import json
import re
from collections import Counter
from typing import Any, Optional, Sequence, Union

import numpy as np
import pytorch_lightning as pl
import torch
from bert_score import BERTScorer
from hydra import compose
from nltk import word_tokenize
from omegaconf import DictConfig
from rouge_score import rouge_scorer
from sklearn.metrics import average_precision_score
from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding

from decontext.experiments.clarification_metric import compute_clf_metric
from decontext.experiments.sari_metric import (
    compute_sentence_generation_scores,
)


class Metric(object):
    """Calculates and stores information for evaluation metrics.

    Attributes:
        name: The name of the metric.
        requires_metadata: True if calculating the metric requires information outside of just the
            (prediction, target) pairs (e.g. the inputs).
    """

    name: str
    requires_metadata: bool

    def __init__(self):
        """Initialize the metric.

        By default, per-example scores are stored in `self.scores` and `self.requires_metadata` is False
        """

        self.scores = []
        self.requires_metadata = False

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        """Calculate the score for the the given (prediction, target) pair

        and add it to the list of all scores.

        Args:
            prediction (str): The string generated by the model.
            target (str): The gold string.
            metadata (Optional[Any]): If required, the additional information needed for calculating the score.
        """

        raise NotImplementedError()

    def process_scores(self) -> dict[str, Any]:
        """Convert the list of scores into summary statistics which are returned

        Returns:
            A dict mapping from summary statistic name to the statistic. This is flexible, so even the entire
            scores list can be returned here.
        """
        raise NotImplementedError()

    def evaluate(
        self,
        predictions: Optional[list[str]] = None,
        targets: Optional[list[str]] = None,
        metadata: Optional[list[Any]] = None,
    ):
        """Evaluate the predictions against the targets.

        Wrap the `self.add` and `self.process_scores` methods so the metric can be called with multiple
        predictions and targets at once. If predictions and targets are not provided, simply process the
        scores (that likely have already been computed).

        Args:
            predictions (Optional[list[str]]): list of model-generated strings.
            targets (Optional[list[str]]): list of gold target strings. `predictions[i]` is the prediction for
                `targets[i]`.
            metadata (Optional[list[Any]]): list of additional information if needed.

        Returns:
            The result of `self.process_scores()`.
        """
        if predictions is not None and targets is not None:
            if metadata is None:
                for prediction, target in zip(predictions, targets):
                    self.add(prediction, target)
            else:
                for prediction, target, metadatum in zip(
                    predictions, targets, metadata
                ):
                    self.add(prediction, target, metadatum)

        return self.process_scores()

    def reset(self) -> None:
        """Reset the accumulated scores."""
        self.scores = []


class ExactMatch(Metric):
    """Implementation of the Exact Match metric."""

    def __init__(self):
        super().__init__()
        self.name = "ExactMatch"

    def process_scores(self) -> dict[str, float]:
        """Return the mean of the exact match scores as the summary statistic."""

        return {"em": np.mean(self.scores)}

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        """Calculate the score as whether the prediction matches the target."""

        self.scores.append(int(prediction == target))


class FirstPersonPerctage(Metric):
    """Calculates what proportion of the samples contain first-person pronouns.

    Also calculates the frequency of different first-person pronouns
    """

    def __init__(self):
        super().__init__()
        self.name = "FirstPersonPercentage"
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        pattern = r"\bour\b|\bours\b|\bthis paper\b|\bthis work\b"
        matches = re.findall(pattern, prediction)
        self.scores["matches"].append(int(bool(matches)))
        for match in matches:
            self.scores["counter"][match] += 1

    def process_scores(self) -> dict[str, Any]:
        """Return the mean percentage and the frequency of first-person prounouns."""
        return {
            "mean": np.mean(self.scores["matches"]) * 100,
            "counter": self.scores["counter"],
        }

    def reset(self):
        self.scores = {"counter": Counter(), "matches": []}


class QuestionPerctage(Metric):
    """Counts the number of predictions that contain at least one question mark."""

    def __init__(self):
        super().__init__()
        self.name = "QuestionPercentage"

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        score = int("?" in prediction)
        self.scores.append(score)

    def process_scores(self) -> dict[str, Any]:
        return {"mean": float(np.mean(self.scores))}


class QuestionNumber(Metric):
    """Counts the number of question marks in each sample."""

    def __init__(self):
        super().__init__()
        self.name = "QuestionNumberComparison"

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        score = prediction.count("?") - target.count("?")
        self.scores.append(score)

    def process_scores(self) -> dict[str, Any]:
        return {
            "mean": float(np.mean(self.scores)),
            "mse": int(np.sum(np.array(self.scores) ** 2)),
            "mad": float(np.mean(np.abs(self.scores))),
        }


class Rouge(Metric):
    """Calculates ROUGE scores.

    Specifically, calculates the rouge1, rouige2, rougeL, and rougeLsum scores. The precision, recall,
    and f1 scores are tracked. Also caluclates the average F1 score across these types.

    Attributes:
        rouges: the rouge scores to use.
    """

    def __init__(self):
        """Initializes the rouge scorer using a stemmer."""
        super().__init__()
        self.name = "ROUGE"
        self.rouges = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
        self.scorer = rouge_scorer.RougeScorer(self.rouges, use_stemmer=True)
        self.reset()

    def reset(self) -> None:
        self.scores = {r_name: [] for r_name in self.rouges}

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        try:
            score = self.scorer.score(target, prediction)
        except AttributeError:
            score = self.scorer.score(target[0], prediction)
        for k in score:
            self.scores[k].append(score[k])

    def process_scores(self) -> dict:
        rouge_results = {}
        for key, value in self.scores.items():
            rouge_results[key] = {
                "precision": [score.precision * 100 for score in value],
                "recall": [score.recall * 100 for score in value],
                "fmeasure": [score.fmeasure * 100 for score in value],
                "fmeasure_mean": np.mean([score.fmeasure for score in value])
                * 100,
            }
        # Compute the arithmetic mean of ROUGE-1, ROUGE-2 and ROUGE-L following: https://arxiv.org/abs/2110.08499
        rouge_results["rouge_avg_fmeasure"] = np.mean(
            [
                rouge_results[key]["fmeasure"]
                for key in ["rouge1", "rouge2", "rougeL"]
            ],
            axis=0,
        ).tolist()
        rouge_results["rouge_avg_fmeasure_mean"] = np.mean(
            rouge_results["rouge_avg_fmeasure"]
        ).item()

        return rouge_results


class BertScore(Metric):
    """Calculates the BERT Score metric.

    Uses the deberta-xlarge-mnli model to calculate BERT Score.

    Attributes:
        post_process: whether to perform post-processing to better match the target data format.
        batch_size: batch size to use to calculate BERT Score.
        device: "cuda" if GPU is available, otherwise "cpu".
        bertscore: the object that computes the BERT Score.
    """

    def __init__(
        self,
        model_type: str = "microsoft/deberta-xlarge-mnli",
        post_process: bool = False,
    ):
        """Initialize the BERTScorer.

        Args:
            model_type (str): the model used (deberta-xlarge-mnli is best, but it's too large for testing)
            post_process (bool): whether to perform post-processing of the predictions to better match the
                target data format.
        """
        super().__init__()
        self.name = "BERTScore"
        self.post_process = post_process
        self.batch_size = 64
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # Parameters taken from
        # https://github.com/allenai/retrieval-exploration/blob/main/src/retrieval_exploration/metrics.py
        # which were based on recommendations from https://github.com/Tiiiger/bert_score
        self.bertscore = BERTScorer(
            model_type=model_type,
            lang="en",
            rescale_with_baseline=True,
            use_fast_tokenizer=True,
            batch_size=self.batch_size,
            device=self.device,
        )
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ) -> None:
        del metadata
        if self.post_process:
            prediction.replace("[REF0]", "the authors")
            prediction.replace("[", "")
            prediction.replace("]", "")
        self.predictions.append(prediction)
        self.targets.append(target)

    def process_scores(self) -> dict:
        p, r, f1 = self.bertscore.score(
            self.predictions,
            self.targets,
        )
        results = {
            "f1_mean": np.mean(f1.tolist()),
            "f1": f1.tolist(),
            "precision": p.tolist(),
            "recall": r.tolist(),
            "hash": self.bertscore.hash,
        }
        return results

    def reset(self) -> None:
        self.predictions: list[str] = []
        self.targets: list[str] = []


class AveragePrecision(Metric):
    def __init__(self):
        self.name = "AveragePrecision"
        self.requires_metadata = True  # needs the classification scores
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        """
        Assumes that the labels are 'pos' and 'neg'
        """
        del prediction
        if metadata is not None:
            label2id = {
                label: int(idx) for idx, label in metadata["label_map"].items()
            }
            p1 = metadata["probs"][label2id["pos"]]
            self.scores.append(p1)
            self.targets.append(label2id[target])
        else:
            raise ValueError("Must pass metadata.")

    def process_scores(self) -> dict[str, float]:
        ap = average_precision_score(self.targets, self.scores)
        return {"macro": ap}

    def reset(self) -> None:
        self.targets: list[int] = []
        self.scores: list[str] = []


class FilterModel(Metric):
    def __init__(
        self,
        model_config: str,
        input_data_file: str,
        model_checkpoint_path: str,
    ):
        """
        input_data_file: this is the file with the predictions, specifically the x's and the idxs. Ideally we
            shouldn't care about these because the prediction should just require the y's and targets, but
            this one is a bit different... In any case, I think we should probably just use the metadata
            here.... it's also unfortunate that we're alrready loadi
        """
        super().__init__()
        self.name = "Filter"
        self.inputs: list[dict[str, Any]] = []
        self.text_inputs: list[str] = []
        self.pred_idx = 0

        # open up the model config. Assumes that hydra has already been initialized
        filter_cfg = compose(
            config_name="config",
            overrides=[
                "data=rewrite_filter_clf",
                f"model={model_config}",
                "wandb=False",
            ],
        )

        import decontext.experiments.model  # import module here to avoid circular imports

        model, self.tokenizer = decontext.experiments.model.load_model(
            filter_cfg
        )
        self.model = decontext.experiments.model.LocalClassificationModel(
            filter_cfg, model, self.tokenizer, num_training_batches=0
        )
        self.trainer = pl.Trainer(accelerator="auto")
        self.args = filter_cfg
        self.checkpoint_path = model_checkpoint_path
        with open(input_data_file) as f:
            self.input_data = [json.loads(line.strip()) for line in f]

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        """
        Assumes that the predictions are in the same order as the data, which they should be because
        the `evaluate` method loads the predictions in the same way as done in `__init__`.

        Does not use target
        """
        del target, metadata
        x = self.input_data[self.pred_idx]["x"]
        inpt = f"{x} [SEP] {prediction.strip()}"  # TODO: use tokenizer.sep instead of [SEP]
        self.text_inputs.append(inpt)
        self.inputs.append(self.tokenizer(inpt))
        self.pred_idx += 1

    def process_scores(self) -> dict[str, Any]:
        # create a dataset from the inputs... this is rather annoying
        # but is possible...
        dataloader = DataLoader(
            self.inputs,
            batch_size=self.args.model.eval_batch_size,
            shuffle=False,
            collate_fn=DataCollatorWithPadding(self.tokenizer, padding=True),
        )
        predictions = self.trainer.predict(
            model=self.model,
            dataloaders=dataloader,
            return_predictions=True,
            ckpt_path=self.checkpoint_path,
        )

        # assumes that 1 is the index for the positive class. TODO make this more explicit by saving label2idx
        # with the huggingface model somehow.
        scores = [
            score[1].item()
            for prediction_batch in predictions
            for score in prediction_batch["probs"]
        ]
        results = {}
        for input_data, text_input, score in zip(
            self.input_data, self.text_inputs, scores
        ):
            results[str(input_data["idx"])] = {"score": score, "x": text_input}

        return results

    def reset(self) -> None:
        self.inputs = []
        self.text_inputs = []
        self.pred_idx = 0
        # self.args.data["val"].path =
        # this is erally effing annoying.... like i can't just use what i
        # already wrote because I have to massage it into the right form with all the effing
        # reading files and args and stuff.... ughhhhhhhh

        # Ok one hacky0-thing i can do is just send in random args (somehow)
        # and then reset the effing data to be what I want
        # dataset = FilterDataset(self.args, self.tokenizer, "val")
        # overwrite the data to be what i want. it's dumb but it works
        # dataset.data = self.inputs
        # self.trainer.predict(dataset, self.model)

        # it's annoying to not use predict because I have handle the batch size and stuff here...
        # dataset =
        # self.model.eval()
        # for batch in dataset.dataloader:
        #     batch.to(self.args.device)
        #     preds = self.model(**batch)
        #     torch.softmax(logits)

        # return scores


def parse_original_sentence(metadata):
    """Parse the original snippet to decontextualize out of the metadata.

    For the experiments in the paper, the metadata has a key "x_no_parse", which contains the original
    snippet without any parsing necessary. In prior formative experiments, the metadata doesn't have this
    key, so the original snippet was parsed out of the prompt. (This *usually* worked, but was not reliable,
    hence in later experiments, the key "x_no_parse" was used.)

    Args:
        metadata: the metadata dictionary for the sample.
    """
    # first find the prompt
    if "x_no_parse" in metadata:
        return metadata["x_no_parse"]
    elif "params" in metadata:
        print(
            "WARNING: Attempting to parse out the prompt from the metadata. "
            "The parser is VERY BRITTLE so confirm this does what you want."
        )
        # the results come from an openai model
        if "messages" in metadata["params"]:
            # the results come from chatgpt
            prompt = metadata["params"]["messages"][-1]["content"]
        else:
            prompt = metadata["params"]["prompt"]
    else:
        prompt = metadata["x"]  # type: ignore
        # original_sent =  metadata["params"]["prompt"] # type: ignore

    start_texts = [
        "section:\n",
        "\nSnippet: ",
        "\nText snippet: ",
        "\nText snippet: ",
        "\nText snippet: ",
        "\nText snippet: ",
        "\nText snippet: ",
    ]
    end_texts = [
        "Please rewrite this snippet according to the instructions.",
        "\nQuestion",
        "\nClarified Snippet",
        "\nWhat is the clarified snippet?",
        "\nCan you please rewrite this snippet according to the instructions above?",
        "\nRewrite",
        -1,
    ]

    hasnt_warned = True
    original_sent = prompt
    for st, et in zip(start_texts, end_texts):
        try:
            i_start = prompt.index(st)
            if et == -1:
                i_end = len(prompt)
            else:
                i_end = prompt.index(et)
            original_sent = prompt[i_start:i_end].lstrip(st)
            if hasnt_warned:
                print(f"WARNING: Found original sentence: {original_sent}")
                hasnt_warned = False
            break
        except ValueError:
            continue
    else:
        raise ValueError(
            "None of start_texts or end_texts were found in snippet."
        )

    return original_sent


class Sari(Metric):
    """Implement the SARI metric.

    Implements the SARI metric from Xu et al., 2016 (https://aclanthology.org/Q16-1029/).
    """

    def __init__(self):
        self.name = "Sari"
        self.requires_metadata = True
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        """Parse out the original sentence and add the (prediction, targets) to a list."""
        if metadata is None:
            idx = self.counter
            self.counter += 1
        else:
            idx = metadata["idx"]

        self.ref_sent_dict[idx] = [target]

        # this is ugly... but we need to parse out the original sentence...
        # this is kind of difficult because there are a few different
        # ways to get at the original sentence
        try:
            original_sent = parse_original_sentence(metadata)
        except TypeError:
            # assume the...input/output is the original sent bc this is the identity baseline...
            print(
                "WARNING [SARI]: using the prediction as the original"
                " sentence because no metadata was found! This is only ok if doing the identity baseline."
            )
            original_sent = prediction

        self.original_sent_dict[idx] = original_sent
        self.pred_dict[idx] = prediction

    def process_scores(self) -> dict[str, Any]:
        """Calculate all of the SARI scores at once."""
        result = compute_sentence_generation_scores(
            original_sent_dict=self.original_sent_dict,
            reference_dict=self.ref_sent_dict,
            pred_dict=self.pred_dict,
        )
        return result

    def reset(self) -> None:
        self.counter = 0
        self.ref_sent_dict: dict[Union[str, int], list[str]] = {}
        self.original_sent_dict: dict[Union[str, int], str] = {}
        self.pred_dict: dict[Union[str, int], str] = {}


class LengthChange(Metric):
    """Calculates the percentage increase of length at the token and character level.

    Also calculates the difference in token count between prediction and targets.
    """

    def __init__(self):
        super().__init__()
        self.name = "LengthChange"
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        pred_words = word_tokenize(prediction)
        target_words = word_tokenize(target)
        self.scores["token"].append(len(pred_words) / len(target_words))
        self.scores["token_diff"].append(len(pred_words) - len(target_words))
        self.scores["character"].append(len(prediction) / len(target))

    def process_scores(self) -> dict[str, Any]:
        return {
            "token": float(np.mean(self.scores["token"])),
            "token_diff": float(np.mean(self.scores["token_diff"])),
            "character": float(np.mean(self.scores["character"])),
        }

    def reset(self) -> None:
        self.scores = {"token": [], "character": [], "token_diff": []}


class Clarification(Metric):
    """Implements the Clarification (CLF) metric.

    Extracts the the spans in square brackets in both the target and prediction and compares them.
    """

    def __init__(self):
        super().__init__()
        self.name = "Clarification"
        self.requires_metadata = True
        self.reset()

    def add(
        self, prediction: str, target: str, metadata: Optional[Any] = None
    ):
        # extract the stuff in square brackets from prediction and target
        pattern = r"\[(.*?)\]"
        prediction_additions = re.findall(pattern, prediction)
        target_additions = re.findall(pattern, target)

        p, r, f1, iou, alignments = compute_clf_metric(
            prediction_additions, target_additions
        )

        self.scores["iou"].append(iou)
        self.scores["precision"].append(p)
        self.scores["recall"].append(r)
        self.scores["f1"].append(f1)
        self.scores["alignments"].append(alignments)

    def process_scores(self) -> dict[str, Any]:
        """Return a number of the following CLF scores.

        * iou_mean: Average token-level intersection-over-union between the closest span in
            the prediction and target.
        * p_mean: Average per-snippet precision between tokens in the closest spans in the
            prediction and target.
        * r_mean: Average per-snippet recall between tokens in the closest spans in the
            prediction and target.
        * f1_mean: Average per-snippet f1 score between tokens in the closest spans in the
            prediction and target.
        * alignments: Alignments between the spans in the predictions and targets.
        * iou: The per-snippet IoU between the closest spans.
        * p: The per-snippet token precisions between the closest spans.
        * r: The per-snippet recall for each snippet between the closest spans.
        * f1: The per-snippet f1 scores for each snippet between the closest pans.
        """
        return {
            "iou_mean": np.nanmean(self.scores["iou"]),
            "p_mean": np.mean(self.scores["precision"]),
            "r_mean": np.mean(self.scores["recall"]),
            "f1_mean": np.mean(self.scores["f1"]),
            "alignments": self.scores["alignments"],
            "iou": self.scores["iou"],
            "p": self.scores["precision"],
            "r": self.scores["recall"],
            "f1": self.scores["f1"],
        }

    def reset(self) -> None:
        self.scores = {
            "iou": [],
            "alignments": [],
            "recall": [],
            "precision": [],
            "f1": [],
        }


# Map each metric name in the config to the correct class.
METRIC_MAP = {
    "em": ExactMatch,
    "rouge": Rouge,
    "average_precision": AveragePrecision,
    "filter": FilterModel,
    "bert_score": BertScore,
    "1st_pers_pct": FirstPersonPerctage,
    "question_pct": QuestionPerctage,
    "question_num": QuestionNumber,
    "sari": Sari,
    "length_change": LengthChange,
    "clf": Clarification,
}


def load_metrics(metric_params: DictConfig) -> Sequence[Metric]:
    """Return the list of metrics we are going to evaluate"""
    metrics: list[Metric] = []

    for metric_param in metric_params:
        if isinstance(metric_param, DictConfig):
            metric_name = metric_param["name"]
            metric_param_dict = {
                k: v for k, v in metric_param.items() if k != "name"
            }
        else:
            metric_name = metric_param
            metric_param_dict = {}

        metrics.append(METRIC_MAP[metric_name](**metric_param_dict))
    return metrics
