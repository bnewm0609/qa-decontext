{"idx": "103595", "paper_id": "4957206", "title": "Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource", "abstract": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "context_section_header": "", "context_paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "sentence": "Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "cited_ids": [{"paper_id": "13475624", "citation": "Jiang et al. (2016)"}], "y": "[There are three differences between Jiang et al's and the authors work on relation extraction. First, a difference in scale. Second, a difference in granularity.] Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations [which look like semantic frames] from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "snippet_surface": "Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "questions": {"103595.uW2Z3umvdO": "What do Jiang et al. (2016) extract time-sensitive relations from?", "103595.C24FBetHE+": "How do we extract relations from unstructured natural language text?"}}
{"idx": "104377", "paper_id": "15193840", "title": "Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers", "abstract": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "context_section_header": "", "context_paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "sentence": "The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "cited_ids": [{"paper_id": "2462277", "citation": "(Komiya and Okumura, 2011)"}], "y": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD [(Word sense disambiguation)], source data, and target data, whereas the authors determined the method for each instance [the individual data contained in the triple].", "snippet_surface": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, whereas the authors determined the method for each instance.", "questions": {"104377.rplvzpdEZj": "What is WSD?", "104377.67rS9h53Ag": "What is DA?", "104377.x1SpN9pqV8": "What is the difference between Komiya and Okumura's approach and the approach in this paper?"}}
{"idx": "112709", "paper_id": "227230401", "title": "Formality Style Transfer with Shared Latent Space", "abstract": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "context_section_header": "", "context_paragraph": "To verify the effectiveness and generalization of our method, we conduct experiments in three different settings: Data Limited, Data Augmentation, and Pre-training. In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset. When we use large-scale non-parallel data to enhance our method in the data augmentation and pre-training settings, our method still consistently outperforms the baselines by 1 BLEU score. The ablation test further studies the effectiveness of the joint training, the auto-encoding training, and the auxiliary losses in different scenarios, showing the robustness of our method.", "sentence": "In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "cited_ids": [{"paper_id": "4859003", "citation": "(Rao and Tetreault, 2018)"}], "y": "In the data-limited scenario, experimental results show that the authors' method [Sequence-to-Sequence with Shared Latent Space trained with two auxiliary losses with joint training of bi-directional transfer and auto-encoding] is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains [categories of informal sentences] (namely, F&R [family and relationships] and E&M [entertainment and music]) of the GYAFC dataset.", "snippet_surface": "In the data-limited scenario, experimental results show that the authors' method is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "questions": {"112709.LLb9cETQ/U": "What is the data-limited scenario?", "112709.iEgp4QHmGG": "What is the GYAFC dataset?", "112709.9+NAlxCVt8": "What is BLEU score?"}}
{"idx": "113060", "paper_id": "202538019", "title": "Mixture Content Selection for Diverse Sequence Generation", "abstract": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "context_section_header": "", "context_paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "sentence": "While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation.", "cited_ids": [{"paper_id": "67787922", "citation": "Shen et al. (2019)"}], "y": "Shen et al. (2019) makes a RNN decoder as a MOE [deep Mixture of Experts], whereas the authors make a SELECTOR [a general plug-and-play module specialized for diversification that wraps around and guides an existing encoder-decoder model] as a MoE to diversify content selection [selection of individual tokens from the source to be fed into the encoder-decoder model] and enable the encoder-decoder model's one-to-one generation.", "snippet_surface": "Shen et al. (2019) makes a RNN decoder as a MoE, whereas the authors make a SELECTOR as a MoE to diversify content selection and let the encoder-decoder models one-to-one generation.", "questions": {"113060.U7WgCNwMx6": "What is a MoE?", "113060.L7vMb7FAjD": "What does SELECTOR do?", "113060.DHSDE+Zj+l": "How does SELECTOR diversify content selection?"}}
{"idx": "114795", "paper_id": "196471395", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning", "abstract": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "context_section_header": "", "context_paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "sentence": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "cited_ids": [{"paper_id": "1294169", "citation": "(Henderson et al., 2014)"}], "y": "The authors' system [consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty]. The system operates in the well-known DSTC2 domain [seed data that contains the dialogues between human asking for information and the machine providing it] which concerns information about restaurants in Cambridge. However, the authors' multi-agent system [a system with multiple agent create novel languages by each playing a different role and having their own objective]. Supports any slot filling / information-seeking domain.", "snippet_surface": "The authors' system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, their multi-agent system supports any slotfilling / information-seeking domain.", "questions": {"114795.6YCyUdHw5q": "What is the DSTC2 domain?", "114795.NHaqDvd9M6": "What is a slotfilling/information-seeking domain?", "114795./GgWPZuX2t": "How does the multi-agent system support these domains?"}}
{"idx": "116565", "paper_id": "234324760", "title": "Modeling German Word Order Acquisition via Bayesian Inference", "abstract": "The question of how children acquire the grammatical principles of their native language has been debated in cognitive science and linguistics for over 50 years (e.g., Chomsky, 1965; Xu and Tenenbaum, 2007). Perfors et al. (2011) suggest that children compare various syntactic hypotheses in order to determine which is most compatible with the socalled primary linguistic data (cf. Chomsky, 1965; Wexler and Culicover, 1980; Clark and Lappin, 2013; among many others), and that a Bayesian model selection approach might be able to do some of this work. More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG) that used hierarchical phrase structure over a regular grammar representing linear phrase structure, without any initial prior bias towards one grammar type or another. Therefore, they argued that correct linguistic generalizations can be achieved by a learner equipped with domain-general Bayesian inference capacities but without language-specific innate knowledge. In their work, the various syntactic hypotheses were represented by grammars that were all able to parse the entirety of a pre-processed English childdirected speech corpus. However, a computational model without the ability to compare competing hypotheses that are supported by different subsets of the data is severely limited due to the inevitability of noise. Children are also very likely to hear errors, actual or only perceived (Krentz and Corina, 2008; Friederici et al., 2011). Furthermore, hypotheses concerning typological tendencies might not warrant comparison over fully congruent data sets. The premise of this work is therefore to extend the Bayesian system designed by Perfors et al. (2011) to allow for comparing models supported by different data subsets. We evaluated this system in the context of word order acquisition in German. To this end, we designed PCFGs representing different word order hypotheses a child might entertain. Many linguists agree that German word order is Subject-Object-Verb (SOV) Verb-Second (V2) (henceforth: SOV+V2) (e.g., Bierwisch, 1963). Yet many simple German sentences also maintain Subject-Verb-Object (SVO) order. Because of the SVO word order of many simple German sentences, a child might initially consider a left-branching grammar only able to account for SVO sentences, only to reject it in order to account for embedded sentences and sentences with auxiliaries. Note that we do not commit to a specific theoretical position on the plausibility of humans performing verb movement operations, and merely lean on this theory to specify conceivable word order hypotheses, assuming that a PCFG representing SVO word order should be able to parse a decent number of German sentences, but fewer than the SOV+V2 PCFG. Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011). Our PCFGs can thus serve as snapshots of different hypotheses comparable in a priori complexity that a child may at some point compare in order to determine the word order of their language, though they by no means exhaust the space of hypotheses that a child may consider.", "context_section_header": "", "context_paragraph": "Note that we do not commit to a specific theoretical position on the plausibility of humans performing verb movement operations, and merely lean on this theory to specify conceivable word order hypotheses, assuming that a PCFG representing SVO word order should be able to parse a decent number of German sentences, but fewer than the SOV+V2 PCFG. Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011). Our PCFGs can thus serve as snapshots of different hypotheses comparable in a priori complexity that a child may at some point compare in order to determine the word order of their language, though they by no means exhaust the space of hypotheses that a child may consider.", "sentence": "Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "cited_ids": [{"paper_id": "17269147", "citation": "Perfors et al. (2011)"}], "y": "Additionally, the authors' PCFGs [probabilistic context free grammars] are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "snippet_surface": "Additionally, the authors' PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "questions": {"116565.02/wdHnQdQ": "What are PCFGs?", "116565.CFDA04wSZ+": "How do the complexities of the grammars used by Perfors et al. (2011) differ from the complexity of the PCFGs?"}}
{"idx": "119638", "paper_id": "17786494", "title": "Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University Joint Learning for Coreference Resolution with Markov Logic Joint Learning for Coreference Resolution with Markov Logic", "abstract": "Pairwise coreference resolution models must merge pairwise coreference decisions to generate \ufb01nal outputs. Traditional merging methods adopt different strategies such as the best-\ufb01rst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classi\ufb01cation and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance.", "context_section_header": "", "context_paragraph": "In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009;Yoshikawa et al., 2009;Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. Compared with it, our methods are more applicable for real dataset. Huang et al. (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results. Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework.", "sentence": "Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches.", "cited_ids": [{"paper_id": "7124715", "citation": "Poon and Domingos (2008)"}], "y": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches. [To be more specific, the entity-mention model is an unsupervised machine learning method which performs joint inference across mentions and uses Markov logic as a representation language. The mention-pair model is a supervised method which splits the task into mention detection, pairwise classification and mention clustering.]", "snippet_surface": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches.", "questions": {"119638.pOT+9fxlJw": "What is the entity-mention model?", "119638.2zS2O0i0U3": "What is the mention-pair model?"}}
{"idx": "123037", "paper_id": "9453296", "title": "Building Specialized Bilingual Lexicons Using Word Sense Disambiguation", "abstract": "This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach.", "context_section_header": "", "context_paragraph": "Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as 'anchor points'. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar\u00e8s (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors.", "sentence": "One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors.", "cited_ids": [{"paper_id": "459519", "citation": "Gaussier et al. (2004)"}], "y": "One important difference between Gaussier et al. (2004) and the authors' work is Gaussier et al. focus on words ambiguities on source and target languages [iin machine translation], whereas the authors consider that it is sufficient to disambiguate only translated source context vectors.", "snippet_surface": "One important difference between the authors and Gaussier et al. (2004) is that the latter focus on word ambiguities on source and target languages, whereas the authors consider it sufficient to disambiguate only translated source context vectors.", "questions": {"123037.TBT2lp3cdO": "What is the difference between the authors' approach and Gaussier et al. (2004)?", "123037.AaFvwqHSd5": "What do they focus on in their approach?", "123037.t506noOE9j": "What do they consider sufficient to disambiguate?"}}
{"idx": "135732", "paper_id": "206467", "title": "Modeling Syntactic Context Improves Morphological Segmentation", "abstract": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "context_section_header": "", "context_paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "sentence": "Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "cited_ids": [{"paper_id": "9519654", "citation": "(Poon et al., 2009"}], "y": "The author's full model [which is to learn words with common affixes and use learned syntactic categories to refine morphological segmentation] yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5% [by integrating POS categorization and grammatical agreement realization].", "snippet_surface": "The authors' full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "questions": {"135732.i1GSeGblMl": "What accuracy did Poon et al. (2009) achieve?", "135732.QZmNTnuxtM": "What is the accuracy of the full model?"}}
{"idx": "137696", "paper_id": "12186762", "title": "Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis", "abstract": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "context_section_header": "", "context_paragraph": "Several differences between this work and previous approaches make direct comparisons challenging and possibly not very informative. (Socher et al., 2013) use syntactic trees, as opposed to discourse trees, as recursive structures for training. Thus we cannot compare with his \"All\"-level results. For \"Root\"-level, (Socher et al., 2013) reports 45.7% fine-grained sentiment accuracy compared to 44.82% of our Multi-tasking. This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "sentence": "This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "cited_ids": [{"paper_id": "12252194", "citation": "(Bhatia et al., 2015)"}], "y": "The authors suggest that [root-level report difference between their work and other approaches] is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU [elementary discourse unit] level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach [of creating a vector, applying three different recursive neural net models and then combining these models in two joint models].", "snippet_surface": "The authors suggest that this difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach.", "questions": {"137696.ldvhxfQnmZ": "What is the difference between the two approaches?", "137696.RBraHGux0t": "What features do Bhatia et al. use for EDUs?", "137696.5LzmkahF/U": "What is the difference between the binary model and the authors' approach?"}}
{"idx": "138193", "paper_id": "661892", "title": "Regularizing Mono- and Bi-Word Models for Word Alignment", "abstract": "Conditional probabilistic models for word alignment are popular due to the elegant way of handling them in the training stage. However, they have weaknesses such as garbage collection and scale poorly beyond single word based models (DeNero et al., 2006): not all parameters should actually be used. To alleviate the problem, in this paper we explore regularity terms that penalize the used parameters. They share the advantages of the standard training in that iterative schemes decompose over the sentence pairs. We explore the models IBM-1 and HMM, then generalize to models we term Bi-word models, where each target word can be aligned to up to two source words. We give two optimization strategies for the arising tasks, using EM and projected gradient descent. While both are well-known, to our knowledge they have never been compared experimentally for the task of word alignment. As a side-effect, we show that, against common belief, for parametric HMMs the M-step is not solved by renormalizing expectations. We demonstrate that the regularity terms improve on the f-measures of the standard HMMs and that they improve translation quality.", "context_section_header": "", "context_paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "sentence": "In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "cited_ids": [{"paper_id": "8518269", "citation": "(Schoenemann, 2011)"}], "y": "In contrast to the authors' recent work [on machine translation] (Schoenemann, 2011) (where they used an L 0 -norm [for single-word based alignment of bilingual sentence pairs]) they do not use the maximum approximation and also address Bi-word models [, where each target word is allowed to align to up to two source words].", "snippet_surface": "In contrast to the authors' recent work (Schoenemann, 2011) (where they used an L 0 -norm) they do not use the maximum approximation and also address Bi-word models.", "questions": {"138193.kzMW4exX2v": "What is the L 0 -norm?", "138193.6jdmob+ZRt": "How is the maximum approximation used?", "138193.HjvJtm6Kr/": "What are Bi-word models?"}}
{"idx": "145596", "paper_id": "102351751", "title": "Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models", "abstract": "We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.", "context_section_header": "", "context_paragraph": "We evaluate the ability of our model in generating graph paths for previously unseen textual definitions on seven ontologies (Section 3). We show that our technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings. The code and resources for the paper can be found at https://github.com/VictorProkhorov/ Text2Path.", "sentence": "We show that our technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings.", "cited_ids": [{"paper_id": "52076072", "citation": "(Kartsaklis et al., 2018)"}], "y": "[The authors present a new technique that maps a textual description of a concept to a hierarchical sequence of the concepts' hypernyms to find other senses of the concept word] on par or outperforming the competitive multi-sense LSTM model thanks to a better use of external information in the form of word embedding.", "snippet_surface": "The authors show that their technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings.", "questions": {"145596.PbHl4LTiHG": "What is a multi-sense LSTM model?", "145596.3YZ2gZnzoA": "How does our technique better utilize external information?", "145596.0NvDnjy3xw": "What form does the external information take?"}}
{"idx": "146809", "paper_id": "52183735", "title": "Multi-view Models for Political Ideology Detection of News Articles", "abstract": "A news article\u2019s title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.", "context_section_header": "", "context_paragraph": "Several works study the detection of political ideology through the lens of computational linguistics and natural language processing (Laver et al., 2003;Monroe and Maeda, 2004;Thomas et al., 2006;Lin et al., 2008;Carroll et al., 2009;Ahmed and Xing, 2010;Gentzkow and Shapiro, 2010;Gerrish and Blei, 2011;Sim et al., 2013). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called \"slant index\" which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei (2011) predict the voting patterns of Congress members based on supervised topic models. Other works use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010;Lin et al., 2008). Sim et al. (2013) propose a novel HMM-based model to infer the ideological proportions of the rhetoric used by political candidates in their campaign speeches which relies on a fixed lexicon of bigrams associated with ideologies. The work that is most closely related to our work is that of Iyyer et al. (2014);Preo\u0163iuc-Pietro et al. (2017). Iyyer et al. (2014) use recurrent neural networks to predict political ideology of congressional debates and articles in the ideological book corpus (IBC) and demonstrate the importance of compositionality in predicting ideology where modifier phrases and punctuality affect the political ideological position. Preo\u0163iuc-Pietro et al. (2017) propose models to infer political ideology of Twitter users based on their everyday language. Most crucially, they also show how to effectively use the relationship between user groups to improve prediction accuracy. Our work draws inspiration from both of these works but differentiates itself from these in the following aspects: We leverage the structure of a news article by noting that an article is just not free-form text, but has a rich structure to it. In particular, we model cues from the title, the inferred network, and the content in a joint generic neural variational inference framework to yield improved models for this task. Furthermore, differing from Iyyer et al. (2014), we also incorporate attention mechanisms in our model which enables us to inspect which sentences (or words) have the most predictive power as captured by our model. Finally, since we work with news articles (which also contain hyperlinks), naturally our setting is different from all other previous works in general (which mostly focus on congressional debates) and in particular from Iyyer et al. (2014) where only textual content is modeled or Preo\u0163iuc-Pietro et al. (2017) which focuses on social media users.", "sentence": "Furthermore, differing from Iyyer et al. (2014), we also incorporate attention mechanisms in our model which enables us to inspect which sentences (or words) have the most predictive power as captured by our model.", "cited_ids": [{"paper_id": "216636598", "citation": "Iyyer et al. (2014)"}], "y": "The authors' model [a recurrent neural network] also incorporates attention mechanisms [word level and sentence level], unlike the one from Iyyer et al. (2014), which enables them to inspect which sentences (or words) have the most predictive power as captured by the model.", "snippet_surface": "Furthermore, the authors' model also incorporates attention mechanisms, unlike the one from Iyyer et al. (2014), which enables them to inspect which sentences (or words) have the most predictive power as captured by the model.", "questions": {"146809.Sqwot8F6Ey": "What did Iyyer et al. (2014) incorporate?", "146809.7N5HOUzEk0": "What is the purpose of attention mechanisms in the authors' model?", "146809.al9RriVQir": "How does the model capture predictive power?"}}
{"idx": "147259", "paper_id": "18492550", "title": "Experiments on Active Learning for Croatian Word Sense Disambiguation", "abstract": "Supervised word sense disambiguation (WSD) has been shown to achieve state-ofthe-art results but at high annotation costs. Active learning can ameliorate that problem by allowing the model to dynamically choose the most informative word contexts for manual labeling. In this paper we investigate the use of active learning for Croatian WSD. We adopt a lexical sample approach and compile a corresponding senseannotated dataset on which we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance.", "context_section_header": "", "context_paragraph": "All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences. We study three sampling methods in this work, but leave the issues of stopping criterion and class imbalance for future work.", "sentence": "In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences.", "cited_ids": [{"paper_id": "403206", "citation": "Chen et al. (2006)"}], "y": "In contrast to Chen et al. (2006), the authors opt for [two features: 1. A simple binary bag-of-words vector (BoW) 2. The skip-gram model proposed by Mikolov et al. (2013),] derived from [given a sentence in which a polysemous word occurs, the context vector is calculated based on the words it co-occurs within the sentence].", "snippet_surface": "In contrast to Chen et al. (2006), the authors opt for simple, readily available features derived from cooccurrences.", "questions": {"147259.noJzJpmyQ/": "No questions."}}
{"idx": "154425", "paper_id": "248780060", "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression", "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "context_section_header": "", "context_paragraph": "Knowledge Distillation. Knowledge distillation (Hinton et al., 2015) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015) first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019); Sun et al. (2019);Liang et al. (2020) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019) proposed a contrastive distillation framework where the teacher's representations were treated as positives to the corresponding student's representations. Sun et al. (2020); Fu et al. (2021) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019); Tung and Mori (2019); Park et al. (2019) pointed out that the relations of the image representations of the teacher should be preserved in the student's feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020); Wang et al. ( , 2021 used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "sentence": "Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "cited_ids": [{"paper_id": "237091399", "citation": "Shao and Chen (2021)"}], "y": "The authors jointly transfer the token-level, span-level and sample-level structural knowledge [hierarchically across layers], which is different from previous works that only considered a single [level or granularity] of representations. Compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality and presents a different definition of granularity [as a distillation mechanism].\"", "snippet_surface": "Different from previous works that only considered a single granularity of representations, the authors jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality, presents a different definition of granularity.", "questions": {"154425.UREzKU2QcD": "What are the previous works that only considered a single granularity of representations?", "154425.TE6bh7++pb": "What are the multi-granularity visual features in an image?", "154425.9FFFhEDkpo": "What is the different modality and definition of granularity of this method?"}}
{"idx": "1603.09631.1.1.1", "paper_id": "1603.09631", "title": "Data Collection for Interactive Learning through the Dialog", "abstract": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "context_section_header": "Dataset Collection Process", "context_paragraph": "However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "sentence": "Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "cited_ids": [], "y": "The authors used the crowdsourcing platform CrowdFlower (CF) for their data collection [which consisted of natural dialogs].", "snippet_surface": "Therefore, the authors used the crowdsourcing platform CrowdFlower (CF) for their data collection.", "questions": {"1603.09631.1.1.1.kgjQPoEmlA": "What is CrowdFlower?", "1603.09631.1.1.1.J5HzxFP0rm": "What data was collected?"}}
{"idx": "1604.00400.1.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Data", "context_paragraph": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "sentence": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "cited_ids": [], "y": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera) [which assess the content relevance between system generated summaries and human written summaries], they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "snippet_surface": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera), they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "questions": {"1604.00400.1.1.1.RqZRYN67Nb": "What is the TAC 2014 summarization track?", "1604.00400.1.1.1.jRTHA6AKS9": "What is the benchmark used for evaluating Rouge variants and Sera?", "1604.00400.1.1.1.l41KR/A0Q+": "What is the format of the benchmark?"}}
{"idx": "1604.00400.5.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Introduction", "context_paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "sentence": "Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "cited_ids": [], "y": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. The authors also show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "snippet_surface": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, they show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "questions": {"1604.00400.5.1.1.jwdvz/jfqY": "What is the common belief?", "1604.00400.5.1.1.nQooxROIl7": "What are the different Rouge variants?", "1604.00400.5.1.1.ohqIdSMtYw": "What are the manual evaluations?"}}
{"idx": "1604.00727.1.1.1", "paper_id": "1604.00727", "title": "Character-Level Question Answering with Attention", "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.", "context_section_header": "Dataset and Experimental Settings", "context_paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "sentence": "In contrast, our models are trained only on the 76K questions in the training set.", "cited_ids": [], "y": "The authors' models are trained only on the 76K questions in the training set [which comprises the random extract of 76k single-relation questions and their corresponding triples from the SimpleQuestions dataset].", "snippet_surface": "In contrast, the authors' models are trained only on the 76K questions in the training set.", "questions": {"1604.00727.1.1.1.FfzBAcGfEB": "What is the training set?", "1604.00727.1.1.1.oqrXUEugGw": "How many questions are in the training set?"}}
{"idx": "1606.02891.1.1.1", "paper_id": "1606.02891", "title": "Edinburgh Neural Machine Translation Systems for WMT 16", "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English<->Czech, English<->German, English<->Romanian and English<->Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.", "context_section_header": "Baseline System", "context_paragraph": "Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", "sentence": "Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", "cited_ids": [], "y": "The authors' systems are attentional encoder-decoder networks. The authors based implementation on the dl4mt-tutorial [which consists of using minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024], which the authors enhanced with new features such as ensemble decoding and pervasive dropout.", "snippet_surface": "The authors' systems are attentional encoder-decoder networks BIBREF0. They based their implementation on the dl4mt-tutorial, which they enhanced with new features such as ensemble decoding and pervasive dropout.", "questions": {"1606.02891.1.1.1.THyWP0LS1U": "What are attentional encoder-decoder networks?", "1606.02891.1.1.1.lYHXOepR1K": "What is dl4mt-tutorial?", "1606.02891.1.1.1.2eWCdDo7Ku": "What are ensemble decoding and pervasive dropout?"}}
{"idx": "1606.03676.1.1.1", "paper_id": "1606.03676", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "abstract": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "context_section_header": "Corpora", "context_paragraph": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43 .", "sentence": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "cited_ids": [], "y": "The authors carried out their experiments [comparing the performance of systems on datasets covering 16 languages] on the Universal Dependencies v1.2 treebanks BIBREF21, (UD1.2), from which morphosyntactically annotated corpora can be trivially extracted.", "snippet_surface": "The authors carried out their experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "questions": {"1606.03676.1.1.1.lsRMdikoLR": "What is Universal Dependencies v1.2 treebanks?", "1606.03676.1.1.1.RKgF29LV1o": "What does \"morphosyntactically annotated corpora\" mean?"}}
{"idx": "1609.00425.1.1.1", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism data", "context_paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "sentence": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "cited_ids": [], "y": "To collect a diverse training dataset [for training their classifier to predict dogmatism across different subreddits], the authors randomly sampled 1000 posts from each of the subreddits: \"politics\", \"business\", \"science\", \"AskReddit\", and 1000 additional posts from the Reddit front page.", "snippet_surface": "To collect a diverse training dataset, the authors have randomly sampled 1000 posts from each of the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "questions": {"1609.00425.1.1.1.W+AAYHec79": "What are the subreddits used for the training dataset?", "1609.00425.1.1.1.HbsMKMuiV4": "How many posts were collected from each subreddit?", "1609.00425.1.1.1.kvWvuYWRyv": "How many posts were collected from the Reddit frontpage?"}}
{"idx": "1609.00425.1.2.2", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism in the Reddit Community ", "context_paragraph": "We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions.", "sentence": "Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "cited_ids": [], "y": "The authors apply the [bag-of-words and linguistic features] model trained on [different subreddit representing different topics, such as politics, business, science and other other posts in the Reddit home page] to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "snippet_surface": "The authors apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "questions": {"1609.00425.1.2.2.B1cXufqsKn": "What is the BOW+LING model?", "1609.00425.1.2.2.wF/jt1BfT6": "What are the two classes labeled by the classifier?"}}
{"idx": "1611.03599.1.2.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "sentence": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "cited_ids": [], "y": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns), taking into account only the post content. Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "snippet_surface": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns), taking into account only the post content. Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "questions": {"1611.03599.1.2.1.vxaikMcEdv": "What is the FBFans dataset?", "1611.03599.1.2.1.mTzHquscpe": "What is the stance labeling task?", "1611.03599.1.2.1.l50Z/W8dAi": "What is the anti-reconstruction topic?"}}
{"idx": "1611.03599.3.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "sentence": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "cited_ids": [], "y": "The authors collected the CreateDebate dataset from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR) [to develop a deep learning model for stance classification].", "snippet_surface": "The authors collected the CreateDebate dataset from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "questions": {"1611.03599.3.1.1.noJzJpmyQ/": "No questions."}}
{"idx": "1611.03599.4.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "To test whether the assumption of this paper \u2013 posts attract users who hold the same stance to like them \u2013 is reliable, we examine the likes from authors of different stances. Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. As the numbers of authors in the Sup, Neu and Uns stances are largely imbalanced, these numbers are normalized by the number of users of each stance. Table TABREF13 shows the results. Posts with stances (i.e., not neutral) attract users of the same stance. Neutral posts also attract both supportive and neutral users, like what we observe in supportive posts, but just the neutral posts can attract even more neutral likers. These results do suggest that users prefer posts of the same stance, or at least posts of no obvious stance which might cause annoyance when reading, and hence support the user modeling in our approach.", "sentence": "Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts.", "cited_ids": [], "y": "For this analysis, the authors use posts [from FBFans, a single-topic Chinese unbalanced social media dataset obtained from Facebook]. They calculate the like statistics of each distinct author from these 32,595 posts.", "snippet_surface": "The authors use posts in FBFans dataset for this analysis. They calculate the like statistics of each distinct author from these 32,595 posts.", "questions": {"1611.03599.4.1.1.vxaikMcEdv": "What is the FBFans dataset?", "1611.03599.4.1.1.yyuCy4lclR": "How many distinct authors are there?", "1611.03599.4.1.1.bydFgQ+kSa": "How many posts are in the FBFans dataset?"}}
{"idx": "1611.03599.5.2.2", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "sentence": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "cited_ids": [], "y": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9, BIBREF5.", "snippet_surface": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of", "questions": {"1611.03599.5.2.2.MN+zYuuaUu": "What is the data format of posts and replies?", "1611.03599.5.2.2.vxaikMcEdv": "What is the FBFans dataset?", "1611.03599.5.2.2.LPBgY8JbjC": "What are the annotation results in Table TABREF12?"}}
{"idx": "1611.03599.6.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Baselines", "context_paragraph": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. All these models were trained on the training set, and parameters as well as the SVM kernel selections (linear or RBF) were fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.", "sentence": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information;", "cited_ids": [], "y": "\"The authors pit their model [UTCNN, \"user-topic-comment neural network\"] against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings, where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the\"", "snippet_surface": "The authors pit their model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the", "questions": {"1611.03599.6.1.1.m8wa8x9xlz": "What are unigram, bigram and trigram features?", "1611.03599.6.1.1.Hy0MQZQbvi": "What is the INLINEFORM0 equation in EQREF6?", "1611.03599.6.1.1.wvc3OaIxhu": "What comment information is used?"}}
{"idx": "1612.03226.1.1.1", "paper_id": "1612.03226", "title": "Active Learning for Speech Recognition: the Power of Gradients", "abstract": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "context_section_header": "Expected Gradient Length", "context_paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "sentence": "Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "cited_ids": [], "y": "Since labels [annotations of the training data with correct transcriptions for use in training automatic speech recognition systems] are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL [expected gradient length, which is an approach in active learning for end-to-end speech recognition] as \u201cexpected model change\u201d. The authors then formalize the intuition for for EGL and show that it follows naturally from reducing the variance of an estimator.", "snippet_surface": "Since labels are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. The following section formalizes the intuition for EGL and shows that it follows naturally from reducing the variance of an estimator.", "questions": {"1612.03226.1.1.1.p1Dt3w9qt9": "What is INLINEFORM1?", "1612.03226.1.1.1.AbctXp7JD2": "What is \"expected model change\"?", "1612.03226.1.1.1.BHNtSAtRO3": "What is meant by reducing the variance of an estimator?"}}
{"idx": "1612.06685.1.1.1", "paper_id": "1612.06685", "title": "Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "abstract": "People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.", "context_section_header": "Psycholinguistic and Semantic Maps", "context_paragraph": "LIWC. In addition to individual words, we can also create maps for word categories that reflect a certain psycholinguistic or semantic property. Several lexical resources, such as Roget or Linguistic Inquiry and Word Count BIBREF9 , group words into categories. Examples of such categories are Money, which includes words such as remuneration, dollar, and payment; or Positive feelings with words such as happy, cheerful, and celebration. Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories. For instance, figure FIGREF8 shows the maps created for two categories: Positive Feelings and Money. The maps are not surprising, and interestingly they also reflect an inverse correlation between Money and Positive Feelings .", "sentence": "Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories.", "cited_ids": [], "y": "The authors can use the distribution of the individual words [from carefully selected blogs from across the 50 U.S states that have the highest frequency of appearance in the blogs] in a category to compile distributions for the entire category, and therefore generate maps for these word categories.", "snippet_surface": "The authors can use the distribution of the individual words in a category to compile distributions for the entire category, and therefore generate maps for these word categories.", "questions": {"1612.06685.1.1.1.1db616lUxT": "What are the individual words in a category?", "1612.06685.1.1.1.Uvqyicf5j3": "How do we compile distributions for the entire category?", "1612.06685.1.1.1.vjRLxpNEzt": "What are the maps generated for these word categories?"}}
{"idx": "164331", "paper_id": "14151217", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "context_section_header": "", "context_paragraph": "LSTMs have proven to be very effective language models (Sundermeyer et al., 2010). Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "sentence": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "cited_ids": [{"paper_id": "5923323", "citation": "Gulcehre et al. (2015)"}], "y": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach [for video description generation, Deep Fusion, which combines a frozen high quality LSTM LM with a trained captioning model] is that the output of the monolingual language model is used as an input when training the full underlying video description network [an RNN that automatically describes videos in natural language].", "snippet_surface": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach is that the output of the monolingual language model is used as an input when training the full underlying video description network.", "questions": {"164331.b2p+cmDhB7": "What is the difference between Gulcehre et al. (2015) and the authors' approach?", "164331.Cy18T+G5uw": "What is the key advantage of the authors' approach?", "164331.dnUif+p9/r": "How is the monolingual language model used in the authors' approach?"}}
{"idx": "168296", "paper_id": "5673257", "title": "Using word alignments to assist computer-aided translation users by marking which target-side words to change or keep unedited", "abstract": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "context_section_header": "", "context_paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "sentence": "In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "cited_ids": [{"paper_id": "1079699", "citation": "Kranias and Samiotou (2004)"}], "y": "In addition, the authors' system is independent of any extermanl resources such as MT systems [Machine Translation] or dictionaries as opposed to the work by Kranias and Samiotou (2004).", "snippet_surface": "In addition, the authors' system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "questions": {"168296.VjVYi6vVzE": "What is the approach by Kranias and Samiotou (2004)?", "168296.OqvIzGqM0o": "How does their approach differ from the authors' approach?"}}
{"idx": "168810", "paper_id": "17519578", "title": "Question classification using head words and their hypernyms", "abstract": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "context_section_header": "", "context_paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "sentence": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set.", "cited_ids": [{"paper_id": "16137770", "citation": "Li and Roth (2006)"}], "y": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set [a head word feature and two approaches to augment semantic features of head words using WordNet], the authors propose to use a compact yet effective feature set [which includes five binary feature sets: question whword, head word, WordNet semantic feature for head word, word grams and word shape feature].", "snippet_surface": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, the authors propose to use a compact yet effective feature set.", "questions": {"168810.CWEBkyo9Yd": "What is Li and Roth's (2006) approach?", "168810.av5n4HXlA+": "What is the proposed feature set?", "168810.7ukazOkPYd": "What makes the feature set compact yet effective?"}}
{"idx": "1702.03856.1.2.1", "paper_id": "1702.03856", "title": "Towards speech-to-text translation without speech recognition", "abstract": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.", "context_section_header": "Introduction", "context_paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "sentence": "We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ).", "cited_ids": [], "y": "The authors [created a system which builds on unsupervised speech processing, using unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech. They ] test this system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects.", "snippet_surface": "The authors test their system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects.", "questions": {"1702.03856.1.2.1.Uuhntm7QUx": "What is the CALLHOME Spanish-English speech translation corpus?", "1702.03856.1.2.1.R1rMSCsB48": "What is the variety of Spanish dialects?", "1702.03856.1.2.1.Mdzt61sYyI": "What does \u00a7 SECREF3 describe?"}}
{"idx": "1703.10344.1.1.2", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Introduction", "context_paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "sentence": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha.", "cited_ids": [], "y": "While [cyclone] Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone, which has 5 times more human casualties, is not mentioned in the [wikipedia] page for Odisha.", "snippet_surface": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties is not mentioned in the page for Odisha.", "questions": {"1703.10344.1.1.2.skmkgDVgjR": "What is the difference in human casualties between Katrina and Odisha Cyclone?", "1703.10344.1.1.2.ftRmZS0X7s": "What is shown in Figure FIGREF2?", "1703.10344.1.1.2.cw1h9I3nMs": "What is the entity page for Odisha?"}}
{"idx": "1703.10344.4.1.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Section Placement", "context_paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "sentence": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "cited_ids": [], "y": "The authors model the ASP [Article-Sectionp placement] task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to [match news sections to] the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "snippet_surface": "The authors model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "questions": {"1703.10344.4.1.1.m7GgCj5lK8": "What is the AEP task?", "1703.10344.4.1.1.OYGOLwocle": "What is the ASP placement task?", "1703.10344.4.1.1.twApkov51c": "How are Wikipedia entity pages structured?"}}
{"idx": "1703.10344.6.2.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Entity Placement", "context_paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "sentence": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "cited_ids": [], "y": "The authors reimplemented baseline features. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.", "snippet_surface": "The authors reimplemented baseline features discussed in Section SECREF2. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "questions": {"1703.10344.6.2.1.aeT3wKnnna": "What are the positional features?", "1703.10344.6.2.1.NFHVQl5it6": "What are the occurrence frequency features?", "1703.10344.6.2.1.axgvI1FVEh": "What is the internal POS structure of the entity?"}}
{"idx": "1704.06194.1.1.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "KBQA End-Task Results", "context_paragraph": "Finally, like STAGG, which uses multiple relation detectors (see yih2015semantic for the three models used), we also try to use the top-3 relation detectors from Section \"Relation Detection Results\" . As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP.", "sentence": "As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP", "cited_ids": [], "y": "[Using the top 3 relation detectors] gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP.", "snippet_surface": "This gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP.", "questions": {"1704.06194.1.1.1.rLzNv8jlIP": "What is the result on SimpleQuestions?", "1704.06194.1.1.1.kS+gcB0qFw": "What is the result on WebQSP?", "1704.06194.1.1.1.2syInCSdBN": "What is the performance boost?"}}
{"idx": "1704.06194.1.2.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "Relation Detection Results", "context_paragraph": "Table 2 shows the results on two relation detection tasks. The AMPCNN result is from BIBREF20 , which yielded state-of-the-art scores by outperforming several attention-based methods. We re-implemented the BiCNN model from BIBREF4 , where both questions and relations are represented with the word hash trick on character tri-grams. The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "sentence": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "cited_ids": [], "y": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP [(WQ; a multi-relation KBQA task used for analysis purposes)] and is close to the previous best result of AMPCNN on SimpleQuestions [(SQ; a single-relation KBQA task)]. The authors' proposed HR-BiLSTM [(Hierarchical Residual BiLSTM)] outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "snippet_surface": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. The authors' proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "questions": {"1704.06194.1.2.1.Ji8NU2vUzl": "What is BiLSTM?", "1704.06194.1.2.1.fE2AvL2uQ5": "What is AMPCNN?", "1704.06194.1.2.1.YKJPHb+Abb": "What are the tasks SQ and WQ?"}}
{"idx": "1704.06194.2.1.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "Introduction", "context_paragraph": "This paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", "sentence": "First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", "cited_ids": [], "y": "The authors first propose to deal with the unseen relations by breaking the relation names into word sequences for question-relation matching. The authors also propose to build both relation-level and word-level relation representations, as original relation names can sometimes help to match longer question contexts. The authors also use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Lastly, the authors propose a residual learning method, [such as HR-BiLSTM,] for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improving hierarchical matching.", "snippet_surface": "The authors propose to deal with the unseen relations by breaking the relation names into word sequences for question-relation matching. Additionally, they propose to build both relation-level and word-level relation representations, as original relation names can sometimes help to match longer question contexts. The authors also use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, they propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improving hierarchical matching.", "questions": {"1704.06194.2.1.1.yJ3x32GWIN": "What is the purpose of breaking the relation names into word sequences?", "1704.06194.2.1.1.aekKTihIlK": "What does deep bidirectional LSTMs do?", "1704.06194.2.1.1.anEKzDsRS+": "What is the residual learning method for sequence matching?"}}
{"idx": "1706.01678.4.1.1", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Baselines", "context_paragraph": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally. The Lead-3 model simply produces the leading three sentences of the document as its summary.", "sentence": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "cited_ids": [], "y": "For the CNN-Dailymail dataset [300k document summary pairs with stories], the Lead-3 model [the leading three sentences of the document] is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "snippet_surface": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "questions": {"1706.01678.4.1.1.LCfb5xn/Ze": "What is the Lead-3 model?", "1706.01678.4.1.1.Zy/tWDmKk0": "What are the abstractive and extractive state-of-the-art methods?"}}
{"idx": "1706.01678.4.1.2", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Baselines", "context_paragraph": "For the proxy report section of the AMR bank, we consider the Lead-1-AMR model as the baseline. For this dataset we already have the gold-standard AMR graphs of the sentences. Therefore, we only need to nullify the error introduced by the generator.", "sentence": "For the proxy report section of the AMR bank, we consider the Lead-1-AMR model as the baseline.", "cited_ids": [], "y": "For the proxy report section of the AMR bank, the authors consider the Lead-1-AMR model as the baseline. [It combines a model which produces the leading sentence of the document as a summary, with an abstract meaning representation step which produces an AMR graph of a story, extracts a summary graph and then generates summary sentences.]", "snippet_surface": "For the proxy report section of the AMR bank, the authors consider the Lead-1-AMR model as the baseline.", "questions": {"1706.01678.4.1.2.kYIk7NUTpE": "What is the Lead-1-AMR model?", "1706.01678.4.1.2.t3XI+qB19t": "What is the AMR bank?"}}
{"idx": "1706.01678.5.1.1", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Step 2: Story AMR to Summary AMR", "context_paragraph": "After parsing (Step 1) we have the AMR graphs for the story sentences. In this step we extract the AMR graphs of the summary sentences using story sentence AMRs. We divide this task in two parts. First is finding the important sentences from the story and then extracting the key information from those sentences using their AMR graphs.", "sentence": "In this step we extract the AMR graphs of the summary sentences using story sentence AMRs. We divide this task in two parts. First is finding the important sentences from the story and then extracting the key information from those sentences using their AMR graphs.", "cited_ids": [], "y": "AMR graphs [that visualize \"abstract meaning representation\" in rooted, directed, edge and vertex labeled form] of the summary sentences are extracted by the authors using story sentence AMRs. The authors divide this task into two parts. First, important sentences from a [news] story are found and then key information is extracted from the sentences found using their AMR graphs.", "snippet_surface": "The authors extract the AMR graphs of the summary sentences using story sentence AMRs. They divide this task into two parts. First, they find the important sentences from the story and then extract the key information from those sentences using their AMR graphs.", "questions": {"1706.01678.5.1.1.xVSnKE+X+4": "What is story sentence AMRs?", "1706.01678.5.1.1.WRzMOBZU07": "What is the key information from the sentences?"}}
{"idx": "1707.06806.3.1.1", "paper_id": "1707.06806", "title": "Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "abstract": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.", "context_section_header": "Evaluation", "context_paragraph": "In this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", "sentence": "In this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", "cited_ids": [], "y": "The authors evaluate their method [online content popularity prediction using on bidirectional recurrent neural network] and compare its performance against the competitive approaches, [such as Bag-of-Words and Convectional Neural Network]. They use an evaluation protocol with random dataset split. The authors measure [the accuracy of the predictions of their proposed model] using standard accuracy metric which they define as a ratio between correctly classified data samples from test dataset and all test samples.", "snippet_surface": "The authors evaluate their method and compare its performance against the competitive approaches. They use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. They measure the performance using standard accuracy metric which they define as a ratio between correctly classified data samples from test dataset and all test samples.", "questions": {"1707.06806.3.1.1.n2thS8LY+2": "What is the evaluation protocol?", "1707.06806.3.1.1.p1Dt3w9qt9": "What is INLINEFORM1?", "1707.06806.3.1.1.sE1JtQACxs": "What is the accuracy metric?"}}
{"idx": "1707.06806.3.2.1", "paper_id": "1707.06806", "title": "Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "abstract": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.", "context_section_header": "Introduction", "context_paragraph": "In this paper we propose a method for online content popularity prediction based on a bidirectional recurrent neural network called BiLSTM. This work is inspired by recent successful applications of deep neural networks in many natural language processing problems BIBREF5 , BIBREF6 . Our method attempts to model complex relationships between the title of an article and its popularity using novel deep network architecture that, in contrast to the previous approaches, gives highly interpretable results. Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "sentence": "Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "cited_ids": [], "y": "The proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy [(greater than 15%)] over the standard shallow approach [(a classification method called as Support Vector Machine with linear kernel)]. It also outperforms the current state-of-the-art on two distinct datasets with over 40,000 samples.", "snippet_surface": "The proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "questions": {"1707.06806.3.2.1.fhvfUmykWN": "What is the BiLSTM method?", "1707.06806.3.2.1.7emIuqrMEY": "What are the two distinct datasets?", "1707.06806.3.2.1.+haINHa1pN": "What is the current state-of-the-art?"}}
{"idx": "1708.01464.1.1.1", "paper_id": "1708.01464", "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "abstract": "Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and automatic speech recognition systems. Most g2p systems are monolingual: they require language-specific data or handcrafting of rules. Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available. As an alternative, we present a neural sequence-to-sequence approach to g2p which is trained on spelling--pronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems. We show an 11% improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages. Our model is also much more compact relative to previous approaches.", "context_section_header": "High Resource Results", "context_paragraph": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "sentence": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "cited_ids": [], "y": "LangID-High does not present a more accurate result [than wFST (weighted Finite State Transducer)], although it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource [grapheme-to-phoneme conversion models] are 197.5 MB.", "snippet_surface": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "questions": {"1708.01464.1.1.1.dOUsqO8IBM": "What is LangID-High?", "1708.01464.1.1.1.TIZKrFdniu": "What is the combined wFST high resource models?", "1708.01464.1.1.1.Cj+T19yNIN": "How much more compact is LangID-High compared to the combined wFST high resource models?"}}
{"idx": "1709.07814.1.1.1", "paper_id": "1709.07814", "title": "Attention-based Wav2Text with Feature Transfer Learning", "abstract": "Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as\"Attention-based Wav2Text\". To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.", "context_section_header": "Attention-based Encoder Decoder for Raw Speech Recognition", "context_paragraph": "In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 .", "sentence": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part.", "cited_ids": [], "y": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, the authors construct an encoder with several convolutional layers followed by [network-in-network (NIN)] layers as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) at the higher part.", "snippet_surface": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, the authors construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part.", "questions": {"1709.07814.1.1.1.zqWIh4A433": "What are RNNs?", "1709.07814.1.1.1.GKVd9z9mL2": "What are NIN layers?", "1709.07814.1.1.1.7J7e5gQiB0": "What is Bi-LSTM?"}}
{"idx": "1710.03348.1.1.1", "paper_id": "1710.03348", "title": "What does Attention in Neural Machine Translation Pay Attention to?", "abstract": "Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.", "context_section_header": "Introduction", "context_paragraph": "Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "sentence": "Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "cited_ids": [], "y": "[The authors investigated the differences between attention and alignment which are both used to encode the relevant parts of the sources sentence during the translation process. They investigate if the attention model can model alignment and how similar attention is to alignment in different syntactic phenomena]. The authors' analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information ratehr than only the translational equivalent in the case of verbs [because the correct translation of verbs requires more distributed attention compared to nouns].", "snippet_surface": "The authors' analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For example, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "questions": {"1710.03348.1.1.1.lQdZC28eo8": "What is traditional alignment?", "1710.03348.1.1.1.eRa+0nZZix": "How does attention agree with traditional alignments in the case of nouns?", "1710.03348.1.1.1.+1wu2KWsH+": "How does attention capture information beyond alignment in the case of verbs?"}}
{"idx": "171837", "paper_id": "5679499", "title": "Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues", "abstract": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "context_section_header": "", "context_paragraph": "The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "sentence": "Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "cited_ids": [{"paper_id": "2687019", "citation": "(Riedel et al., 2013)"}], "y": "Key differences between the previous approach and the authors' [approach which is to use latent edge labels in addition to surface-level labels] include their use of syntactic information [expressive lexicalized labels] as opposed to surface-level patterns, and also the ability of the proposed PRA-based method [Path Ranking Algorithm that performs inference over a knowledge base using syntactic information from parsed text] to generate useful inference rules [used for increasing coverage of facts in Knowledge Bases] which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "snippet_surface": "Key differences between the previous approach and the authors' include their use of syntactic information as opposed to surface-level patterns, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "questions": {"171837.ILaSevtF5o": "What is the difference between the authors' approach and the approach from (Riedel et al., 2013)?", "171837.3qC1kt6vCs": "What is syntactic information?", "171837.7ca+PwP1TC": "What are inference rules?"}}
{"idx": "173414", "paper_id": "229363636", "title": "Learning Dense Representations of Phrases at Scale", "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "context_section_header": "", "context_paragraph": "tically improves phrase retrieval performance and allows for transfer learning to new domains, without re-building billions of phrase representations. As a result, all these improvements lead to a much stronger phrase retrieval model, without the use of any sparse representations (Table 1). We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets. Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020;. Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.", "sentence": "We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "cited_ids": [{"paper_id": "189762341", "citation": "(Seo et al., 2019;"}, {"paper_id": "189762341", "citation": "(Seo et al., 2019;"}], "y": "The authors evaluate their model, DensePhrases [which is designed to learn phrase representations for retrieving phrase-level knowledge from a large text corpus], on five standard open-domain QA datasets [Natural Questions, SQuAD, TQA, Web Questions, CuratedTREC] and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019), with 15%-25% absolute improvement on most datasets.", "snippet_surface": "The authors evaluate their model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "questions": {"173414.fWN1XxFx47": "What are the five standard open-domain QA datasets?", "173414.xETg+oiKSD": "What are the accuracies of previous phrase retrieval models?"}}
{"idx": "176206", "paper_id": "216642069", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube", "abstract": "Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos, a domain that, a priori, we expect to be relatively\"easy:\"speakers in instructional videos will often reference the literal objects/actions being depicted. Because instructional videos make up only a fraction of the web's diverse video content, we ask: can similar models be trained on broader corpora? And, if so, what types of videos are\"grounded\"and what types are not? We examine the diverse YouTube8M corpus, first verifying that it contains many non-instructional videos via crowd labeling. We pretrain a representative model on YouTube8M and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set still results in representations that generalize to both non-instructional and instructional domains.", "context_section_header": "", "context_paragraph": "porally corresponding (clip, ASR caption) pairs are sampled (\"Positive\" cases). For each positive case, a set of mismatched \"N egative\" cases is also sampled both from other videos and from the same video in equal proportion. In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change. 3 The following hinge loss is minimized for margin \u03b4:", "sentence": "In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change.", "cited_ids": [{"paper_id": "182952863", "citation": "Miech et al. (2019)"}], "y": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this makes their [simple] error analysis significantly more straightforward, and results in [a minimal positive] performance change.", "snippet_surface": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this simplifying choice makes their error analysis significantly more straightforward, and results in minimal performance change.", "questions": {"176206.x2FyabObiK": "What is the approach taken by Miech et al. (2019)?", "176206.jabSQbNumS": "What is the simplifying choice made by the authors?", "176206.Fz8BUHPK31": "What is the performance change resulting from this choice?"}}
{"idx": "176260", "paper_id": "42957292", "title": "Effect Functors for Opinion Inference", "abstract": "Sentiment analysis has so far focused on the detection of explicit opinions. However, of late implicit opinions have received broader attention, the key idea being that the evaluation of an event type by a speaker depends on how the participants in the event are valued and how the event itself affects the participants. We present an annotation scheme for adding relevant information, couched in terms of so-called effect functors, to German lexical items. Our scheme synthesizes and extends previous proposals. We report on an inter-annotator agreement study. We also present results of a crowdsourcing experiment to test the utility of some known and some new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation.", "context_section_header": "", "context_paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "sentence": "Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations.", "cited_ids": [{"paper_id": "193721", "citation": "Ruppenhofer and Brandes (2015)"}], "y": "[Additional functors proposed for verbs embedding states of possibility] are added by the authors to functor types [such as possession, possibility, sentiment, and scalarity], unlike alternative approaches.", "snippet_surface": "The authors add explicit annotations of functor types to the annotations, unlike Ruppenhofer and Brandes (2015).", "questions": {"176260.spN0SwaNX2": "What are functor types?", "176260.llzDd0oUH9": "What did Ruppenhofer and Brandes (2015) do differently?"}}
{"idx": "178903", "paper_id": "53082875", "title": "A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images", "abstract": "Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely, on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.", "context_section_header": "", "context_paragraph": "All the aforementioned methods rely on independently pre-trained linguistic embeddings and visual features. In this work, we propose a different strategy, which consists in adapting those representations so that the information can be fused in earlier stages. In this respect, the closest work to ours is (Lazaridou et al., 2015), which proposes to augment the SKIP-GRAM objective function with a term mapping the textual embeddings to the visual features. Crudely, the linguistic embeddings must therefore predict both the text co-occurrences and (pre-trained) visual features. We emphasize two key differences with our approach. First, instead of performing a regression or mapping from the textual embeddings to the visual features, our model learns to infer perceptual latent factors to retain only the portion of visual information that can supplement the linguistic embeddings in representing words. Second, while Lazaridou et al. (2015) combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way. Specifically, our model seeks latent factors that are good at explaining the word-context co-occurrences. For instance, a visual feature of (an image of) OCEAN often contains information about SKY and BLUE -such visual information could be beneficial to predict cooccurrence of tokens in the context of OCEAN. This desiderata further strengthens the learned embeddings to be visually grounded. In our experiments, we show that our approach tends to group concrete visually similar concepts together.", "sentence": "Second, while Lazaridou et al. (2015) combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way.", "cited_ids": [{"paper_id": "6618571", "citation": "Lazaridou et al. (2015)"}], "y": "While Lazaridou et al. (2015) [predicts both the text co-occurrences and pre-trained visual features], the authors use a joint probabilistic model integrating both visual and text information in a principled way", "snippet_surface": "While Lazaridou et al. (2015) combines two objectives, the authors use a joint probabilistic model integrating both visual and text information in a principled way.", "questions": {"178903.fndHp/Xz42": "What are the two objectives combined by Lazaridou et al. (2015)?", "178903.CbtjsDB/kl": "What is the joint probabilistic model used by the authors?"}}
{"idx": "1801.05147.2.2.1", "paper_id": "1801.05147", "title": "Adversarial Learning for Chinese NER from Crowd Annotations", "abstract": "To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.", "context_section_header": "Data Sets", "context_paragraph": "With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two domains: Dialog and E-commerce domain. We hire undergraduate students to annotate the sentences. They are required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators are educated some tips in fifteen minutes and also provided with 20 exemplifying sentences.", "sentence": "With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two domains: Dialog and E-commerce domain. We hire undergraduate students to annotate the sentences. They are required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators are educated some tips in fifteen minutes and also provided with 20 exemplifying sentences.", "cited_ids": [], "y": "The authors collected sentences from two domains, Dialog and E-commerce, [to show the results of crowdsourcing NER labels from non-experts]. They hired undergraduate students to annotate the sentences. The students were required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators were taught some tips in fifteen minutes and also provided with 20 example sentences.", "snippet_surface": "With the purpose of obtaining evaluation datasets from crowd annotators, the authors collected the sentences from two domains: Dialog and E-commerce. They hired undergraduate students to annotate the sentences. The students were required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators were taught some tips in fifteen minutes and also provided with 20 example sentences.", "questions": {"1801.05147.2.2.1.JjeStUO27V": "What are the predefined types of entities?", "1801.05147.2.2.1.PH5VVaIGX+": "What are the tips given to the annotators?", "1801.05147.2.2.1.NeQIJD/kGA": "How many exemplifying sentences were provided?"}}
{"idx": "1804.05918.1.1.1", "paper_id": "1804.05918", "title": "Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph", "abstract": "We argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph, or independently from all discourse relations and the overall paragraph-level discourse structure. With the goal of improving implicit discourse relation classification, we introduce a paragraph-level neural networks that model inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predict a sequence of discourse relations in a paragraph. Experimental results show that our model outperforms the previous state-of-the-art systems on the benchmark corpus of PDTB.", "context_section_header": "Experimental Results", "context_paragraph": "The second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph. Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations. Especially on the contingency relation, the classification performance was improved by another 1.42 percents. Moreover, the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).", "sentence": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).", "cited_ids": [], "y": "The basic [discourse-level neural network] model yields good performance for recognizing explicit discourse relations as well. This is comparable with the previous best result of a 92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11.", "snippet_surface": "The basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with the previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11).", "questions": {"1804.05918.1.1.1.hjPztwskhC": "What is the best result that was reported in BIBREF11?", "1804.05918.1.1.1./VOyEch8QL": "What is the performance of the basic model for recognizing explicit discourse relations?", "1804.05918.1.1.1.eYxF9V/nY3": "What is macro F1-score?"}}
{"idx": "1805.08241.1.1.1", "paper_id": "1805.08241", "title": "Sparse and Constrained Attention for Neural Machine Translation", "abstract": "In NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.", "context_section_header": "Experiments", "context_paragraph": "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit BIBREF21 with the default parameters . We used a validation set to tune hyperparameters introduced by our model. Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.", "sentence": "We evaluated our attention transformations on three language pairs.", "cited_ids": [], "y": "The authors evaluated their attention transformations on three language pairs [German-English, Japanese-English, and Romanian-English].", "snippet_surface": "The authors evaluated their attention transformations on three language pairs.", "questions": {"1805.08241.1.1.1.noJzJpmyQ/": "No questions."}}
{"idx": "1806.04511.1.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Discussion", "context_paragraph": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. Building separate models for each language requires both labeled and unlabeled data. Even though having lots of labeled data in every language is the perfect case, it is unrealistic. Therefore, eliminating the resource requirement in this resource-constrained task is crucial. The fact that machine translation can be used in reusing models from different languages is promising for reducing the data requirements.", "sentence": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.", "cited_ids": [], "y": "The authors' results show that their RNN model achieved significant improvements over the majority baseline for both non-English (on the average 22.76% improvement [relative to the RNN-based framework]; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement). This demonstrates that the model is robust to handle multiple languages.", "snippet_surface": "The authors' results show that their RNN model achieved significant improvements over the majority baseline for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement). This demonstrates that the model is robust to handle multiple languages.", "questions": {"1806.04511.1.1.1.yypBDPHB+y": "What is the majority baseline?", "1806.04511.1.1.1.xcKSkSekyF": "What is the RNN model?", "1806.04511.1.1.1.GJiXRWSdfx": "What were the average relative improvements on the non-English test sets?"}}
{"idx": "1806.04511.3.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Corpora", "context_paragraph": "For evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus.", "sentence": "These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus.", "cited_ids": [], "y": "Datasets [comprising restaurant reviews in four different languages (Spanish, Turkish, Dutch, Russian)] are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. A table shows the number of reviews in each test corpus alongside whether it is a positive or negative review.", "snippet_surface": "These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. Table TABREF7 shows the number of observations in each test corpus.", "questions": {"1806.04511.3.1.1.RHL2P9xQ3T": "What is SemEval-2016 Challenge Task 5?", "1806.04511.3.1.1.5D/23IcH6J": "What is the content of Table TABREF7?"}}
{"idx": "1806.04511.4.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Experimental Results", "context_paragraph": "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", "sentence": "In addition to the majority baseline, we also compare our results with a lexicon-based approach.", "cited_ids": [], "y": "In addition to the majority baseline, [which is defined as the language model's accuracy if it always predict the majority class in the dataset] the authors also compare their results with a lexicon-based approach.", "snippet_surface": "In addition to the majority baseline, the authors also compare their results with a lexicon-based approach.", "questions": {"1806.04511.4.1.1.yypBDPHB+y": "What is the majority baseline?", "1806.04511.4.1.1.QUiR4c7Vik": "What is a lexicon-based approach?"}}
{"idx": "1808.09111.1.1.1", "paper_id": "1808.09111", "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "abstract": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "context_section_header": "Data", "context_paragraph": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. To create the observed data embeddings, we train skip-gram word embeddings BIBREF7 that are found to capture syntactic properties well when trained with small context window BIBREF8 , BIBREF9 . Following BIBREF9 , the dimensionality INLINEFORM0 is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset BIBREF21 in addition to the WSJ corpus.", "sentence": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "cited_ids": [], "y": "[The authors evaluate their approach of using Markov and tree-structured priors on the part-of-speech (POS) induction and unsupervised dependency parsing]. These experiments were run on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "snippet_surface": "The authors run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing.", "questions": {"1808.09111.1.1.1.4lGj4Q7Z38": "What is the Penn Treebank?", "1808.09111.1.1.1.yxCXweyiqo": "What is the Wall Street Journal portion of the Penn Treebank?"}}
{"idx": "1809.01541.1.2.1", "paper_id": "1809.01541", "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "abstract": "This paper documents the Team Copenhagen system which placed first in the CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the\"inflection in context\"task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.", "context_section_header": "Our system", "context_paragraph": "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", "sentence": "Multilingual training is performed by randomly alternating between languages for every new minibatch.", "cited_ids": [], "y": "The authors perform multilingual training by randomly alternating between languages for every new [randomly selected] minibatch.", "snippet_surface": "The authors perform multilingual training by randomly alternating between languages for every new minibatch.", "questions": {"1809.01541.1.2.1.noJzJpmyQ/": "No questions."}}
{"idx": "1809.09194.1.1.1", "paper_id": "1809.09194", "title": "Stochastic Answer Networks for SQuAD 2.0", "abstract": "This paper presents an extension of the Stochastic Answer Network (SAN), one of the state-of-the-art machine reading comprehension models, to be able to judge whether a question is unanswerable or not. The extended SAN contains two components: a span detector and a binary classifier for judging whether the question is unanswerable, and both components are jointly optimized. Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. To facilitate the research on this field, we release our code: https://github.com/kevinduh/san_mrc.", "context_section_header": "Model", "context_paragraph": "Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2", "sentence": "The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2", "cited_ids": [], "y": "The authors use the attention function BIBREF11 to compute the similarity score between passages and questions as: INLINEFORM2.", "snippet_surface": "The authors use the attention function BIBREF11 to compute the similarity score between passages and questions as: INLINEFORM2.", "questions": {"1809.09194.1.1.1.noJzJpmyQ/": "No questions."}}
{"idx": "1810.08699.1.1.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Experiments", "context_paragraph": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .", "sentence": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .", "cited_ids": [], "y": "The authors describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on their data. They trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model that uses bidirectional LSTM cells for character-based feature extraction and CRF [Conditional Random Field], described in Guillaume Genthial's \"Sequence Tagging with Tensorflow\" blog post.", "snippet_surface": "The authors describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on their data. They trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", "questions": {"1810.08699.1.1.1.VWh/WuSQLr": "What is Stanford NER?", "1810.08699.1.1.1.4A+Vdp9YQT": "What is spaCy 2.0?", "1810.08699.1.1.1.8gapZuDLR9": "What is bidirectional LSTM cells?"}}
{"idx": "1810.08699.1.1.3", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Models", "context_paragraph": "spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.", "sentence": "spaCy 2.0 uses a CNN-based transition system for named entity recognition.", "cited_ids": [], "y": "SpaCy 2.0 [one of the NER models evaluated] uses a CNN-based transition system for named entity recognition [NER].", "snippet_surface": "SpaCy 2.0 uses a CNN-based transition system for named entity recognition.", "questions": {"1810.08699.1.1.3.noJzJpmyQ/": "No questions."}}
{"idx": "1810.08699.1.1.4", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Models", "context_paragraph": "The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.", "sentence": "The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.", "cited_ids": [], "y": "The main model that authors focused on was the recurrent model with a CRF [Conditional Random Field] top layer, and the above-mentioned methods[, the Stanford NER and spaCy 2.0,] served mostly as baselines.", "snippet_surface": "The main model that authors focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.", "questions": {"1810.08699.1.1.4.fh4YOlWuFT": "What is the main model?", "1810.08699.1.1.4.X5O0i649dN": "What are the above-mentioned methods?", "1810.08699.1.1.4.yQyRvYVobq": "How did the main model and the above-mentioned methods compare?"}}
{"idx": "1810.08699.2.1.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Test dataset", "context_paragraph": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .", "sentence": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "cited_ids": [], "y": "In order to evaluate the models trained on the generated data [consisting of annotated sentences and tokens from the Armenian Wikipedia and manually curated datasets], the authors manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "snippet_surface": "In order to evaluate the models trained on generated data, the authors manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "questions": {"1810.08699.2.1.1.9/jQF3E4oL": "What is ilur.am?", "1810.08699.2.1.1.C/u0QqqtuP": "What is the named entities dataset?", "1810.08699.2.1.1.q0Wr+Bd0ib": "What type of data is the dataset comprised of?"}}
{"idx": "1810.08699.2.2.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Automated training corpus generation", "context_paragraph": "We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "sentence": "This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "cited_ids": [], "y": "The approach using [Sysoev and Andrianov's modification of Nothman et al. to automatically generate data for the Russian language] uses [extracted text and outgoing] links from Wikipedia articles to generate sequences of named-entity annotated tokens.", "snippet_surface": "This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "questions": {"1810.08699.2.2.1.jto9Dab8/K": "What are named-entity annotated tokens?", "1810.08699.2.2.1.zNQeJAowxW": "What is the purpose of this approach?", "1810.08699.2.2.1.9jF1A1YRKW": "How does this approach use links between Wikipedia articles?"}}
{"idx": "1810.08699.3.1.2", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Test dataset", "context_paragraph": "During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. Only named entities corresponding to BBN's person name category were tagged as PER. Those include proper names of people, including fictional people, first and last names, family names, unique nicknames. Similarly, organization name categories, including company names, government agencies, educational and academic institutions, sports clubs, musical ensembles and other groups, hospitals, museums, newspaper names, were marked as ORG. However, unlike BBN, we did not mark adjectival forms of organization names as named entities. BBN's gpe name, facility name, location name categories were combined and annotated as LOC.", "sentence": "During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "cited_ids": [], "y": "During annotation [of named entity datasets using three different classes: PER, ORG, LOC], the authors generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "snippet_surface": "During annotation, the authors generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "questions": {"1810.08699.3.1.2.ooVSz65BWb": "What are categories and guidelines assembled by BBN Technologies?", "1810.08699.3.1.2.nngo/ARuvD": "What is the TREC 2002 question answering track?"}}
{"idx": "1810.12196.2.1.1", "paper_id": "1810.12196", "title": "ReviewQA: a relational aspect-based opinion reading dataset", "abstract": "Deep reading models for question-answering have demonstrated promising performance over the last couple of years. However current systems tend to learn how to cleverly extract a span of the source document, based on its similarity with the question, instead of seeking for the appropriate answer. Indeed, a reading machine should be able to detect relevant passages in a document regarding a question, but more importantly, it should be able to reason over the important pieces of the document in order to produce an answer when it is required. To motivate this purpose, we present ReviewQA, a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that we expect a model to master. Indeed, each question comes with an associated type that characterizes the required competency. With this framework, it is possible to benchmark the main families of models and to get an overview of what are the strengths and the weaknesses of a given model on the set of tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of the recent datasets, but selected among a set of candidates that contains all the possible answers to the questions of the dataset. Finally, we present several baselines over this dataset.", "context_section_header": "Models", "context_paragraph": "Logistic regression: To produce the representation of the input, we concatenate the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question. It produces an array of size INLINEFORM0 where INLINEFORM1 is the vocabulary size. Then we use a logistic regression to select the most probable answer among the INLINEFORM2 possibilities.", "sentence": "Logistic regression: To produce the representation of the input, we concatenate the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question.", "cited_ids": [], "y": "Logistic regression: To produce the representation of the input [hotel reviews], by concatenating the Bag-Of-Words [obtained from the source document for which deep reading models have been used] for representation of the document and the question.", "snippet_surface": "The authors used logistic regression to produce the representation of the input, by concatenating the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question.", "questions": {"1810.12196.2.1.1.jL2dC20A0u": "What is the Bag-Of-Words representation?", "1810.12196.2.1.1.jYRpvOD7k/": "How do we produce the representation of the input?"}}
{"idx": "1811.00147.1.1.1", "paper_id": "1811.00147", "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "abstract": "We introduce a new method DOLORES for learning knowledge graph embeddings that effectively captures contextual cues and dependencies among entities and relations. First, we note that short paths on knowledge graphs comprising of chains of entities and relations can encode valuable information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings for entities and relations using deep neural models that capture such contextual usage. In particular, our model is based on Bi-Directional LSTMs and learn deep representations of entities and relations from constructed entity-relation chains. We show that these representations can very easily be incorporated into existing models to significantly advance the state of the art on several knowledge graph prediction tasks like link prediction, triple classification, and missing relation type prediction (in some cases by at least 9.5%).", "context_section_header": "Dolores: Learner", "context_paragraph": "While it is obvious that our embeddings can be used as features for new predictive models, it is also very easy to incorporate our learned Dolores embeddings into existing predictive models on knowledge graphs. The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings. In our evaluation below, we show how to improve several state-of-the-art models on various tasks simply by incorporating Dolores as a drop-in replacement to the original embedding layer.", "sentence": "The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings.", "cited_ids": [], "y": "The only requirement is that the model accepts as input an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), the authors can just use Dolores embeddings [which are created using connections between language models and random walks on knowledge graphs] as a drop-in replacement. The authors just need to initialize the corresponding embedding layer with Dolores embeddings.", "snippet_surface": "The only requirement is that the model accepts as input an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), the authors can just use Dolores embeddings as a drop-in replacement. They just need to initialize the corresponding embedding layer with Dolores embeddings.", "questions": {"1811.00147.1.1.1.rRPGaaHsnU": "What is an embedding layer?", "1811.00147.1.1.1.H8173MNvSn": "What are Dolores embeddings?", "1811.00147.1.1.1.L/mThdSpVV": "How do we initialize the corresponding embedding layer?"}}
{"idx": "1811.00383.1.2.1", "paper_id": "1811.00383", "title": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on the assisting-target language pair (parent model) which is later fine-tuned for the source-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, We propose to pre-order the assisting language sentence to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.", "context_section_header": "Network", "context_paragraph": "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 . These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29 .", "sentence": "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 .", "cited_ids": [], "y": "The authors use the pre-existing CFILT-preorder system for reordering English sentences to match the Indian language word order before running translation. The system contains two re-ordering systems: (1) generic rules that apply to all Indian languages, and (2) Hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.", "snippet_surface": "The authors use the prexisting CFILT-preorder system for reordering English sentences to match the Indian language word order before running translation. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 .", "questions": {"1811.00383.1.2.1.bo1hh4UW4G": "What is CFILT-preorder?", "1811.00383.1.2.1.xULfVxDXX/": "What are the generic rules?", "1811.00383.1.2.1.gw58LUxVoK": "What are the hindi-tuned rules?"}}
{"idx": "1811.00383.2.1.1", "paper_id": "1811.00383", "title": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on the assisting-target language pair (parent model) which is later fine-tuned for the source-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, We propose to pre-order the assisting language sentence to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.", "context_section_header": "Languages", "context_paragraph": "We experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order.", "sentence": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.", "cited_ids": [], "y": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks [the target language pair].", "snippet_surface": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.", "questions": {"1811.00383.2.1.1.Hm3I2JaO57": "What are the primary source languages?", "1811.00383.2.1.1.5B/HeObggU": "What are the child tasks?"}}
{"idx": "1811.05711.1.1.1", "paper_id": "1811.05711", "title": "From Free Text to Clusters of Content in Health Records: An Unsupervised Graph Partitioning Approach", "abstract": "Electronic Healthcare records contain large volumes of unstructured data in different forms. Free text constitutes a large portion of such data, yet this source of richly detailed information often remains under-used in practice because of a lack of suitable methodologies to extract interpretable content in a timely manner. Here we apply network-theoretical tools to the analysis of free text in Hospital Patient Incident reports in the English National Health Service, to find clusters of reports in an unsupervised manner and at different levels of resolution based directly on the free text descriptions contained within them. To do so, we combine recently developed deep neural network text-embedding methodologies based on paragraph vectors with multi-scale Markov Stability community detection applied to a similarity graph of documents obtained from sparsified text vector similarities. We showcase the approach with the analysis of incident reports submitted in Imperial College Healthcare NHS Trust, London. The multiscale community structure reveals levels of meaning with different resolution in the topics of the dataset, as shown by relevant descriptive terms extracted from the groups of records, as well as by comparing a posteriori against hand-coded categories assigned by healthcare personnel. Our content communities exhibit good correspondence with well-defined hand-coded categories, yet our results also provide further medical detail in certain areas as well as revealing complementary descriptors of incidents beyond the external classification. We also discuss how the method can be used to monitor reports over time and across different healthcare providers, and to detect emerging trends that fall outside of pre-existing categories.", "context_section_header": "Multiscale graph partitioning for text analysis: description of the framework", "context_paragraph": "Figure 1 shows a summary of our pipeline. First, we pre-process each document to transform text into consecutive word tokens, where words are in their most normalised forms, and some words are removed if they have no distinctive meaning when used out of context BIBREF5 , BIBREF6 . We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. This training step is only done once. This Doc2Vec model is subsequently used to infer high-dimensional vector descriptions for the text of each of the 3229 documents in our target analysis set. We then compute a matrix containing pairwise similarities between any pair of document vectors, as inferred with Doc2Vec. This matrix can be thought of as a full, weighted graph with documents as nodes and edges weighted by their similarity. We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF8 , a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The derived MST-kNN graph is analysed with Markov Stability BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , a multi-resolution dynamics-based graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need for choosing a priori the number of clusters, scale or organisation. To analyse a posteriori the different partitions across levels of resolution, we use both visualisations and quantitative scores. The visualisations include word clouds to summarise the main content, graph layouts, as well as Sankey diagrams and contingency tables that capture the correspondences across levels of resolution and relationships to the hand-coded classifications. The partitions are also evaluated quantitatively to score: (i) their intrinsic topic coherence (using pairwise mutual information BIBREF13 , BIBREF14 ), and (ii) their similarity to the operator hand-coded categories (using normalised mutual information BIBREF15 ). We now expand on the steps of the computational framework.", "sentence": "We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results.", "cited_ids": [], "y": "[After pre-processing documents to transform text into consecutive word tokens,] the authors then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records[, which were obtained by tokenizing their documents, removing punctuation and digits, stemming the tokens, and removing stop-words]. Training on smaller sets (1 million) also produces good results.", "snippet_surface": "The authors then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results.", "questions": {"1811.05711.1.1.1.nb/jiXu+sI": "What is Doc2Vec?", "1811.05711.1.1.1.21Vjy7GqVX": "What is the difference between training on 13 million records and 1 million records?"}}
{"idx": "181932", "paper_id": "8241948", "title": "Fast Recursive Multi-class Classification of Pairs of Text Entities for Biomedical Event Extraction", "abstract": "Extracting biomedical events from scientific articles to automatically update dedicated knowledge bases has become a popular research topic with important applications. Most existing approaches are either pipeline models of specific classifiers, usually subject to cascading errors, or joint structured models, more efficient but also more costly and complicated to train. This paper proposes a system based on a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. Such pairs are recursively provided to the classifier, allowing to extract events involving other events as arguments. This model facilitates inference compared to joint models while % relying on a single main classifier compared to being more direct and efficient than usual pipeline approaches. This method yields the best results reported so far on the BioNLP 2011 and 2013 Genia tasks.", "context_section_header": "", "context_paragraph": "Biomedical event extraction is attracting more and more attention, especially thanks to the organization of recurrent dedicated BioNLP challenges (Kim et al., 2009;Kim et al., 2011b;Kim et al., 2013). We propose here a new approach which relies on a single multi-class classifier for recursively detecting events from (trigger, argument) pairs. Compared to standard pipeline approaches based on sequences of classifiers (Bj\u00f6rne and Salakoski, 2013;Hakala et al., 2013), we avoid the intermediate problem of associating isolated triggers to event types, relying on a tricky multi-label classification problem. Instead, we directly extract compounds of events in the form of (trigger, argument) pairs, simply relying on a multi-class problem, whereby (trigger, argument) pairs are associated to event types. Considering pairs of words also allows us to characterize examples by sophisticated joint features such as shortest path in the dependency parse tree, and hence to achieve much accurate trigger detection than pipeline models. Besides, compared to Markov random fields (Riedel and McCallum, 2011a), our discriminant model does not represent the full joint distribution of words and events. We thus have a simpler inference process, which results into drastically reduced training times (15  Figure 1: Part of a sentence and corresponding extracted events for the BioNLP 2013 Genia task.", "sentence": "Besides, compared to Markov random fields (Riedel and McCallum, 2011a), our discriminant model does not represent the full joint distribution of words and events.", "cited_ids": [{"paper_id": "18198203", "citation": "(Riedel and McCallum, 2011a"}], "y": "Compared to Markov random fields (Riedel and McCallum, 2011a), the authors [proposed a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. This] discriminant model does not represent the full joint distribution of words and events.", "snippet_surface": "Additionally, compared to Markov random fields (Riedel and McCallum, 2011a), the authors' discriminant model does not represent the full joint distribution of words and events.", "questions": {"181932.B64loBuSdc": "What is a Markov random field?", "181932.rLPkxtDcYm": "What does a discriminant model represent?"}}
{"idx": "183543", "paper_id": "2281724", "title": "Combining Heterogeneous Models for Measuring Relational Similarity", "abstract": "In this work, we study the problem of measuring relational similarity between two word pairs (e.g., silverware:fork and clothing:shirt). Due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources. Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models. When evaluated in the setting of a recently proposed SemEval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman\u2019s rank correlation.", "context_section_header": "", "context_paragraph": "In this paper, we explore the problem of measuring relational similarity in the same task setting. We argue that due to the large number of possible relations, building an ensemble of relational simi-larity models based on heterogeneous information sources is the key to advance the state-of-the-art on this problem. By combining two general-purpose relational similarity models with three specific wordrelation models covering relations like IsA and synonymy/antonymy, we improve the previous stateof-the-art substantially -having a relative gain of 54.1% in Spearman's rank correlation and 14.7% in the MaxDiff accuracy! Our main contributions are threefold. First, we propose a novel directional similarity method based on the vector representation of words learned from a recurrent neural network language model. The relation of two words is captured by their vector offset in the latent semantic space. Similarity of relations can then be naturally measured by a distance function in the vector space. This method alone already performs better than all existing systems. Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well. Third, we demonstrate that by augmenting existing word-relation models, which cover only a small number of relations, the overall system can be further improved.", "sentence": "Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well.", "cited_ids": [{"paper_id": "15637201", "citation": "(Rink and Harabagiu, 2012)"}], "y": "Unlike the previous finding, where SVMs learn a much poorer model than naive Bayes  (Rink and Harabagiu, 2012), the authors show that using a highly regularized log-linear model [for assessing relational similarity] on simple contextual pattern features collected from a document collection of 20 GB,  a discriminative approach can learn a strong model as well.", "snippet_surface": "Secondly, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), the authors show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well.", "questions": {"183543.s6+Rlb85OX": "What is the previous finding?", "183543.1F+CGnqc1o": "What is a log-linear model?", "183543.dxW+nUNKXp": "What are contextual pattern features?"}}
{"idx": "183813", "paper_id": "10910870", "title": "Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora", "abstract": "We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available.", "context_section_header": "", "context_paragraph": "2 In (Deng et al., 2006), p. 5, the p(a k ) = p(x, y) which determines the prior probability of having an alignment containing x source and y target sentences, is equal to 0 if x < 1 or y < 1. As p(a k ) is a multiplicative factor of the model, the probability of having an insertion or a deletion is always equal to 0. finds a model-optimal alignment composed of the smallest possible correspondences, namely 1-to-0/0-to-1 and 1-to-1, and then merges those correspondences into larger alignments. This allows the finding of 1-to-0/0-to-1 alignments as well as high quality 1-to-many/many-to-1 alignments, leading to high accuracy on parallel texts but also on corpora containing large blocs of inserted or deleted text. Furthermore, our approach keeps the computational costs of the alignment procedure low: our aligner is, on average, about 550 times faster than our implementation 3 of (Deng et al., 2006).", "sentence": "Furthermore, our approach keeps the computational costs of the alignment procedure low: our aligner is, on average, about 550 times faster than our implementation 3 of (Deng et al., 2006).", "cited_ids": [{"paper_id": "11879506", "citation": "(Deng et al., 2006)"}], "y": "The authorss' approach [use a two-step clustering approach for sentence alignment] that keeps the computational costs of the alignment procedure [finding a large amount of sentences with a dynamic programming search] low: their aligner is, on average, about 550 times faster than their implementation 3 of (Deng et al., 2006).", "snippet_surface": "Furthermore, the authors' approach keeps the computational costs of the alignment procedure low: their aligner is, on average, about 550 times faster than their implementation 3 of (Deng et al., 2006).", "questions": {"183813.gZwrHfhfeh": "What is the aligner?", "183813.2iYvYv/vrp": "What does \"implementation 3\" refer to?", "183813.b29+6y7k90": "How much faster is the aligner compared to the implementation?"}}
{"idx": "184958", "paper_id": "16919810", "title": "One Tree is not Enough: Cross-lingual Accumulative Structure Transfer for Semantic Indeterminacy", "abstract": "We address the task of parsing semantically indeterminate expressions, for which several correct structures exist that do not lead to differences in meaning. We present a novel non-deterministic structure transfer method that accumulates all structural information based on cross-lingual word distance derived from parallel corpora. Our system\u2019s output is a ranked list of trees. To evaluate our system, we adopted common IR metrics. We show that our system outperforms previous cross-lingual structure transfer methods significantly. In addition, we illustrate that tree accumulation can be used to combine partial evidence across languages to form a single structure, thereby making use of sparse parallel data in an optimal way.", "context_section_header": "", "context_paragraph": "Our system is inspired by Ziering and Van der Plas (2015), who exploit cross-lingual surface variation for bracketing 3NCs. There are various ways of translating English noun compounds. Germanic languages such as Swedish frequently use closed compounds (i.e., single nouns), whereas Romance languages such as French use open compounds (i.e., lexemes composed of several words). Paraphrased translations (e.g., human rights abuse aligned to the partially closed German Verletzung der Menschenrechte (abuse of human rights)) can reveal the internal structure of a compound. While Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, we gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "sentence": "While Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, we gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "cited_ids": [{"paper_id": "10070164", "citation": "Ziering and Van der Plas (2015)"}], "y": "Ziering and Van der Plas (2015) follow the deterministic take [(i.e., the notion that syntactic structures are usually understood that for every structure there exists conditions that can have no other structure)] by producing a single tree output, whereas the authors gather all structural information and produce a ranked list of plausible [system outputs], where similarly-ranked [system outputs] indicate semantic indeterminacy.", "snippet_surface": "Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, whereas the authors gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "questions": {"184958.wqzWYqDPyw": "What is the deterministic take?", "184958.ZrCyWGsBRY": "What does gathering structural information entail?", "184958.giMULur4Nq": "What does producing a ranked list of plausible trees indicate?"}}
{"idx": "188418", "paper_id": "1397387", "title": "Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data", "abstract": "We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets.", "context_section_header": "", "context_paragraph": "Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains.", "sentence": "When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points.", "cited_ids": [{"paper_id": "1916754", "citation": "(Koo et al., 2008)"}], "y": "When comparing to the strong baseline of using Brown-clusters based features [created using a maximum likelihood clustering model] (Koo et al., 2008), we find that our triplets-based method [which involves looking at dependencies between sets of 3 words] outperform them by over 0.27 UAS [unlabeled attachment score] points.", "snippet_surface": "When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), the authors find that their triplets-based method outperforms them by over 0.27 UAS points.", "questions": {"188418.94FeDvmPt9": "What are Brown-clusters based features?", "188418.sEVDxaDhyQ": "What is the triplets-based method?", "188418.9pVQjc83j1": "How is performance measured (UAS points)?"}}
{"idx": "188615", "paper_id": "51976920", "title": "Rapid Adaptation of Neural Machine Translation to New Languages", "abstract": "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \u201cseed models\u201d, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \u201csimilar-language regularization\u201d, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.", "context_section_header": "", "context_paragraph": "While adapting MT systems to new languages is a long-standing challenge (Schultz and Black, 2006;Jabaian et al., 2013), multilingual NMT is highly promising in its ability to abstract across language boundaries (Firat et al., 2016;Ha et al., 2016;Johnson et al., 2016). Results on multilingual training for low-resource translation (Gu et al., 2018; further demonstrates this potential, although these works do not consider adaptation to new languages, the main focus of our work. Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined. Finally, unsupervised NMT approaches (Artetxe et al., 2017;Lample et al., 2018Lample et al., , 2017 require no parallel data, but rest on strong assumptions about high-quality comparable monolingual data. As we show, when this assumption breaks down these methods fail to function, while our cold-start methods achieve non-trivial accuracies even with no monolingual data.", "sentence": "Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined.", "cited_ids": [{"paper_id": "16631020", "citation": "(Zoph et al., 2016)"}], "y": "The authors did not examine partial freezing of parameters, a method that has proven useful for cross-lingual adaptation by Zoph et al. (2016); this is orthogonal to their multi-lingual training approach but the two methods [partial freezing of parameters and multilingual training] could potentially be combined.", "snippet_surface": "Notably, the authors did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to their multi-lingual training approach but the two methods could potentially be combined.", "questions": {"188615.Z/Pr4u1BZd": "What is partial freezing of parameters?", "188615.WXhuY/JFHg": "How is it different from the multi-lingual training approach?", "188615.e2OAQwNS2N": "Could the two methods be combined?"}}
{"idx": "1902.09666.1.2.2", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Experiments and Evaluation", "context_paragraph": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "sentence": "Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "cited_ids": [], "y": "The authors also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs [(FastText embeddings and updatable embeddings learned by the model during training)] as the above BiLSTM [(Bidirectional Long Short-Term-Memory)].", "snippet_surface": "The authors also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", "questions": {"1902.09666.1.2.2.Uu2HoW+toH": "What is a BiLSTM?", "1902.09666.1.2.2.4/cCrnRYzO": "What is the architecture of BIBREF15?", "1902.09666.1.2.2.Hv/cSaBeb2": "What are the multi-channel inputs?"}}
{"idx": "1902.09666.5.3.1", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Hierarchically Modelling Offensive Content", "context_paragraph": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10 .", "sentence": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.", "cited_ids": [], "y": "The authors use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language in the OLID dataset, [which contains tweets that have been categorised and annotated according to whether their content is offensive, the type of offensive language, and who the targets of the offensive language are.].", "snippet_surface": "The authors use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language in the OLID dataset.", "questions": {"1902.09666.5.3.1.MyS6o+sVA5": "What is the OLID dataset?", "1902.09666.5.3.1.3SJ4CjVdJX": "What are the three levels of the hierarchical annotation model?", "1902.09666.5.3.1.THYo8/pqQ9": "What is the difference between type (B) and target (C) of offensive language?"}}
{"idx": "1902.09666.5.3.3", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Level B: Categorization of Offensive Language", "context_paragraph": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "sentence": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "cited_ids": [], "y": "[To predict the type and target of offensive posts in social media, the authors use a trained system on pre-tagged tweets to discriminate between insults/threats and untargeted offensive tweets (e.g., swearing). The authors categorize] Level B to include the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "snippet_surface": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "questions": {"1902.09666.5.3.3.rSnclAoEiB": "What does \"Level B\" refer to?", "1902.09666.5.3.3.fKnZPg7ebQ": "What do \"TIN\" and \"INT\" stand for?"}}
{"idx": "1904.01548.2.2.1", "paper_id": "1904.01548", "title": "Understanding language-elicited EEG data by predicting it from a fine-tuned language model", "abstract": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.", "context_section_header": "Related Work", "context_paragraph": "This work is most closely related to the paper from which we get the ERP data: BIBREF0 . In that work, the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals we consider here. The authors do not directly train a model to predict ERPs. Instead, models of the probability distribution of each word in context are used to compute a surprisal for each word, which is input into a mixed effects regression along with word frequency, word length, word position in the sentence, and sentence position in the experiment. The effect of the surprisal is assessed using a likelihood-ratio test. In BIBREF7 , the authors take an approach similar to BIBREF0 . The authors compare the explanatory power of surprisal (as computed by an LSTM or a Recurrent Neural Network Grammar (RNNG) language model) to a measure of syntactic complexity they call \u201cdistance\" that counts the number of parser actions in the RNNG language model. The authors find that surprisal (as predicted by the RNNG) and distance are both significant factors in a mixed effects regression which predicts the P600, while the surprisal as computed by an LSTM is not. Unlike BIBREF0 and BIBREF7 , we do not use a linking function (e.g. surprisal) to relate a language model to ERPs. We thus lose the interpretability provided by the linking function, but we are able to predict a significant proportion of the variance for all of the ERP components, where prior work could not. We interpret our results through characterization of the ERPs in terms of how they relate to each other and to eye-tracking data rather than through a linking function. The authors in BIBREF8 also use a recurrent neural network to predict neural activity directly. In that work the authors predict magnetoencephalography (MEG) activity, a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer\u2019s Stone BIBREF9 . Their approach to characterization of processing at each MEG sensor location is to determine whether it is best predicted by the context vector of the recurrent network (prior to the current word being processed), the embedding of the current word, or the probability of the current word given the context. In future work we also intend to add these types of studies to the ERP predictions.", "sentence": "This work is most closely related to the paper from which we get the ERP data: BIBREF0 .", "cited_ids": [], "y": "The author's [new approach to fine-tuning a language model to predict event-related potentials (ERP)] is most closely related to the paper from which they get the ERP data.", "snippet_surface": "This work is most closely related to the paper from which the authors get the ERP data: BIBREF0.", "questions": {"1904.01548.2.2.1.noJzJpmyQ/": "No questions."}}
{"idx": "1904.04358.2.1.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "Dataset", "context_paragraph": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "sentence": "In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "cited_ids": [], "y": "The authors explore [the challenge of decoding intended speech or motor activity from brain signals, specifically electroencephalography (EEG) data] by specifically targeting five binary classification problems addressed in BIBREF17, BIBREF18, i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "snippet_surface": "The authors explore the problem space by specifically targeting five binary classification problems addressed in BIBREF17, BIBREF18, i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "questions": {"1904.04358.2.1.1.ohTDbuY68c": "What are the five binary classification problems?", "1904.04358.2.1.1.KgOfvSCNij": "What is being classified as present/absent?", "1904.04358.2.1.1.xcfa/jSA0o": "What is the source of the five binary classification problems?"}}
{"idx": "1904.04358.3.1.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "CNN & LSTM", "context_paragraph": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "sentence": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "cited_ids": [], "y": "The authors decode spatial connections between electrodes [features that are decoded to determine the dimensionality of matrices] between the [parts of channel covariance matrices that are used to determine their dimensionality of these matrices] from the channel covariance matrix by using a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The feature map at a given CNN layer with input INLINEFORM1, weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4. At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, they apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, they exploit an LSTM with two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as a CNN [trained on the channel covariant matrices].", "snippet_surface": "The authors decode spatial connections between the electrodes from the channel covariance matrix by using a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The feature map at a given CNN layer with input INLINEFORM1, weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4. At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, they apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, they exploit an LSTM BIB", "questions": {"1904.04358.3.1.1.PQB4EOAoJB": "What is INLINEFORM0?", "1904.04358.3.1.1.p1Dt3w9qt9": "What is INLINEFORM1?", "1904.04358.3.1.1.XLf2vmYSJc": "What is INLINEFORM2?", "1904.04358.3.1.1.Oa47PoOjQk": "What is INLINEFORM3?", "1904.04358.3.1.1.400KIHCx4A": "What is INLINEFORM4?"}}
{"idx": "1904.04358.4.2.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "Dataset", "context_paragraph": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "sentence": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "cited_ids": [], "y": "The authors evaluate their model [, a BCI system for classification, ]on a publicly available dataset, KARA ONE, composed of multimodal data [from 14 participants] for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since the authors' intention is to classify the phonological categories from human thoughts [(i.e., he relationship between imagined speech and active thoughts)], they discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is reportedly challenging to attain a pairwise EEG-ph [because of the mixed nature of EEG signals.]", "snippet_surface": "The authors evaluate their model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since the authors' intention is to classify the phonological categories from human thoughts, they discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is reportedly challenging to attain a pairwise EEG-ph", "questions": {"1904.04358.4.2.1.kBibnbgxcu": "What is KARA ONE?", "1904.04358.4.2.1.jhiaC3/hVY": "What is the task of the classification problem?", "1904.04358.4.2.1.NnFV77n9Bg": "What is the purpose of discarding facial and audio information?"}}
{"idx": "190534", "paper_id": "9519654", "title": "Unsupervised Morphological Segmentation with Log-Linear Models", "abstract": "Morphological segmentation breaks words into morphemes (the basic semantic units). It is a key component for natural language processing systems. Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources. However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efficient algorithms for learning and inference by combining contrastive estimation with sampling. Our system, based on monolingual features only, outperforms a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor.", "context_section_header": "", "context_paragraph": "We focus on inflectional morphology and test our approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder & Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007).", "sentence": "On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007).", "cited_ids": [{"paper_id": "8819802", "citation": "(Creutz and Lagus, 2007)"}], "y": "[The authors present a log-linear model for unsupervised morphological segmentation,] reducing F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007) [which uses information about word frequency] on the Arabic Penn Treebank.", "snippet_surface": "The authors' system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007) on the Arabic Penn Treebank.", "questions": {"190534.TWEkwPVRzg": "What is the Arabic Penn Treebank?", "190534.E5kkZLj6Yy": "What is F1 error?", "190534.E66NXjcYOl": "What is Morfessor Categories-MAP?"}}
{"idx": "1906.08593.2.1.1", "paper_id": "1906.08593", "title": "Conflict as an Inverse of Attention in Sequence Relationship", "abstract": "Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.", "context_section_header": "The model", "context_paragraph": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", "sentence": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input.", "cited_ids": [], "y": "The authors create two models [for their language understanding task,] both of which constitute of three main parts: encoder, interaction and classifier and take two sequences [(individual words or phrases)] as input.", "snippet_surface": "The authors create two models both of which constitute of three main parts: encoder, interaction and classifier and take two sequences as input.", "questions": {"1906.08593.2.1.1.exQewJE7Cu": "What are the three main parts of the models?", "1906.08593.2.1.1.4wgDFORsuH": "What are the two sequences that are taken as input?"}}
{"idx": "1907.03060.1.2.1", "paper_id": "1907.03060", "title": "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation", "abstract": "This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking. Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario.", "context_section_header": "Conclusion", "context_paragraph": "In this paper, we challenged the difficult task of Ja INLINEFORM0 Ru news domain translation in an extremely low-resource setting. We empirically confirmed the limited success of well-established solutions when restricted to in-domain data. Then, to incorporate out-of-domain data, we proposed a multilingual multistage fine-tuning approach and observed that it substantially improves Ja INLINEFORM1 Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53 . This paper contains an empirical comparison of several existing approaches and hence we hope that our paper can act as a guideline to researchers attempting to tackle extremely low-resource translation.", "sentence": "Then, to incorporate out-of-domain data, we proposed a multilingual multistage fine-tuning approach and observed that it substantially improves Ja INLINEFORM1 Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53 .", "cited_ids": [], "y": "The authors proposed a multilingual multistage fine-tuning approach to incorporate out-of-domain data, and observed that it substantially improves Ja\u2192Ru [Japanese to Russian news] translation by over 3.7 BLEU points compared to a strong baseline.", "snippet_surface": "The authors then proposed a multilingual multistage fine-tuning approach to incorporate out-of-domain data, and observed that it substantially improves Ja\u2192Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53.", "questions": {"1907.03060.1.2.1.Ea8MPAIiTa": "What is the multilingual multistage fine-tuning approach?", "1907.03060.1.2.1.rPQE0Rf8PG": "What is Ja-Ru translation?", "1907.03060.1.2.1.vvHnUtEg8+": "What is the strong baseline used for comparison?"}}
{"idx": "1907.09369.3.1.2", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "sentence": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "cited_ids": [], "y": "In the [CrowdFlower dataset annotated for emotions], the authors reported results from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. They only reported part of their result for CrowdFlower dataset that can be mapped to one of the seven labels.", "snippet_surface": "In the second one, the authors reported results from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. They only reported part of their result for CrowdFlower dataset that can be mapped to one of the seven labels.", "questions": {"1907.09369.3.1.2.AXbadyw8eb": "What is the maximum entropy classifier?", "1907.09369.3.1.2.S8P2N2DWpV": "What is the bag of words model?", "1907.09369.3.1.2.DbecUDz0vm": "What is the CrowdFlower dataset?"}}
{"idx": "1907.09369.3.2.1", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "sentence": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "cited_ids": [], "y": "The authors' first experiment was conducted by Wang et al. BIBREF21, who downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "snippet_surface": "The authors' first experiment was conducted by Wang et al. BIBREF21, who downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "questions": {"1907.09369.3.2.1.+W+bUG4W9N": "What are Parrott's three-level categorization of emotions?", "1907.09369.3.2.1.3MQdbadYYT": "How many categories are there in Parrott's categorization?", "1907.09369.3.2.1.Pv/NC9JSDQ": "How many tweets were downloaded?"}}
{"idx": "1907.09369.4.2.2", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "sentence": "Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "cited_ids": [], "y": "The authors only report part of their result for CrowdFlower dataset [comprising emotional tweets] [whereby emotions are annotated] that can be mapped to one of their seven labels.", "snippet_surface": "The authors only report part of their result for CrowdFlower dataset that can be mapped to one of their seven labels.", "questions": {"1907.09369.4.2.2.VNcXeOSSGE": "What are the seven labels?", "1907.09369.4.2.2.74Fa00Pp8w": "What part of their result is reported?"}}
{"idx": "1907.09369.5.1.1", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Model", "context_paragraph": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature. As mentioned in the Introduction, Recurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets.", "sentence": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature.", "cited_ids": [], "y": "The authors' architecture [a bidirectional GRU network used to classify emotions in tweets] was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods [that infer emotion by looking for specific n-grams] commonly used in the literature.", "snippet_surface": "The authors' architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature.", "questions": {"1907.09369.5.1.1.+h7T4+12XR": "What is a lexicon-based method?", "1907.09369.5.1.1.GqKGnsgzoF": "How does the architecture capture better information?", "1907.09369.5.1.1.hcBIIirzxB": "What type of text does the model outperform lexicon-based methods on?"}}
{"idx": "1908.05828.1.1.1", "paper_id": "1908.05828", "title": "Named Entity Recognition for Nepali Language", "abstract": "Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created. We discuss and compare the performance of various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.", "context_section_header": "Conclusion and Future work", "context_paragraph": "We also present a neural architecture BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration. We believe this will not only help Nepali language but also other languages falling under the umbrellas of Devanagari languages. Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively.", "sentence": "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively.", "cited_ids": [], "y": "The authors' model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperform all other models the authors generated in the [Nepali Named Entity Recognition] (OurNepali) and ILPRL datasets, respectively.", "snippet_surface": "The authors' model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other models experimented in OurNepali and ILPRL dataset respectively.", "questions": {"1908.05828.1.1.1.ET7W17dq3h": "What are the \"BiLSTM+CNN(grapheme-level)\" and \"BiLSTM+CNN(G)+POS\" models?", "1908.05828.1.1.1.XWJ2WgiTeg": "What are the OurNepali and ILPRL datasets?"}}
{"idx": "1908.06941.1.1.2", "paper_id": "1908.06941", "title": "Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "abstract": "In distributional semantics, the pointwise mutual information ($\\mathit{PMI}$) weighting of the cooccurrence matrix performs far better than raw counts. There is, however, an issue with unobserved pair cooccurrences as $\\mathit{PMI}$ goes to negative infinity. This problem is aggravated by unreliable statistics from finite corpora which lead to a large number of such pairs. A common practice is to clip negative $\\mathit{PMI}$ ($\\mathit{\\texttt{-} PMI}$) at $0$, also known as Positive $\\mathit{PMI}$ ($\\mathit{PPMI}$). In this paper, we investigate alternative ways of dealing with $\\mathit{\\texttt{-} PMI}$ and, more importantly, study the role that negative information plays in the performance of a low-rank, weighted factorization of different $\\mathit{PMI}$ matrices. Using various semantic and syntactic tasks as probes into models which use either negative or positive $\\mathit{PMI}$ (or both), we find that most of the encoded semantics and syntax come from positive $\\mathit{PMI}$, in contrast to $\\mathit{\\texttt{-} PMI}$ which contributes almost exclusively syntactic information. Our findings deepen our understanding of distributional semantics, while also introducing novel $PMI$ variants and grounding the popular $PPMI$ measure.", "context_section_header": "PMI & Matrix Factorization", "context_paragraph": "such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "sentence": "In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "cited_ids": [], "y": "The authors find that this [the normalized method used in BIBREF7] works poorly if done symmetrically, so they introduce a variant called NNEGPMI which only normalizes -PMI.", "snippet_surface": "The authors find that this works poorly if done symmetrically, so they introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "questions": {"1908.06941.1.1.2.Y1qm7e5bGx": "What is $\\mathit {NNEGPMI}$?", "1908.06941.1.1.2.jqOYd3AC8y": "What is $\\mathit {\\texttt {-}PMI}$?", "1908.06941.1.1.2.1SktNNzxNp": "What does \"normalizes\" mean in this context?"}}
{"idx": "1908.09246.3.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Experimental Setup", "context_paragraph": "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12 , Twitter, and Google datasets) are employed. Details are summarized below:", "sentence": "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12 , Twitter, and Google datasets) are employed.", "cited_ids": [], "y": "The authors validate the effectiveness of AEM [Adversarial-neural Event Mode] for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news) using three datasets.", "snippet_surface": "The authors validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news) using three datasets (FSD BIBREF12, Twitter, and Google datasets).", "questions": {"1908.09246.3.1.1.aax53iUpEr": "What is AEM?", "1908.09246.3.1.1.gvX17lTzp+": "What are the three datasets?", "1908.09246.3.1.1.RiLCn+jTAf": "What are the sources of the three datasets?"}}
{"idx": "1908.09246.4.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Generative Adversarial Nets", "context_paragraph": "Although various GAN based approaches have been explored for many applications, none of these approaches tackles open-domain event extraction from online texts. We propose a novel GAN-based event extraction model called AEM. Compared with the previous models, AEM has the following differences: (1) Unlike most GAN-based text generation approaches, a generator network is employed in AEM to learn the projection function between an event distribution and the event-related word distributions (entity, location, keyword, date). The learned generator captures event-related patterns rather than generating text sequence; (2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration; (3) The discriminative features learned by the discriminator of AEM provide a straightforward way to visualize the extracted events.", "sentence": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration;", "cited_ids": [], "y": "Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM [where Gibbs sampler computes the conditional posterior distribution and assigns an event for each document], AEM could extract the events more efficiently due to CUDA acceleration [ie using GPUs].", "snippet_surface": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to CUDA acceleration.", "questions": {"1908.09246.4.1.1.nVdsKoI3rE": "What is LEM and DPEMM?", "1908.09246.4.1.1.3s6sXba10e": "What is AEM's generator network?", "1908.09246.4.1.1.ploY3y9Poj": "What is the traditional inference procedure used in LEM and DPEMM?"}}
{"idx": "1908.09246.5.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "sentence": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns.", "cited_ids": [], "y": "Instead of providing an analytic approximation, [the Adversarial-neural Event Model] uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. The supervision signal provided by the discriminator will help the generator to capture the event-related patterns.", "snippet_surface": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. The supervision signal provided by the discriminator will help the generator to capture the event-related patterns.", "questions": {"1908.09246.5.1.1.GWrIcWzPYe": "What is the role of the discriminator network?", "1908.09246.5.1.1.pfWFJs2wUQ": "What is the Dirichlet distribution?", "1908.09246.5.1.1.HzEWuYC763": "How does the supervision signal help the generator?"}}
{"idx": "1908.09246.5.2.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To extract structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al. zhou2014simple proposed an unsupervised Latent Event Model (LEM) for open-domain event extraction. To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "sentence": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 .", "cited_ids": [], "y": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document [concatenation of four multinomial distributions of the document] are generated from a single event which can be represented by a quadruple (entity, location, keyword, date).\" [(2) this is a time-consuming process and takes a long time to converge]", "snippet_surface": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple (entity, location, keyword, date).", "questions": {"1908.09246.5.2.1.u1GScHc8SQ": "What are the two limitations of LEM and DPEMM?", "1908.09246.5.2.1.lH/DFvvNhP": "What are the four components of the quadruple?"}}
{"idx": "1908.09246.5.2.2", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "sentence": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "cited_ids": [], "y": "To deal with the limitations of previous work [assuming words in a document are generated from a single event, and the long time it takes the Gibbs sampler to run], the authors propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "snippet_surface": "To deal with these limitations, the authors propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "questions": {"1908.09246.5.2.2.4ZCpOJC1un": "What are the limitations of existing approaches?", "1908.09246.5.2.2.ukKLr6ley1": "What is the Adversarial-neural Event Model?", "1908.09246.5.2.2.qT1SdAQE7K": "What are the four event related word distributions?"}}
{"idx": "1908.10084.1.3.1", "paper_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.  ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "context_section_header": "Introduction", "context_paragraph": "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", "sentence": "On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder.", "cited_ids": [], "y": "On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points [of cosine-similarity score] compared to InferSent and 5.5 points compared to Universal Sentence Encoder.", "snippet_surface": "The authors find that SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder on seven Semantic Textual Similarity (STS) tasks.", "questions": {"1908.10084.1.3.1.3Q0u5X2k4U": "What are the STS tasks?", "1908.10084.1.3.1.02N3PtlXpx": "What is an improvement of 11.7 points compared to InferSent?", "1908.10084.1.3.1.hxOpU6wfn/": "What is Universal Sentence Encoder?"}}
{"idx": "1908.10084.2.1.1", "paper_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.  ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "context_section_header": "Evaluation - Semantic Textual Similarity ::: Unsupervised STS", "context_paragraph": "We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, the STS benchmark BIBREF10, and the SICK-Relatedness dataset BIBREF21. These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine-similarity. The results are depicted in Table TABREF6.", "sentence": "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", "cited_ids": [], "y": "The authors showed in BIBREF22 that Pearson correlation [a well-known correlation measure in statistics] is badly suited for STS. Instead, they compute the Spearman's rank correlation between the cosine-similarity [which is computed by dividing the dot product of the vectors by the product of their magnitudes] of the sentence embeddings and the gold labels", "snippet_surface": "The authors showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, they compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", "questions": {"1908.10084.2.1.1.X2ciBT7ssn": "What is STS?", "1908.10084.2.1.1.tgSp4AG7Xx": "What is Spearman's rank correlation?", "1908.10084.2.1.1.NpbwPFF8gU": "What are sentence embeddings?"}}
{"idx": "1909.00512.1.1.1", "paper_id": "1909.00512", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings", "abstract": "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.", "context_section_header": "Approach ::: Measures of Contextuality", "context_paragraph": "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "sentence": "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "cited_ids": [], "y": "The authors measure how contextual a word representation is using three different metrics: self-similarity [the average cosine similarities between its contextualized representation in a certain context], intra-sentence similarity [the average cosine similarity between its word representations and the sentence vector], and maximum explainable variance [the proportion of variance in contextualized representations for a given layer that can be explained by the first principal component].", "snippet_surface": "The authors measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "questions": {"1909.00512.1.1.1.tB+UNCmGFW": "What is self-similarity?", "1909.00512.1.1.1.CgmjQ6z7MW": "What is intra-sentence similarity?", "1909.00512.1.1.1.ULomci+gfl": "What is maximum explainable variance?"}}
{"idx": "1909.07575.1.2.1", "paper_id": "1909.07575", "title": "Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation", "abstract": "End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model outperforms baselines 2.2 BLEU on a large benchmark dataset.", "context_section_header": "Experiments ::: Baseline Models and Implementation", "context_paragraph": "We compare our method with following baselines.", "sentence": "We compare our method with following baselines.", "cited_ids": [], "y": "The authors compare their method with the baselines [Vanilla Speech Translation, Pre-training, and multi-task]", "snippet_surface": "The authors compare their method with the following baselines.", "questions": {"1909.07575.1.2.1.noJzJpmyQ/": "No questions."}}
{"idx": "1909.08089.1.1.1", "paper_id": "1909.08089", "title": "Extractive Summarization of Long Documents by Combining Global and Local Context", "abstract": "In this paper, we propose a novel neural single document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.", "context_section_header": "Results and Analysis", "context_paragraph": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate randomization as the statistical significance test method BIBREF32 with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries.", "sentence": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively.", "cited_ids": [], "y": "The performance of all models [in the cited previous work in extractive summarisation] on arXiv and Pubmed is shown in Tables TABREF28 and TABREF29, respectively.", "snippet_surface": "The performance of all models on arXiv and Pubmed is shown in Tables TABREF28 and TABREF29, respectively.", "questions": {"1909.08089.1.1.1.6MmeKAR33t": "What are the models being compared?", "1909.08089.1.1.1.9T4oO363u3": "What is the performance being measured?", "1909.08089.1.1.1.AL6tmNckYE": "How are the results displayed in Table TABREF28 and Table TABREF29?"}}
{"idx": "1909.09067.1.1.1", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Introduction", "context_paragraph": "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).", "sentence": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "cited_ids": [], "y": "The focus of this work is on representing information that is valuable for tasks [that around information that is not inherently linguistic (e.g., text structure, typography, position, or dimensions)] but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "snippet_surface": "The focus of this work is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "questions": {"1909.09067.1.1.1.ABgyinQUV4": "What tasks are addressed in this publication?", "1909.09067.1.1.1.MkmFiFxro8": "What language does the machine learning approach center around?", "1909.09067.1.1.1.FFcpu2GwEd": "What typography and image information is considered?"}}
{"idx": "1909.09067.1.1.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "sentence": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "cited_ids": [], "y": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element [consisting on information about text structure] in the textstructure layer [which contains information about physical pages including paragraphs and lines].", "snippet_surface": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer.", "questions": {"1909.09067.1.1.2.noJzJpmyQ/": "No questions."}}
{"idx": "1909.09067.2.1.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "sentence": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "cited_ids": [], "y": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes of the token layer [which contained information about the tokens and the linking to other annotation layers].", "snippet_surface": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes of the token elements of the tokens layer.", "questions": {"1909.09067.2.1.2.tUCN7yRCRC": "What is the \"tokens layer\"?", "1909.09067.2.1.2.UttnxAuTlj": "What are the attributes specified for token elements?", "1909.09067.2.1.2.hbTr3PWhF3": "What is an example of the token elements?"}}
{"idx": "1909.09067.2.2.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "sentence": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "cited_ids": [], "y": "A separate fonts layer [for preserving information about font configurations referenced in the tokens layer] was introduced to preserve detailed information on the font configurations referenced in the tokens layer.", "snippet_surface": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer.", "questions": {"1909.09067.2.2.2.noJzJpmyQ/": "No questions."}}
{"idx": "1909.09551.1.1.1", "paper_id": "1909.09551", "title": "Natural Language Processing via LDA Topic Model in Recommendation Systems", "abstract": "Today, Internet is one of the widest available media worldwide. Recommendation systems are increasingly being used in various applications such as movie recommendation, mobile recommendation, article recommendation and etc. Collaborative Filtering (CF) and Content-Based (CB) are Well-known techniques for building recommendation systems. Topic modeling based on LDA, is a powerful technique for semantic mining and perform topic extraction. In the past few years, many articles have been published based on LDA technique for building recommendation systems. In this paper, we present taxonomy of recommendation systems and applications based on LDA. In addition, we utilize LDA and Gibbs sampling algorithms to evaluate ISWC and WWW conference publications in computer science. Our study suggest that the recommendation systems based on LDA could be effective in building smart recommendation system in online communities.", "context_section_header": "Discussion, Open Issues and Future Directions", "context_paragraph": "In this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. We succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field. According to our studies, some issues require further research, which can be very effective and attractive for the future.", "sentence": "In this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. We succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field.", "cited_ids": [], "y": "The authors focused on the LDA approaches to recommendation systems and given the importance of research, they have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. They evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. They succeeded in discovering the relationship between LDA topics and paper features [titles and abstracts of the papers] and also obtained the researchers' interest in research field.", "snippet_surface": "The authors focused on the LDA approaches to recommendation systems and given the importance of research, studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. They evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. They succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field.", "questions": {"1909.09551.1.1.1.P+kcwUHBbp": "What is the Gibbs sampling algorithm?", "1909.09551.1.1.1.Ul3qcZste1": "What are the paper features?", "1909.09551.1.1.1.bfj8/oEl7K": "What are the researchers' interests?"}}
{"idx": "1909.11687.1.1.1", "paper_id": "1909.11687", "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections", "abstract": "Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT_BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.", "context_section_header": "Introduction", "context_paragraph": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "sentence": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "cited_ids": [], "y": "A significant bottleneck [where a large proportion of the model's memory is used] that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix [an embedding matrix consists of teacher and student WordPiece's which can be used as context to predict a word tokenised by either student/teacher vocabulary], often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over 21% of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these since they require the student and teacher models to share the same vocabulary and output space, which profoundly limits their potential to further reduce model sizes.", "snippet_surface": "A significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these since they require the student and teacher models to share the same vocabulary and output space, which profoundly limits their potential to further reduce model sizes.", "questions": {"1909.11687.1.1.1.Jhf+DE2ldf": "What is the BERTBASE model?", "1909.11687.1.1.1.fyrWGS3FXO": "What is WordPiece tokens?", "1909.11687.1.1.1.GdiV5/LIh9": "What is distillation techniques?"}}
{"idx": "1909.13104.3.1.1", "paper_id": "1909.13104", "title": "Attention-based method for categorizing different types of online harassment language", "abstract": "In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter, because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual or racist harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. Previous studies have been focused on collecting data about sexism and racism in very broad terms. However, there is not much study focusing on different types of online harassment alone attracting natural language processing techniques. In this work, we present an attention-based approach for the detection of harassment in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classification specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach.", "context_section_header": "Experiments ::: Evaluation and Results", "context_paragraph": "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "sentence": "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "cited_ids": [], "y": "The authors have [compared eight models which vary in terms of their attention-based approach. The models include: LateStateRNN, AvgRNN, Attention RNN, and MultiAttentionRNN--which includes four additional attention heads] and evaluated their models using the F1 Score, which is the harmonic mean of precision and recall. They have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro, the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "snippet_surface": "The authors have evaluated their models considering the F1 Score, which is the harmonic mean of precision and recall. They have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro, the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "questions": {"1909.13104.3.1.1.or20NmdjGb": "What is the F1 Score?", "1909.13104.3.1.1.dw9lKqw0JR": "What is the F1 Macro?", "1909.13104.3.1.1.mJs9IgkFfd": "What is the multi-attention mechanism?"}}
{"idx": "1909.13104.7.1.1", "paper_id": "1909.13104", "title": "Attention-based method for categorizing different types of online harassment language", "abstract": "In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter, because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual or racist harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. Previous studies have been focused on collecting data about sexism and racism in very broad terms. However, there is not much study focusing on different types of online harassment alone attracting natural language processing techniques. In this work, we present an attention-based approach for the detection of harassment in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classification specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach.", "context_section_header": "Experiments ::: Evaluation and Results", "context_paragraph": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "sentence": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9.", "cited_ids": [], "y": "[For the task of classifying tweets as harassing,] LastStateRNN is presented by the authors as the classic RNN model, in which the last state is passed through an MLP [that obtains the state, $h_{*}$, as a result of the passing of the weighted sum ($h_{sum}$)]. Then the LR Layer evaluates the corresponding probability. In contrast, in the AvgRNN model the authors examine the average vector of all states that come out of the [GRU] cells. The AttentionRNN model is the one provided in BIBREF9.", "snippet_surface": "The authors present LastStateRNN as the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model the authors consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9.", "questions": {"1909.13104.7.1.1.VJW7kpWYk1": "What is LastStateRNN?", "1909.13104.7.1.1.vT5xJF1VGk": "What is AvgRNN?", "1909.13104.7.1.1.84MlaxZ4N1": "What is AttentionRNN?"}}
{"idx": "1910.01863.1.1.1", "paper_id": "1910.01863", "title": "Template-free Data-to-Text Generation of Finnish Sports News", "abstract": "News articles such as sports game reports are often thought to closely follow the underlying game statistics, but in practice they contain a notable amount of background knowledge, interpretation, insight into the game, and quotes that are not present in the official statistics. This poses a challenge for automated data-to-text news generation with real-world news corpora as training data. We report on the development of a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product. The new dataset and system source code are available for research purposes at this https URL.", "context_section_header": "Text Generation ::: Baseline Experiments on the E2E Dataset", "context_paragraph": "In Table TABREF15 we measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers. Our generation system is compared to the official shared task baseline system, TGen BIBREF24, as well as to the top performing participant system on each score (ST top). Our system outperforms the TGen baseline on 3 out of 5 metrics (BLEU, METEOR and ROUGE-L), which is on par with the official shared task results, where not a single one participant system was able to surpass the baseline on all five metrics. On two metrics, BLEU and METEOR, our system outperforms the best shared task participants.", "sentence": "In Table TABREF15 we measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers.", "cited_ids": [], "y": "The authors measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data [(i.e., the published benchmark of end-to-end language generation in spoken dialogue systems)] using the evaluation script provided by the organizers.", "snippet_surface": "The authors measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers.", "questions": {"1910.01863.1.1.1.DLI1rlSrHi": "What is BLEU?", "1910.01863.1.1.1.vCxq/AKRTl": "What is NIST?", "1910.01863.1.1.1.ZiBkxvKSHu": "What is METEOR?", "1910.01863.1.1.1.hNWcRcub5V": "What is ROUGE-L?", "1910.01863.1.1.1.yqtmxRX3Fq": "What is CIDEr?"}}
{"idx": "1910.06592.1.1.1", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Experiments and Results", "context_paragraph": "Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "sentence": "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1.", "cited_ids": [], "y": "The authors build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts [which have been labelled as containing one of the main fake news types (clickbait, propaganda, satire and hoax)], they rely on a list of 180 Twitter accounts from BIBREF1.", "snippet_surface": "The authors build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, they rely on a list of 180 Twitter accounts from BIBREF1.", "questions": {"1910.06592.1.1.1.Yu+ecSBZnI": "What are the two lists annotated in previous works?", "1910.06592.1.1.1.V5/05I7uCb": "What is the list of 180 Twitter accounts from BIBREF1?"}}
{"idx": "1910.06592.11.2.2", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Experiments and Results", "context_paragraph": "LR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here, we do not aggregate over tweets and thus view each tweet independently.", "sentence": "LR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier.", "cited_ids": [], "y": "The authors extract all the features from each tweet [collected from the factual accounts being studied] and feed them into a LR classifier.", "snippet_surface": "The authors extract all the features from each tweet and feed them into a LR classifier.", "questions": {"1910.06592.11.2.2.M/6ZLgXtYG": "What features are extracted from each tweet?", "1910.06592.11.2.2.kOGvqUFs+A": "What is a LR classifier?"}}
{"idx": "1910.06592.4.1.1", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Methodology", "context_paragraph": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", "sentence": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "cited_ids": [], "y": "Given a news Twitter account, the authors read its tweets. They then sort the tweets by the posting date ascending and split them into $N$ [approximately equally size] chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "snippet_surface": "The authors read the tweets from the timeline of a given news Twitter account. Then, they sorted the tweets by the posting date in ascending way and split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "questions": {"1910.06592.4.1.1.rjKYUuLJe2": "What is the purpose of splitting the tweets into chunks?", "1910.06592.4.1.1.pgkBlP48s8": "How is each chunk labeled?", "1910.06592.4.1.1.yvkbVWO/OQ": "What is the meaning of '$N$' in this context?"}}
{"idx": "1910.09982.2.1.1", "paper_id": "1910.09982", "title": "Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection", "abstract": "We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at this http URL.", "context_section_header": "Data", "context_paragraph": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators. More details about the data collection and the annotation, as well as statistics about the corpus can be found in BIBREF10, where an earlier version of the corpus is described, which includes 450 news articles. We further annotated 47 additional articles for the purpose of the shared task using the same protocol and the same annotators.", "sentence": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "cited_ids": [], "y": "The input for [the Fragment level classification task, FLC, and the Sentence level classification task, SLC,] consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "snippet_surface": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "questions": {"1910.09982.2.1.1.vd4R/Rfyon": "What are the 36 propagandist and 12 non-propagandist news outlets?", "1910.09982.2.1.1.Sj8tAxVyTy": "How were the news articles annotated?"}}
{"idx": "1911.03059.1.1.1", "paper_id": "1911.03059", "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification", "abstract": "QA classification system maps questions asked by humans to an appropriate answer category. A sound question classification (QC) system model is the pre-requisite of a sound QA system. This work demonstrates phases of assembling a QA type classification model. We present a comprehensive comparison (performance and computational complexity) among some machine learning based approaches used in QC for Bengali language.", "context_section_header": "Question Collection and Categories", "context_paragraph": "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "sentence": "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "cited_ids": [], "y": "The authors have collected a total of 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. [The questions were classified into a two-layer taxonomy with six coarse classes and fifty finer classes.] The corpus contains the questions and the classes each question belongs to.", "snippet_surface": "The authors have collected a total of 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "questions": {"1911.03059.1.1.1.nZb1RhBkvy": "What sources were used to collect the questions?", "1911.03059.1.1.1.HlsOUqQ1cC": "What classes do the questions belong to?", "1911.03059.1.1.1.RMQfebRjsq": "How many questions were collected?"}}
{"idx": "1911.03343.1.1.1", "paper_id": "1911.03343", "title": "Negated LAMA: Birds cannot fly", "abstract": "Pretrained language models have achieved remarkable improvements in a broad range of natural language processing tasks, including question answering (QA). To analyze pretrained language model performance on QA, we extend the LAMA (Petroni et al., 2019) evaluation framework by a component that is focused on negation. We find that pretrained language models are equally prone to generate facts (\"birds can fly\") and their negation (\"birds cannot fly\"). This casts doubt on the claim that pretrained language models have adequately learned factual knowledge.", "context_section_header": "Introduction", "context_paragraph": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d). In our experiments, we query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions in terms of rank correlation and overlap of top predictions. We find that the predicted filler words often have high overlap. Thus, negating a cloze statement does not change the predictions in many cases \u2013 but of course it should as our example \u201cbirds can fly\u201d vs. \u201cbirds cannot fly\u201d shows. We identify and analyze a subset of cloze statements where predictions are different. We find that BERT handles negation best among pretrained language models, but it still fails badly on most negated statements.", "sentence": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "cited_ids": [], "y": "The authors analyze the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, they introduce the negated LAMA [LAnguage Model Analysis] dataset. They construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statements (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "snippet_surface": "The authors analyze the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, they introduce the negated LAMA dataset. They construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statements (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "questions": {"1911.03343.1.1.1.SZYEroVEnC": "What is LAMA?", "1911.03343.1.1.1.9RupgFHMIb": "What is LAMA cloze statement?", "1911.03343.1.1.1.B40LauZ6+A": "What is the negated LAMA dataset?"}}
{"idx": "1911.06964.1.1.1", "paper_id": "1911.06964", "title": "Learning Autocomplete Systems as a Communication Game", "abstract": "We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (interpretability). We propose an unsupervised approach which tackles all three desiderata by constraining the communication scheme to keywords extracted from a source sentence for interpretability and optimizing the efficiency-accuracy tradeoff. Our experiments show that this approach results in an autocomplete system that is 52% more accurate at a given efficiency level compared to baselines, is robust to user variations, and saves time by nearly 50% compared to typing full sentences.", "context_section_header": "Experiments", "context_paragraph": "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "sentence": "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "cited_ids": [], "y": "The authors evaluate their approach [learning encoder-decoder pairs] by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6. They quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "snippet_surface": "The authors evaluate their approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). They quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "questions": {"1911.06964.1.1.1.xmax0rOaUp": "What is the retention rate of tokens?", "1911.06964.1.1.1.JI6WSAKDpb": "How is accuracy measured?", "1911.06964.1.1.1.Z7w4Z88Qf1": "What does greedily decoding the model mean?"}}
{"idx": "1911.07555.1.2.1", "paper_id": "1911.07555", "title": "Short Text Language Identification for Under Resourced Languages", "abstract": "The paper presents a hierarchical naive Bayesian and lexicon based classifier for short text language identification (LID) useful for under resourced languages. The algorithm is evaluated on short pieces of text for the 11 official South African languages some of which are similar languages. The algorithm is compared to recent approaches using test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) shared tasks' datasets. Remaining research opportunities and pressing concerns in evaluating and comparing LID approaches are also discussed.", "context_section_header": "Related Works", "context_paragraph": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "sentence": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "cited_ids": [], "y": "Existing NLP datasets, models and services are available for South African languages. These include a [language identification (LID) algorithm that identifies the language a particular document is written in] and uses a character level n-gram language model. Multiple [(6)] papers have shown that 'shallow' naive Bayes classifiers and similar models work very well for doing LID. The DSL 2017 paper gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach used an SVM with character n-gram, parts of speech tag features and some other [features beyond character n-grams and parts of speech tags]. The winning approach for", "snippet_surface": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1 gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for", "questions": {"1911.07555.1.2.1.XE4ozP8BSm": "What are South African languages?", "1911.07555.1.2.1.NoRzpvf17K": "What is LID?", "1911.07555.1.2.1.yLYKlN7pM1": "What is the DSL 2017 paper?"}}
{"idx": "1911.07555.1.2.3", "paper_id": "1911.07555", "title": "Short Text Language Identification for Under Resourced Languages", "abstract": "The paper presents a hierarchical naive Bayesian and lexicon based classifier for short text language identification (LID) useful for under resourced languages. The algorithm is evaluated on short pieces of text for the 11 official South African languages some of which are similar languages. The algorithm is compared to recent approaches using test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) shared tasks' datasets. Remaining research opportunities and pressing concerns in evaluating and comparing LID approaches are also discussed.", "context_section_header": "Related Works", "context_paragraph": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", "sentence": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task.", "cited_ids": [], "y": "Researchers have investigated deeper LID [language identification] models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% accuracy in the DSL 2015 shared task.", "snippet_surface": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% accuracy in the DSL 2015 shared task.", "questions": {"1911.07555.1.2.3.JALLFtgkK7": "What is a deeper LID model?", "1911.07555.1.2.3.9e1It/fzqK": "What is the DSL 2015 shared task?", "1911.07555.1.2.3.9St530qUOV": "What is the reported accuracy of the ensembles of recurrent neural networks?"}}
{"idx": "1912.00667.2.2.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Experiments and Results ::: Experimental Setup", "context_paragraph": "Comparison Methods. To demonstrate the generality of our approach on different event detection models, we consider Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models. As the goal of our experiments is to demonstrate the effectiveness of our approach as a new model training technique, we use these widely used models. Also, we note that in our case other neural network models with more complex network architectures for event detection, such as the bi-directional LSTM BIBREF17, turn out to be less effective than a simple feedforward network. For both LR and MLP, we evaluate our proposed human-AI loop approach for keyword discovery and expectation estimation by comparing against the weakly supervised learning method proposed by BIBREF1 (BIBREF1) and BIBREF17 (BIBREF17) where only one initial keyword is used with an expectation estimated by an individual expert.", "sentence": "To demonstrate the generality of our approach on different event detection models, we consider Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "cited_ids": [], "y": "The authors demonstrate the generality of a [ human-AI loop] approach [to discover informative keywords and estimate their expectations ] by considering Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "snippet_surface": "The authors demonstrate the generality of their approach on different event detection models by considering Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "questions": {"1912.00667.2.2.1.Fwwbi1iDNK": "What is Logistic Regression (LR)?", "1912.00667.2.2.1.3hOmmIw/8o": "What is Multilayer Perceptron (MLP)?", "1912.00667.2.2.1.9FdkPszaw7": "How do LR and MLP relate to the target models?"}}
{"idx": "1912.00667.4.1.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Related Work", "context_paragraph": "Human-in-the-Loop Approaches. Our work extends weakly supervised learning methods by involving humans in the loop BIBREF13. Existing human-in-the-loop approaches mainly leverage crowds to label individual data instances BIBREF9, BIBREF10 or to debug the training data BIBREF30, BIBREF31 or components BIBREF32, BIBREF33, BIBREF34 of a machine learning system. Unlike these works, we leverage crowd workers to label sampled microposts in order to obtain keyword-specific expectations, which can then be generalized to help classify microposts containing the same keyword, thus amplifying the utility of the crowd. Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability. In contrast, our approach directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model.", "sentence": "Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "cited_ids": [], "y": "The authors' work [presents a new approach to event detection on microblogging platform using a human-AI loop to find and evaluate keywords.] This work is further connected to the topic of interpretability and transparency of machine learning models for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "snippet_surface": "The authors' work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "questions": {"1912.00667.4.1.1.Z27YMHfUGS": "What is the topic of interpretability and transparency of machine learning models?", "1912.00667.4.1.1.E6jMDlr+AC": "How are humans involved in post-hoc evaluations?", "1912.00667.4.1.1.2R3PIPKyme": "What are post-hoc evaluations?"}}
{"idx": "1912.00667.5.2.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Experiments and Results ::: Experimental Setup", "context_paragraph": "Evaluation. Following BIBREF1 (BIBREF1) and BIBREF3 (BIBREF3), we use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of our proposed approach. We note that due to the imbalance in our datasets (20% positive microposts in CyberAttack and 27% in PoliticianDeath), accuracy is dominated by negative examples; AUC, in comparison, better characterizes the discriminative power of the model.", "sentence": "Following BIBREF1 (BIBREF1) and BIBREF3 (BIBREF3), we use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of our proposed approach.", "cited_ids": [], "y": "[The authors propose a Human-AI loop to reliably discover informative keywords]. Following BIBREF1 and BIBREF3, the authors use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of their approach.", "snippet_surface": "Following BIBREF1 and BIBREF3, the authors use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of their proposed approach.", "questions": {"1912.00667.5.2.1.ZNp7VjqGPn": "What are the accuracy and AUC metrics?", "1912.00667.5.2.1.qpiwCaYw+3": "What is the proposed approach?"}}
{"idx": "1912.01214.1.1.2", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Main Results", "context_paragraph": "Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.", "sentence": "The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.", "cited_ids": [], "y": "The authors' results show that their approaches [using one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages] consistently outperform other approaches across languages and datasets. The authors surpass pivoting [i.e. translating a source language into the pivot language which is then translated to the target language], which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat.", "snippet_surface": "The authors' results show that their approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.", "questions": {"1912.01214.1.1.2.A1F6cahRNj": "What approaches are being compared?", "1912.01214.1.1.2.cCrq2w6HeC": "What is pivoting?", "1912.01214.1.1.2.UBNFnkEg/G": "What is the zero-shot scenario?"}}
{"idx": "1912.01214.2.1.2", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Main Results ::: Results on MultiUN Dataset.", "context_paragraph": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\rightarrow $ Ar and Es $\\rightarrow $ Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.", "sentence": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\rightarrow $ Ar and Es $\\rightarrow $ Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "cited_ids": [], "y": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of [the public dataset] MultiUN, MLM+BRLM-SA still achieves better performances on [Spanish (Es)] \u2192[Arabic] and Es\u2192[Russian] than strong pivoting$_{\\rm m}$, which uses MNMT [multilingual neural machine translation] to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "snippet_surface": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es\u2192Ar and Es\u2192Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "questions": {"1912.01214.2.1.2.2eLRk6aebp": "What is MultiUN?", "1912.01214.2.1.2.d/IfL77fiP": "What is MLM+BRLM-SA?", "1912.01214.2.1.2.EKenmX5jL5": "What is strong pivoting$_{\\rm m}$?"}}
{"idx": "1912.01214.4.2.1", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Setup ::: Datasets.", "context_paragraph": "The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\rightarrow $Es and De$\\rightarrow $Fr. For distant language pair Ro$\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.", "sentence": "For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\rightarrow $Es and De$\\rightarrow $Fr. For distant language pair Ro$\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets.", "cited_ids": [], "y": "For the Europarl corpus, the authors evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language [which facilitates translation between two languages that don't have a lot of training data], its left side is the source language, and its right side is the target language. They remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. They use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For the distant language pair Ro\u2192De, they extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there are no official validation and test sets.", "snippet_surface": "For the Europarl corpus, the authors evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. They remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. They use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For the distant language pair Ro\u2192De, they extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there are no official validation and test sets.", "questions": {"1912.01214.4.2.1.3T9FLjVB+l": "What is the Europarl corpus?", "1912.01214.4.2.1.C/rW5H8KnZ": "What is the devtest2006 and test2006?", "1912.01214.4.2.1.pUNfLeYl/4": "What is the newstest2016?"}}
{"idx": "1912.03804.1.1.1", "paper_id": "1912.03804", "title": "Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "abstract": "Online propaganda is central to the recruitment strategies of extremist groups and in recent years these efforts have increasingly extended to women. To investigate ISIS' approach to targeting women in their online propaganda and uncover implications for counterterrorism, we rely on text mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar", "context_section_header": "Related Work", "context_paragraph": "With their ability to operate freely on social media now curtailed, ISIS recruiters and propagandists increased their attentiveness to another longstanding tool\u2013English language online magazines targeting western audiences. Al Hayat, the media wing of ISIS, published multiple online magazines in different languages including English. The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017. The content of these magazines provides a valuable but underutilized resource for understanding ISIS strategies and how they appeal to recruits, specifically English-speaking audiences. They also provide a way to compare ISIS' approach with other radical groups. Ingram compared Dabiq contents with Inspire (Al Qaeda publication) and suggested that Al Qaeda heavily emphasized identity-choice, while ISIS' messages were more balanced between identity-choice and rational-choice BIBREF7. In another research paper, Wignell et al. BIBREF8 compared Dabiq and Rumiah by examining their style and what both magazine messages emphasized. Despite the volume of research on these magazines, only a few researchers used lexical analysis and mostly relied on experts' opinions. BIBREF9 is one exception to this approach where they used word frequency on 11 issues of Dabiq publications and compared attributes such as anger, anxiety, power, motive, etc.", "sentence": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017.", "cited_ids": [], "y": "The English online magazine of ISIS [the Islamic State of Iraq and Syria] was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah [ISIS's online English language publication] which produced 13 English language issues through September 2017.", "snippet_surface": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web in July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017.", "questions": {"1912.03804.1.1.1.A1Xof4BzaG": "What is the dark web?", "1912.03804.1.1.1.NCgvAgfdvU": "What is ISIS?", "1912.03804.1.1.1.cQTSmuQ3zO": "What was the content of Dabiq and Rumiyah?"}}
{"idx": "1912.04961.1.1.1", "paper_id": "1912.04961", "title": "Medication Regimen Extraction From Clinical Conversations", "abstract": "Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.", "context_section_header": "Experiments ::: Model variations", "context_paragraph": "We consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "sentence": "We consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "cited_ids": [], "y": "The authors consider QA PGNet and Multi-decoder QA PGnet with lookup table embedding as baseline models and improve on the baselines with other variants [like 'Nearest number' for dosage extraction and 'Random Top 3' for frequency extraction].", "snippet_surface": "The authors consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "questions": {"1912.04961.1.1.1.kMcf/6o9Ue": "What is QA PGNet?", "1912.04961.1.1.1.O/onNID7yj": "What is Multi-decoder QA PGNet?", "1912.04961.1.1.1.ok2ehvxm6j": "What are the variations described?"}}
{"idx": "1912.06262.1.1.1", "paper_id": "1912.06262", "title": "Extracting clinical concepts from user queries", "abstract": "Clinical concept extraction often begins with clinical Named Entity Recognition (NER). Often trained on annotated clinical notes, clinical NER models tend to struggle with tagging clinical entities in user queries because of the structural differences between clinical notes and user queries. User queries, unlike clinical notes, are often ungrammatical and incoherent. In many cases, user queries are compounded of multiple clinical entities, without comma or conjunction words separating them. By using as dataset a mixture of annotated clinical notes and synthesized user queries, we adapt a clinical NER model based on the BiLSTM-CRF architecture for tagging clinical entities in user queries. Our contribution are the following: 1) We found that when trained on a mixture of synthesized user queries and clinical notes, the NER model performs better on both user queries and clinical notes. 2) We provide an end-to-end and easy-to-implement framework for clinical concept extraction from user queries.", "context_section_header": "Experiments ::: Results", "context_paragraph": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", "sentence": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", "cited_ids": [], "y": "[The authors compared two models; the hybrid and ib2b NER models. The main difference between models is their drastic structural differences]. With the [best hyperparameter setting after tuning], the hybrid NER model achieved a F1 score of 0.995 on synthesized queries and 0.948 on clinical notes while the i2b2 NER model achieved a F1 score of 0.441 on synthesized queries and 0.927 on clinical notes.", "snippet_surface": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes.", "questions": {"1912.06262.1.1.1.aViot9UN2o": "What are the hyperparameter settings?", "1912.06262.1.1.1.lI0vSO0GNA": "What is a F1 score?", "1912.06262.1.1.1.me9mKfF/R+": "What are synthesized queries?"}}
{"idx": "1912.08904.2.2.1", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Introduction", "context_paragraph": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data. Each interaction in Macaw (from both user and system) is a Message object, thus a conversation is a list of Messages. Macaw consists of multiple actions, each action is a module that can satisfy the information needs of users for some requests. For example, search and question answering can be two actions in Macaw. Even multiple search algorithms can be also seen as multiple actions. Each action can produce multiple outputs (e.g., multiple retrieved documents). For every user interaction, Macaw runs all actions in parallel. The actions' outputs produced within a predefined time interval (i.e., an interaction timeout constant) are then post-processed. Macaw can choose one or combine multiple of these outputs and prepare an output Message object as the user's response.", "sentence": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "cited_ids": [], "y": "The authors designed Macaw based on a modular architecture [composed of different parts] to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "snippet_surface": "The authors designed Macaw based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "questions": {"1912.08904.2.2.1.nB8aVDij/J": "What are the components of the Macaw modular architecture?", "1912.08904.2.2.1.PZTqSbWZoc": "How does Macaw support different information seeking tasks?", "1912.08904.2.2.1.2bay3MmBvP": "What types of data does Macaw support?"}}
{"idx": "1912.08904.3.2.1", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Macaw Architecture", "context_paragraph": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "sentence": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "cited_ids": [], "y": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. Using Macaw the [information] seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS [conversational information seeking] system and thus is useful for collecting high-quality data from real users for CIS research.", "snippet_surface": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The authors present the architecture of Macaw for such setup in FIGREF16. As shown, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "questions": {"1912.08904.3.2.1.TSnlTSy2IZ": "What is the architecture of Macaw for Wizard of Oz studies?", "1912.08904.3.2.1.VhN587pcf7": "What type of interactions does Macaw support?", "1912.08904.3.2.1.i6AlT9deZt": "What kind of data is collected for CIS research?"}}
{"idx": "1912.08904.6.1.2", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Retrieval and Question Answering in Macaw", "context_paragraph": "Query Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.", "sentence": "Query Generation: This component generates a query based on the past user-system interactions.", "cited_ids": [], "y": "The authors' query generation component [of Macaw, an open-source framework with modular architecture for conversational information seeking], generates a query based on the past user-system interactions.", "snippet_surface": "The authors' query generation component generates a query based on the past user-system interactions.", "questions": {"1912.08904.6.1.2.noJzJpmyQ/": "No questions."}}
{"idx": "1912.08960.1.1.1", "paper_id": "1912.08960", "title": "Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "abstract": "Image captioning as a multimodal task has drawn much interest in recent years. However, evaluation for this task remains a challenging problem. Existing evaluation metrics focus on surface similarity between a candidate caption and a set of reference captions, and do not check the actual relation between a caption and the underlying visual content. We introduce a new diagnostic evaluation framework for the task of image captioning, with the goal of directly assessing models for grammaticality, truthfulness and diversity (GTD) of generated captions. We demonstrate the potential of our evaluation framework by evaluating existing image captioning models on a wide ranging set of synthetic datasets that we construct for diagnostic evaluation. We empirically show how the GTD evaluation framework, in combination with diagnostic datasets, can provide insights into model capabilities and limitations to supplement standard evaluations.", "context_section_header": "Introduction", "context_paragraph": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. We empirically demonstrate that the existing metrics BLEU and SPICE do not capture true caption-image agreement in all scenarios, while the GTD framework allows a fine-grained investigation of how well existing models cope with varied visual situations and linguistic constructions.", "sentence": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "cited_ids": [], "y": "The authors conducted practical evaluation of GTD [Gramaticality, truthfulness and diversity] on synthetic data [constructed by the authors themselves]. They constructed a range of datasets designed for image captioning evaluation, called ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). They illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "snippet_surface": "The authors conducted practical evaluation of GTD on synthetic data. They constructed a range of datasets designed for image captioning evaluation, called ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). They illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "questions": {"1912.08960.1.1.1.uJ/V4G9LJz": "What is GTD?", "1912.08960.1.1.1.fOqjn2HS7x": "What is ShapeWorldICE?", "1912.08960.1.1.1.2CFFnD1H8z": "How is ShapeWorldICE used to evaluate image captioning models?"}}
{"idx": "191502", "paper_id": "258794", "title": "From Natural Language Specifications to Program Input Parsers", "abstract": "We present a method for automatically generating input parsers from English specifications of input file formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then translated into a C++ input parser. We model the problem as a joint dependency parsing and semantic role labeling task. Our method is based on two sources of information: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples. Our results show that our approach achieves 80.0% F-Score accuracy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format specifications from the ACM International Collegiate Programming Contest (which were written in English for humans with no intention of providing support for automated processing). 1", "context_section_header": "", "context_paragraph": "We evaluate our method on a dataset of input specifications from ACM International Collegiate Programming Contests, along with the corresponding input examples. These specifications were written for human programmers with no intention of providing support for automated processing. However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%). The strength of our model in the face of such weak supervision is also highlighted by the fact that it retains an F-Score of 77% even when only one input example is provided for each input Your program is supposed to read the input from the standard input and write its output to the standard output. The first line of the input contains one integer N. N lines follow, the i-th of them contains two real numbers Xi, Yi separated by a single space -the coordinates of the i-th house. Each of the following lines contains four real numbers separated by a single space. These numbers are the coordinates of two different points (X1, Y1) and (X2, Y2), lying on the highway.  Figure 3: An example of generating input parser code from text: (a) a natural language input specification; (b) a specification tree representing the input format structure (we omit the background phrases in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints.", "sentence": "However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "cited_ids": [{"paper_id": "5667590", "citation": "(Clarke et al., 2010)"}], "y": "However, when trained using the noisy supervision, the authors' method [based on joint probabilistic approach using a Bayesian generative model] achieves substantially more accurate translations than a state-of-the-art semantic parser [which uses a model that relies on human supervision] (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "snippet_surface": "However, when trained using the noisy supervision, the authors' method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "questions": {"191502.g4fZra3mrB": "What is a semantic parser?", "191502.i8W7Y4ofwT": "What is the F-Score?", "191502.+BltuwfJMQ": "What is the difference between noisy supervision and other forms of supervision?"}}
{"idx": "191560", "paper_id": "236486232", "title": "Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers", "abstract": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.", "context_section_header": "", "context_paragraph": "Offensive Language Detection. Offensive language detection (OLD) has become an active research topic in recent years (Araujo De Souza and Da Costa Abreu, 2020). Nikolov and Radivchev (2019) experimented with a variety of models and observe promising results with BERT and SVC based models. Han et al. (2019) employed a GRU based RNN with 100 dimensional glove word embeddings (Pennington et al., 2014). Additionally, they develop a Modified Sentence Offensiveness Calculation (MSOC) model which makes use of a dictionary of offensive words. Liu et al. (2019) evaluated three models on the OLID dataset, including logistic regression, LSTM and BERT, and results show that BERT achieves the best performance. The concept of transfer learning mentioned in (Liu et al., 2019) is closely related to our work, since the BERT model is also pretrained on external text corpus. However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we propose a new training strategy for domain adaptation.", "sentence": "However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we propose a new training strategy for domain adaptation.", "cited_ids": [{"paper_id": "184482578", "citation": "(Liu et al., 2019)"}], "y": "Different from (Liu et al., 2019), the authors approach [training using the ALBERT loss function for detection of offensive language] exploits external data [by training with different domains of of offensive language] that are closely related to the OLD [Offensive language detection] task, and we propose a new training strategy for domain adaptation [including a predictive layer that should help in aligning the two domains].", "snippet_surface": "However, different from (Liu et al., 2019), the authors' approach exploits external data that are closely related to the OLD task, and they propose a new training strategy for domain adaptation.", "questions": {"191560.KBroXU+7R2": "What is \"the OLD task\"?", "191560.PPvJYpANud": "What is the new training strategy for domain adaptation?"}}
{"idx": "194176", "paper_id": "33671165", "title": "Supervised Attention for Sequence-to-Sequence Constituency Parsing", "abstract": "The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.", "context_section_header": "", "context_paragraph": "In this paper, we define several linguisticallymotivated annotations between surface words and nonterminals as \"gold standard alignments\" to enhance the attention mechanism of the constituency parser (Vinyals et al., 2015) by supervised attention. The PTB corpus results showed that our method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "sentence": "The PTB corpus results showed that our method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "cited_ids": [{"paper_id": "14223", "citation": "Vinyals et al. (2015)"}], "y": "The [English Penn Treebank] (PTB) corpus results showed that the authors' method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure. [Their approach defined numerous linguistically motivated annotations between surface words and non-terminals as \"gold standard alignments\" in order to enhance the attention mechanism of the constituency parser via supervised attention--where attentions are learned from given alignments.]", "snippet_surface": "The PTB corpus results showed that the authors' method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "questions": {"194176.bZtuRk81Gm": "What is the PTB corpus?", "194176.SEodeY1eAX": "What is the bracketing F-measure?"}}
{"idx": "195310", "paper_id": "202783441", "title": "Identifying Predictive Causal Factors from News Streams", "abstract": "We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years. We then manually validated PCG, finding that 67% of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.", "context_section_header": "", "context_paragraph": "We constructed PCG from news streams of around 700, 000 articles from Google News API and New York Times spread across over 6 years and evaluated it to extract features for stock price predictions. We obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017). The longitudinal PCG provided insights into the variation in importance of the predictive causal factors over time, while consistently maintaining a low prediction error rate between 1.5-5% in predicting 10 stock prices. Using full text of more than 1.5 million articles of Times of India news archives for over 10 years, we performed a fine-grained qualitative analysis of PCG and validated that 67% of the semantic causation arguments found in the news text is connected by a direct edge in PCG while the rest were linked by a path of length 2. In summary, PCG provides a powerful framework for identifying predictive causal factors from news streams to accurately predict and interpret price fluctuations.", "sentence": "We obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017).", "cited_ids": [{"paper_id": "793863", "citation": "(Kang et al., 2017)"}], "y": "The authors obtained two orders lower prediction error compared to a similar semantic causal graph-based method [that uses news outlet information to predict stock price evolution] (Kang et al., 2017).", "snippet_surface": "The authors obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017).", "questions": {"195310.wcCI8zn+ce": "What is the similar semantic causal graph-based method?", "195310.RHOJsH+76C": "What is the prediction error?"}}
{"idx": "195919", "paper_id": "218974038", "title": "Evaluating Sentence Segmentation in Different Datasets of Neuropsychological Language Tests in Brazilian Portuguese", "abstract": "Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain: the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of parsers used in metrics, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives \u2014 visual and oral \u2014 via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture: (i) the inclusion of a Linear Chain CRF; (ii) the inclusion of a self-attention mechanism; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set.", "context_section_header": "", "context_paragraph": "Since the majority of studies on diagnosing cognitive impairments by NLP methods deal with English-speaking patients (Filiou et al., 2019), in this study we will evaluate the Brazilian Portuguese (BP) language in order to contribute with datasets and studies to develop automatic analysis of connected speech in BP. Our motivation for this study was to explore the performance of a single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets. Therefore, here, we evaluate four datasets used to diagnose cognitive impairments (see Section 3), comprising different stories and two type of stimulus presentation for eliciting narratives: (i) oral stimuli presentation with retelling, where sequencing discourse marks, such as \"e\",\"a\u00ed\", \"da\u00ed\" and \"ent\u00e3o\" (and, then, in English) and confirmatory discourse marks \"n\u00e9\" and \"ok\" (ok, in English) are frequent and (ii) visual stimuli, via both illustrated story book and sequence of scenes of a common event, where deictic expressions (place deixis) are pervasive, such as \"aqui\" and \"a\u00ed\" (here) and \"ali\" and \"l\u00e1\" (there), besides presenting sequencing discourse marks and confirmatory discourse. Figure 1 shows the result of a manual transcription in BP of a narrative of the ABCD story telling task, which presents a story about a woman who is unaware of having lost her wallet while doing the shopping; she then receives a call from a little girl who found the wallet. As we can see in (a) the transcript without punctuation prevents the direct application of NLP methods that rely on sentence segmentation for the correct use of tools as taggers and parsers. These tools are used to implement metrics of syntactic complexity, basic counts of PoS tags and to analyze other levels of language to diagnose cognitive impairments. When the architecture developed in the project Deep-BonDD 1 was trained with The Cinderella Story dataset (a production task elicited via an illustrated story book) and 1 https://github.com/mtreviso/deepbond (a) ahm uma senhora foi fazer compras no me foi no mercado n\u00e3o lembrava o local no me fazer compras e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira a\u00ed ela foi deixou a mercadoria n\u00e3o levou a mercadoria voltou para casa chegando em casa toca o telefone era uma garotinha avisando ela que que tinha achado a carteira \u00e9 isso tem mais coisa n\u00e3o cortei eu resumi o que eu ouvi (b) ahm uma senhora foi fazer compras no me foi no mercado. n\u00e3o lembrava o local . no me fazer compras . e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira . a\u00ed ela foi deixou a mercadoria . n\u00e3o levou a mercadoria . voltou para casa . chegando em casa toca o telefone . era uma garotinha avisando ela que que tinha achado a carteira . \u00e9 isso . tem mais coisa . n\u00e3o cortei . eu resumi o que eu ouvi . Figure 1: (a) Narrative transcribed where there is no punctuation or capitalization, besides presenting several disfluencies, such as unlexicalized filled pauses, restarts and patient's comments, shown in bold. (b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1). The average of F 1 in the three datasets, for all classes, is 0.59. In the original evaluation with training and cross-validation testing in the same dataset, the best F 1 value for Controls was 0.76, for MCIs, 0.74, and for ADs, 0.66. However, Table 1 shows that, in general, for sentence segmentation, more data is beneficial, independently of task and topic of datasets. Given this motivation scenario, where the main goal was to develop a robust and generic segmentation system for narratives of neuropsychological language tests, the present study tries to answer three questions:", "sentence": "(b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1).", "cited_ids": [{"paper_id": "32649543", "citation": "(Treviso et al., 2017a)"}], "y": "The authors evaluated a narrative of a retelling task manually segmented using \"The Wallet Story\" [a story about a woman unaware of having lost her wallet who receives a call from a girl who found it], along with the \"Cinderella\", \"Dog Story\" and \"Lucia Story\" datasets. F 1 values were much lower than the work done by (Treviso et al., 2017a) [which developed a narrative segmentation system based on a Cinderella story transcribed from a picture book].", "snippet_surface": "(b) Narrative manually segmented of a retelling task using The Wallet Story was evaluated with the other three datasets analyzed in this paper. F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1).", "questions": {"195919.0DdXKb376T": "What is The Wallet Story?", "195919.sx8ALcxYPP": "What is the original work on Cinderella?", "195919.cDiFYWvm1H": "What is the F1 metric?"}}
{"idx": "197372", "paper_id": "248693452", "title": "Extracting Latent Steering Vectors from Pretrained Language Models", "abstract": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.", "context_section_header": "", "context_paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "sentence": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "cited_ids": [{"paper_id": "4421747", "citation": "Cer et al. (2017)"}, {"paper_id": "4421747", "citation": "Cer et al. (2017)"}], "y": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), the authors' [steering vectors, that are used to generate a target sentence nearly perfectly when they are added to the hidden states of the language model], outperform extractive methods such as averaging language model. The author's approach adds a vector z to steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "snippet_surface": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), the authors' steering vectors outperform extractive methods such as averaging language model. Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "questions": {"197372.3WgDPF/shV": "What is the STS-B benchmark?", "197372.lMlVRqCkwE": "What are extractive methods?", "197372.nYMpiPReOv": "What is the transformer decoder?"}}
{"idx": "197455", "paper_id": "33889881", "title": "Collecting bilingual technical terms from patent families of character-segmented Chinese sentences and morpheme-segmented Japanese sentences", "abstract": "In manual translation of patent documents, a technical term bilingual lexicon is inevitable for a translator to efficiently translate patent documents. Dong et al. (2015) proposed a method of generating bilingual technical term lexicon from morpheme-segmented parallel patent sentences. The proposed method estimates Japanese-Chinese translation of technical terms using the phrase translation table of a statistical machine translation model. The procedure of generating bilingual technical term lexicon consists of the following four steps: (1) extracting Japanese technical terms from Japanese side of parallel patent sentences, (2) collecting all the sentences that contain the extracted Japanese term, (3) generating Chinese translation of the Japanese technical term referring to the phrase translation table of a statistical machine translation model, and (4) applying the Support Vector Machines (SVMs) to the task of identifying bilingual technical terms. In this paper, we segment the Chinese sentences into characters instead of segmenting them into morphemes as in Dong et al. (2015), and represent JapaneseChinese patent families in terms of character-segmented Chinese sentences and morphemesegmented Japanese sentences. Then, to those Japanese-Chinese patent families, we apply the framework (Dong et al., 2015) of identifying bilingual technical terms. As a result, we achieve the performance of over 90% precision with the condition of more than or equal to 60% recall.", "context_section_header": "", "context_paragraph": "Among related works on acquiring bilingual lexicon from text, Itagaki et al. (2007) focused on automatic validation of translation pairs available in the phrase translation table trained by an SMT model. Itagaki et al. (2007) especially studied to apply a Gaussian mixture model based classifier to the task of automatic validation of translation pairs available in the phrase translation table. Yasuda and Sumita (2013) also studied to extract bilingual terms from comparable patents, where, they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences. Yasuda and Sumita (2013) especially studied to exploit kanji character similarity between Japanese and Chinese languages in the task of extracting Japanese-Chinese bilingual term pairs. It is also reported that two types of SMT phrase translation tables are integrated in this task. Haque et al. (2014a) also presented a bilingual terminology extraction method using the phrase translation table trained by a phrase-based SMT. One of the major differences of our approach and those proposed in Itagaki et al. (2007) , Yasuda and Sumita (2013) and Haque et al. (2014b) is that we apply the SVM-based classifier learning framework to the task of identifying bilingual technical term pairs from parallel patent sentences, where we examine various features extracted from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Lu and Tsou (2009) also studied to extract English-Chinese bilingual terms from patent families, where they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences based on an SVM classifier. One of the major differences of our approach and that proposed in Lu and Tsou (2009) is that our features studied in this paper are much finer-grained and cover wider range of information that are available from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Morishita et al. (2008) studied to acquire Japanese-English technical term translation lexicon from the phrase translation tables, which are trained by a phrase-based SMT model with parallel sentences automatically extracted from patent families. The approach taken in Morishita et al. (2008) is based on integrating the phrase translation table and compositional translation generation based on an existing bilingual lexicon for human use. This approach is quite effective in the case of language pairs such as Japanese and English, where an existing bilingual lexicon for human use is widely available. However, this is not always the case in the case of other language pairs such as Japanese and Chinese. Compared with the approach of Morishita et al. (2008), our approach is advantageous in that we concentrate on utilizing information that are available from patent families, but not rely on information source other than patent fami-lies. Also, compared with the features of SVM examined in Morishita et al. (2008), those we employed in this paper cover much wider range of information that are available from patent families. Especially, we concentrate more on utilizing features that are based on statistics of all the parallel sentences of the patent families rather than a single parallel sentence pair. In our proposed framework, we introduce the feature of the number of Chinese translation candidates (f 7 ), which was not examined in Morishita et al. (2008). We also use the rate of parallel sentences where phrase alignment is consistent with word alignments as a feature (f 8 ), while Morishita et al. (2008) used a binary feature which judges for each parallel sentence pair whether a phrase alignment is consistent with word alignments. Finally, we use the feature of the translation probability of compositional translation generation, which, through a preliminary evaluation, is proved to perform better than the binary feature of compositional translation generation employed in Morishita et al. (2008).", "sentence": "Also, compared with the features of SVM examined in Morishita et al. (2008), those we employed in this paper cover much wider range of information that are available from patent families.", "cited_ids": [{"paper_id": "7184744", "citation": "Morishita et al. (2008)"}], "y": "Compared with the features of SVM examined in Morishita et al. (2008), the authors [employed features including term frequency, translation probability, number of translation candidates] in this paper to cover a much wider range of [parallel sentences in two different languages] than is available from patent families.", "snippet_surface": "Additionally, compared with the features of SVM examined in Morishita et al. (2008), the authors' employed features in this paper cover a much wider range of information that is available from patent families.", "questions": {"197455.Q9VCrSvDKr": "What features does SVM examine?", "197455.TP+g9g9+pO": "What information do the features employed in this paper cover?", "197455.SLPiuO8o6X": "What are patent families?"}}
{"idx": "2001.02284.1.2.1", "paper_id": "2001.02284", "title": "Multipurpose Intelligent Process Automation via Conversational Assistant", "abstract": "Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.", "context_section_header": "Model ::: Dialogue Modules", "context_paragraph": "Natural Language Understanding (NLU): We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API. The NLU module contains following functionalities:", "sentence": "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API.", "cited_ids": [], "y": "The authors implemented an NLU unit utilizing handcrafted rules [particular explicit words that are associated with organisational, contextual, or mathematical intent], Regular Expressions (RegEx) and Elasticsearch (ES) API.", "snippet_surface": "The authors implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API.", "questions": {"2001.02284.1.2.1.riYhg2zs91": "What is a Regular Expression (RegEx)?", "2001.02284.1.2.1.gKiXog1hH8": "What is Elasticsearch (ES) API?"}}
{"idx": "2002.01664.1.1.1", "paper_id": "2002.01664", "title": "Identification of Indian Languages using Ghost-VLAD pooling", "abstract": "In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features.", "context_section_header": "INTRODUCTION", "context_paragraph": "In this paper, we explore multiple pooling strategies for language identification task. Mainly we propose Ghost-VLAD based pooling method for language identification. Inspired by the recent work by W. Xie et al. [9] and Y. Zhong et al. [10], we use Ghost-VLAD to improve the accuracy of language identification task for Indian languages. We explore multiple pooling strategies including NetVLAD pooling [11], Average pooling and Statistics pooling( as proposed in X-vectors [7]) and show that Ghost-VLAD pooling is the best pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset. In section 4, we describe the experiments, and in section 5, we describe the results.", "sentence": "We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.", "cited_ids": [], "y": "The authors conducted experiments [to determine a new pooling strategy (GhostVLAD) for language identification in Indian languages] on 635hrs of audio data for 7 Indian languages collected from \"All India Radio\" news channel.", "snippet_surface": "The authors conducted all their experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.", "questions": {"2002.01664.1.1.1.yzvDWvRzCI": "What is the source of the audio data?", "2002.01664.1.1.1.zXtTGK4wxC": "How many hours of audio data is collected for each language?", "2002.01664.1.1.1.85bJ2oPgL3": "What are the 7 Indian languages?"}}
{"idx": "2002.02070.1.1.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "sentence": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "cited_ids": [], "y": "The authors train a series of classifiers in order to classify car-speak [an abstract language that relates to the physical specifications of a car]. They train three classifiers on the review vectors that they prepared in Section SECREF8. The classifiers they use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "snippet_surface": "The authors train a series of classifiers in order to classify car-speak. They train three classifiers on the review vectors that they prepared in Section SECREF8. The classifiers they use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "questions": {"2002.02070.1.1.2.o35DlXVE16": "What are review vectors?", "2002.02070.1.1.2.5PrD+oQ454": "What is K Nearest Neighbors (KNN)?", "2002.02070.1.1.2.WSSD21bYac": "What is Random Forest (RF)?"}}
{"idx": "2002.02070.1.2.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak", "context_paragraph": "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.", "sentence": "In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.", "cited_ids": [], "y": "The authors need to filter their dataset [which includes 3,209 reviews about 553 different cars from 49 car manufacturers] so that they have only the most relevant terms in order to accomplish the goal of translating and classifying car-speak [an abstract language related to the physical specifications of a car that is used by potential buyers in order to communicate their requirements]. The authors need to be able to weight each word in each review in order to determine the most relevant ideas in each document for the purpose of classification. The authors train classification models [to understand car-speak data, and aid in finding a path to automate car dealers in dealerships].", "snippet_surface": "The authors need to filter their data set so that they only have the most relevant terms in order to accomplish their goal of translating and classifying car-speak. They then need to be able to weight each word in each review, so that they can determine the most relevant ideas in each document for the purpose of classification. Finally, they need to train various classification models and evaluate them.", "questions": {"2002.02070.1.2.2.ZBZ3LNoqom": "How do we filter the data set?", "2002.02070.1.2.2.COzIxE5Zjj": "How do we weight each word in each review?", "2002.02070.1.2.2./qNwAySZIC": "What classification models are used?"}}
{"idx": "2002.02070.5.1.1", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "sentence": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "cited_ids": [], "y": "The authors evaluate [four classifiers: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron] by performing 4-fold cross validation on a shuffled data set. The work shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier appear to perform the best across all four metrics. This is likely due to the multi-class nature of the data set.", "snippet_surface": "The authors evaluate their classifiers by performing 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier appear to perform the best across all four metrics. This is likely due to the multi-class nature of the data set.", "questions": {"2002.02070.5.1.1.0j4IILWXxm": "What is 4-fold cross validation?", "2002.02070.5.1.1.9IsACOp1ZL": "What are F1 micro and F1 macro scores?", "2002.02070.5.1.1.KfFiz0hRQC": "What is the multi-class nature of the data set?"}}
{"idx": "2002.02070.6.1.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "sentence": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers.", "cited_ids": [], "y": "The authors evaluated their classifiers [K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)] by performing 4-fold cross validation on a shuffled data set.", "snippet_surface": "The authors evaluated their classifiers by performing 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers.", "questions": {"2002.02070.6.1.2.0j4IILWXxm": "What is 4-fold cross validation?", "2002.02070.6.1.2.3ogqmFKUZZ": "What is F1 micro and F1 macro?"}}
{"idx": "2002.02070.6.2.1", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "sentence": "The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "cited_ids": [], "y": "The authors use K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) [to classify car-speak].", "snippet_surface": "The authors use K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13 as classifiers.", "questions": {"2002.02070.6.2.1.zLl/ZGEhqY": "What is KNN?", "2002.02070.6.2.1.w8yZ8wI8/x": "What is RF?", "2002.02070.6.2.1.MM1+e9iHx5": "What is SVM?"}}
{"idx": "2003.03106.1.1.1", "paper_id": "2003.03106", "title": "Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "abstract": "Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.", "context_section_header": "Results ::: Experiment A: NUBes-PHI", "context_paragraph": "To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems. Despite the fact that, in general, the systems obtain high values, BERT outperforms them again. BERT's F1-score is 1.9 points higher than the next most competitive result in the comparison. More remarkably, the recall obtained by BERT is about 5 points above.", "sentence": "To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems.", "cited_ids": [], "y": "The authors finish an experiment [that uses data from NUBes-PHI, a corpus of medical reports that classifies clinical text through] strict classification precision, recall and F1-score.", "snippet_surface": "The authors finish this experiment set with a Table showing the strict classification precision, recall and F1-score for the compared systems.", "questions": {"2003.03106.1.1.1.Oo1etsYCCs": "What is the strict classification precision?", "2003.03106.1.1.1.DkK5buk2HA": "What is the recall?", "2003.03106.1.1.1.Bwsx7Pl3vT": "What is the F1-score?"}}
{"idx": "2003.08380.1.1.1", "paper_id": "2003.08380", "title": "TTTTTackling WinoGrande Schemas", "abstract": "We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the\"entailment\"token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.", "context_section_header": "Results", "context_paragraph": "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al. BIBREF7, T5's \u201cgenerative capability\u201d, i.e., its ability to generate fluent text, honed through pretraining, seems to play an important role. The fact that the choice of target tokens affects prediction accuracy is consistent with this observation. How and why is the subject of ongoing work.", "sentence": "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture.", "cited_ids": [], "y": "Regarding the WinoGrande leaderboard, the then state-of-the-art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", "snippet_surface": "Looking at the WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture.", "questions": {"2003.08380.1.1.1.UYKq1gPA9Y": "What is WinoGrande?", "2003.08380.1.1.1.6DMylS4iIM": "What is RoBERTa?", "2003.08380.1.1.1.ZLb6Y9lJac": "What is an encoder-only transformer architecture?"}}
{"idx": "2003.13032.5.1.1", "paper_id": "2003.13032", "title": "Named Entities in Medical Case Reports: Corpus and Experiments", "abstract": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "context_section_header": "A Corpus of Medical Case Reports with Medical Entity Annotation ::: Corpus Overview", "context_paragraph": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24.", "sentence": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "cited_ids": [], "y": "The authors' corpus consists of 53 documents [medical case reports with medical entity annotation which are used for annotating the document for medically relevant categories such as conditions, findings and other things], which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "snippet_surface": "The authors' corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "questions": {"2003.13032.5.1.1.noJzJpmyQ/": "No questions."}}
{"idx": "2003.13032.6.2.2", "paper_id": "2003.13032", "title": "Named Entities in Medical Case Reports: Corpus and Experiments", "abstract": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "context_section_header": "", "context_paragraph": "Baseline systems for Named Entity Recognition in medical case reports ::: BiLSTM-CRF", "sentence": "Baseline systems for Named Entity Recognition in medical case reports ::: BiLSTM-CRF", "cited_ids": [], "y": "The authors present a baseline system for Named Entity Recognition in medical case reports, BiLSTM-CRF, [which consists of a BioWordVec, followed by a bidirectional LSTM and a CRF layer].", "snippet_surface": "The authors present a baseline system for Named Entity Recognition in medical case reports ::: BiLSTM-CRF.", "questions": {"2003.13032.6.2.2.noJzJpmyQ/": "No questions."}}
{"idx": "2004.01853.1.1.1", "paper_id": "2004.01853", "title": "STEP: Sequence-to-Sequence Transformer Pre-training for Document Summarization", "abstract": "Abstractive summarization aims to rewrite a long document to its shorter form, which is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Seq2Seq Transformers are powerful models for this problem. Unfortunately, training large Seq2Seq Transformers on limited supervised summarization data is challenging. We, therefore, propose STEP (as shorthand for Sequence-to-Sequence Transformer Pre-training), which can be trained on large scale unlabeled documents. Specifically, STEP is pre-trained using three different tasks, namely sentence reordering, next sentence generation, and masked document generation. Experiments on two summarization datasets show that all three tasks can improve performance upon a heavily tuned large Seq2Seq Transformer which already includes a strong pre-trained encoder by a large margin. By using our best task to pre-train STEP, we outperform the best published abstractive model on CNN/DailyMail by 0.8 ROUGE-2 and New York Times by 2.4 ROUGE-2.", "context_section_header": "Introduction", "context_paragraph": "Based on the above observations, we propose Step (as shorthand for Sequence-to-Sequence TransformEr Pre-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences. NSG generates the next segment of a document based on its preceding segment. MDG recovers a masked document to its original form. After pre-trianing Step using the three tasks on unlabeled documents, we fine-tune it on supervised summarization datasets.", "sentence": "Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG).", "cited_ids": [], "y": "The authors create three tasks specifically for the seq2seq model pre-training [of an abstractive document summarisation model which uses an encode and decode transformer]. These tasks are sentence reordering (SR) [for texts having multiple sentences], next sentence generation (NSG) [that assumes correlated sentences], and masked document generation (MDG).", "snippet_surface": "The authors specifically design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG).", "questions": {"2004.01853.1.1.1.5N31t0T2t/": "What is seq2seq model pre-training?", "2004.01853.1.1.1.lWhD9X+IXf": "What are Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG)?"}}
{"idx": "201666", "paper_id": "235313592", "title": "Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning", "abstract": "Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction.", "context_section_header": "", "context_paragraph": "Relation Fact Extraction. In this work, we show that all of the relational fact extraction models can be unified into a graph-oriented output structure analytical framework. From the perspective of graph representation, the prior models can be divided into three categories. Edge List, this type of model usually employs sequence-to-sequence fashion, such as NovelTagging (Zheng et al., 2017), CopyRE (Zeng et al., 2018), CopyRL (Zeng et al.,    2019), and PNDec (Nayak and Ng, 2020). Some models of this category may suffer from the triplet overlapping problem and expensive extraction cost. Adjacency Matrices, many early pipeline approaches (Zelenko et al., 2003;Zhou et al., 2005;Mintz et al., 2009) and recent neural network-based models (Bekoulis et al., 2018;Dai et al., 2019;Fu et al., 2019), can be classified into this category. The main problem for this type of model is the graph representation efficiency. Adjacency List, the recent state-of-the-art model CasRel (Wei et al., 2020) is a partially adjacency list oriented model. In this work, we propose DIRECT that is a fully adjacency list oriented relational fact extraction model. To the best of our knowledge, few previous works analyze this task from the output data structure perspective. GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure. Our work is a pioneer investigation to analyze the output data structure of relational fact extraction.", "sentence": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure.", "cited_ids": [{"paper_id": "196211486", "citation": "(Fu et al., 2019)"}], "y": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while the authors analyze it from the perspective of output structure [the specific type of variable used to encode the output data, e.g., a list or a (sparse) matrix.]", "snippet_surface": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while the authors analyze it from the perspective of output structure.", "questions": {"201666.NO3ss/891u": "What is GraphRel?", "201666.MPY8kCo2Xm": "What is the difference between an encoding perspective and an output structure perspective?"}}
{"idx": "201707", "paper_id": "1582646", "title": "Stacking for Statistical Machine Translation", "abstract": "We propose the use of stacking, an ensemble learning technique, to the statistical machine translation (SMT) models. A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model.", "context_section_header": "", "context_paragraph": "Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "sentence": "Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "cited_ids": [{"paper_id": "7437692", "citation": "Duan et al. (2010)"}], "y": "The authors' approach [is the same as Duan et al.'s approach, except] in a number of ways: i) the authors use cross-validation-style partitioning for creating training subsets while Duan et al. (2010) do sampling without replacement (80% of the training set); ii) in the authors' approach a number of base models [SMT Log-linear models] are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features [(i.e., offline combinations of phrases in tables)]; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, the authors' method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) the authors' approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "snippet_surface": "The authors' approach differs from this approach in a number of ways: i) they use cross-validation-style partitioning for creating training subsets while Duan et al. (2010) do sampling without replacement (80% of the training set); ii) in their approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, the authors' method not only uses all available data for training, but promotes", "questions": {"201707.HCplTptUJX": "What is cross-validation-style partitioning?", "201707.a2cUnHnT28": "What is ensemble decoding?", "201707.uLfwNEzxZJ": "What is the difference between Duan et al.'s method and our method?"}}
{"idx": "201907", "paper_id": "202121966", "title": "Aligning Cross-Lingual Entities with Multi-Aspect Information", "abstract": "Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.", "context_section_header": "", "context_paragraph": "In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features.", "sentence": "However, in contrast to Wang et al. (2018), we regard relation features as input to our models.", "cited_ids": [{"paper_id": "53082628", "citation": "Wang et al. (2018)"}], "y": "The authors [utilize] relation features, [i.e. using the number in parentheses after entitiy names] as input to their models [two variants of GCNbased models], in contrast to Wang et al. (2018).", "snippet_surface": "However, in contrast to Wang et al. (2018), the authors regard relation features as input to their models.", "questions": {"201907.V+JUbY3hdx": "What do Wang et al. (2018) regard relation features as?", "201907.Xz6LS2ZdW6": "What is the purpose of the relation features in our models?"}}
{"idx": "201966", "paper_id": "3795383", "title": "Grammatical Error Correction Using Feature Selection and Confidence Tuning", "abstract": "This paper proposes a novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors. Most previous machine learning based researches empirically collected features which may bring about noises and increase the computational complexity. Meanwhile, the predicted result is largely affected by the threshold setting of a classifier which can easily lead to low performance but hasn\u2019t been well developed yet. To address these problems, we employ genetic algorithm for feature selection and confidence tuning to reinforce the motivation of correction. Comparative experiments on the NUCLE corpus show that our approach could efficiently reduce feature dimensionality and enhance the final F1 value for the article error correction problem.", "context_section_header": "", "context_paragraph": "In this paper, we focus on the machine learning based approach on error annotated corpus and propose a novel strategy to solve article error correction problem. Primarily, we extract a large number of related syntactic and semantic features from the context. With the help of genetic algorithm, a best feature subset is selected out which could greatly reduce the feature dimensionality. For each testing instance, according to the predicted confidence scores generated by the classifier, our tuning approach measures the trade-off between scores in order to enhance the confidence to a certain category. We didn't include any external corpora as references in our work which is to be further exploited. Experiments on NUCLE corpus show that our approach could efficiently reduce feature dimensionality and take full advantage of predicted scores generated by the classifier. The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "sentence": "The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "cited_ids": [{"paper_id": "14113283", "citation": "(Dahlmeier and Ng, 2011)"}], "y": "The evaluation result shows that the authors\u2019 [machine learning] approach outperforms the state-of-the-art work [based on Alternating Structure Optimization] (Dahlmeier and Ng, 2011) by 2.2% in F 1 on [the NUCLE corpus].", "snippet_surface": "The evaluation result shows that the authors' approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "questions": {"201966.fMfuv5Wx8h": "What is the evaluation result?", "201966.q1c3wV+ev0": "What is the state-of-the-art work?", "201966.e+GrWEMyBs": "What is F 1?"}}
{"idx": "207780", "paper_id": "198622", "title": "SuperTagging and Full Parsing", "abstract": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser.", "context_section_header": "", "context_paragraph": "We are not aware of any other work that directly investigates the extent to which supertagging determines parsing. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree). Rogers (1994) proposes a different context-free variant, \"regular-form TAG\". The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. Our conversion to FSMs is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., reconstructing the derived tree), while we encode  Figure 4: Results (accuracy) for different models using the Gold-Standard supertag on development corpus (Section 00, first 800 sentences) with add-0.001 smoothing, and for the best performing model as well as the baselines on the test corpus (Section 23) the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree (not unlike (Eisner, 2000)). As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in standard TAG parsing using FSMs.", "sentence": "Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "cited_ids": [{"paper_id": "1471139", "citation": "Chiang (2000)"}], "y": "Chiang (2000) parses with an automatically extracted TIG [Tree insertion grammar], but [while the authors use lexical information in the first phase, known as supertagging, in which lexical syntactic properties are determined without building structure], he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "snippet_surface": "Chiang (2000) also parses with an automatically extracted TIG, but unlike the authors' approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "questions": {"207780.GBRgM2bR/k": "What is TIG?", "207780.MJX1hJBGkT": "What is TAG/TIG parsing?", "207780.RkkX2cQyFo": "What is the difference between the derived tree and the derivation tree?"}}
{"idx": "208014", "paper_id": "2765046", "title": "Easy-first Coreference Resolution", "abstract": "We describe an approach to coreference resolution that relies on the intuition that easy decisions should be made early, while harder decisions should be left for later when more information is available. We are inspired by the recent success of the rule-based system of Raghunathan et al. (2010), which relies on the same intuition. Our system, however, automatically learns from training data what constitutes an easy decision. Thus, we can utilize more features, learn more precise weights, and adapt to any dataset for which training data is available. Experiments show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.\u2019s system as well as a competitive baseline that uses a pairwise classifier.", "context_section_header": "", "context_paragraph": "We propose a coreference resolution approach that like Raghunathan et al. (2010) aims to consider global consistency while performing fast and deterministic greedy search. Similar to Raghunathan et al. (2010), our algorithm operates by making the easy (most confident) decisions first. It builds up coreference clusters as it goes and uses the information from these clusters in the form of features to make later decisions. However, while Raghunathan et al. (2010) use hand-written rules for their system, we learn feature weights from training data.", "sentence": "However, while Raghunathan et al. (2010) use hand-written rules for their system, we learn feature weights from training data.", "cited_ids": [{"paper_id": "7691746", "citation": "Raghunathan et al. (2010)"}], "y": "However, while Raghunathan et al. (2010) use hand-written rules for their [coreference resolution] system, the authors learn feature weights from training data [with a method that performs supervised perceptron style updates].", "snippet_surface": "However, while Raghunathan et al. (2010) use hand-written rules for their system, the authors learn feature weights from training data.", "questions": {"208014.q7U4qAHkq3": "What are \"hand-written rules\"?", "208014.56Wd7DV5N8": "How do they learn feature weights from training data?"}}
{"idx": "208412", "paper_id": "11156254", "title": "New Inflectional Lexicons and Training Corpora for Improved Morphosyntactic Annotation of Croatian and Serbian", "abstract": "In this paper we present newly developed inflectional lexcions and manually annotated corpora of Croatian and Serbian. We introduce hrLex and srLex - two freely available inflectional lexicons of Croatian and Serbian - and describe the process of building these lexicons, supported by supervised machine learning techniques for lemma and paradigm prediction. Furthermore, we introduce hr500k, a manually annotated corpus of Croatian, 500 thousand tokens in size. We showcase the three newly developed resources on the task of morphosyntactic annotation of both languages by using a recently developed CRF tagger. We achieve best results yet reported on the task for both languages, beating the HunPos baseline trained on the same datasets by a wide margin.", "context_section_header": "", "context_paragraph": "For both languages morphological lexicons were developed in the past, but with limited availability. For Croatian the Croatian Morphological Lexicon (Tadi\u0107 and Fulgosi, 2003) was available for search through a web interface since 2005 (Tadi\u0107, 2005). Since 2012 this lexicon is available through Meta-Share, with a size of ca 113,000 lemmas (60% of which are proper names) in version 5.0. However, it is distributed under a non-commercial license in the form of (token, lemma, tag) triples only, and is therefore not useful for expansion or enrichment. \u0160najder et al. (2008) provide another line of work on Croatian inflectional lexica, but the resulting resource is not freely available. For Serbian the SrpMD dictionary (Krstev, 2008), 85,721 lemmas in size, is published under a non-commercial license and indexed on Meta-Share, but is not available for download. The lexicons we present in this paper are freely downloadable, published under the GNU GPL license, organised by lexemes and paired with their inflectional paradigms, thereby enabling a wide range of applications and easy extensibility. Similar to inflectional lexicons, the line of work in annotated corpora of Croatian is reasonably extensive, in contrast to a fairly limited amount of research carried out for Serbian (Vitas et al., 2012), especially considering syntactic annotations. By and large, however, these contributions do not result in freely available resources; for a more detailed overview, see Agi\u0107 et al. (2013b). On top of providing two sizable new inflectional lexicons for the two languages, our hr500k corpus marks a significant new development for Croatian, and by virtue of direct transfer of tagging models, for Serbian as well. While Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, our contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages. With these recent developments, we can safely assume that through our line of work in free-culture resources, Croatian and Serbian are leaving the realm of severely underresourced languages.", "sentence": "While Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, our contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages.", "cited_ids": [{"paper_id": "5139332", "citation": "Agi\u0107 and Ljube\u0161i\u0107 (2015)"}], "y": "Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, and the authors' contribution significantly improves over the previous top scores [80 points LAS] in morphosyntactic tagging for the two languages, [Croatian and Serbian].", "snippet_surface": "Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, and the authors' contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages.", "questions": {"208412.OmGuUwgIQG": "What are the top-level results in dependency parsing?", "208412.12Akthdyro": "What are the previous top scores in morphosyntactic tagging?"}}
{"idx": "210070", "paper_id": "9400830", "title": "Neural Sequence-to-sequence Learning of Internal Word Structure", "abstract": "Learning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison. In this paper, we present a neural encoder-decoder model for learning canonical morphological segmentation. Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments. We obtain up to 4% improvement over a strong character-level encoder-decoder baseline for three languages. Our model outperforms the previous state-of-the-art for two languages, while eliminating the need for external resources such as large dictionaries. Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.", "context_section_header": "", "context_paragraph": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data sets.", "sentence": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data sets.", "cited_ids": [{"paper_id": "44472515", "citation": "Gulcehre et al. (2017)"}], "y": "Gulcehre et al. (2017) used a [statistical language model with canonical segmentation and sequence-to-sequence transformation] with recurrent neural networks, while the authors employ [a fused statistical model], which is better adapted to their settings with small data sets.", "snippet_surface": "Last, while Gulcehre et al. (2017) use a language model implemented with recurrent neural networks, the authors employ a statistical language model, which is better adapted to their settings with small data sets.", "questions": {"210070.Yyyk0nCDm5": "What is a statistical language model?", "210070.N+4CNWylXL": "How is it better adapted to the authors' settings?", "210070.czb27XNH6p": "What are the authors' settings?"}}
{"idx": "210176", "paper_id": "218501192", "title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards", "abstract": "Throughout a conversation, participants make choices that can orient the flow of the interaction. Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. In this work, we develop an unsupervised methodology to quantify how counselors manage this balance. Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range. Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range. By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards. This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis. We also illustrate how our measure can be indicative of a conversation\u2019s progress, as well as its effectiveness.", "context_section_header": "", "context_paragraph": "Prior work has similarly considered how utterances relate to the preceding and subsequent discourse (Webber, 2001). Frameworks like centering theory (Grosz et al., 1995) aim at identify-ing referenced entities, while we aim to more abstractly model interlocutor choices. Past work has also examined how interlocutors mediate a conversation's trajectory through taking or ceding control (Walker and Whittaker, 1990) or shifting topic (Nguyen et al., 2014); Althoff et al. (2016) considers the rate at which counselors in our setting advance across stages of a conversation. While these actions can be construed as forwards-oriented, we focus more on the interplay between forwards-and backwards-oriented actions. A counselor's objectives may also cut across these concepts: for instance, the training stresses the need for empathetic reflecting across all stages and topics.", "sentence": "Frameworks like centering theory (Grosz et al., 1995) aim at identify-ing referenced entities, while we aim to more abstractly model interlocutor choices.", "cited_ids": [{"paper_id": "18229335", "citation": "(Grosz et al., 1995)"}], "y": "Frameworks like centering theory [which says that some utterances sound more coherent because the focus attention (or \"center\") around one entity] (Grosz et al., 1995) aim at identifying referenced entities, while the authors aim to more abstractly model interlocutor choices.", "snippet_surface": "Frameworks like centering theory (Grosz et al., 1995) aim at identifying referenced entities, while the authors aim to more abstractly model interlocutor choices.", "questions": {"210176.Ypz+FdsFOo": "What is centering theory?", "210176.6rGTdSbkX2": "What do referenced entities refer to?", "210176./oNpuHNonF": "What is the goal of the authors' model?"}}
{"idx": "212619", "paper_id": "198977494", "title": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects", "abstract": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "context_section_header": "", "context_paragraph": "2. Given that, DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature are learned, we feed whole n-grams to our model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "sentence": "Given that, DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature are learned, we feed whole n-grams to our model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "cited_ids": [{"paper_id": "886027", "citation": "(Tang et al., 2014"}], "y": "Given that DA [Dialectal Arabic] has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature [the full diversity of Arabic dialects] is learned, the authors feed whole n-grams to their model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "snippet_surface": "Given that DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature is learned, the authors feed whole n-grams to their model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "questions": {"212619.67rS9h53Ag": "What is DA?", "212619.E2fWnk/AaJ": "What are \"corrupted input ngrams\"?", "212619.xr3VMW552h": "What are the training objectives of the model?"}}
{"idx": "212620", "paper_id": "198977494", "title": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects", "abstract": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "context_section_header": "", "context_paragraph": "3. In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings introduced here are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "sentence": "In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings introduced here are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "cited_ids": [{"paper_id": "216848261", "citation": "(Iyyer et al., 2015)"}], "y": "In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings the authors introduced are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "snippet_surface": "In contrast to previous studies, that composed unordered embeddings within deep neural models (BIBREF15), the authors introduced embeddings that are generated and learned within a shallow feed-forward neural model as they are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "questions": {"212620.DRDlIacIZX": "What are unordered embeddings?", "212620.6n2TDSH1A+": "What is SA of DA?", "212620.mXnUvoKVn0": "What do the authors mean by \"less complicated neural architectures\"?"}}
{"idx": "214438", "paper_id": "248721870", "title": "NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension", "abstract": "NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question per entity) are considered at the same time for a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC) architecture for this formulation. NER-MQMRC architecture considers all entities as input to BERT for learning token embeddings with self-attention and leverages BERT-based entity representation for further improving these token embeddings for NER task. Evaluation on three NER datasets show that our proposed architecture leads to average 2.5 times faster training and 2.3 times faster inference as compared to NER-SQMRC framework based models by considering all entities together in a single pass. Further, we show that our model performance does not degrade compared to single-question based MRC (NER-SQMRC) (Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for AE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this architecture primarily to solve large scale e-commerce attribute (or entity) extraction from unstructured text of a magnitude of 50k+ attributes to be extracted on a scalable production environment with high performance and optimised training and inference runtimes.", "context_section_header": "", "context_paragraph": "\u2022 Using entity information: Leveraging entity information (such as entity name) for learning better representations.  As summarized in Table 1, our proposed NER-MQMRC architecture combines the best of NER-SL and NER-SQMRC. NER-MQMRC considers extraction of multiple entities based on multiple questions on same text, and is novel in three ways -1) Token representations are learnt to incorporate information of all the entities, unlike using single entity as in Mengge et al., 2020). 2) We introduce leveraging BERT-based entity representations for further improving token representations for NER task. 3) Our architecture leads to faster training and inference. E.g. scoring of five entities can be done using a single forward pass with our NER-MQMRC as compared to five passes required earlier with NER-SQMRC based models (Devlin et al., 2019;Mengge et al., 2020). Experiments on three NER datasets establish the effectiveness of NER-MQMRC architecture. NER-MQMRC achieves 2.5x faster training and 2.3x faster inference as compared to single question based MRC (NER-SQMRC) framework based models by considering multiple entities together in training and inference. Further, we show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively. Rest of the paper is organized as follows. We describe our proposed NER-MQMRC architecture in Section 2. We discuss our experimental setup in Section 3 followed by results in Section 4. We discuss the industry impact of our work in Section 5 and summarize the paper in Section 6.", "sentence": "Further, we show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively.", "cited_ids": [{"paper_id": "52967399", "citation": "(Devlin et al., 2019)"}], "y": "The authors show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub [a dataset for E-commerce attribute extraction collected from the AliExpress Sports & Entertainment category], Ecommerce5PT [an extracted set of data from five different product types from Amazon catalogue] and Twitter datasets respectively.", "snippet_surface": "The authors show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively.", "questions": {"214438.VNVrVi7f9+": "What is \"SOTA NER-SQMRC\"?", "214438.Wy7nH8qx23": "What are the AE-Pub, Ecommerce5PT and Twitter datasets?", "214438.cDiFYWvm1H": "What is the F1 metric?"}}
{"idx": "215684", "paper_id": "7243556", "title": "Edinburgh Research Explorer A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability", "abstract": "We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We de\ufb01ne the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed \ufb01rst, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.", "context_section_header": "", "context_paragraph": "In particular, although the GGJ model achieves high segmentation accuracy on phonemic (nonvariable) input and makes errors that are qualitatively similar to human learners (tending to undersegment the input), its accuracy drops considerably on phonetically noisy data and it tends to oversegment rather than undersegment. Here, we demonstrate that when the model is augmented to account for phonetic variability, it is able to learn common phonetic changes and by doing so, its accuracy improves and its errors return to the more human-like undersegmentation pattern. In addition, we find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012). We analyze the model's phonetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants.", "sentence": "In addition, we find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012).", "cited_ids": [{"paper_id": "6447567", "citation": "(Elsner et al., 2012)"}], "y": "The authors find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning [a bigram language model and a model of phonetic variation are learned simultaneously] (Elsner et al., 2012).\"", "snippet_surface": "The authors find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012).", "questions": {"215684.gm3QevovU5": "What are the lexicon accuracy improvements?", "215684.6cUOcov+wP": "What is a pipeline model?", "215684.8W2ljI6BOg": "What is lexical-phonetic learning?"}}
{"idx": "217138", "paper_id": "235421827", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network", "abstract": "We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as\"class prototypes\"as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor's entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks.", "context_section_header": "", "context_paragraph": "We present a new approach (MOLEMAN 1 ) that maintains the dual-encoder architecture, but with the same mention-encoder on both sides. Entity linking is modeled entirely as a mapping between mentions, where inference involves a nearest neighbor search against all known mentions of all entities in the training set. We build MOLEMAN using exactly the same mention-encoder architecture and training data as Model F (Botha et al., 2020). We show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "sentence": "We show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "cited_ids": [{"paper_id": "15156124", "citation": "Tsai and Roth (2016)"}], "y": "The authors show that MOLEMAN significantly outperforms Model F on both Mewsli-9 [a multilingual entity-linking dataset] and the Tsai and Roth (2016) dataset [a dataset in 12 languages based on Wikipedia] particularly for low-coverage languages, and rarer entities.", "snippet_surface": "The authors show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "questions": {"217138.lv/1Gf6nok": "What is MOLEMAN?", "217138.m5VEjEK5lp": "What is Model F?", "217138.+tXw5paEvJ": "What are the Mewsli-9 and Tsai and Roth (2016) datasets?"}}
{"idx": "218457", "paper_id": "12186549", "title": "Multimodal Semantic Learning from Child-Directed Input", "abstract": "Children learn the meaning of words by being exposed to perceptually rich situations (linguistic discourse, visual scenes, etc). Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information (visual and social cues), handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure.", "context_section_header": "", "context_paragraph": "While there is work on learning from multimodal data (Roy, 2000;Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007;Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)'s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multimodal features, while in Yu's approach words are simply distributions over categories. It is therefore not clear how Yu's approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words-all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)'s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu's, but it would then have the same limitations.", "sentence": "For example, in comparison to Yu (2005)'s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations.", "cited_ids": [{"paper_id": "15689940", "citation": "Yu (2005)"}], "y": "In comparison to Yu (2005)'s model, the authors\u2019 approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures [(i.e., how visually similar a word is to others)] without positing a categorical level on which to learn word-symbol/category-symbol associations.", "snippet_surface": "The authors' approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations, in comparison to Yu (2005)'s model.", "questions": {"218457.iFnNb2djiI": "What is Yu (2005)'s model?", "218457.+dzgRnMJtT": "What are the distributed representations based on?", "218457.M18zu8RUlN": "How does the model operate on distributed representations?"}}
{"idx": "221085", "paper_id": "1454594", "title": "The Role of Semantic Roles in Disambiguating Verb Senses", "abstract": "We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicate-argument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles.", "context_section_header": "", "context_paragraph": "(Lee and Ng, 2002) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Vector Machines (SVM) and included local collocations and syntactic relations, and also found that adding syntactic features improved accuracy. Our features are similar to theirs, but we added semantic class features for the verb arguments. We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Gomez, 2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and iden-tify thematic roles and adjuncts; our work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "sentence": "We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Gomez, 2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and iden-tify thematic roles and adjuncts; our work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "cited_ids": [{"paper_id": "7437033", "citation": "(Gomez, 2001)"}], "y": "The authors found that the difference in machine learning algorithms did not play a large role in performance; when they used their features in SVMs they obtained almost no difference in performance over using maximum entropy models with Gaussian priors. Gomez (2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and identify thematic roles [the ties that indicate what part of each argument plays in the predicate's description of the event] and adjuncts [a class of arguments whose labels are derived from the treebank functional tags]; the authors' work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "snippet_surface": "The authors found that the difference in machine learning algorithms did not play a large role in performance; when they used their features in SVM they obtained almost no difference in performance over using maximum entropy models with Gaussian priors. Gomez (2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and identify thematic roles and adjuncts; their work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "questions": {"221085.MM1+e9iHx5": "What is SVM?", "221085.vsOBT79YRO": "What is WordNet?", "221085.iyFUL8M/My": "What is the difference between Gomez's algorithm and this work?"}}
{"idx": "221159", "paper_id": "9431715", "title": "Learning-based Multi-Sieve Co-reference Resolution with Knowledge", "abstract": "We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or \"grounds\") expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learning-based multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.", "context_section_header": "", "context_paragraph": "(2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009;Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise \"co-reference\" vs. \"non-coreference\" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. In our running example, the decision of whether {[vessel]} m1 refers to {[Kursk]} m2 is made before the decision of whether {[vessel]} m1 refers to {Norwegian [ship]} m 4 since decisions in the same sentence are believed to be easier than cross-sentence ones. We describe our learningbased multi-sieve approach in Sec. 4.", "sentence": "Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available.", "cited_ids": [{"paper_id": "7691746", "citation": "(Raghunathan et al., 2010)"}], "y": "The authors' multi-sieve approach [which splits data into co-reference vs non-coreference for coreference resolution] is different from (Raghunathan et al., 2010) in several respects: (a) their sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing recovery from errors as additional evidence becomes available.", "snippet_surface": "The authors' multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) their sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing recovery from errors as additional evidence becomes available.", "questions": {"221159.aMZmm+rC9c": "What is the multi-sieve approach?", "221159.vBpWN3foUO": "What is the difference between the multi-sieve approach and the approach from Raghunathan et al. (2010)?", "221159.SSzXuuV41p": "How do later sieves override the decisions of earlier sieves?"}}
{"idx": "221221", "paper_id": "14729876", "title": "Top-Down Predictive Linking and Complex-Feature-Based Formalisms", "abstract": "Automatic compilation of the linking relation employed in certain parsing algorithms for context-free languages is examined. Special problems arise in the extension of these algorithms to the possibly infinite domain of feature structures. A technique is proposed which is designed specifically for left-recursive categories and is based on the generalization of their occurrences in a derivation. Particular attention is drawn to the top-down predictive character of the linking relation and to its significance not only as a filter for increasing the efficiency of syntactic analysis but as a device for the top-down instantiation of information, which then serves as a key to the directed analysis of inflected forms as well as \"unknown\" or \"new\" words.", "context_section_header": "", "context_paragraph": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing. We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991)--but rather as a device for the top-down predictive instantiation of information, as Shieber et al. (1990) have shown for semantic-head-driven generation. In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of \"unknown\" or \"new\" lexical items.", "sentence": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing.", "cited_ids": [{"paper_id": "215762145", "citation": "Shieber et al. (1990)"}], "y": "Whereas Shieber et al. (1990) have discussed similar techniques [syntax-based ones that are top-down and allows for left-recursion] in the context of semantic head-driven generation [which generates language in a manner that is related to the semantic structure of the generated string], the authors are concerned here with parsing.", "snippet_surface": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, the authors are concerned here with parsing.", "questions": {"221221.uugiAVfsZo": "What are similar techniques discussed by Shieber et al.?", "221221.JiZgY5fjMz": "What are we concerned with here?"}}
{"idx": "221866", "paper_id": "8401287", "title": "Sentiment Flow - A General Model of Web Review Argumentation", "abstract": "Web reviews have been intensively studied in argumentation-related tasks such as sentiment analysis. However, due to their focus on content-based features, many sentiment analysis approaches are effective only for reviews from those domains they have been specifically modeled for. This paper puts its focus on domain independence and asks whether a general model can be found for how people argue in web reviews. Our hypothesis is that people express their global sentiment on a topic with similar sequences of local sentiment independent of the domain. We model such sentiment flow robustly under uncertainty through abstraction. To test our hypothesis, we predict global sentiment based on sentiment flow. In systematic experiments, we improve over the domain independence of strong baselines. Our findings suggest that sentiment flow qualifies as a general model of web review argumentation.", "context_section_header": "", "context_paragraph": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation.", "sentence": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment.", "cited_ids": [{"paper_id": "8715425", "citation": "(Wachsmuth et al., 2014a)"}], "y": "The authors' previous approach (Wachsmuth et al., 2014a) [involved cluster analysis to capture overall argument structure of a review]. [The author\u2019s new approach involve analyzing] the major abstraction steps, [such as content and domain differences,] when modeling sentiment flow to represent global sentiment.", "snippet_surface": "Unlike in the authors' previous approach (Wachsmuth et al., 2014a), they analyze the major abstraction steps when modeling sentiment flow to represent global sentiment.", "questions": {"221866.WKBru8ZS4g": "What was the previous approach?", "221866.AXruHBF3iO": "What are the major abstraction steps?", "221866.0H9n6Y4Z4q": "How does the sentiment flow get represented?"}}
{"idx": "222062", "paper_id": "14365022", "title": "A Syntactified Direct Translation Model with Linear-time Decoding", "abstract": "Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity. \n \nThis paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag-operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system.", "context_section_header": "", "context_paragraph": "Recently, (Shen et al., 2008) introduced an approach for incorporating a dependency-based language model into SMT. They proposed to extract String-to-Dependency trees from the parallel corpus. As the dependency trees are not constituents by nature, they handle non-constituent phrases as well. While this work is in the same general direction as our work, namely aiming at incorporating dependency parsing into SMT, there remain three major differences. Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory. Secondly, their decoder works bottom-up and uses a chart parser with a limited language model capability (3-grams), while we build on the efficient, linear-time decoder commonly used in phrase-based SMT. Thirdly, (Shen et al., 2008) deploys the dependency language model to augment the lexical language model probability be-tween two head words but never seek a full dependency graph. In contrast, our approach integrates an incremental parsing capability, that produces the partial dependency structures incrementally while decoding, and thus provides for better guidance for the search of the decoder for more grammatical output. To the best of our knowledge, our approach is the first to incorporate incremental dependency parsing capabilities into SMT while maintaining the linear-time and -space decoder.", "sentence": "Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory.", "cited_ids": [{"paper_id": "832217", "citation": "(Shen et al., 2008)"}], "y": "Firstly, Shen et al. (2008) resorted to heuristics to extract the String-to-Dependency trees, whereas the authors' approach [of a string-to-dependency algorithm for statistical machine translation] employs the well formalized CCG [combinatory categorial grammar] grammatical theory.", "snippet_surface": "Firstly, Shen et al. (2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas the authors' approach employs the well formalized CCG grammatical theory.", "questions": {"222062.i9SIXJ+v2I": "What are String-to-Dependency trees?", "222062.AjVzIH3AHa": "What is CCG grammatical theory?"}}
{"idx": "222510", "paper_id": "10148319", "title": "A Multithreaded Conversational Interface for Pedestrian Navigation and Question Answering", "abstract": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "context_section_header": "", "context_paragraph": "While most applications address these two problems independently, some like Google Now, Google Field Trip, etc, mix navigation with exploration. However, such applications present information primarily visually on the screen for the user to read. In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, our system allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1).", "sentence": "In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003).", "cited_ids": [{"paper_id": "5125584", "citation": "(Kray et al., 2003)"}], "y": "The authors' [android mobile app helps with pedestrian navigation in urban environments for tourist information and question answering] and has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003) as [the authors' model doesn't demand the user's attention/visual attention and has a speech-only interface that works in a question and answer way].", "snippet_surface": "In contrast, the authors' system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003).", "questions": {"222510.CgVLykz3o7": "What is the objective of the system?", "222510.r8Mdt6EwEJ": "How does the system prevent users from being distracted?", "222510.7XZiL7hJol": "What is the danger of being distracted while walking in the city?"}}
{"idx": "223314", "paper_id": "219720870", "title": "Iterative Edit-Based Unsupervised Sentence Simplification", "abstract": "We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.", "context_section_header": "", "context_paragraph": "candidate sentence according to the scoring function. Compared with Narayan and Gardent (2016), the order of our simplification operations is not fixed and is decided by the model. Figure 1 illustrates an example in which our model first chooses to delete a sentence fragment, followed by reordering the remaining fragments and replacing a word with a simpler synonym.", "sentence": "Compared with Narayan and Gardent (2016), the order of our simplification operations is not fixed and is decided by the model.", "cited_ids": [{"paper_id": "568368", "citation": "Narayan and Gardent (2016)"}], "y": "The authors' results show that compared with previous work, the order of their simplification operations [lexical substitution, sentence splitting and word/phrase deletion] is not fixed and is decided by the model [the model has iterative edits with no fixed order to achieve the highest score] .", "snippet_surface": "The authors' results show that compared with Narayan and Gardent (2016), the order of their simplification operations is not fixed and is decided by the model.", "questions": {"223314.caDctM8H7S": "What are the simplification operations?", "223314.jB+S+U8Jk9": "How is the order of the operations decided?"}}
{"idx": "224246", "paper_id": "5896413", "title": "Word Sense Induction by Community Detection", "abstract": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word. Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses. Our results show competitive performance on the SemEval-2010 WSI Task.", "context_section_header": "", "context_paragraph": "We highlight those related works with connections to community detection. V\u00e9ronis (2004) demon-strated that word co-occurrence graphs follow a small-world network pattern. In his scheme, word senses are discovered by iteratively deleting the more connected portions of the subgraph to reveal the different senses' network structure. Our work capitalizes on this intuition of discovering senserelated subgraphs, but leverages formalized methods for community detection to identify them. Dorow and Widdows (2003) identify senserelated subgraphs in a similar method to community detection for local region of the co-occurrence graph. They use a random walk approach to identify regions of the graph that are sense-specific. Though not identical, we note that the random walk model has been successfully applied to community detection (Rosvall et al., 2009). Furthermore, Dorow and Widdows (2003) performs graph clustering on a perword basis; in contrast, the proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "sentence": "Furthermore, Dorow and Widdows (2003) performs graph clustering on a perword basis; in contrast, the proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "cited_ids": [{"paper_id": "15155425", "citation": "Dorow and Widdows (2003)"}], "y": "[To perform word sense induction (WSI),] Dorow and Widdows (2003) perform graph clustering on a per-word basis; in contrast, the authors' proposed approach identifies communities for the entire [co-occurrence] graph, effectively performing an all-word WSI.", "snippet_surface": "Furthermore, Dorow and Widdows (2003) perform graph clustering on a per-word basis; in contrast, the authors' proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "questions": {"224246.WuOIhp8UFm": "What is graph clustering?", "224246.6OH30m7Feu": "How does the proposed approach identify communities?", "224246.SC4VQ5krIU": "What is all-word WSI?"}}
{"idx": "224972", "paper_id": "5389438", "title": "What good are \u2018Nominalkomposita\u2019 for \u2018noun compounds\u2019: Multilingual Extraction and Structure Analysis of Nominal Compositions using Linguistic Restrictors", "abstract": "Finding a definition of compoundhood that is cross-lingually valid is a non-trivial task as shown by linguistic literature. We present an iterative method for defining and extracting English noun compounds in a multilingual setting. We show how linguistic criteria can be used to extract compounds automatically and vice versa how the results of this extraction can shed new lights on linguistic theories about compounding. The extracted compound nouns and their multilingual contexts are a rich source that serves several purposes. In an additional case study we show how the database serves to predict the internal structure of tripartite noun compounds using spelling variations across languages, which leads to a precision of over 91%.", "context_section_header": "", "context_paragraph": "This multilingual perspective on a considerable number of languages has been adopted as well by Macherey et al., (2011), who present a multilingual language-independent approach to compound splitting. Moreover, they learned morphological operations on compounding automatically. Here, Macherey et al., (2011) extract training instances using a method related to Garera and Yarowsky (2008): select a single word f in a language l translated to several English words e i . If there is a translation for each e i to a word g i that shows a (partial) substring match with f , (f ; e 1 , . . . , e n ; g 1 , . . . , g n ) is extracted. While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages. This token-based perspective has the advantage that we can process English NCs for which there is no literal translation to the target language (e.g., health insurance aligned to Krankenversicherung (lit. invalid insurance)).", "sentence": "While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages.", "cited_ids": [{"paper_id": "8497268", "citation": "Macherey et al., (2011)"}], "y": "Macherey et al., (2011) extract training instances type-based in a bilingual setting, whereas the authors directly extract NC [noun compound] instances with a set of four closed compounding languages [languages like German that make one-word compounds].", "snippet_surface": "Macherey et al., (2011) extract training instances type-based in a bilingual setting, whereas the authors directly extract NC instances with a set of four closed compounding languages.", "questions": {"224972.hhumH3VuYE": "What is a bilingual setting?", "224972.ZAnO65h4Nz": "What are NC instances?", "224972.Isl6T2n+Ym": "What are closed compounding languages?"}}
{"idx": "225325", "paper_id": "52010508", "title": "RNN Simulations of Grammaticality Judgments on Long-distance Dependencies", "abstract": "The paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism. The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested.", "context_section_header": "", "context_paragraph": "This paper is a step in the direction of this research program. We trained two types of RNNs (GRU and LSTM) on a large corpus of English (Wikipedia and parts of UKWAC, Ferraresi et al. 2008), then tested their performances or a range of artificially constructed language structures, contrasting grammatical and ungrammatical examples. Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, we only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap). As a preliminary task (Task A), we check whether the network is sensitive to the difference in processing complexity between subject and object relatives, a much-studied domain in the psycholinguistic literature; next, we turn to two cases of ungrammaticality, one due to a violation of the principle of Full Interpretation (Chomsky, 1986a, p.98-99) (Task B), one to extraction out of \"strong\" syntactic islands (Ross, 1982), specifically, subject and relative clause islands (Task C). Violations of these types have been regarded as sharp both in the theoretical (Szabolcsi and den Dikken, 1999) and the experimental literature (Sorace and Keller, 2005;Cowart, 1997;Sprouse et al., 2013).", "sentence": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, we only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap).", "cited_ids": [{"paper_id": "1056628", "citation": "Lau et al. (2017)"}], "y": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, the authors only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal [a question word that starts with \"wh\" and replaces a noun] or the head of a relative clause and its gap).\"", "snippet_surface": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, the authors only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap).", "questions": {"225325.BqnyW1EXP5": "What are LSTM?", "225325.QAyw/icH02": "What is the graded nature of human grammaticality?", "225325.m8pQxACSdD": "What are long distance dependencies?"}}
{"idx": "227076", "paper_id": "52111066", "title": "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction", "abstract": "We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.", "context_section_header": "", "context_paragraph": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "sentence": "Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.", "cited_ids": [{"paper_id": "13335042", "citation": "Artetxe et al. (2017)"}], "y": "The only difference between the authors' approach and Artetxe et al. (2017), the strongest baseline, is that the authors admit both one-to-one alignments [the mapping of each word in the source language to one word in the target language] or one-to-many alignments [the mapping of each word in the source language to many words in the target language] between the words of the languages' respective lexicons, whereas Artexe et al. did not.", "snippet_surface": "Viewed in this light, the difference between the authors' approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.", "questions": {"227076.HLZ8mVrgLP": "What is the difference between the authors' approach and Artetxe et al. (2017)?", "227076.Fc/ffbW4SA": "What kind of alignments are admitted between the words of the languages' respective lexicons?", "227076.j01Vo0sTUn": "What is the strongest baseline?"}}
{"idx": "227077", "paper_id": "52111066", "title": "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction", "abstract": "We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.", "context_section_header": "", "context_paragraph": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "sentence": "Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "cited_ids": [{"paper_id": "13335042", "citation": "Artetxe et al. (2017)"}], "y": "Thus, the authors conclude that their hard constraint on one-to-one alignments [i.e., that there exist a single source for every word in the target lexicon, and that this source cannot be used more than once] is primarily responsible for the improvements [on databases and tasks they tested on] over Artetxe et al. (2017).", "snippet_surface": "Thus, the authors conclude that their hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "questions": {"227077.noJzJpmyQ/": "No questions."}}
{"idx": "230395", "paper_id": "16049704", "title": "Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models", "abstract": "This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.", "context_section_header": "", "context_paragraph": "Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "sentence": "Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "cited_ids": [{"paper_id": "5344007", "citation": "(He, 2007)"}], "y": "The authors' system [a system that used as a monolingual model improves the state of the art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, it outperforms the state-of-the-art word alignment model as measured by alignment error rate], used as a bilingual model, outperforms the current state-of-the-art WDHMM word alignment model as measured by alignment error rate (AER).", "snippet_surface": "The authors' system, used as a bilingual model, outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "questions": {"230395.plPvUhBkPb": "What is WDHMM?", "230395.AZENmHBLuR": "What is AER?"}}
{"idx": "236031", "paper_id": "248780089", "title": "Towards Detecting Political Bias in Hindi News Articles", "abstract": "Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles.We propose a transformer-based transfer learning method to fine-tune the pre-trained network on our data for this bias detection. As the required dataset for this particular task was not available, we created our dataset comprising 1388 Hindi news articles and their headlines from various Hindi news media outlets. We marked them on whether they are biased towards, against, or neutral to BJP, a political party, and the current ruling party at the centre in India.", "context_section_header": "", "context_paragraph": "We present several baseline Machine learning and Deep Learning approaches to detecting political bias on our dataset. We observe that XLM-RoBERTa (Conneau et al. (2020)), a transformerbased model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "sentence": "We observe that XLM-RoBERTa (Conneau et al. (2020)), a transformerbased model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "cited_ids": [{"paper_id": "207880568", "citation": "(Conneau et al. (2020)"}], "y": "The authors observe that XLM-RoBERTa (Conneau et al. (2020)), a transformer-based model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC [Matthews Correlation].", "snippet_surface": "The authors observe that XLM-RoBERTa (Conneau et al. (2020)), a transformer-based model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "questions": {"236031.GJm9sNsxa+": "What are the other baseline models?", "236031.8dHpW+JOQP": "What is the score of 83% accuracy?", "236031.fDtruRin93": "What is F1 macro and MCC?"}}
{"idx": "236106", "paper_id": "86111", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "abstract": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "context_section_header": "", "context_paragraph": "Discriminative models in MT have been proposed before. Carpuat and Wu (2007) trained a maximum entropy classifier for each source phrase type which used source context information to disambiguate its translations. The models did not capture target-side information and they were independent; no parameters were shared between classifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the \"discriminative word lexicon\" and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type of features. Our implementation is freely available and can be further extended by other researchers in the future.", "sentence": "Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features.", "cited_ids": [{"paper_id": "11410088", "citation": "Jeong et al. (2010)"}], "y": "Jeong et al. (2010) proposed a discriminative lexicon with a rich [feature set split into three parts--the first set contains label-independent features which depend on the source sentence; second set contains shared features and depends on target-side context; and, the third set contains label-dependent features which describe the currently predicted phrasal translation--] tailored to translation into morphologically rich languages; unlike the authors' [new discriminative model for MT which utilises source and target context information], their model only used source-context features [either full word forms or lemmata].", "snippet_surface": "Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike the authors' work, their model only used source-context features.", "questions": {"236106.EJR1woh0Wd": "What is a discriminative lexicon?", "236106.ZJUa9YiXpJ": "What are source-context features?", "236106.bKsSQM1D+y": "What features does the authors' work use?"}}
{"idx": "236242", "paper_id": "8332619", "title": "Generalized Character-Level Spelling Error Correction", "abstract": "We present a generalized discriminative model for spelling error correction which targets character-level transformations. While operating at the character level, the model makes use of wordlevel and contextual information. In contrast to previous work, the proposed approach learns to correct a variety of error types without guidance of manuallyselected constraints or language-specific features. We apply the model to correct errors in Egyptian Arabic dialect text, achieving 65% reduction in word error rate over the input baseline, and improving over the earlier state-of-the-art system.", "context_section_header": "", "context_paragraph": "Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints. We present a detailed analysis of mistakes and demonstrate that the proposed model indeed learns to correct a wider variety of errors.", "sentence": "Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints.", "cited_ids": [{"paper_id": "16622975", "citation": "(Eskander et al., 2013)"}], "y": "The authors show a 65% reduction in word-error-rate (WER) over a do-nothing input benchmark, and they outperform a state-of-the-art system (Eskander et al., 2013) that depends extensively on language-specific and manually-selected constraints where selections are besed on the [classification of the character additions or removals].", "snippet_surface": "The authors demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and they improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints.", "questions": {"236242.yT7FJ7YBUz": "What is the word-error-rate (WER)?", "236242.HxVwYV5Vnx": "What is the do-nothing input baseline?", "236242.AvQ/oup2rG": "What are language-specific and manually-selected constraints?"}}
{"idx": "239068", "paper_id": "13118529", "title": "A Probabilistic Framework for Answer Selection in Question Answering", "abstract": "This paper describes a probabilistic answer selection framework for question answering. In contrast with previous work using individual resources such as ontologies and the Web to validate answer candidates, our work focuses on developing a unified framework that not only uses multiple resources for validating answer candidates, but also considers evidence of similarity among answer candidates in order to boost the ranking of the correct answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework.", "context_section_header": "", "context_paragraph": "In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "sentence": "Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "cited_ids": [{"paper_id": "237199732", "citation": "(Voorhees, 2004)"}], "y": "Experimental results on TREC factoid questions (Voorhees, 2004) show that the authors' framework [a probabilistic answer selection framework which uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation and similarity features] significantly improved answer selection performance for four different extraction techniques [feature representation, answer validation features and answer similarity features], when compared to default selection using the individual candidate scores produced by each extractor.", "snippet_surface": "Experimental results on TREC factoid questions (Voorhees, 2004) show that the authors' framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "questions": {"239068.OaY0SqKA/j": "What are TREC factoid questions?", "239068.YHA3FmGHw7": "What are the four different extraction techniques?", "239068./l+P5E3PMV": "What is the default selection using the individual candidate scores?"}}
{"idx": "239373", "paper_id": "12126824", "title": "The Effect of Corpus Size in Combining Supervised and Unsupervised Training for Disambiguation", "abstract": "We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins' parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning.", "context_section_header": "", "context_paragraph": "Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below).", "sentence": "We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below).", "cited_ids": [{"paper_id": "129886", "citation": "(Ratnaparkhi et al., 1994)"}], "y": "The authors did not use the PP dataset described by (Ratnaparkhi et al., 1994) [containing prepositional phrase-attachments] because we are using more context than the limited context available in that set (see below).", "snippet_surface": "The authors did not use the PP data set described by (Ratnaparkhi et al., 1994) because they are using more context than the limited context available in that set (see below).", "questions": {"239373.Sj89zMh6KL": "What is the PP data set?", "239373.AkUGZXjX02": "What is the limited context available in that set?"}}
{"idx": "239597", "paper_id": "2687347", "title": "Chinese Segmentation with a Word-Based Perceptron Algorithm", "abstract": "Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.", "context_section_header": "", "context_paragraph": "One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "sentence": "Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "cited_ids": [{"paper_id": "5651543", "citation": "Sproat et al. (1996)"}], "y": "A difference from the authors' model [perceptron training algorithm with a beamsearch decoder for de-segmenting Chinese text] is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996) [starting at one end of the sentence sentence, finding the longest word and then repeating].", "snippet_surface": "Another difference from the authors' model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "questions": {"239597.0W8jObTi7l": "What is the rule-based submodel?", "239597.2SRcLcpXsO": "How does the dictionary-based forward maximum match method work?"}}
{"idx": "239760", "paper_id": "235294133", "title": "Higher-order Derivatives of Weighted Finite-state Machines", "abstract": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time\u2014from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A\u02c62 N\u02c64) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.", "context_section_header": "", "context_paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "sentence": "In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "cited_ids": [{"paper_id": "5548799", "citation": "(Mohri, 1997)"}], "y": "In contrast to many presentations of WFSMs [Weighted finite-state machines - computationally efficient mathematical models used in speech processing and NLP], the authors' work provides a purely linear-algebraic take on them. This connection allows them to develop [their algorithm that is an extension to the solved case of acyclic WFSMs. It solves the problem in the general case for efficient computation of mth order derivatives over cyclic WFSM].", "snippet_surface": "In contrast to many presentations of WFSMs (Mohri, 1997), the authors' work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows them to develop their general algorithm.", "questions": {"239760.7uAMGJIZ1O": "What are WFSMs?", "239760.0hbYdIxI5T": "What is the connection between WFSMs and linear algebra?", "239760.7cJHfuKWf5": "What is the general algorithm developed by the authors?"}}
{"idx": "242385", "paper_id": "5815524", "title": "Improving Automatic Speech Recognition for Lectures through Transformation-based Rules Learned from Minimal Data", "abstract": "We demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9%. Our method is distinguished from earlier related work by its robustness to small amounts of training data, and its resulting efficiency, in spite of its use of true word error rate computations as a rule scoring function.", "context_section_header": "", "context_paragraph": "What we show in this paper is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one-hour lecture is necessary to learn the TBL rules, and that this smaller amount of transcribed data in turn makes the true WER calculation computationally feasible. With this combination, we achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of our reimplementation of their heuristics on our lecture data (3.6%). This is on top of the average 11% RER from language model adaptation on the same data. We also achieve the RER from TBL without the obligatory round of development-set parameter tuning required by their heuristics, and in a manner that is robust to perplexity. Less is more.", "sentence": "With this combination, we achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of our reimplementation of their heuristics on our lecture data (3.6%).", "cited_ids": [{"paper_id": "16803157", "citation": "Peters and Drexel (2004)"}], "y": "[By correcting ASR output,] the authors achieve a greater average relative error reduction [RER] (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of their reimplementation of their heuristics on their lecture data (3.6%).", "snippet_surface": "The authors achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of their reimplementation of their heuristics on their lecture data (3.6%).", "questions": {"242385.Z3x3fPAIyO": "What is the average relative error reduction reported by Peters and Drexel (2004)?", "242385.0CQ9MgHuyw": "What is the RER of the reimplementation of their heuristics?"}}
{"idx": "244942", "paper_id": "220446825", "title": "Human Verb Associations as the Basis for Gold Standard Verb Classes: Validation against GermaNet and FrameNet", "abstract": "We describe a gold standard for semantic verb classes which is based on human associations to verbs. The associations were collected in a web experiment and then applied as verb features in a hierarchical cluster analysis. We claim that the resulting classes represent a theory-independent gold standard classification which covers a variety of semantic verb relations, and whose features can be used to guide the feature selection in automatic processes. To evaluate our claims, the association-based classification is validated against two standard approaches to semantic verb classes, GermaNet and FrameNet.", "context_section_header": "", "context_paragraph": "This paper suggests a resource for gold standard semantic verb classes which is independent from manual definitions. We collected human associations of German verbs in a web experiment, and performed a simple hierarchical clustering on the verbs, as based on the human associations. We claim that the resulting verb classes and their underlying features (i.e. the verb associations) represent a valuable basis for a theory-independent semantic classification of the German verbs. To support this claim, we validate the associationbased classes against existing verb classes. Why are we interested in a gold standard semantic verb classification? There are a variety of manual semantic verb classifications; major frameworks are the Levin classes (Levin, 1993), WordNet (Fellbaum, 1998) with its German counterpart GermaNet (Kunze, 2000), and FrameNet (Fontenelle, 2003) with the Salsa project (Erk et al., 2003) creating its German counterpart. Different frameworks depend on different instantiations of semantic similarity, e.g. Levin relies on syntactic similarity and verb alternations, WordNet uses synonymy, and FrameNet relies on situationbased agreement as defined in Fillmore's frame semantics (Fillmore, 1982). As an alternative to the resourceintensive manual classifications, automatic methods such as classification and clustering techniques have been applied to induce verb classes from corpus data, e.g. (Schulte im Walde, 2000;Merlo and Stevenson, 2001;Joanis and Stevenson, 2003;Korhonen et al., 2003;Schulte im Walde, 2003;Ferrer, 2004). When evaluating such induced classifications, it is difficult to define a gold standard that is generally accepted and covers various aspects of semantic similarity. 1 Human associations should provide a useful source 1 Note that we refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "sentence": "1 Human associations should provide a useful source 1 Note that we refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "cited_ids": [{"paper_id": "9680240", "citation": "Merlo and Stevenson (2001)"}], "y": "The authors suggest that human associations should provide a useful source. They note that they refer to cases where it is desirable to have a generally accepted gold standard [that covers various aspects of semantic similarity]\n  (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations)", "snippet_surface": "The authors suggest that human associations should provide a useful source. They note that they refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "questions": {"244942.gdwWm4YeDY": "What is the generally accepted gold standard?", "244942.1ml4gGBgGV": "What are Merlo and Stevenson (2001) aiming for?", "244942.UFBwWyqWdF": "What are the three types of intransitive-transitive alternations?"}}
{"idx": "247151", "paper_id": "220047806", "title": "A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization", "abstract": "Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.", "context_section_header": "", "context_paragraph": "We propose an architecture (shown in Figure 1) that is able to consider both morphological and semantic information. We first apply a candidate generator to generate a list of candidate concepts, and then use a BERT-based list-wise classifier to rank the candidate concepts. This two-step architecture allows unlikely concept candidates to be filtered out prior to the final classification, a necessary step when dealing with ontologies with millions of concepts. In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, our BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data. We further enhance this list-wise approach with a semantic type regularizer that allows our ranker to leverage semantic type information from  the ontology during training.", "sentence": "In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, our BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data.", "cited_ids": [{"paper_id": "46896270", "citation": "(Murty et al., 2018)"}], "y": "[There are two previous list-wise classifiers for bio-medical concept classification. The first treats the selection of the best candidate concept as a flat classification problem that loses the ability to handle concepts not encountered in training. The second uses a generate-and-rank approach that does not take advantage of the synonyms and semantic type information from UMLS.] In contrast to these previous list-wise classifiers (Murty et al., 2018) which only take the [biomedical] concept mention and the candidate concept name as input, the authors' BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the [clinical notes] training data.\"", "snippet_surface": "In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, the authors' BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data.", "questions": {"247151.mm6y2DEoeF": "What are \"list-wise classifiers\"?", "247151.iGz2nK8EtL": "What is the difference between the authors' approach and previous list-wise classifiers?", "247151.TKDc+pHCfN": "What does the BERT-based list-wise classifier take as input?"}}
{"idx": "248183", "paper_id": "218974285", "title": "Alignment Annotation for Clinic Visit Dialogue to Clinical Note Sentence Language Generation", "abstract": "For every patient\u2019s visit to a clinician, a clinical note is generated documenting their medical conversation, including complaints discussed, treatments, and medical plans. Despite advances in natural language processing, automating clinical note generation from a clinic visit conversation is a largely unexplored area of research. Due to the idiosyncrasies of the task, traditional methods of corpus creation are not effective enough approaches for this problem. In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies. Here we provide data statistics and qualitative analysis describing the unique annotation challenges. Given enough annotated data, such a resource would support multiple modeling methods including information extraction with template language generation, information retrieval type language generation, or sequence to sequence modeling.", "context_section_header": "", "context_paragraph": "In (Finley et al., 2018a), members of the EMR.AI team describe one intended approach to the problem by bridging information from clinic visit dialogue, first by classifying conversation sentences by intended note sections, then applying information extraction techniques. This data is then used to fill note templates generated by finite-state grammars. In another work (Finley et al., 2018b), they describe their method to automatically produce a parallel machine translation corpus for the special case of dictations to clinical note letter, but this focuses on just a narrow portion of the general problem. We posit that the task of clinical note generation based on dialogue is best represented as an amalgamation of different language transformations and thus the annotation efforts should not be tied to specific end to end methods. Our proposed method associates a note sentence to associated dialogue sentence sets using different tags (e.g. DICTA-TION, QA, STATEMENT, etc) and provides a higher level ordering of these sets. Compared to (Kazi and Kahanda, 2019), (Jeblee et al., 2019), and (Finley et al., 2018a), we actually annotate the final note content output. Compared to the work in (Finley et al., 2018b), we manually create our alignments and do it for the entire conversation and note. Therefore our dataset does not rely on a specific sequence of domain-dependent NLU tasks or a specific relationship between the extracted information and the final output (e.g. template-filling), nor assumes a narrow part of the problem 1 Anaphora is the phenomenon when an expression can only be understood within context of another expression (e.g. dictation), freeing users to choose their own intermediate methods. Our annotation objectives are inspired by and bear much similarity to the idea of machine translation corpus creation, the goal of which is to create sentence pairs which can be consumed by other algorithms (Koehn, 2005;Tiedemann, 2011). Several significant differences emerge from the end points being distinct mediums: one dialogue, one clinical note. For instance, dialogue data contains question and answer modes which must be mapped to prose. Additionally, the associations between the two mediums often occur out of sequence. Therefore, in contrast to machine translation corpus creation algorithms, our process cannot be easily automatized. Our annotation methodology bears most similarity to that of (Hwang et al., 2015) and (Tian et al., 2014) who create parallel corpora by manually labeling paired sentences for aligned documents as good, good partial, partial, bad, etc., between Wikipedia and Simple Wikipedia and between Chinese-English online web articles, respectively. In contrast to their work, we distinguish between different types of dialogue to clinical note transformations, e.g. dictation, question-answering etc, as well as attempt to organize related sentences on the dialogue side into groups.", "sentence": "Compared to the work in (Finley et al., 2018b), we manually create our alignments and do it for the entire conversation and note.", "cited_ids": [{"paper_id": "44178410", "citation": "(Finley et al., 2018b)"}], "y": "Compared to the work in (Finley et al., 2018b), the authors [propose a new annotation methodology that is content- and technique- agnostic while associate note sentences to sets of dialogue sentences, and] manually create their alignments [between dialogue and notes] and do so for the entire conversation and note.", "snippet_surface": "The authors compared their work to (Finley et al., 2018b), and manually created alignments for the entire conversation and note.", "questions": {"248183.Qjnkpjzj6N": "What is the work in (Finley et al., 2018b)?", "248183.BDNCGsG1HR": "What do the authors manually create?", "248183.41S1wAeWM1": "What is the entire conversation and note?"}}
{"idx": "249184", "paper_id": "118589015", "title": "Pun Generation with Surprise", "abstract": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \u201cdied\u201d and \u201cdyed\u201d). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.", "context_section_header": "", "context_paragraph": "We test our approach on 150 pun-alternative word pairs. 1 First, we show a strong correlation between our surprisal metric and funniness ratings from crowdworkers. Second, human evaluation shows that our system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "sentence": "Second, human evaluation shows that our system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "cited_ids": [{"paper_id": "51874992", "citation": "(Yu et al., 2018)"}], "y": "Human evaluation shows that the [an unsupervised approach to pun generation based on raw text and a susprisal principle] generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores [mean z-scores of ratings of each worker].", "snippet_surface": "Second, human evaluation shows that the authors' system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "questions": {"249184.RPTqEr2jYB": "What is the neural generation baseline?", "249184.o/lqszIgrr": "What are the funniness scores?"}}
{"idx": "250701", "paper_id": "402181", "title": "Using Semantic Roles to Improve Question Answering", "abstract": "Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneficial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models.", "context_section_header": "", "context_paragraph": "In line with previous work, our method exploits syntactic information in the form of dependency relation paths together with FrameNet-like semantic roles to smooth lexical and syntactic divergences between question and answer sentences. Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database. In contrast to Kaisser (2006), we model the semantic role assignment and answer extraction tasks numerically, thereby alleviating the coverage problems encountered previously.", "sentence": "Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database.", "cited_ids": [{"paper_id": "6541034", "citation": "Narayanan and Harabagiu (2004)"}], "y": "The authors' approach [that is comprised of 3 stages, determining the expected answer type of the question, retrieving passages likely to contain answers to the question and performing a match between the question words and retrieved passaged to extract the answer], is less domain dependent and resource intensive than Narayanan and Harabagiu (2004). It solely employs a dependency parser and the FrameNet database, [which includes surface syntactic realizations of semantic roles and provides annotated example sentences from British National Corpus].", "snippet_surface": "The authors' approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database.", "questions": {"250701.sQTxa/TZTF": "What is the approach of Narayanan and Harabagiu (2004)?", "250701.95gErpR/Ym": "What does the FrameNet database consist of?", "250701.uxSRAGlDda": "How does the dependency parser help?"}}
{"idx": "254182", "paper_id": "5859332", "title": "Character-Level Chinese Dependency Parsing", "abstract": "Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods.", "context_section_header": "", "context_paragraph": "Zhao (2009) was the first to study character-level dependencies; they argue that since no consistent word boundaries exist over Chinese word segmentation, dependency-based representations of word structures serve as a good alternative for Chinese word segmentation. Thus their main concern is to parse intra-word dependencies. In this work, we extend their formulation, making use of largescale annotations of , so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012 and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner-and inter-word structures, studying their influences on each other.  was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing.", "sentence": "Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "cited_ids": [{"paper_id": "10011032", "citation": "Hatori et al. (2012)"}], "y": "The authors' proposed arc-standard model [a combination of character-level arceager system with the arc-standard model] is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "snippet_surface": "The authors' proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "questions": {"254182.F7qyCstiH7": "What is the arc-standard model?", "254182.2G77hOROmZ": "What performance metrics do the authors compare?"}}
{"idx": "255776", "paper_id": "16956667", "title": "Joint Inference for Knowledge Base Population", "abstract": "Populating Knowledge Base (KB) with new knowledge facts from reliable text resources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs. However, the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors. In this paper, we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions: whether the types of entities meet the expectations of relations explicitly or implicitly, and whether the local predictions are globally compatible. We further measure the confidence of the extracted triples by looking at the details of the complete extraction process. Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts, and outperforms competitive baselines with state-of-the-art relation extraction models.", "context_section_header": "", "context_paragraph": "Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework Li and Ji, 2014;Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specific sentence describes (between a pair of entity mentions in this sentence). Li and Ji (2014) follow the ACE task definitions and present a neat incremental joint framework to simultaneously extract entity mentions and relations by structure perceptron. In contrast, we link entity mentions from a text corpus to their corresponding entities in an ex-isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "sentence": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "cited_ids": [{"paper_id": "250761", "citation": "Choi et al. (2006)"}], "y": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while the authors focus on jointly modeling [Entity Linking (EL)] and [Relation Extraction (RE)] in open domain, which is a different and challenging task.", "snippet_surface": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while the authors focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "questions": {"255776.yxBaFZD5Sd": "What is EL?", "255776.eVl8SmyK7a": "What is RE?", "255776.9YpexPm1Vp": "What is the difference between Choi et al.'s approach and the authors' approach?"}}
{"idx": "256673", "paper_id": "8790940", "title": "Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks", "abstract": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "context_section_header": "", "context_paragraph": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "sentence": "Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "cited_ids": [{"paper_id": "11616343", "citation": "Chen and Manning (2014)"}], "y": "Compared to the parser of Chen and Manning (2014), the authors receive 0.6% (UAS [Unlabeled attachment score]) and 0.9% (LAS[ Labelled attachment score ]) improvement on PTB3 [Section 23 of the English Penn Treebank 3] test set, while they receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 [Chinese Penn Treebank dataset ]test set.", "snippet_surface": "Compared to the parser of Chen and Manning (2014), the authors receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while they receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "questions": {"256673.UB35u6AWxQ": "What is UAS and LAS?", "256673./9VkJWt6hA": "What is PTB3 test set?", "256673.spQfHwU3mp": "What is CTB5 test set?"}}
{"idx": "256730", "paper_id": "14203516", "title": "A Statistical Tree Annotator and Its Applications", "abstract": "In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.", "context_section_header": "", "context_paragraph": "As for predicting projectable constituent, it is related to the work described in (Xiong et al., 2010), where they were predicting translation boundaries. A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while translation boundaries in our work are defined from source parse trees. Our work uses more resources, but the prediction accuracy is higher (modulated on a different test data): we get a F-measure 84.7%, in contrast with (Xiong et al., 2010)'s 71%.", "sentence": "A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while translation boundaries in our work are defined from source parse trees.", "cited_ids": [{"paper_id": "16085803", "citation": "(Xiong et al., 2010)"}], "y": "[The authors work was in applications to predict function tags, null elements and whether a tree constituent is projectable in machine translation.] They define projectable spans on a left-branching derivation tree solely for their phrase decoder and models. In contrast, (Xiong et al., 2010) defines translation boundaries in their work from source parse trees.", "snippet_surface": "A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while the authors define translation boundaries in their work from source parse trees.", "questions": {"256730.7IvgDWOlBz": "What is a left-branching derivation tree?", "256730.+VDdTjsING": "What are projectable spans?", "256730.AXYgpOO6kk": "What are translation boundaries in the authors' work?"}}
{"idx": "256897", "paper_id": "10409321", "title": "Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective", "abstract": "For the task of relation extraction, distant supervision is an efficient approach to generate labeled data by aligning knowledge base with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top precision improvements over the traditional state-of-the-art approaches.", "context_section_header": "", "context_paragraph": "Our work is closest to (Fan et al., 2014), since we focus on the same noisy corpus problem. Although from different perspectives, we study it along with the same line of using matrix factorization (Petroni et al., 2015) for relation extraction. In this line, (Riedel et al., 2013) initially considered the task as a matrix factorization problem. Their method consists of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, the data noise brought by the assumption of distant supervision (Mintz et al., 2009), is not considered in the work. Another line addressing the problem uses deep neural networks (Zeng et al., 2015;. The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously. In addition, (Han and Sun, 2016) explored Markov logic technique to enrich supervision knowledge, which can incorporate indirect supervision globally. Our method could be further augmented by that idea, using additional logical constraint to reduce the uncertainty for the clustered noise modeling.", "sentence": "Another line addressing the problem uses deep neural networks (Zeng et al., 2015;. The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "cited_ids": [{"paper_id": "2778800", "citation": "(Zeng et al., 2015;"}], "y": "One line of research addressing the problem [the noisy corpus problem] uses deep neural networks. The difference is that it is a supervised learning approach, while the authors' focused one [the authors approach models noisy data corpus using adaptive variance modeling approach based on Dirichlet Process instead of a fixed way of controlling complex noise weighting] is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "snippet_surface": "Another line addressing the problem uses deep neural networks (Zeng et al., 2015). The difference is that it is a supervised learning approach, while the authors' focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "questions": {"256897.Gqya/Y8xvI": "What is the problem being addressed?", "256897.ldvhxfQnmZ": "What is the difference between the two approaches?", "256897.VJFV9IXJ4R": "What is a joint learning approach?"}}
{"idx": "260139", "paper_id": "220045476", "title": "Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations", "abstract": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.", "context_section_header": "", "context_paragraph": "Our end-2-end transcription-free approach is similar and perhaps even motivated some of the previous works. There have been some works (Serdyuk et al., 2018;Lugosch et al., 2019) which perform prediction tasks directly from speech signals but lack in capturing the underlying linguis-tic structure of a language (sentences break into words for semantics). We believe capturing some of the important linguistic units (e.g. words) are important for spoken language understanding. (Qian et al., 2017) is most similar to our work in terms of overall architecture as they also first get word level representations and then use the encoder for utterance level prediction. However (Qian et al., 2017) uses transcribed word transcriptions but we only use word boundaries for ASR-free end-2-end spoken language understanding. As shown in Figure  1, most previous works follow the upper pipeline. They start with a transcript (manually generated or through an ASR), which is first segmented into utterances. They then use word-embeddings for each word in the transcript before feeding it into a classifier to predict target behavior codes.", "sentence": "However (Qian et al., 2017) uses transcribed word transcriptions but we only use word boundaries for ASR-free end-2-end spoken language understanding.", "cited_ids": [{"paper_id": "1451605", "citation": "(Qian et al., 2017)"}], "y": "Qian et al. (2017) uses transcribed word transcriptions but the authors only use word boundaries for ASR-free end-2-end spoken language understanding [that predicts labels from only audio features] .", "snippet_surface": "However, Qian et al. (2017) uses transcribed word transcriptions but the authors only use word boundaries for ASR-free end-2-end spoken language understanding.", "questions": {"260139.+gWNcccDt3": "What is the approach used by Qian et al.?", "260139.6E3luAPqes": "What do we use for ASR-free end-2-end spoken language understanding?"}}
{"idx": "261809", "paper_id": "52126537", "title": "Rule induction for global explanation of trained models", "abstract": "Understanding the behavior of a trained network and finding explanations for its outputs is important for improving the network\u2019s performance and generalization ability, and for ensuring trust in automated systems. Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network. However, the relations between different features and classes are lost in most cases. We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network. We first calculate the importance of the features in the trained network. We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and finally fit a rule induction model to explain the model predictions. We find that the output rule-sets can explain the predictions of a neural network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80. We make the code available at https://github.com/clips/interpret_with_rules.", "context_section_header": "", "context_paragraph": "In our approach, we aim to generate a set of if-then-else rules that approximate the interaction between the most important features and classes for a trained model. As opposed to Lakkaraju et al. (2017), before learning an explanation model, we modify the input data based on the importance of the features in the trained network. In doing so, we already encode some information about the network's performance within these input features.", "sentence": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, we modify the input data based on the importance of the features in the trained network.", "cited_ids": [{"paper_id": "19598815", "citation": "Lakkaraju et al. (2017)"}], "y": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, [which uses transparent approximations to explain the behaviour of black box classifiers by optimizing for fidelity to the original model and interpretability of the explanation], the authors modify the input data based on the importance of the features in the trained network.", "snippet_surface": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, the authors modify the input data based on the importance of the features in the trained network.", "questions": {"261809.Lq7Lnn9g4n": "What are Lakkaraju et al.'s approach?", "261809.yaFO+4pBFh": "What is the modification to the input data?", "261809.4ALUxgC6Jd": "How is the importance of the features determined?"}}
{"idx": "269699", "paper_id": "4902154", "title": "Towards a Probabilistic Model for Lexical Entailment", "abstract": "While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. The impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems.", "context_section_header": "", "context_paragraph": "In contrary to these systems, our model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003). As Majumdar and Bhattacharyya (2010), our model considers the impact of hypothesis length, however it does not require the tuning of a unique threshold for each length. Finally, most of the above systems do not differentiate between the various lexical resources they use, even though it is known that resources reliability vary considerably (Mirkin et al., 2009b). Our probabilistic model, on the other hand, learns a unique reliability parameter for each resource it utilizes. As mentioned above, this work extends the base model in (Shnarch et al., 2011), which is described in the next section.", "sentence": "In contrary to these systems, our model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003).", "cited_ids": [{"paper_id": "2903805", "citation": "(Habash and Dorr, 2003)"}], "y": "The authors' model showed improvement when utilizing high quality resources, such as WordNet and the CatVar (Categorial Variation) databases, [that comprise English language lexemes and categorical variants] (Habash and Dorr, 2003).", "snippet_surface": "In contrast to these systems, the authors' model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003).", "questions": {"269699.JA/f4cB7+4": "What are other systems that our model is compared to?", "269699.TbPz3ZYP9Q": "What does the CatVar database contain?"}}
{"idx": "27645", "paper_id": "3781940", "title": "ICRC-HIT: A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge", "abstract": "In this paper, we present a comment labeling system based on a deep learning strategy. We treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments. In the recurrent architecture of our system, our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question-comment pair, and assigns the labels to the comment sequence with a recurrent neural network over CNN. Compared with the conditional random fields based method, our approach performs better performance on Macro-F1 (53.82%), and achieves the highest accuracy (73.18%), F1-value (79.76%) on predicting the Good class in this answer selection challenge.", "context_section_header": "", "context_paragraph": "In this work, we present a novel comment labeling system based on deep learning. We propose the recurrent convolutional neural networks (R&CNN) approach to assign the labels to comments given a question. Based on the distributed representations learned form 2-dimensional CNN (2D-CNN) matching, our approach achieves to comment sequence learning and predict the classes of comments. Using the word embedding trained by provided Qatar Living data, R&CNN not only models the semantic relevance for question and comment, but also captures the correlative context in comment sequence for predicting the class of comment. The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "sentence": "The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "cited_ids": [{"paper_id": "13243922", "citation": "(Ding et al., 2008)"}], "y": "The experiment results show that the authors' system [recurrent convolutional neural networks (RCNN) to assign the labels to comments given a question] performs better than the CRF [Conditional Random Fields] based method [which is used to detect the contexts and answers of questions from forum threads] (Ding et al., 2008) on recognizing good comments, and performs more adaptively [on the Qatar Living data].", "snippet_surface": "The experiment results show that the authors' system performs better than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptively on the development and test dataset.", "questions": {"27645.T3JvA1Z39S": "What is the CRF based method?", "27645.+LQQu+COxt": "What is the system's purpose?", "27645.+bCUssX/6F": "How does it perform more adaptive on the development and test dataset?"}}
{"idx": "279713", "paper_id": "236486137", "title": "Unsupervised Paradigm Clustering Using Transformation Rules", "abstract": "This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering. Our system generates paradigms using morphological transformation rules which are discovered from raw data. We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios.", "context_section_header": "", "context_paragraph": "We experiment with two methods for discovering rules, described in Section 3.3. Our first approach is inspired by work on morphology discovery by Soricut and Och (2015), who generate prefix and suffix transformations between similar strings. This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule demonstrate that prefix and suffix rules deliver stronger performance for most languages in the shared task dataset but our more general transformations rules are beneficial for templatic languages like Maltese and languages with a high morpheme-to-word ratio like Basque.", "sentence": "This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets.", "cited_ids": [{"paper_id": "16326127", "citation": "Soricut and Och (2015)"}], "y": "This idea closely parallels the authors' approach for extracting rules. Unlike Soricut and Och (2015), the authors do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets, [and instead generate paradigms from raw data using not only prefix and suffix rules but also more general transformation rules.]", "snippet_surface": "This idea closely parallels the authors' approach for extracting rules. Unlike Soricut and Och (2015), the authors do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets.", "questions": {"279713.EWO9bnypxZ": "What is the idea being discussed?", "279713.lLq3X6hJ68": "What do Soricut and Och (2015) use when extracting rules?", "279713.IOYX2ZohzP": "What is the size of the shared task datasets?"}}
{"idx": "282589", "paper_id": "13694072", "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings", "abstract": "The positive effect of adding subword information to word embeddings has been demonstrated for predictive models. In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.", "context_section_header": "", "context_paragraph": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "sentence": "These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "cited_ids": [{"paper_id": "8796808", "citation": "Cotterell et al. (2016)"}], "y": "The authors' work differs from Cotterell et al. (2016) in that [the presented model] incorporates morphological information at training time, and that only [some of the pre existing models are] able to generate embeddings for [out-of-vocabulary] (OOV) words.", "snippet_surface": "These differ from the authors' work in that they incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "questions": {"282589.xhak8QqCw0": "What is morphological information?", "282589.hUKGzVV25y": "How do the other approaches differ from Cotterell et al. (2016)?"}}
{"idx": "28347", "paper_id": "14922514", "title": "A Comprehensive Gold Standard for the Enron Organizational Hierarchy", "abstract": "Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems.", "context_section_header": "", "context_paragraph": "We show the usefulness of this resource by investigating a simple predictor for hierarchy based on social network analysis (SNA), namely degree centrality of the social network induced by the email correspondence (Section 4). We call this a lower bound for SNA-based systems because we are only using a single simple metric (degree centrality) to establish dominance. Degree centrality is one of the features used by Rowe et al. (2007), but they did not perform a quantitative evaluation, and to our knowledge there are no published experiments using only degree centrality. Current systems using natural language processing (NLP) are restricted to making informed predictions on dominance pairs for which email exchange is available. We show (Section 5) that the upper bound performance of such NLP-based systems is much lower than our SNAbased system on the entire gold standard. We also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if we restrict ourselves to pairs for which email exchange is available, our simple SNA-based systems outperforms the NLP-based system.", "sentence": "We also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if we restrict ourselves to pairs for which email exchange is available, our simple SNA-based systems outperforms the NLP-based system.", "cited_ids": [{"paper_id": "16452861", "citation": "(Gilbert, 2012)"}], "y": "The authors contrast the simple SN-based [, also called SNA-based, social network analysis-based] system with a specific NLP system based on (Gilbert, 2012), and show that even if they restrict themselves to pairs for which email exchange is available, their simple SNA-based systems outperform the NLP-based system.", "snippet_surface": "The authors also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if they restrict themselves to pairs for which email exchange is available, their simple SNA-based systems outperform the NLP-based system.", "questions": {"28347.ib5We5B9iG": "What is SN-based system?", "28347.+6eMMb++qG": "What does \"email exchange is available\" mean?", "28347.FIh82XmYdS": "What is SNA-based system?"}}
{"idx": "291353", "paper_id": "11170627", "title": "Enriching a statistical machine translation system trained on small parallel corpora with rule-based bilingual phrases", "abstract": "Work funded by the Spanish Ministry of Science and Innovation through project TIN2009-14009-C02-01 and by Generalitat Valenciana through grant ACIF/2010/174 (VALi+d programme).", "context_section_header": "", "context_paragraph": "When both parallel corpora and linguistic information exist, hybrid approaches (Thurmair, 2009) may be followed in order to make the most of such resources. We focus on alleviating the data sparseness problem suffered by phrase-based statistical machine translation (PBSMT) systems (Koehn, 2010, ch. 5) when trained on small parallel corpora. We present a new hybrid approach which enriches a PBSMT system with resources from shallow-transfer RBMT. Shallow-transfer RBMT systems, which are described in detail below, do not perform a complete syntactic analysis of the input sentences, but rather work with much simpler intermediate representations. Hybridisation between shallow-transfer RBMT and SMT has not yet been explored. Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, our approach directly uses the RBMT dictionaries and rules.", "sentence": "Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, our approach directly uses the RBMT dictionaries and rules.", "cited_ids": [{"paper_id": "6612052", "citation": "(Eisele et al., 2008)"}], "y": "Existing hybridisation strategies involve more complex RBMT [Rule-based machine translation] systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, the authors' approach directly uses the RBMT dictionaries and rules.", "snippet_surface": "Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, the authors' approach directly uses the RBMT dictionaries and rules.", "questions": {"291353.RJIwiQNUHk": "What are RBMT systems?", "291353.PKWkf+CfQ8": "What do existing hybridisation strategies involve?", "291353.Yk8GgtahOI": "What is the difference between existing hybridisation strategies and the authors' approach?"}}
{"idx": "29321", "paper_id": "222290903", "title": "When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models", "abstract": "We address hypernymy detection, i.e., whether an is-a relationship exists between words (x, y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x, y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework achieves competitive improvements and the case study shows its better interpretability.", "context_section_header": "", "context_paragraph": "For the second question, we present ComHyper, a complementary framework (Sec. 4.1) which takes advantage of both pattern-based models' superior performance on Type-I cases and the broad coverage of distributional models on Type-II ones. Specifically, to deal with Type-II sparsity, instead of directly using unsupervised distributional models, ComHyper uses a training stage (Sec. 4.3) to sample from output space of a pattern-based model to train another supervised distribution model implemented by different context encoders (Sec. 4.2). In the inference stage, ComHyper uses the two models to separately handle the type of sparsity they are good at, as illustrated in Figure 1. In this manner, ComHyper relies on the partial use of pattern-based models on Type-I sparsity to secure performance no lower than distributional ones, and further attempts to lift the performance by fixing the former's blind spots (Type-II sparsity) with the latter. On several benchmarks and evaluation settings, the distributional model in ComHyper proves effective on its targeted cases, making our complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018). Further analysis also suggests that ComHyper is robust when facing different mixtures of Type-I and -II sparsity.", "sentence": "On several benchmarks and evaluation settings, the distributional model in ComHyper proves effective on its targeted cases, making our complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018).", "cited_ids": [{"paper_id": "47016219", "citation": "(Roller et al., 2018)"}], "y": "On several benchmarks and evaluation settings, the authors' distributional model in ComHyper[, a complementary framework,] proves effective on its targeted cases[ (i.e. pattern-based approaches that need to be complemented with distributional ones)] making their complementary approach outperform a competitive class of pattern-based baselines [(approaches that employ pattern pairs)] (Roller et al., 2018).", "snippet_surface": "On several benchmarks and evaluation settings, the authors' distributional model in ComHyper proves effective on its targeted cases, making their complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018).", "questions": {"29321.1Z/xzMcgru": "What are the benchmark and evaluation settings used?", "29321.bd4ZcIlJjR": "What is ComHyper?", "29321.0EMocFBgO2": "What is the competitive class of pattern-based baselines?"}}
{"idx": "299877", "paper_id": "102350797", "title": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training", "abstract": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.", "context_section_header": "", "context_paragraph": "We conduct a series of experiments with six different language pairs (in both directions) comprising European, non-European, and low-resource languages from two different datasets. Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures. Our method also gives better initial mapping compared to other existing methods (Artetxe et al., 2018b).", "sentence": "Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures.", "cited_ids": [{"paper_id": "3470398", "citation": "Conneau et al. (2018)"}], "y": "The authors' results show that their model [an adversarial autoencoder with a target encoder as an extra adversary and a regularization term to enforce cycle consistency] is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks [using the European language datasets] in all evaluation measures [mapping accuracy and convergence].", "snippet_surface": "The authors' results show that their model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures.", "questions": {"299877.nAE1jTO/dd": "What is the model?", "299877.8KwNAhSdJk": "What are the translation tasks?", "299877.idb9T1A4ee": "What evaluation measures were used?"}}
{"idx": "303207", "paper_id": "6440511", "title": "Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts", "abstract": "This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co-occurrence and error-driven filtering. We present initial evaluation results and discuss future directions.", "context_section_header": "", "context_paragraph": "Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints. The fourth third difference is in the level of abstraction of transfer rules candidates; in (Meyers et al., 1998), the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminal nodes), while in our approach the nodes of transfer rules do not have to be lexicalized.", "sentence": "The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction.", "cited_ids": [{"paper_id": "5088141", "citation": "(Meyers et al., 1998)"}], "y": "Meyers et al (1998) designs the alignment of source and target nodes in a way that preserves node dominancy [nodes in the input tree being above the corresponding the output tree] in the source and target parses, while the authors\u2019 approach [using syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process] does not have such restriction.", "snippet_surface": "The second difference concerns the node alignment; (Meyers et al., 1998) designs the alignment of source and target nodes in a way that preserves node dominancy in the source and target parses, while the authors' approach does not have such restriction.", "questions": {"303207.NGPNx+TZ4l": "What is the difference between the node alignment in (Meyers et al., 1998) and the authors' approach?", "303207.pHgNEfkUUA": "What is node dominancy?", "303207.G8WHxQt6xV": "What is the purpose of node dominancy preservation?"}}
{"idx": "303210", "paper_id": "6440511", "title": "Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts", "abstract": "This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co-occurrence and error-driven filtering. We present initial evaluation results and discuss future directions.", "context_section_header": "", "context_paragraph": "Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints. The fourth third difference is in the level of abstraction of transfer rules candidates; in (Meyers et al., 1998), the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminal nodes), while in our approach the nodes of transfer rules do not have to be lexicalized.", "sentence": "The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "cited_ids": [{"paper_id": "5088141", "citation": "(Meyers et al., 1998)"}], "y": "The third difference is in the process of identification of transfer rules candidates[, which can be compiled into a transfer dictionary for use in the translation process]; (Meyers et al., 1998) use the exact tree fragments in the source and target parse that are delimited by the alignment [which which preserved the dominant relationship], while the authors use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "snippet_surface": "The third difference is in the process of identification of transfer rules candidates; (Meyers et al., 1998) use the exact tree fragments in the source and target parse that are delimited by the alignment, while the authors use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "questions": {"303210.KIz0S3o08+": "What are the parse features?", "303210.lUVQHLCI9e": "What are the alignment constraints?", "303210.N+8NqaAaet": "What are the attribute constraints?"}}
{"idx": "310204", "paper_id": "6354513", "title": "Learning Hidden Markov Models with Distributed State Representations for Domain Adaptation", "abstract": "Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains. In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance. We reformulate the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectationmaximization algorithm to jointly learn distributed state representations and model parameters. We empirically investigate the proposed model on cross-domain part-ofspeech tagging and noun-phrase chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation.", "context_section_header": "", "context_paragraph": "In terms of performing distributed representation learning for output variables, our proposed model shares similarity with the structured output representation learning approach developed by Srikumar and Manning (2014), which extends the structured support vector machines to simultaneously learn the prediction model and the distributed representations of the output labels. However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), and the spectral learning method (Stratos et al., 2013). But none of them performs representation learning to address cross-domain adaptation problems.", "sentence": "However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning.", "cited_ids": [{"paper_id": "716020", "citation": "(Srikumar and Manning, 2014)"}], "y": "The approach presented in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while the authors' proposed distributed [Hidden Markov Models] address cross-domain learning problems by performing unsupervised representation learning.", "snippet_surface": "However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while the authors' proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning.", "questions": {"310204.Qg3cujT9vG": "What is the approach in (Srikumar and Manning, 2014)?", "310204.V5cFKZEudh": "What does \"standard supervised in-domain setting\" mean?", "310204.9FoN5Q8LlD": "What is the difference between in-domain and cross-domain learning?"}}
{"idx": "316712", "paper_id": "52186890", "title": "Unsupervised Cross-lingual Transfer of Word Embedding Spaces", "abstract": "Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.", "context_section_header": "", "context_paragraph": "Unsupervised Domain Transfer: Generally speaking, learning the cross-lingual transfer of word embedding can be viewed as a domain transfer problem, where the domains are word sets in different languages. Thus various work in the field of unsupervised domain adaptation or unsupervised transfer learning can shed light on our problem. For example, He et al. (2016) proposed a semi-supervised method for machine translation to utilize large monolingual corpora. Shen et al. (2017) used unsupervised learning to transfer sentences of different sentiments. Recent work in computer vision addresses the problem of image style transfer without any annotated training data (Zhu et al., 2017;Taigman et al., 2016;Yi et al., 2017). Among those, our work is mostly inspired by the work on CycleGAN (Zhu et al., 2017), and we adopt their cycled consistent loss over images into our back-translation loss. One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, we introduce the Sinkhorn distance in our objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "sentence": "One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, we introduce the Sinkhorn distance in our objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "cited_ids": [{"paper_id": "26873455", "citation": "(Zhang et al., 2017a)"}], "y": "One key difference between the authors method [an unsupervised cross-lingual transformation of monolingual embeddings] from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, the authors introduce the Sinkhorn distance in their objective function and demonstrate its superiority over the representative method [that showed the feasibility of connecting word embeddings of various languages without cross-lingual signal] using adversarial loss (Zhang et al., 2017a).", "snippet_surface": "One key difference between the authors' method and CycleGAN is that the latter used the training loss of an adversarial classifier as an indicator of the distributional distance, while the authors introduce the Sinkhorn distance in their objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "questions": {"316712.wBxEbjTGPj": "What is CycleGAN?", "316712.VCBbXHf/am": "What is the adversarial classifier?", "316712.cAwf6t3GqI": "What is the Sinkhorn distance?"}}
{"idx": "318102", "paper_id": "249204487", "title": "Synthetic Data Generation for Multilingual Domain-Adaptable Question Answering Systems", "abstract": "Deep learning models have significantly advanced the state of the art of question answering systems. However, the majority of datasets available for training such models have been annotated by humans, are open-domain, and are composed primarily in English. To deal with these limitations, we introduce a pipeline that creates synthetic data from natural text. To illustrate the domain-adaptability of our approach, as well as its multilingual potential, we use our pipeline to obtain synthetic data in English and Dutch. We combine the synthetic data with non-synthetic data (SQuAD 2.0) and evaluate multilingual BERT models on the question answering task. Models trained with synthetically augmented data demonstrate a clear improvement in performance when evaluated on the domain-specific test set, compared to the models trained exclusively on SQuAD 2.0. We expect our work to be beneficial for training domain-specific question-answering systems when the amount of available data is limited.", "context_section_header": "", "context_paragraph": "These approaches, however, focus on potential answers that are primarily named entities or noun phrases (Tang et al., 2017;Alberti et al., 2019;Puri et al., 2020;Shakeri et al., 2020). For our use-case, we are interested in finding answers of longer spans that might contain administrative procedures in a multilingual setting (e.g. answers to such questions as \"how do I request an interna-OG Results tend to be scattered across different websites that often lack any guarantee of quality or reliability, and significant information gaps remain in many areas, leaving important questions unanswered MT Resultaten zijn meestal verspreid over verschillende websites die vaak geen enkele garantie voor kwaliteit of betrouwbaarheid hebben, en er blijven op veel gebieden aanzienlijke informatielacunes, waardoor belangrijke vragen onbeantwoord blijven OG information gaps MT informatie hiaten Table 1: Translation of Segments via Google Translate tional passport?\" or \"Waar kan ik mijn wagen registreren?\" -\"Where can I register my car?\"). Moreover, we are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "sentence": "Moreover, we are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "cited_ids": [{"paper_id": "4570782", "citation": "(Dhingra et al., 2018)"}], "y": "The authors are interested in finding right answers in a document that might contain multiple [administrative procedures in a multilingual setting], i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018) [which trains a neural network model on cloze-style questions and fine-tuning].", "snippet_surface": "Moreover, the authors are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "questions": {"318102.BtOo6wCunm": "What assumptions do the methods proposed in (Dhingra et al., 2018) make?", "318102.gJsSdGW/Bt": "How does this differ from our approach?", "318102.W4ZjIpWV25": "What is the goal of our approach?"}}
{"idx": "320211", "paper_id": "237010913", "title": "Sentiment Preservation in Review Translation using Curriculum-based Re-inforcement Framework", "abstract": "Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment and emotion and gender traits and etc.) to the target and especially in a low-resource scenario. Such loss can affect the performance of any downstream Natural Language Processing (NLP) task and such as sentiment analysis and that heavily relies on the output of the MT systems. The susceptibility to sentiment polarity loss becomes even more severe when an MT system is employed for translating a source content that lacks a legitimate language structure (e.g. review text). Therefore and we must find ways to minimize the undesirable effects of sentiment loss in translation without compromising with the adequacy. In our current work and we present a deep re-inforcement learning (RL) framework in conjunction with the curriculum learning (as per difficulties of the reward) to fine-tune the parameters of a pre-trained neural MT system so that the generated translation successfully encodes the underlying sentiment of the source without compromising the adequacy unlike previous methods. We evaluate our proposed method on the English\u2013Hindi (product domain) and French\u2013English (restaurant domain) review datasets and and found that our method brings a significant improvement over several baselines in the machine translation and and sentiment classification tasks.", "context_section_header": "", "context_paragraph": "Recently, Tebbifakhr et al. (2019) proposed Machine-Oriented (MO) Reinforce, a policybased method to pursue a machine-oriented objective 2 in a sentiment classification task unlike the traditional human-oriented objective 3 . It gives a new perspective for a use-case of the MT system (i.e. machine translation for machine). To perform this task-specific adaption (i.e. produce output to feed a machine), Tebbifakhr et al. (2019) adapted the REINFORCE of Williams (1992) by incorporating an exploration-oriented sampling strategy. As opposed to one sampling of REINFORCE, MO Reinforce samples k times, (k = 5), and obtains a reward for each sample from the sentiment classifier. A final update of the model parameters are done w.r.t the highest rewarding sample. Although they achieved a performance boost in the sentiment classification task, they had to greatly compromise with the translation quality. In contrast to Tebbifakhr et al. (2019), we focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL. The adapted NMT system, thus obtained, is expected to produce a more accurate (high-quality) translation as well as improve the performance of a sentiment analyser. Bahdanau et al. (2017); Nguyen et al. (2017), unlike us, used the popular AC method, and focused only on preserving the semantics (translation quality) of a text. Additionally, we develop a CL based strategy to guide the training. Recently, Zhao et al. (2020) also studied AC method in the context of NMT. However, they used this method to learn the curriculum for re-selecting influential data samples from the existing training set that can further improve the performance (translation quality) of a pre-trained NMT system.", "sentence": "In contrast to Tebbifakhr et al. (2019), we focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL.", "cited_ids": [{"paper_id": "202786381", "citation": "Tebbifakhr et al. (2019)"}], "y": "In contrast to Tebbifakhr et al. (2019), the authors focus on performing a task-specific customisation of a pre-trained MT [Machine Translation] system via a harmonic reward-based deep reinforcement framework that uses an AC method [actor-critic reinforcement learning framework] in conjunction with CL [Curriculum Learning].", "snippet_surface": "In contrast to Tebbifakhr et al. (2019), the authors focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL.", "questions": {"320211.l2AaaHNgIE": "What is the task-specific customisation?", "320211.eKI8p9yp3i": "What is the deep reinforcement framework?", "320211.VWHQuO1YzL": "What is the AC method and CL?"}}
{"idx": "324225", "paper_id": "184488238", "title": "Identifying Visible Actions in Lifestyle Vlogs", "abstract": "We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.", "context_section_header": "", "context_paragraph": "Although we use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, we search for routine videos that contain rich audio descriptions of the actions being performed, and we use this transcribed audio to extract actions. In these lifestyle vlogs, a vlogger typically performs an action while also describing it in detail. To the best of our knowledge, we are the first to build a video action recognition dataset using both transcribed audio and video information.", "sentence": "Although we use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, we search for routine videos that contain rich audio descriptions of the actions being performed, and we use this transcribed audio to extract actions.", "cited_ids": [{"paper_id": "22264672", "citation": "(Fouhey et al., 2018)"}], "y": "Although the authors use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, they search for routine videos that contain rich audio descriptions of the actions being performed, and they use this transcribed audio to extract [activities performed in the videos]. [Actions are extracted in several steps; data gathering, manual selection, transcript filtering, and then using the Stanford parser to split the transcript into sentences and verb phrases].", "snippet_surface": "The authors use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, they search for routine videos that contain rich audio descriptions of the actions being performed, and they use this transcribed audio to extract actions.", "questions": {"324225.9KkeKZc9sj": "What is implicit data gathering?", "324225.76pMIElYLO": "What is different about the human action recognition datasets compared to the dataset used in this paper?", "324225.yKMoiJklka": "How do the authors use the transcribed audio?"}}
{"idx": "325708", "paper_id": "15307540", "title": "Automatically Creating General-Purpose Opinion Summaries from Text", "abstract": "We present and evaluate the first method known to us that can create rich nonextract-based opinion summaries from general text (e.g. newspaper articles). We first describe two possible representations for opinion summaries and then present our system OASIS, which identifies, and optionally aggregates, fine-grained opinions from the same source on the same topic. We propose new evaluation measures for both types of opinion summary and employ the metrics in an evaluation of OASIS on a standard opinion corpus. Our results are encouraging \u2014 OASIS substantially outperforms a competitive baseline when creating document-level aggregate summaries that compute the average polarity value across the multiple opinions identified for each source about each topic. We further show that as state-ofthe-art performance on fine-grained opinion extraction improves, we can expect to see opinion summaries of very high quality \u2014 with F-scores of 54-78% using our OSEM evaluation measure.", "context_section_header": "", "context_paragraph": "Our works falls in the area of fine-grained subjectivity analysis concerned with analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "sentence": "In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "cited_ids": [{"paper_id": "388", "citation": "Pang and Lee (2004)"}], "y": "In contrast to the opinion extracts [movie reviews] produced by Pang and Lee (2004), the authors' summaries are not text extracts, but rather explicitly identify and characterize [by using an opinion polarity classifier from Choi and Cardie (2009)] the relations between opinions and their source [either positive, negative, or neutral].", "snippet_surface": "In contrast to the opinion extracts produced by Pang and Lee (2004), the authors' summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "questions": {"325708.zx3kgbkolj": "What are opinion extracts?", "325708.joquT3sPNY": "What do the summaries explicitly identify and characterize?"}}
{"idx": "326512", "paper_id": "8586038", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT\u201914 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT\u201914 English-to-German task.", "context_section_header": "", "context_paragraph": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45 (Sutskever et al., 2014). Our models are also validated on the more difficult WMT'14 English-to-German task.", "sentence": "With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points.", "cited_ids": [{"paper_id": "1245593", "citation": "(Luong et al., 2015)"}, {"paper_id": "1245593", "citation": "(Luong et al., 2015)"}], "y": "The authors' deep attention model improved the BLEU score to 37.7, outperforming the shallow model which has six layers (Luong et al., 2015) [as opposed to the authors' model which has 16] by 6.2 BLEU points.", "snippet_surface": "The authors' deep attention model improved the BLEU score to 37.7, outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points.", "questions": {"326512.7A1D5DNCfe": "What is the BLEU score?", "326512.UekZvb/ojR": "What is the shallow model?", "326512.dOnXr/ntTO": "How many layers does the shallow model have?"}}
{"idx": "328432", "paper_id": "13320571", "title": "Reducing Lexical Features in Parsing by Word Embeddings", "abstract": "The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words.", "context_section_header": "", "context_paragraph": "Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003;Nivre et al., 2006;Zhang and Clark, 2008;Huang and Sagae, 2010; Templates: and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s 0 w and q 0 w, respectively) are typically used as features to calculate scores of transitions. When s 0 w is used as a feature template, the features in this template (e.g. s 0 w saw and s 0 w look ) can be viewed as one-hot vectors of a dimension of the lexicon size ( Figure 1). Corresponding to s 0 w, a weight is assigned to each word (e.g. W (s 0 w saw ) and W (s 0 w look )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s 0 w by d features, namely s 0 e 1 , . . . , s 0 e d . Given the vector representation of a word (e.g., e saw = (0.6, . . . , 0.2)), we replace the lexical feature (e.g. s 0 w saw ) by a linear combination of the d features (e.g., s 0 e saw := 0.6s 0 e 1 + . . . + 0.2s 0 e d ). Then, instead of the weights in a number of lexicon size assigned to s 0 w, now we use d weights (i.e., W (s 0 e 1 ), . . . , W (s 0 e d )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s 0 wq 0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008;Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014).", "sentence": "In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline.", "cited_ids": [{"paper_id": "1356465", "citation": "Andreas and Klein (2014)"}], "y": "The authors' framework [using a parser that has has low-dimensional features induced from word embeddings] effects that significantly improve the baseline [by connecting unseen words to known ones and encouraging common behaviors among in-vocabulary words], in contrast to the negative results reported in Andreas and Klein (2014) [which used word vectors in their parser.]", "snippet_surface": "The authors' framework has effects that significantly improve the baseline, in contrast to the negative results reported in Andreas and Klein (2014).", "questions": {"328432.BFus3lILQi": "What were the negative results reported in Andreas and Klein (2014)?", "328432.4uaXIWkjn4": "What effects does the authors' framework have?", "328432.P/JDu5P+Da": "How does it improve the baseline?"}}
{"idx": "333254", "paper_id": "202775978", "title": "Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching", "abstract": "Sentence matching is a key issue in natural language inference and paraphrase identification. Despite the recent progress on multi-layered neural network with cross sentence attention, one sentence learns attention to the intermediate representations of another sentence, which are propagated from preceding layers and therefore are uncertain and unstable for matching, particularly at the risk of error propagation. In this paper, we present an original semantics-oriented attention and deep fusion network (OSOA-DFN) for sentence matching. Unlike existing models, each attention layer of OSOA-DFN is oriented to the original semantic representation of another sentence, which captures the relevant information from a fixed matching target. The multiple attention layers allow one sentence to repeatedly read the important information of another sentence for better matching. We then additionally design deep fusion to propagate the attention information at each matching layer. At last, we introduce a self-attention mechanism to capture global context to enhance attention-aware representation within each sentence. Experiment results on three sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN has the ability to model sentence matching more precisely.", "context_section_header": "", "context_paragraph": "Our proposed OSOA-DFN conducts original semantics-oriented cross sentence attention to model the matching. We design deep fusion to augment the propagation of attention information. At last, we introduce a self-attention mechanism to capture global context to enhance semantic representation. Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "sentence": "Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "cited_ids": [{"paper_id": "51610106", "citation": "(Duan et al., 2018)"}], "y": "Compared to AF-DMN (Duan et al., 2018), the authors [use a model model containing three mechanisms where the first two are combined to form one unit of cross-attention] and just use one self-attention layer instead of multiple layers, which reduces model complexity [by setting the number of cross attention layers to 3] but achieves outperformed accuracy.", "snippet_surface": "Compared to AF-DMN (Duan et al., 2018), the authors just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "questions": {"333254.M5gMVJMoLh": "What is AF-DMN?", "333254.P96NEcTcVL": "How does using one self-attention layer reduce model complexity?"}}
{"idx": "33357", "paper_id": "14186198", "title": "Personalized Page Rank for Named Entity Disambiguation", "abstract": "The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.", "context_section_header": "", "context_paragraph": "Our method has the following properties: 1) as our system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, our method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) we tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "sentence": "Our method has the following properties: 1) as our system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, our method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) we tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "cited_ids": [{"paper_id": "17826594", "citation": "(Alhelbawy and Gaizauskas, 2014)"}], "y": "The authors' [collective disambiguation approach using a graph model] has the following properties: 1) as their system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike PageRank based approaches in [name entity disambiguation] (NED) (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, the authors' method is able to better utilize the local similarity between a candidate and a KB node; and 3) the authors' tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise.", "snippet_surface": "The authors' method has the following properties: 1) as their system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, their method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) they tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "questions": {"33357.vbjAdUMynL": "What is NED?", "33357.Q3SGY+k9BK": "What is the Personalized PageRank algorithm?", "33357.PgJmY2Dvzg": "How does the Personalized PageRank algorithm reduce noise?"}}
{"idx": "333952", "paper_id": "249394694", "title": "Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation", "abstract": "We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves strong translation performance across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU scores of 31.16 for \\texttt{en}\\rightarrow\\texttt{de} and 38.37 for \\texttt{de}\\rightarrow\\texttt{en} on the IWSLT14 dataset, 30.78 for \\texttt{en}\\rightarrow\\texttt{de} and 35.15 for \\texttt{de}\\rightarrow\\texttt{en} on the WMT14 dataset, and 27.17 for \\texttt{zh}\\rightarrow\\texttt{en} on the WMT17 dataset. SimCut is not a new method, but a version of Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be considered as a perturbation-based method. Given the universality and simplicity of Bi-SimCut and SimCut, we believe they can serve as strong baselines for future NMT research.", "context_section_header": "", "context_paragraph": "\u2022 Our experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "sentence": "\u2022 Our experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "cited_ids": [{"paper_id": "237485586", "citation": "(Xu et al., 2021)"}], "y": "[The authors use a new two-stage training strategy consisting of bidirectional pretraining and unidirectional finetuning, combined with SimCut Regularisation to improve generality of NMT.] Their experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "snippet_surface": "The authors' experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "questions": {"333952.v83AKSuxi7": "What is \"Bi-SimCut\"?", "333952.1aoLOtm8Lu": "What is the Transformer model?", "333952.Sy46QH7NNw": "What does SOTA stand for?"}}
{"idx": "334608", "paper_id": "237332190", "title": "Stylistic approaches to predicting Reddit popularity in diglossia", "abstract": "Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit\u2019s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (\u2018Singlish\u2019) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect\u2019s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity.", "context_section_header": "", "context_paragraph": "Much research has gone into investigating what makes a social media post popular, including some specifically focused on Reddit. Lakkaraju et al. (2013) controlled for the content of the post by concentrating on image submissions, which are frequently re-or cross-posted to different communities by different authors. They found that the title of a submission played a role in determining its success, where titles specifically engineered towards the community it was posted in (for example, by using community-specific words) performed better. Tran and Ostendorf (2016) took this a step further and trained separate models for the content (using Latent Dirichlet Allocation (LDA)) and the style of the language used (by replacing topic words with their part-of-speech tags). They computed the Spearman rank correlation between scores and post representations, and found that the style model was much better at predicting of the success of a post than the content model. In other words, they found that these subreddits had their own community style, and posts which are stylisti-cally more similar to it are more likely to be wellreceived. Fang et al. (2016) is the paper which is closest to the aim of this paper. They divided posts into eight different bins which are automatically determined by the score distribution of that particular subreddit, and evaluated model performance using a modified macro F1 score (details in Section 5.1). However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "sentence": "However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "cited_ids": [{"paper_id": "5958042", "citation": "Fang et al. (2016)"}], "y": "While Fang et al. (2016) focus on modelling the conversational context of a post [text features, response structure and text of the discussion], the authors focus on modelling the community style [i.e. use of punctuation, stopwords, and part-of-speech tags] of subreddits.", "snippet_surface": "However, while Fang et al. (2016) focused on modelling the conversational context of a post, the authors instead focus on modelling the community style.", "questions": {"334608.i/CYMVLWNW": "What did Fang et al. (2016) focus on?", "334608.QHhSg8CYS3": "What do I focus on in this paper?"}}
{"idx": "340739", "paper_id": "15890923", "title": "Extracting Social Networks from Literary Text with Word Embedding Tools", "abstract": "In this paper a social network is extracted from a literary text. The social network shows, how frequent the characters interact and how similar their social behavior is. Two types of similarity measures are used: the first applies co-occurrence statistics, while the second exploits cosine similarity on different types of word embedding vectors. The results are evaluated by a paid micro-task crowdsourcing survey. The experiments suggest that specific types of word embeddings like word2vec are well-suited for the task at hand and the specific circumstances of literary fiction text.", "context_section_header": "", "context_paragraph": "An obvious next step is the actual extraction of social networks from novels. In the method by (Elson et al., 2010) the networks are derived from dialog interactions. Therefore their method includes finding instances of quoted speech, attributing each quote to a character, and identifying when certain characters are in conversation. They construct a weighted graph, where nodes correspond to actors, and the weights on the edges represents the frequency and amount of exchanges. In contrast to our work, (Elson et al., 2010) are solely focus on length and number of dialogues between persons to measure relatedness, whereas our approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings. Similarly, (Celikyilmaz et al., 2010) address a the problem related to the extraction of relations between characters. They attribute utterances in literary dialogues to actors, and apply the similarities in the language used to predict similarity and hidden relations between those actors. In contrast to our work, the approach also is restricted to dialogues between authors, and the evaluation of the method is of limited scope.", "sentence": "In contrast to our work, (Elson et al., 2010) are solely focus on length and number of dialogues between persons to measure relatedness, whereas our approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings.", "cited_ids": [{"paper_id": "1974676", "citation": "(Elson et al., 2010)"}], "y": "In contrast to the authors' work [based on co-occurence statistics and cosine similarity], (Elson et al., 2010) focus solely on length and number of dialogues between persons to measure relatedness, whereas the authors' approach looks at general co-occurrence or similarity as measured by [Language Technology] (LT) tools which use word embeddings.", "snippet_surface": "In contrast to the authors' work, (Elson et al., 2010) focus solely on length and number of dialogues between persons to measure relatedness, whereas their approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings.", "questions": {"340739.wiFS+HEpkF": "What are LT tools?", "340739.VBH4+SrnxN": "How do LT tools measure co-occurrence or similarity?", "340739.l7U800F9jj": "What are word embeddings?"}}
{"idx": "341873", "paper_id": "52118895", "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction", "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.", "context_section_header": "", "context_paragraph": "To explore this problem, we create a dataset SCI-ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b;. In addition, we build a scientific knowledge graph integrating terms and relations extracted from each article. Human evaluation shows that propagating coreference can significantly improve the quality of the automatic constructed knowledge graph.", "sentence": "Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b;. In addition, we build a scientific knowledge graph integrating terms and relations extracted from each article.", "cited_ids": [{"paper_id": "10822819", "citation": "(Luan et al., 2017b;"}], "y": "The authors experiments [comparing their unified model, a multi task model], show that the unified model shows that it is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b. The authors build a scientific knowledge graph integrating terms and relations extracted from each article.", "snippet_surface": "The authors' experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b. In addition, they build a scientific knowledge graph integrating terms and relations extracted from each article.", "questions": {"341873.ToZ7bmeb8g": "What is the unified model?", "341873.GgRpZEUGGk": "How does it outperform previous state-of-the-art systems?", "341873.re5ySIpUwp": "What is the scientific knowledge graph integrating?"}}
{"idx": "354670", "paper_id": "16212021", "title": "Neural Automatic Post-Editing Using Prior Alignment and Reranking", "abstract": "We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a first-stage MT system. Our APE system (APE_Sym) is an extended version of an attention based NMT model with bilingual symmetry employing bidirectional models, mt\u2013pe and pe\u2013mt. APE translations produced by our system show statistically significant improvements over the first-stage MT, phrase-based APE and the best reported score on the WMT 2016 APE dataset by a previous neural APE system. Re-ranking (APE_Rerank) of the n-best translations from the phrase-based APE and APE_Sym systems provides further substantial improvements over the symmetric neural APE model. Human evaluation confirms that the APE_Rerank generated PE translations improve on the previous best neural APE system at WMT 2016.", "context_section_header": "", "context_paragraph": "In this paper we present a neural network based APE system to improve raw first-stage MT output quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt \u2192 pe and pe \u2192 mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the other systems.", "sentence": "Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach.", "cited_ids": [{"paper_id": "1964946", "citation": "Cohn et al. (2016)"}], "y": "Unlike in previous work, the authors employed prior alignment computed by a hybrid multi-alignment approach [an approach to align a monolingual mt-pe parallel corpus based on previous alignments using three different statistical methods].", "snippet_surface": "Unlike Cohn et al. (2016), the authors employed prior alignment computed by a hybrid multi-alignment approach.", "questions": {"354670.1sCVsv3pMg": "What is the hybrid multi-alignment approach?", "354670.W4Nc8+NVM2": "How is this different from Cohn et al. (2016)?"}}
{"idx": "356941", "paper_id": "7426857", "title": "Common Space Embedding of Primal-Dual Relation Semantic Spaces", "abstract": "Explicit continuous vector representation such as vector representation of words, phrases, etc. has been proven effective for various NLP tasks. This paper proposes a novel method of constructing such vector representation for both entity-pairs and relation expressions which link them in text. Based on the insight of the duality of relations, the representation is constructed by embedding of two separately constructed semantic spaces, one for entity-pairs and the other for relation expressions, into a common semantic space. By representing the two different types of objects (i.e. entity-pairs and relation expressions) in the same semantic space, we can treat the two tasks, relation mining and relation expression mining (a.k.a. pattern mining), systematically and in a unified manner. The approach is the first attempt to construct a continuous vector representation for expressions whose validity can be explicitly checked by their proximities to known sets of entity-pairs. We also experimentally validate the effectiveness of the common space for relation mining and relation expression mining.", "context_section_header": "", "context_paragraph": "The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given relation. We first construct two separate semantic spaces, one for pairs of named entities and another for relation expressions in text which link an entity-pair. A relation is supposed to correspond to a subset in each of these two spaces. The subset of entity-pairs is a set of pairs between which the relation holds. The subset is called the extension set of the relation. The subset of relation expressions consists a set of expressions which are used to link entity-pairs in the extension set.", "sentence": "While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given relation.", "cited_ids": [{"paper_id": "1646061", "citation": "Bollegala et al. (2010)"}], "y": "While Bollegala et al. (2010) uses duality [a binary relation where two sets define relations of interest (there is a set of conditions that the pair satisfies)] in the co-clustering algorithm, the authors construct an explicit semantic space which reflects the two aspects of a given relation [two semantic spaces; one aspect is for pairs of named entities and another for relation expressions linking entity-pair] of a given relation.", "snippet_surface": "While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, the authors construct an explicit semantic space which reflects the two aspects of a given relation.", "questions": {"356941.WKsMpfI40C": "What is the duality in Bollegala et al.'s co-clustering algorithm?", "356941.RHP7YGApfj": "What are the two aspects of the given relation?", "356941.6NIoK4RqPC": "How does the semantic space reflect these aspects?"}}
{"idx": "361274", "paper_id": "218502538", "title": "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction", "abstract": "Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.", "context_section_header": "", "context_paragraph": "In this paper, we propose a hill-climbing approach to unsupervised sentence summarization, directly extracting words from the source sentence. This is motivated by the observation that humanwritten reference summaries exhibit high word overlap with the source sentence, even preserving word order to a large extent. To perform word extraction for summarization, we define a scoring function -similar to Miao et al. (2019) and Zhou and Rush (2019) -that evaluates the quality of a candidate summary by language fluency, semantic similarity to the source, and a hard constraint on output length. We search towards our scoring function by first choice hill-climbing (FCHC), shown in Figure 1. We start from a random subset of words of the required output length. For each search step, a new candidate is sampled by randomly swapping a selected word and a non-selected word. We accept the new candidate if its score is higher than the current one. In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "sentence": "In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "cited_ids": [{"paper_id": "196201849", "citation": "(Zhou and Rush, 2019)"}], "y": "In contrast to beam search (Zhou and Rush, 2019), the authors' summary is not generated sequentially from the beginning of a sentence, [with first choice hill-climbing instead] and therefore not biased towards the first few words.", "snippet_surface": "In contrast to beam search (Zhou and Rush, 2019), the authors' summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "questions": {"361274.UqycvPmU7u": "What is beam search?", "361274.hzTTXw5JO0": "What is the difference between beam search and the authors' approach?"}}
{"idx": "3717", "paper_id": "141120", "title": "Sentence Simplification by Monolingual Machine Translation", "abstract": "In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems.", "context_section_header": "", "context_paragraph": "We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases. Our approach differs from Coster and Kauchak (2011) in the sense that instead of focusing on deletion in the PBMT decoding stage, we focus on dissimilarity, as simplification does not necessarily imply shortening (Woodsend and Lapata, 2011), or as the Simple Wikipedia guidelines state, \"simpler does not mean short\" 1 .  (Zhu et al., 2010). These numbers suggest that, although the selection criteria for sentences to be included in this dataset are biased (see Section 2.2), Simple Wikipedia sentences are about 17% shorter, while the average word length is virtually equal. Statistical machine translation (SMT) has already been successfully applied to the related task of paraphrasing (Quirk et al., 2004;Bannard and Callison-Burch, 2005;Madnani et al., 2007;Callison-Burch, 2008;Zhao et al., 2009;Wubben et al., 2010). SMT typically makes use of large parallel corpora to train a model on. These corpora need to be aligned at the sentence level. Large parallel corpora, such as the multilingual proceedings of the European Parliament (Europarl), are readily available for many languages. Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words (\"phrases\") in one go, solving part of the word ordering problem along the way that would be left to the target language model in a word-based SMT system. PMBT operates purely on statistics and no linguistic knowledge is involved in the process: the phrases that are aligned are motivated statistically, rather than linguistically. This makes PBMT adaptable to any language pair for which there is a parallel corpus available. The PBMT model makes use of a translation model, derived from the parallel corpus, and a language model, derived from a monolingual corpus in the target language. The language model is typically an n-gram model with smoothing. For any given input sentence, a search is carried out producing an n-best list of candidate translations, ranked by the decoder score, a complex scoring function including likelihood scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, pro-vided that a parallel corpus is available that pairs text to simplified versions of that text.", "sentence": "We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases.", "cited_ids": [{"paper_id": "15636533", "citation": "Zhu et al. (2010)"}], "y": "The authors differ from the approach of Zhu et al. (2010) in the sense that they do not take syntactic information into account; they rely on PBMT [Phrase-Based Machine Translation] to do its work and implicitly learn simplifying paraphrasings of phrases.", "snippet_surface": "The authors differ from the approach of Zhu et al. (2010) in the sense that they do not take syntactic information into account; they rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases.", "questions": {"3717.gNI1HcCVTX": "What is PBMT?", "3717.7DuIFAlakh": "How does PBMT learn simplifying paraphrasings of phrases?", "3717.qtvDtiBjv2": "What approach did Zhu et al. take?"}}
{"idx": "372943", "paper_id": "5050185", "title": "Cross Language Text Classification by Model Translation and Semi-Supervised Learning", "abstract": "In this paper, we introduce a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semi-supervised learning, and adapt the translated model to better fit the data distribution of the target language.", "context_section_header": "", "context_paragraph": "In this paper, we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm. Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features. The translated model serves as an initial classifier for a semi-supervised process, by which the model is further adjusted to fit the distribution of the target language. Our method does not require any labeled data in the target language, nor a machine translation system. Instead, the only requirement is a reasonable amount of unlabeled data in the target language, which is often easy to obtain.", "sentence": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features.", "cited_ids": [{"paper_id": "1876524", "citation": "(Fortuna and Shawe-Taylor, 2005)"}], "y": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), the authors' method [to automatically build text classifiers in a new language by training on already labeled data in another language] takes into account different possible translations for [individual words from a bag-of-words feature functions rather than just leveraging the single \"best\" translation].", "snippet_surface": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), the authors' method takes into account different possible translations for model features.", "questions": {"372943.WXLr73c7Ew": "What are previous methods based on machine translation?", "372943.JEFCwuEewN": "What does our method take into account?"}}
{"idx": "373206", "paper_id": "18253563", "title": "Large-scale Word Alignment Using Soft Dependency Cohesion Constraints", "abstract": "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.", "context_section_header": "", "context_paragraph": "There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a \"loosely\" tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So 2006a)    out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and they trained the model with only 100 hand-annotated sentence pairs. Therefore, their method is not suitable for large-scale tasks. In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "sentence": "In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "cited_ids": [{"paper_id": "2787289", "citation": "Cherry and Lin (2006b)"}], "y": "The authors also use dependency cohesion, [which is the assumption that disjoint dependency subtrees remain disjoint after translation,] as a soft constraint. But, unlike Cherry and Lin (2006b), they integrate the soft dependency cohesion constraint into a [HMM word alignment] generative model that is more suitable for large-scale word alignment tasks", "snippet_surface": "The authors also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), they integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "questions": {"373206.K2PDQhV0d5": "What is dependency cohesion?", "373206.wyct1xTxFy": "What is the difference between Cherry and Lin (2006b) and the authors' approach?", "373206.xA+84WON8k": "How is the generative model more suitable for large-scale word alignment tasks?"}}
{"idx": "374871", "paper_id": "14171478", "title": "A Graph-Based Approach to String Regeneration", "abstract": "The string regeneration problem is the problem of generating a fluent sentence from a bag of words. We explore the Ngram language model approach to string regeneration. The approach computes the highest probability permutation of the input bag of words under an N-gram language model. We describe a graph-based approach for finding the optimal permutation. The evaluation of the approach on a number of datasets yielded promising results, which were confirmed by conducting a manual evaluation study.", "context_section_header": "", "context_paragraph": "In contrast to the baseline N-gram approach by Wan et al. (2009), our approach finds optimal solutions. We built several models based on 2-gram, 3-gram, and 4-gram language models. We experimentally evaluated the graph-based approach on several datasets. The BLEU scores and example output indicated that our approach is successful in constructing a fairly fluent version of the original sentence. We confirmed the results of automatic evaluation by conducting a manual evaluation. The human judges were asked to compare the outputs of two systems and decide which is more fluent. The results are statistically significant and confirm the ranking of the systems obtained using the BLEU scores. Additionally, we explored computing approximate solutions with time constraints. We found that approximate solutions significantly decrease the quality of the output compared to optimal ones. This paper describes work conducted in the MPhil thesis by Horvat (2013).", "sentence": "In contrast to the baseline N-gram approach by Wan et al. (2009), our approach finds optimal solutions.", "cited_ids": [{"paper_id": "10752264", "citation": "Wan et al. (2009)"}], "y": "In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach[, which is a graph-based approach to evaluate the highest probability permutation from a bag of words,] finds optimal solutions.In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach finds optimal solutions [for N-gram language modeling].", "snippet_surface": "In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach finds optimal solutions.", "questions": {"374871.8Cnaf37sz/": "What is the baseline N-gram approach?", "374871.nsy6KcoISw": "What does optimal solution mean in this context?"}}
{"idx": "375620", "paper_id": "237421077", "title": "Semi-Automated Labeling of Requirement Datasets for Relation Extraction", "abstract": "Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.", "context_section_header": "", "context_paragraph": "2 Related Work Gamallo et al. (2012) propose a simple Open Information Extraction system based on dependency parse trees. The algorithm extracts triples with two arguments and a sentence part relating those. However, the patterns are not very sophisticated and put a large part of the sentence into the relation. Hence, this approach is not suitable for our use case as we would eventually like to generate object diagrams from the relations we extracted. Erkan et al. (2007) use dependency parse trees to extract relations between proteins from sentences. They do so by classifying whether a sentence, given a dependency tree, describes a relation between any pair of proteins occurring in the sentence using semi-supervised harmonic functions and support vector machines. However, their entities (the protein names) are already annotated which is not the case if we only have the raw sentences as in our approach. Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike our approach which does not require to annotate any data manually but instead to produce patterns. While this approach might be able to extract simple triples well, one needs either a larger annotated dataset, defeating the purpose of our work, or the patterns might not generalize well, thus being unsuitable for constructing a qualitative annotated corpus. Reddy et al. (2016) propose an algorithm to automatically extract logical expressions from dependency parse trees for question answering. These were then converted into a graph indicating the relations between the named entities in the sentence by applying semantic parsing. However, this approach always converts the entire sentence into a graph and may include information that is irrelevant for a dataset that is to be generated. Inago et al. (2019) use a rule-based approach on dependency trees to process natural language car parking instructions with decision trees for automated driving systems. Unlike our data (or most datasets in general), sentences of the application domain are very short and similar in structure. While our approach could be effectively converted into a decision tree, it is easier to construct rules with our pattern engine for more complex data.", "sentence": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike our approach which does not require to annotate any data manually but instead to produce patterns.", "cited_ids": [{"paper_id": "74065", "citation": "Mausam et al. (2012)"}], "y": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset [ from the ClueWeb 3 corpus] to automatically generate patterns for information extraction, unlike the authors' approach which does not require any data to be annotated manually but instead to produce [sequences of triples].", "snippet_surface": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike the authors' approach which does not require any data to be annotated manually but instead to produce patterns.", "questions": {"375620.a/EpST5/uq": "What is the labeled bootstrap dataset?", "375620.1T5jKN8Klt": "How does their approach generate patterns?", "375620.4/ZrOYExL+": "How does the authors' approach produce patterns?"}}
{"idx": "376367", "paper_id": "17078659", "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "abstract": "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.", "context_section_header": "", "context_paragraph": "Research on topic adaptation most closely related to our work was performed by (Hasler et al., 2014), but the features proposed there were added to the log-linear model of a phrase-based system. Here, we use the topic information as part of the input to the NMT system. Another difference is that we primarily work with human-labeled topics, whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "sentence": "Another difference is that we primarily work with human-labeled topics, whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "cited_ids": [{"paper_id": "7161937", "citation": "(Hasler et al., 2014)"}], "y": "A difference is that the authors primarily work with human-labeled topics [product categories in an e-commerce setting], whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "snippet_surface": "Another difference is that the authors primarily work with human-labeled topics, whereas (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "questions": {"376367.5AW/zJaL1j": "What are human-labeled topics?", "376367.dJ1drzGBLq": "How is the topic distribution inferred automatically from data?"}}
{"idx": "386656", "paper_id": "16263338", "title": "Enriching SMT Training Data via Paraphrasing", "abstract": "This paper proposes a novel method to resolve the coverage problem of SMT system. The method generates paraphrases for source-side sentences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. Within a statistical paraphrase generation framework, we employ an object function, named Sentence Novelty, to select paraphrases which having the most novel information to the bilingual training corpus of the SMT model. Meanwhile, the context is considered via a language model in the source language to ensure the fluency and accuracy of paraphrase substitution. Compared to a state-of-the-art phrase based SMT system (Moses), our method achieves an improvement of 1.66 points in terms of BLEU on a small training corpus which simulates a resource-poor environment, and 1.06 points on a training corpus of medium size.", "context_section_header": "", "context_paragraph": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen-tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "sentence": "Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "cited_ids": [{"paper_id": "791881", "citation": "Callison-Burch et al. (2006)"}], "y": "The authors' results [resolve the coverage problem of SMT system] indicate that their method [a statistical paraphrase generating (SPG) model] gains a signficant improvement [in terms of BLEU] over the method of Callison-Burch et al. (2006).", "snippet_surface": "The authors' results indicate that their method gains a significant improvement over the method of Callison-Burch et al. (2006).", "questions": {"386656.noJzJpmyQ/": "No questions."}}
{"idx": "391591", "paper_id": "227231569", "title": "Improving Low-Resource NMT through Relevance Based Linguistic Features Incorporation", "abstract": "In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.", "context_section_header": "", "context_paragraph": "The above point motivates us to conduct the present research. We hypothesize that only including the features alongside the word by using a generalized embedding layer or forming a composite representation by taking all features together does not exploit the features completely. There should be some mechanism which can justify the relationship between a word and its supporting features as well. Driven by this idea, we come up with two simple strategies which measure the relevance of the word and the feature embeddings obtained from the output of the embedding layers. The first one is self relevance which considers the relevance of a feature with respect to itself. We apply an attention function to the feature embedding, which in turn generates a mask determining the importance of that feature. Finally, the mask is applied on the feature to effectuate the attention. Our second approach considers the feature relevance with respect to the corresponding word which is the most vital component of a source token. In this case, the attention function operates on a word-feature pair and returns the mask determining the word-based relevance of the input feature. For experimentation, we choose the Transformer network of Vaswani et al. (2017) and assess our proposed techniques on eight low-resource language pairs having diverse morphological variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of  and Vaswani et al. (2017). In the next section, the related works are briefly described.", "sentence": "We achieve up to 3.09 BLEU points gain over the standard baseline models of  and Vaswani et al. (2017).", "cited_ids": [{"paper_id": "13756489", "citation": "Vaswani et al. (2017)"}], "y": "[Using two methods for translation tasks, namely self relevance and word-based relevance,] the authors achieve up to 3.09 BLEU points gain over the [transformer models used in previous papers] of and Vaswani et al. (2017).", "snippet_surface": "The authors achieve up to 3.09 BLEU points gain over the standard baseline models of and Vaswani et al. (2017).", "questions": {"391591.n4Sjaq8lO/": "What are the standard baseline models?", "391591.3vPUDoP8MZ": "What does BLEU points measure?"}}
{"idx": "391894", "paper_id": "218613715", "title": "BioMRC: A Dataset for Biomedical Machine Reading Comprehension", "abstract": "We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.", "context_section_header": "", "context_paragraph": "In this paper, we introduce BIOMRC, a new dataset for biomedical MRC that can be viewed as an improved version of BIOREAD. To avoid crossing sections, extracting text from references, captions, tables etc., we use abstracts and titles of biomedical articles as passages and questions, respectively, which are clearly marked up in PUBMED data, instead of using the full text of the articles. Using titles and abstracts is a decision that favors precision over recall. Titles are likely to be related to their abstracts, which reduces the noise-tosignal ratio significantly and makes it less likely to generate irrelevant questions for a passage. We replace a biomedical entity in each title with a placeholder, and we require systems to guess the hidden entity by considering the entities of the abstract as candidate answers. Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a repository that provides approximately 25 million abstracts and their corresponding titles from PUBMED, with multiple annotations. 2 We use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013). We also perform several checks, discussed below, to discard passage-question instances that are too easy, and we show that the accuracy of experts and nonexpert humans reaches 85% and 82%, respectively, on a sample of 30 instances for each annotator type, which is an indication that the new dataset is indeed less noisy, or at least that the task is more feasible for humans. Following Pappas et al. (2018), we release two versions of BIOMRC, LARGE and LITE, containing 812k and 100k instances respectively, for researchers with more or fewer resources, along with the 60 instances (TINY) humans answered. Random samples from BIOMRC LARGE where selected to create LITE and TINY. BIOMRC TINY is used only as a test set; it has no training and validation subsets.", "sentence": "2 We use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013).", "cited_ids": [{"paper_id": "18987749", "citation": "(Leaman et al., 2013)"}], "y": "The authors use DNORM's [Disease Name Normalization] biomedical entity annotations, which are more accurate than METAMAP's [Metathesaurus Mapping] (Leaman et al., 2013).", "snippet_surface": "The authors use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013).", "questions": {"391894.Xh95pF4JXo": "What is DNORM?", "391894.ajE3PCFVB6": "What are METAMAP's annotations?"}}
{"idx": "400200", "paper_id": "7925642", "title": "Collaborative Partitioning for Coreference Resolution", "abstract": "This paper presents a collaborative partitioning algorithm\u2014a novel ensemble-based approach to coreference resolution. Starting from the all-singleton partition, we search for a solution close to the ensemble\u2019s outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the ensemble and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).", "context_section_header": "", "context_paragraph": "Although coreference is formally a partitioning problem, the setting is rather different from a typical clustering scenario. Thus, individual mentions and mention properties are very important for coreference and should carefully be assessed one by one. The resolution clues are very heterogeneous and different elements (mentions) of clusters (entities) can be rather dissimilar in a strict sense. This is why, for example, clustering evaluation measures are not reliable for coreferenceand, indeed, task-specific metrics have been put forward. While algorithms of (Strehl and Ghosh, 2003) constitute the state of the art in the ensemble clustering in general, we propose a coreferencespecific approach. More specifically, (i) while Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, we explicitly integrate coreference metrics, such as MUC and MELA and (ii) since our partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task. In our future work, we plan to evaluate our approach against the general-purpose algorithms proposed in (Strehl and Ghosh, 2003).", "sentence": "More specifically, (i) while Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, we explicitly integrate coreference metrics, such as MUC and MELA and (ii) since our partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task.", "cited_ids": [{"paper_id": "3068944", "citation": "Strehl and Ghosh (2003)"}], "y": "(i) While Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, the authors explicitly integrate coreference metrics, such as MUC and MELA [average of MUC and two other coreference metrics, B3 and CEAFE] and (ii) Since the authors' partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task [creating an ensemble-based approach to coreference resolution].", "snippet_surface": "More specifically, (i) Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, whereas the authors explicitly integrate coreference metrics, such as MUC and MELA and (ii) since the authors' partitions are much smaller than typical clustering outputs, they can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task.", "questions": {"400200.GIFqHXE/rw": "What are \"MUC\" and \"MELA\"?", "400200.vXwkwjRmcN": "How does the search for mutual information's maximum differ from a greedy agglomerative search strategy?"}}
{"idx": "401162", "paper_id": "9286820", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.", "context_section_header": "", "context_paragraph": "Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of Gubbins and Vlachos (2013), while achieving slightly worse than the recent state-of-the-art results by Mnih and Kavukcuoglu (2013). Finally, we make the code and preprocessed data available to facilitate comparisons with future work.", "sentence": "Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "cited_ids": [{"paper_id": "15076873", "citation": "Mikolov et al. (2011)"}], "y": "The authors' results show that the dependency RNN language model proposed[, which makes use of the syntactic dependency parse of a sentence instead of considering tokens in a sentence in the order they appear,] outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "snippet_surface": "The authors' results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "questions": {"401162.ybw8V5Vvnf": "What is the dependency RNN language model?", "401162.JI6WSAKDpb": "How is accuracy measured?"}}
{"idx": "401269", "paper_id": "5747513", "title": "Semi-Supervised Bootstrapping of Relationship Extractors with Distributional Semantics", "abstract": "Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. We research bootstrapping for relationship extraction using word embeddings to find similar relationships. Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TFIDF to find similar relationships.", "context_section_header": "", "context_paragraph": "We implemented these ideas in BREDS, a bootstrapping system for RE based on word embeddings. BREDS was evaluated with a collection of 1.2 million sentences from news articles. The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "sentence": "The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "cited_ids": [{"paper_id": "7579604", "citation": "Agichtein and Gravano (2000)"}], "y": "The experimental results show that the authors method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations [to expand the seed set with new relationship instances, while limiting the semantic drift through the bootstrapping system].", "snippet_surface": "The experimental results show that the authors' method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "questions": {"401269.rtXuYDW4B+": "What is the baseline bootstrapping system?", "401269.wLAnHPB6cd": "What is TF-IDF?", "401269.8VacaLME2J": "How does our method outperform the baseline?"}}
{"idx": "408504", "paper_id": "52979524", "title": "SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task", "abstract": "Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5% in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at https://yale-lily.github.io/seq2sql/spider.", "context_section_header": "", "context_paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "sentence": "Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module.", "cited_ids": [{"paper_id": "13529592", "citation": "(Rabinovich et al., 2017)"}], "y": "[This work first differentiates itself from previous other work by using a SQL specific grammar instead of a AST. Additionally, different sequence-to-set modules are used to avoid the \"ordering issue\". Lastly] different from (Rabinovich et al., 2017), the authors pass a pre-order traverse of SQL decoding history to each module [representing a component of the grammar].", "snippet_surface": "Third, differently from (Rabinovich et al., 2017), the authors pass a pre-order traverse of SQL decoding history to each module.", "questions": {"408504.BSw7NS2ok1": "What is the approach of Rabinovich et al. (2017)?", "408504.9Ra1BKxDWm": "What is a pre-order traverse?", "408504.y18xhMFf0U": "What is the purpose of passing the pre-order traverse to each module?"}}
{"idx": "413936", "paper_id": "1774259", "title": "Bidirectional Recurrent Convolutional Neural Network for Relation Classification", "abstract": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neural networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset.", "context_section_header": "", "context_paragraph": "Convolutional neural works are widely used in relation classification. Zeng et al. (2014) proposed an approach for relation classification where sentence-level features are learned through a CNN, which has word embedding and position features as its input. In parallel, lexical features were extracted according to given nouns. dos Santos et al. (2015) tackled the relation classification task using a convolutional neural network and proposed a new pairwise ranking loss function, which achieved the state-of-the-art result in SemEval-2010 Task 8. Yu et al. (2014) proposed a Factor-based Compositional Embedding Model (FCM) by deriving sentence-level and substructure embeddings from word embeddings, utilizing dependency trees and named entities. It achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "sentence": "It achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "cited_ids": [{"paper_id": "12873739", "citation": "Zeng et al. (2014)"}], "y": "The authors achieved slightly higher accuracy on the same dataset than Zeng et al. (2014) [by using a bidirectional neural network model], but only when syntactic information [chunking and parse tree features] is used.", "snippet_surface": "The authors achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "questions": {"413936.qKJCXKHR3r": "What dataset was used?", "413936.3qC1kt6vCs": "What is syntactic information?", "413936.qFdSJT+sKZ": "How did accuracy differ with and without syntactic information?"}}
{"idx": "414396", "paper_id": "13875176", "title": "Relation Extraction from Community Generated Question-Answer Pairs", "abstract": "Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users\u2019 interests. Traditional methods for relation extraction from natural language text operate over individual sentences. However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question. This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences. Experiments on 2 publicly available datasets demonstrate that the model can extract from 20% to 40% additional relation triples, not extracted by existing sentence-based models.", "context_section_header": "", "context_paragraph": "Community question-answering data has been a subject of active research during the last decade. Bian et al. (2008) and Shtok et al. (2012) show how such data can be used for question answering, an area with a long history of research, and numerous different approaches proposed over the decades (Kolomiyets and Moens, 2011). One particular way to answer questions is to utilize structured KBs and perform semantic parsing of questions to transform natural language questions into KB queries. Berant et al. (2013) proposed a semantic parsing model that can be trained from QnA pairs, which are much easier to obtain than correct KB queries used previ-ously. However, unlike our approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities. Later Yao and Van Durme (2014) presented an information extraction inspired approach, that predicts which of the entities related to an entity in the question could be the answer to the question.", "sentence": "However, unlike our approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities.", "cited_ids": [{"paper_id": "6401679", "citation": "Berant et al. (2013)"}], "y": "\"However, unlike the authors' approach, which takes noisy answer text [from real users] provided by a CQA website user [to predict relations between entities mentioned in the question and answer], the work of Berant et al. (2013) uses manually created answers in a form of single or lists of [knowledge base (KB)] entities [(named entities, proper nouns or a sequence of at least two tokens)].", "snippet_surface": "However, unlike the authors' approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities.", "questions": {"414396.YlRkYmUAUk": "What is a CQA website?", "414396.C7pnH/lcEV": "What is a KB entity?", "414396./EDWVswf2L": "What is the difference between the authors' approach and Berant et al.'s approach?"}}
{"idx": "41947", "paper_id": "53093808", "title": "What can we gain from language models for morphological inflection?", "abstract": "This paper investigates the attempts to augment neural-based inflection models with characterbased language models. We found that in most cases this slightly improves performance, however, the effect is marginal. We also propose another language-model based approach that can be used as a strong baseline in low-resource setting.", "context_section_header": "", "context_paragraph": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.", "sentence": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "cited_ids": [{"paper_id": "28704517", "citation": "(Makarov et al., 2017)"}], "y": "The authors expected to improve performance [measured by percentage points in average accuracy in the Sigmorphon 2018 shared task,] especially in low and medium resource settings [which is a 1000 word training dataset]. Their approach does not have clear advantages: their joint system[, which combines the model from Makarov et al., 2017 with a language model component following the architecture of Gulcehre,] is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "snippet_surface": "The authors expected to improve performance especially in low and medium resource settings, however, their approach does not have clear advantages: their joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "questions": {"41947./RGPrRNlwK": "What is the baseline system of Makarov et al.?", "41947.6+YBhMqQQd": "What are \"low and medium resource settings\"?", "41947.7DsxUaOEdi": "What are the advantages of the joint system?"}}
{"idx": "425823", "paper_id": "29306617", "title": "Coarse \u201csplit and lump\u201d bilingual language models for richer source information in SMT", "abstract": "Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these \u201ccoarse models\u201d. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing \u201cgeneric\u201d coarse configuration chosen on English > French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English > French, +0.35 for French > English, +1.0 for Arabic > English, and +0.6 for Chinese > English.", "context_section_header": "", "context_paragraph": "This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM -a coarse reordering model -none of the Eng<>Fre experiments do. In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data. Many experiments in this paper involve coarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013;Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one.", "sentence": "In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data.", "cited_ids": [{"paper_id": "18721941", "citation": "Wuebker et al (2013)"}], "y": "The authors also studied coarse phrase translation models in prior experiments [that used discriminative heirarchical RMs], but found that, unlike Wuebker et al (2013), they did not yield significant improvements to the [machine translation] system, except when there is little training data.", "snippet_surface": "The authors also studied coarse phrase translation models in prior experiments, but found that, unlike Wuebker et al (2013), they did not yield significant improvements to the system, except when there is little training data.", "questions": {"425823.vgLZeCgNIE": "What are coarse phrase translation models?", "425823.pOSG27KM2G": "What did Wuebker et al (2013) find?", "425823.n1WSAFAwbB": "What kind of improvement did we find?"}}
{"idx": "434565", "paper_id": "11553116", "title": "A Seed-driven Bottom-up Machine Learning Framework for Extracting Relations of Various Complexity", "abstract": "A minimally supervised machine learning framework is described for extracting relations of various complexity. Bootstrapping starts from a small set of n-ary relation instances as \u201cseeds\u201d, in order to automatically learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Performance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition approaches.", "context_section_header": "", "context_paragraph": "We propose a rule representation that supports this strategy. Therefore, our learning approach is seed-driven and bottom-up. Here we use dependency trees as input for pattern extraction. We consider only trees or their subtrees containing seed arguments. Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account. Our pattern rule ranking and filtering method considers two aspects of a pattern: its domain relevance and the trustworthiness of its origin. We tested our framework in two domains: Nobel Prize awards and management succession. Evaluations have been conducted to investigate the performance with respect to the seed parameters: the number of seeds and the influence of data size and its redundancy property. The whole system has been evaluated for the two domains considering precision and recall. We utilize the evaluation strategy \"Ideal Matrix\" of Agichtein and Gravano (2000) to deal with unannotated test data.", "sentence": "Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "cited_ids": [{"paper_id": "16749388", "citation": "Sudo et al., (2003)"}], "y": "Therefore, the authors' method, [a seed-driven machine learning approach that uses dependency trees as input for relation extraction], is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "snippet_surface": "Therefore, the authors' method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "questions": {"434565.E1UJh2CU9W": "What is the subtree model of Sudo et al., (2003)?", "434565.Hi5y3nNytl": "What does the subtree model consider?", "434565.g4CYGu89yl": "How is the efficiency of the subtree model compared to the authors' method?"}}
{"idx": "43855", "paper_id": "243865635", "title": "A Language Model-based Generative Classifier for Sentence-level Discourse Parsing", "abstract": "Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing.", "context_section_header": "", "context_paragraph": "As for the base parser, we retrained two models, 2-stage Parser 5 (Wang et al., 2017) and span-based Parser 6 (Kobayashi et al., 2020). Different from the setting of Lin et al. (2019), we retrained 2stage Parser in the sentence-level rather than in the document-level. Since the experimental re-4 https://github.com/PKU-TANGENT/ NeuralEDUSeg 5 https://github.com/yizhongw/StageDP 6 https://github.com/nttcslab-nlp/ Top-Down-RST-Parser sults show our retrained 2-stage Parser achieved the highest F 1 scores among several parsers (See Appendix C), we selected it as our base parser in the following experiments.", "sentence": "Different from the setting of Lin et al. (2019), we retrained 2stage Parser in the sentence-level rather than in the document-level.", "cited_ids": [{"paper_id": "153312413", "citation": "Lin et al. (2019)"}], "y": "Different from the setting of Lin et al. (2019) the authors retrained 2stage Parser in the sentence-level rather than in the document-level. [This parser being a model for sentence-level discourse parsing, which extracts several features and does classification with SVMs in two stages.]", "snippet_surface": "Different from the setting of Lin et al. (2019), the authors retrained 2stage Parser in the sentence-level rather than in the document-level.", "questions": {"43855.1ULPIeJEUD": "What is the setting of Lin et al. (2019)?", "43855.0bzevOrDkh": "What does retraining 2stage Parser mean?", "43855.FNCTN30CCq": "What is the difference between sentence-level and document-level?"}}
{"idx": "44656", "paper_id": "6397366", "title": "Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars", "abstract": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "context_section_header": "", "context_paragraph": "It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "sentence": "When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "cited_ids": [{"paper_id": "14829769", "citation": "(Smith and Eisner, 2004)"}], "y": "When applied to unsupervised grammar learning, [deterministic annealing] has been shown to lead to worse parsing accuracy than [the standard expectation-maximization algorithm] (Smith and Eisner, 2004); in contrast, the authors show that their approach, [unambiguity regularization for unsupervised learning of probabilistic natural language grammars, which introduces an inductive bias into grammar learning based on the observation that natural language is remarkably unambiguous], leads to significantly higher parsing accuracy than standard [expectation-maximization] in unsupervised dependency grammar learning.", "snippet_surface": "When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, the authors show that their approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "questions": {"44656.jY0lUKoUof": "What is \"DA\"?", "44656.Vt4oe8XET6": "What is \"EM\"?", "44656.hGNLOGYIkn": "What is unsupervised dependency grammar learning?"}}
{"idx": "447642", "paper_id": "381368", "title": "Generating and Validating Abstracts of Meeting Conversations: a User Study", "abstract": "In this paper we present a complete system for automatically generating natural language abstracts of meeting conversations. This system is comprised of components relating to interpretation of the meeting documents according to a meeting ontology, transformation or content selection from that source representation to a summary representation, and generation of new summary text. In a formative user study, we compare this approach to gold-standard human abstracts and extracts to gauge the usefulness of the different summary types for browsing meeting conversations. We find that our automatically generated summaries are ranked significantly higher than human-selected extracts on coherence and usability criteria. More generally, users demonstrate a strong preference for abstract-style summaries over extracts.", "context_section_header": "", "context_paragraph": "While extraction remains the most common ap-proach to text summarization, one application in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain (Portet et al., 2009) and data from gas turbine sensors (Yu et al., 2007). Our approach is similar except that our input is text data in the form of conversations. We otherwise utilize a very similar architecture of pattern recognition, pattern abstraction, pattern selection and summary generation. Kleinbauer et al. (2007) carry out topic-based meeting abstraction. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "sentence": "Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "cited_ids": [{"paper_id": "6118869", "citation": "(Carletta et al., 2005)"}], "y": "The authors' system differs [from Kleinbaur et al.'s] in two major respects: the summarization process [Kleinbaur et al.] use utilizes human gold-standard annotations of topic segments, topic labels and content items from the ontology, while the authors' summarizer is fully automatic; secondly, the ontology used by them is specific not just to meetings but to the AMI scenario meetings [from the AMI corpus where participants play roles in a fictitious company] (Carletta et al., 2005), while the authors' ontology applies to conversations in general, allowing their approach to be extended to emails, blogs, etc.", "snippet_surface": "The authors' systems differ in two major respects: the summarization process used by them utilizes human gold-standard annotations of topic segments, topic labels and content items from the ontology, while the authors' summarizer is fully automatic; secondly, the ontology used by them is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while the authors' ontology applies to conversations in general, allowing their approach to be extended to emails, blogs, etc.", "questions": {"447642.QbH/2b3Us8": "What are the gold-standard annotations?", "447642.qdXA71bb3e": "What is the AMI scenario meetings?", "447642.anap9LlDnD": "How does our ontology apply to conversations in general?"}}
{"idx": "448821", "paper_id": "226262372", "title": "Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots", "abstract": "Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.", "context_section_header": "", "context_paragraph": "In the third decoupling forward step, we feed e to the same BiLSTM model and calculate a new adversarial loss L . The final loss is a weighted sum of L and L controlled by a hyperparameter \u03b1 3 :   Baselines For a fair comparison, we use the same slot filling architecture BiLSTM (Liu and Lane, 2016) as (Kim et al., 2019;Ray et al., 2019). Kim et al. (2019) proposes two model variants, where random noise means adding random noise in the embeddings of all slot words and cw represents concatenating the context word window as input. Note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from our method. Our adversarial semantic decoupling method can take into account the impact of different contexts (global semantics) on local semantics, thereby enabling more accurate decoupling. Ray et al. (2019) proposes greedy delex and iterative delex methods for open-vocabulary slots. We also validate our method in the BERT-based models (Devlin et al., 2019) for comprehensive analysis.", "sentence": "Note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from our method.", "cited_ids": [{"paper_id": "186206148", "citation": "(Kim et al., 2019)"}], "y": "The authors note that the random noise [(added in the embeddings of all slot words)] in Kim et al., 2019 is independently sampled regardless of the global context, which is significantly different from the authors' [adversarial semantic decoupling method].", "snippet_surface": "The authors note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from their method.", "questions": {"448821.1jymxz9dZj": "What is the random noise in Kim et al.'s method?", "448821.uVVMjqXSmt": "How does it differ from the authors' method?"}}
{"idx": "450638", "paper_id": "7017683", "title": "Function Assistant: A Tool for NL Querying of APIs", "abstract": "In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.", "context_section_header": "", "context_paragraph": "In this paper, we focus on the first usage of our tool. To explore building models for new API collections, we run our pipeline on 27 open source Python projects from the well-known Awesome Python project list. 1 As in previous work, we perform synthetic experiments on these datasets, which measure how well our models can generate function representations for unseen API descriptions, which mimic user queries. 1 github.com/vinta/awesome-python 2 Related Work Natural language querying of APIs has long been a goal in software engineering, related to the general problem of software reuse (Krueger, 1992). To date, a number of industrial scale products are available in this area. 2 To our knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from our methods that use more conventional NLP components and techniques. As we show in this paper and in RK, term matching and related techniques can sometimes serve as a competitive baseline, but are almost always outperformed by our translation approach.", "sentence": "2 To our knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from our methods that use more conventional NLP components and techniques.", "cited_ids": [{"paper_id": "15813825", "citation": "(Lv et al., 2015)"}], "y": "To the authors' knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from the authors' methods [for querying and exploring source code repositories using natural language, stochastic gradient ascent algorithm under a maximum conditional log-likelihood objective] that use more conventional NLP components and techniques.", "snippet_surface": "To the authors' knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from the authors' methods that use more conventional NLP components and techniques.", "questions": {"450638.IpRr0vgKwf": "What are shallow term matching and information-extraction techniques?", "450638.EBh8Y+j2Gm": "What are the more conventional NLP components and techniques?"}}
{"idx": "454256", "paper_id": "202777021", "title": "Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders", "abstract": "Understanding text often requires identifying meaningful constituent spans such as noun phrases and verb phrases. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the model\u2019s labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19% relative error reduction).", "context_section_header": "", "context_paragraph": "Our code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing (19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people).", "sentence": "Our code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing (19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags).", "cited_ids": [{"paper_id": "5930290", "citation": "Haghighi and Klein 2006"}], "y": "The authors' code-enhanced DIORA architecture, which outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10, [a corpus comprising sentences with 10 or fewer words from the WSJ section of the English Penn treebank] when labelling a gold bracketing (19% relative error reduction over the previous best model Haghighi and Klein, 2006, which unlike the authors' approach uses gold part-of-speech tags.", "snippet_surface": "The authors' code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing, resulting in a 19% relative error reduction over the previous best model (Haghighi and Klein 2006), which unlike the authors' approach uses gold part-of-speech tags.", "questions": {"454256.FpLodXuiyQ": "What is the DIORA architecture?", "454256.vpT4hj75Ne": "What is the F1 score?", "454256.llbHmoBJ3j": "What is WSJ-10?"}}
{"idx": "455133", "paper_id": "15876808", "title": "Guided Learning for Bidirectional Sequence Classification", "abstract": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.", "context_section_header": "", "context_paragraph": "Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004;Daum\u00e9 III and Marcu, 2005) for guided learning. We apply this algorithm to POS tagging, a classic sequence learning problem. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005).", "sentence": "Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features.", "cited_ids": [{"paper_id": "14835360", "citation": "(Toutanova et al., 2003)"}], "y": "The authors' system [a novel learning algorithm for bidirectional sequence classification] reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system [using feature-rich part-of-speech tagging with a cyclic dependency network](Toutanova et al., 2003) by using fewer features.", "snippet_surface": "The authors' system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features.", "questions": {"455133.pTu9+N721t": "What is the standard PTB test set?", "455133./DvA9s/dss": "What are the features used in the previous best system?", "455133.2tzQP67IoF": "What is the error rate of the previous best system?"}}
{"idx": "45874", "paper_id": "8692445", "title": "Learning Field Compatibilities to Extract Database Records from Unstructured Text", "abstract": "Named-entity recognition systems extract entities such as people, organizations, and locations from unstructured text. Rather than extract these mentions in isolation, this paper presents a record extraction system that assembles mentions into records (i.e. database tuples). We construct a probabilistic model of the compatibility between field values, then employ graph partitioning algorithms to cluster fields into cohesive records. We also investigate compatibility functions over sets of fields, rather than simply pairs of fields, to examine how higher representational power can impact performance. We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches.", "context_section_header": "", "context_paragraph": "McCallum and Wellner (2005) discriminatively train a model to learn binary coreference decisions, then perform joint inference using graph partitioning. This is analogous to our work, with two distinctions. First, instead of binary coreference decisions, our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance. These higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also,  automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper.", "sentence": "Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices.", "cited_ids": [{"paper_id": "11747348", "citation": "McCallum and Wellner (2005)"}], "y": "[The authors aim to extract contact records from homepages]. McCallum and Wellner (2005) factor [what their models learn before producing results] into pairs of vertices, but their compatibility decisions [used to determine the compatibility functions over sets of fields] are made between sets of vertices.", "snippet_surface": "The authors note that, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, their compatibility decisions are made between sets of vertices.", "questions": {"45874.RUmAE9CmWo": "What is the approach of McCallum and Wellner (2005)?", "45874.C2oAc01sfZ": "What are coreference decisions?", "45874.tGocloJ3TK": "What are compatibility decisions?"}}
{"idx": "470003", "paper_id": "227230509", "title": "Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction", "abstract": "Bilingual dictionary induction (BDI) is the task of accurately translating words to the target language. It is of great importance in many low-resource scenarios where cross-lingual training data is not available. To perform BDI, bilingual word embeddings (BWEs) are often used due to their low bilingual training signal requirements. They achieve high performance, but problematic cases still remain, such as the translation of rare words or named entities, which often need to be transliterated. In this paper, we enrich BWE-based BDI with transliteration information by using Bilingual Orthography Embeddings (BOEs). BOEs represent source and target language transliteration word pairs with similar vectors. A key problem in our BDI setup is to decide which information source \u2013 BWEs (or semantics) vs. BOEs (or orthography) \u2013 is more reliable for a particular word pair. We propose a novel classification-based BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on English-Russian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity.", "context_section_header": "", "context_paragraph": "In this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities. Our aim is to improve BDI systems in two respects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation. We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information. For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors. We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr. seq2seqTr can also be used to extract candidate transliterations for a source word. It is applicable to any language pair (as opposed to Levenshtein distance). To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be semantically translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017), exploiting our pretrained encoder from seq2seqTr. In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision. We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks.", "sentence": "Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set.", "cited_ids": [{"paper_id": "218594574", "citation": "Severini et al. (2020)"}], "y": "The authors show that their classification system [(seq2seqTr)] is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set [(test dictionaries released in three frequency categories: high, middle and low)].", "snippet_surface": "The authors show that their classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set.", "questions": {"470003.SGru/ME/El": "What is the classification system?", "470003.+jvryW4aqh": "What is ensembling?", "470003.kFAdD/OBxb": "What is a frequency set?"}}
{"idx": "470296", "paper_id": "864605", "title": "SPred: Large-scale Harvesting of Semantic Predicates", "abstract": "We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ) and learn the semantic classes that best fit the argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner\u2019s Dictionary with high precision and recall, and perform well against the most similar approach.", "context_section_header": "", "context_paragraph": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "sentence": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "cited_ids": [{"paper_id": "743925", "citation": "Kozareva and Hovy (2010)"}], "y": "The closest technical approach to that of the authors is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns [lexical predicates like \"work for\" or \"fly to\"]. Whereas Kozareva and Hovy's approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, the authors' proposed method [using only recursive relation patterns to create large repositories of semantic predicates] requires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "snippet_surface": "The closest technical approach to that of the authors is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, the authors' proposed method requires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "questions": {"470296.XIm7riSYFZ": "What is the approach of Kozareva and Hovy (2010)?", "470296.iwYLjSBU1C": "What do they require for their approach?", "470296.lUi+WMg2zY": "How does the proposed method differ from Kozareva and Hovy's?"}}
{"idx": "470558", "paper_id": "1160159", "title": "Joint Inference for Knowledge Extraction from Biomedical Literature", "abstract": "Knowledge extraction from online repositories such as PubMed holds the promise of dramatically speeding up biomedical research and drug design. After initially focusing on recognizing proteins and binary interactions, the community has recently shifted their attention to the more ambitious task of recognizing complex, nested event structures. State-of-the-art systems use a pipeline architecture in which the candidate events are identified first, and subsequently the arguments. This fails to leverage joint inference among events and arguments for mutual disambiguation. Some joint approaches have been proposed, but they still lag much behind in accuracy. In this paper, we present the first joint approach for bio-event extraction that obtains state-of-the-art results. Our system is based on Markov logic and adopts a novel formulation by jointly predicting events and arguments, as well as individual dependency edges that compose the argument paths. On the BioNLP'09 Shared Task dataset, it reduced F1 errors by more than 10% compared to the previous best joint approach.", "context_section_header": "", "context_paragraph": "We also presented a heuristic method to fix errors in syntactic parsing by leveraging available semantic information from task input, and showed that this in turn led to substantial performance gain in the task. Overall, our final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "sentence": "Overall, our final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "cited_ids": [{"paper_id": "475213", "citation": "Riedel et al. (2009)"}], "y": "The authors' final system [a bio-events extraction classification system based on Markov logic] reduced F1 error by more than 10% compared to Riedel et al. (2009).", "snippet_surface": "The authors' final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "questions": {"470558.noJzJpmyQ/": "No questions."}}
{"idx": "471099", "paper_id": "15146734", "title": "Sentiment Propagation via Implicature Constraints", "abstract": "Opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities (goodFor/badFor events). We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities. The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.", "context_section_header": "", "context_paragraph": "Most work in NLP addresses explicit sentiment, but some address implicit sentiment. For example,  identify noun product features that imply opinions, and (Feng et al., 2013) identify objective words that have positive or negative connotations. However, identifying terms that imply opinions is a different task than sentiment propagation between entities. (Dasigi et al., 2012) search for implicit attitudes shared between authors, while we address inferences within a single text.", "sentence": "However, identifying terms that imply opinions is a different task than sentiment propagation between entities. (Dasigi et al., 2012) search for implicit attitudes shared between authors, while we address inferences within a single text.", "cited_ids": [{"paper_id": "5283021", "citation": "(Dasigi et al., 2012)"}], "y": "Identifying terms that imply opinions is a different task than sentiment propagation between entities. Dasigi et al. (2012) search for implicit attitudes shared between authors, while the authors address inferences within a single text [using Loopy Belief Propagation].", "snippet_surface": "However, identifying terms that imply opinions is a different task than sentiment propagation between entities. Dasigi et al. (2012) search for implicit attitudes shared between authors, while the authors address inferences within a single text.", "questions": {"471099.NF3gP2/c3u": "What is sentiment propagation?", "471099.wDyWA5Q1v6": "What is the task that Dasigi et al. (2012) address?", "471099.+WW4PN1QNE": "What is the difference between sentiment propagation and inferences within a single text?"}}
{"idx": "481201", "paper_id": "10016808", "title": "Annotating Abstract Pronominal Anaphora in the DAD Project", "abstract": "In this paper we present an extension of the MATE/GNOME annotation scheme for anaphora (Poesio, 2004) which accounts for abstract anaphora in Danish and Italian. By abstract anaphora it is here meant pronouns whose linguistic antecedents are verbal phrases, clauses and discourse segments. The extended scheme, which we call the DAD annotation scheme, allows to annotate information about abstract anaphora which is important to investigate their use, see i.a. (Webber, 1988; Gundel et al., 2003; Navarretta, 2004; Navarretta, 2007) and which can influence their automatic treatment. Intercoder agreement scores obtained by applying the DAD annotation scheme on texts and dialogues in the two languages are given and show that the information proposed in the scheme can be recognised in a reliable way.", "context_section_header": "", "context_paragraph": "There are only few studies describing the annotation of abstract anaphora in English. These studies have been done with different purposes, thus focussing on different types of information. Gundel et al. (2003), Gundel et al. (2004) and Hedberg et al. (2007) in their work focus on the investigation of the salience state of abstract entities in discourse and on their relation to the types of referring pronoun as proposed by Gundel et al. (1993) in their Givenness Hierar-chy. Hedberg et al. (2007) use XML as annotation format. Eckert and Strube (2001) and M\u00fcller (2007) annotate third person singular anaphors and their antecedents as a basis for their automatic recognition and \"resolution\". They do not focus on the format of their annotation. M\u00fcller (2007) lets non experts identify the antecedents of abstract anaphora according to very general instructions and reports low inter-coder agreement. Fraurud (1992) and Navarretta (2002) annotate occurrences of abstract anaphora in Swedish and Danish respectively to investigate the characteristics of abstract pronominal reference in these languages. Their results indicate that there are differences in the way abstract anaphora are used in these two languages respect to English. Navarretta (2007) investigates the occurrences of abstract anaphors in a parallel corpus of fairy tales (Danish, English and Italian) and also her data indicate that there are language specific characteristics in the way abstract anaphora are used in these three languages, both for what regards their frequency, the contexts in which they are used and the types of pronoun used in similar contexts. In Fraurud's and Navarretta's studies only one person (the respective authors) made the annotation following different annotation conventions and focussing on different phenomena. In the DAD project we include much of the information which has been used by i.a. Hedberg et al. (2007) for English and by Navarretta (2007) for Danish and Italian and integrate it in the existing MATE/GNOME scheme for anaphora. Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and our work is that we do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead we provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance. Furthermore we work with abstract anaphora in Danish and Italian which involve more types of pronoun than they do in English.", "sentence": "Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and our work is that we do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead we provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance.", "cited_ids": [{"paper_id": "14697968", "citation": "Hedberg et al. (2007)"}], "y": "Apart from the fact that the authors use the MATE/GNOME scheme [a form of anaphoric annotation of information for conference resolution], the main difference between the work by Hedberg et al. (2007) and the authors' work is that [Hedberg et al., 2007] do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead the authors provide more detailed information about the syntactic type of non nominal antecedents [elements in the MATE/GNOME scheme that are distinguished into several types] and mark information such as [the distance between the anaphor and antecedent].", "snippet_surface": "Apart from the fact that the authors use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and the authors' work is that they do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead they provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance.", "questions": {"481201.3F7gI5R1oy": "What is the MATE/GNOME scheme?", "481201.g8Pl7njFEQ": "What is the cognitive status of the antecedents?", "481201.tyQA6LT1u2": "What kind of detailed information do we provide about syntactic types of non nominal antecedents?"}}
{"idx": "481291", "paper_id": "235097439", "title": "Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction", "abstract": "Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information.", "context_section_header": "", "context_paragraph": "A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011;Zeng et al., 2015;Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3).", "sentence": "Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches.", "cited_ids": [{"paper_id": "13742825", "citation": "Han et al. (2018)"}], "y": "Han et al. (2018) propose a neural attention mechanism between a knowledge graph and supporting texts [for automatic knowledge base construction], outperforming previous approaches [which are based on distant supervision using entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base].", "snippet_surface": "Han et al. (2018) propose a neural attention mechanism between a knowledge graph and supporting texts, outperforming previous approaches.", "questions": {"481291.JbNwq8QXLD": "What is the neural attention mechanism?", "481291.DqM5C5plWg": "What are the previous approaches?", "481291.MICE49GVOK": "How does the proposed approach outperform the previous approaches?"}}
{"idx": "491012", "paper_id": "51869205", "title": "Learning to Control the Specificity in Neural Response Generation", "abstract": "In conversation, a general response (e.g., \u201cI don\u2019t know\u201d) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.", "context_section_header": "", "context_paragraph": "We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. Meanwhile, we assume that each word, beyond the semantic representation which relates to its meaning, also has another representation which relates to the usage preference under different response purpose. We name this representation as the usage representation of words. The specificity control variable then interacts with the usage representation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels. We refer to our model as Specificity Controlled Seq2Seq model . Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model. All the model parameters are learned on the same conversation corpus in an end-to-end way.", "sentence": "Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model.", "cited_ids": [{"paper_id": "9514751", "citation": "(Xing et al., 2017)"}], "y": "The authors note that, unlike the work by (Xing et al., 2017), they do not rely on any external corpus to learn their [Seq2Seq] model.", "snippet_surface": "The authors note that, unlike the work by (Xing et al., 2017), they do not rely on any external corpus to learn their model.", "questions": {"491012.fVhly5oPoD": "What is the work by Xing et al.?", "491012.zkSCJKX/sG": "What does the work by Xing et al. rely on?", "491012.6NGp7f/OLe": "What does the authors' model rely on?"}}
{"idx": "51608", "paper_id": "235097578", "title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs", "abstract": "We present Graformer, a novel Transformerbased encoder-decoder architecture for graphto-text generation. With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph \u2013 not only direct neighbors \u2013 facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these nodenode relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.1", "context_section_header": "", "context_paragraph": "Deficiencies in modeling long-range dependencies in GNNs have been considered a serious limitation before. Various solutions orthogonal to our approach have been proposed in recent work: By 2 abstract meaning representation incorporating a connectivity score into their graph attention network, Zhang et al. (2020) manage to increase the attention span to k-hop neighborhoods but, finally, only experiment with k = 2. Our graph encoder efficiently handles dependencies between much more distant nodes. Pei et al. (2020) define an additional neighborhood based on Euclidean distance in a continuous node embedding space. Similar to our work, a node can thus receive information from distant nodes, given their embeddings are close enough. However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer. This allows Graformer to dynamically change node interaction patterns during training.", "sentence": "However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer.", "cited_ids": [{"paper_id": "210843644", "citation": "Pei et al. (2020)"}], "y": "The authors' approach is to [build a transformer-based graph-to-text generator which facilitates the detection of global patterns by making sure that the embedding of a node relies on all nodes in the input graph. This is tested on popular benchmarks, AGENDA and WebNLG]. Pei et al. (2020) compute these embeddings only once before training whereas in the authors' approach, node similarity is based on the learned representation in each encoder layer.", "snippet_surface": "However, Pei et al. (2020) compute these embeddings only once before training whereas in the authors' approach, node similarity is based on the learned representation in each encoder layer.", "questions": {"51608.41xI3GWvyE": "What embeddings does Pei et al. (2020) compute?", "51608.NZueioMUAJ": "How is node similarity calculated in the authors' approach?"}}
{"idx": "52500", "paper_id": "5776384", "title": "On the Automatic Generation of Medical Imaging Reports", "abstract": "Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.", "context_section_header": "", "context_paragraph": "for images, Krause et al. (2017) and Liang et al. (2017) generate paragraph captions using a hierarchical LSTM. Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "sentence": "Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "cited_ids": [{"paper_id": "14464447", "citation": "Krause et al. (2017)"}], "y": "The authors' method adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), they use a co-attention network [which uses visual word recognition (letter shape recognition) along with semantic relevance of the word for tasks like transcribing captions], to generate topics.", "snippet_surface": "The authors' method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), they use a co-attention network to generate topics.", "questions": {"52500.c64wh00Rsw": "What is a hierarchical LSTM?", "52500.MRlHAVa55T": "What is a co-attention network?", "52500.5nlttlI1eu": "How does it differ from Krause et al.'s approach?"}}
{"idx": "52710", "paper_id": "32879266", "title": "Verb Replacer: An English Verb Error Correction System", "abstract": "According to the analysis of Cambridge Learner Corpus, using a wrong verb is the most common type of grammatical errors. This paper describes Verb Replacer, a system for detecting and correcting potential verb errors in a given sentence. In our approach, alternative verbs are considered to replace the verb based on an error-annotated corpus and verb-object collocations. The method involves applying regression on channel models, parsing the sentence, identifying the verbs, retrieving a small set of alternative verbs, and evaluating each alternative. Our method combines and improves channel and language models, resulting in high recall of detecting and correcting verb misuse.", "context_section_header": "", "context_paragraph": "According to the analysis of a sample of the Cambridge Learner Corpus (CLC) with 1,244 exam scripts for First Certificate English (FCE), verb selection errors (Replace-Verb errors, RV) is the most common error type, not counting spelling errors. In content word (e.g., verb and noun) errors correction, previous systems relied on mostly manually constructed resources (e,g., (Shei and Pain, 2000;Lee and Seneff, 2008;Liu et al., 2009)). It is not clear whether these manual resources can be easily scaled up and extended to other types of writing error and domains. Classifiers have been used for correcting verb errors. (Wu et al., 2010) describe an approach based on a classifier to predict the verb in the context of a given sentence. The main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used. Similarly, (Rozovskaya et al., 2014) use classifiers with the notion of verb finiteness to identify certain types of verb errors. (Rozovskaya et al., 2014) only address the agreement, tense, and form verb errors related to a small candidate set, while we deal with the verb selection problem with an open candidate set. In a noisy-channel approach closer to our work, (Sawai et al., 2013) use large learner corpus to construct candidate sets. They show that an GEC system that uses learner corpus outperforms systems that use WordNet and roundtrip translations, improving the performance of verb error detection and suggestion.", "sentence": "The main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used.", "cited_ids": [{"paper_id": "17049904", "citation": "(Wu et al., 2010)"}], "y": "The authors are presenting a similar work to the one made by Wu (2010) albeit the context alone isnt determining the outcome while they use the channel model information related to the potentially wrong verb.", "snippet_surface": "The main difference from the authors' current work is that in (Wu et al., 2010), the context alone determines the outcome, the channel model information related to the potentially wrong verb is not used.", "questions": {"52710.v9CmNWK1su": "What did Wu et al. (2010) use to determine the outcome?", "52710.HWsGqDWbkv": "How does the channel model information play a role in the current work?"}}
{"idx": "5882", "paper_id": "6383080", "title": "A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction", "abstract": "In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an infinite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction.", "context_section_header": "", "context_paragraph": "One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "sentence": "Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "cited_ids": [{"paper_id": "7923429", "citation": "(Goldwater et al., 2009)"}], "y": "The authors' segmentation model [unigram word segmentation model using a blocked sampler] is in principle the same as the unigram word segmentation model and the main difference is that the authors are using a pointwise Gibbs sampler [where the variables of interest are potential word boundaries, and each of them can take on two values] while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "snippet_surface": "The authors' segmentation model is in principle the same as the unigram word segmentation model and the main difference is that the authors are using a blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "questions": {"5882.ko6sPpPjz5": "What is a blocked sampler?", "5882.cp5zruZZJu": "How does it differ from a point-wise Gibbs sampler?", "5882.oErvzJNUc8": "What does it mean to draw the presence or absence of a word border?"}}
{"idx": "59705", "paper_id": "216914626", "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "abstract": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.", "context_section_header": "", "context_paragraph": "Two recent works also perturb the input sequence for model interpretability Li et al., 2019b). However, these works only perturb the sequence once.  utilize the original MLM objective to estimate each word's \"reducibility\" and import simple heuristics into a right-chain baseline to construct dependency trees. Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike our two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "sentence": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike our two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "cited_ids": [{"paper_id": "196176486", "citation": "Li et al. (2019b)"}], "y": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike the authors' two-step masking strategy [which consists of subsequently masking out two elements x_i, x_j to get two corrupted sequences], they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "snippet_surface": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike the authors' two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "questions": {"59705.nS8n9SwcCq": "What is NMT?", "59705.6tUMs1oiM5": "What is the two-step masking strategy?", "59705.q3uU1SJYWE": "What does \"replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary\" mean?"}}
{"idx": "609", "paper_id": "53109320", "title": "GraphIE: A Graph-Based Framework for Information Extraction", "abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.", "context_section_header": "", "context_paragraph": "In this paper, we propose GraphIE, a framework that improves predictions by automatically learning the interactions between local and non-local dependencies in the input space. Our approach integrates a graph module with the encoder-decoder architecture for sequence tagging. The algorithm operates over a graph, where nodes correspond to textual units (i.e. words or sentences) and edges describe their relations. At the core of our model, a recurrent neural network sequentially encodes local contextual representations and then the graph module iteratively propagates information between neighboring nodes using graph convolutions (Kipf and Welling, 2016). The learned representations are finally projected back to a recurrent decoder to support tagging at the word level. We evaluate GraphIE on three IE tasks, namely textual, social media, and visual (Aumann et al., 2006) information extraction. For each task, we provide in input a simple task-specific graph, which defines the data structure without access to any major processing or external resources. Our model is expected to learn from the relevant dependencies to identify and extract the appropriate information. Experimental results on multiple benchmark datasets show that GraphIE consistently outperforms a strong and commonly adopted sequential model (SeqIE, i.e. a bidirectional long-short term memory (BiLSTM) followed by a conditional random fields (CRF) module). Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015). In the social media IE task, GraphIE improves over SeqIE by 3.7% in extracting the EDU-CATION attribute from twitter users. In visual IE, finally, we outperform the baseline by 1.2%.", "sentence": "Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "cited_ids": [{"paper_id": "5973003", "citation": "(Krallinger et al., 2015)"}], "y": "In the textual IE task, the authors obtain an improvement of 0.5% over SeqIE [Sequential Information Extraction] on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "snippet_surface": "Specifically, in the textual IE task, the authors obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "questions": {"609.al6CDgrkjb": "What is SeqIE?", "609.qG2VMEqG0c": "What is the CONLL03 dataset?", "609.73eArVbi2s": "What is chemical entity extraction?"}}
{"idx": "64788", "paper_id": "231709298", "title": "Representations for Question Answering from Documents with Tables and Text", "abstract": "Tables in web documents are pervasive and can be directly used to answer many of the queries searched on the web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset (Kwiatkowski et al., 2019).", "context_section_header": "", "context_paragraph": "Most work on QA with tables prior to BERT involves first converting the table to a Knowledge Graph (KG) where cell entries are entities with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of M\u00fceller et al. (2019), where the authors generalized the BERT architecture similarly to Shaw et al. (2018) with new types of relations to encode table-specific relationships. The main differences between our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1), while in their work the authors use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values. Finally, the two most recent works on table representation learning, TaPaS (Herzig et al., 2020) and GraPPa (Yu et al., 2020), also use pretraining on the Wikitables dataset (Bhagavatula et al., 2013) that we use in our work. Therefore, our table representations based on transformers and our pretraining method are comparable to those in recent and concurrent work.", "sentence": "The main differences between our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1), while in their work the authors use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "cited_ids": [{"paper_id": "201698399", "citation": "M\u00fceller et al. (2019)"}], "y": "The main differences between the authors' table representation and the one from M\u00fceller et al. (2019) is that the authors use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row, while the latter use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "snippet_surface": "The main differences between the authors' table representation and the one from M\u00fceller et al. (2019) is that the authors use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1) while the latter use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "questions": {"64788.CkiJJC1tx5": "What are the 5 types of relations used in our table representation?", "64788.bZccnP8LZI": "What are the differences between our table representation and M\u00fceller et al. (2019)?", "64788.nCvgFkbY0d": "What are question-cell relations?"}}
{"idx": "69385", "paper_id": "1085023", "title": "Constituency to Dependency Translation with Forests", "abstract": "Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts.", "context_section_header": "", "context_paragraph": "The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models.  and  use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models  respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002).", "sentence": "Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that our model runs very faster.", "cited_ids": [{"paper_id": "832217", "citation": "Shen et al. (2008)"}], "y": "Compared with another work [a string-to-dependency algorithm for statistical machine translation], the authors put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that they can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, their rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that the authors' model runs much faster.", "snippet_surface": "The authors put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules compared with this work; the other difference is that they can also extract more expressive constituency to dependency rules, since the source side of their rule can encode multi-level reordering and contain more variables being larger than two; furthermore, their rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that their model runs very faster.", "questions": {"69385.2+xzUQNa1V": "What restrictions were put on the definition of well-formed dependency structures?", "69385.3JcX6bV35W": "What is the difference between the authors' approach and Shen et al. (2008)'s scenario?", "69385.NpyXwDhuGB": "What makes the authors' model run faster?"}}
{"idx": "71087", "paper_id": "2982769", "title": "Building RDF Content for Data-to-Text Generation", "abstract": "In Natural Language Generation (NLG), one important limitation is the lack of common benchmarks on which to train, evaluate and compare data-to-text generators. In this paper, we make one step in that direction and introduce a method for automatically creating an arbitrary large repertoire of data units that could serve as input for generation. Using both automated metrics and a human evaluation, we show that the data units produced by our method are both diverse and coherent.", "context_section_header": "", "context_paragraph": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure. In particular, one important difference is that we produce trees of varying shapes and depths while the graphs produced by (Cheng et al., 2015) are restricted to trees of depth one i.e., set of DBpedia triples whose subject is the entity to be described. As discussed in Section 5.1, this allows us to produce knowledge trees which, because they vary in shape, will give rise to different linguistic structures and will therefore better support the creation of a linguistically varied benchmark for Natural Language Generation.", "sentence": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure.", "cited_ids": [{"paper_id": "2699851", "citation": "(Cheng et al., 2015)"}], "y": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, the authors' goal is to produce a large set of content units [which are snippets of texts generated by a Language model starting from a DBPedia entity,] that are varied both in terms of content and in terms of structure.", "snippet_surface": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, the authors' goal is to produce a large set of content units that are varied both in terms of content and in terms of structure.", "questions": {"71087.6UMLkrbxJg": "What is the goal of Cheng et al. (2015)?", "71087.bWFwmvWhQp": "What does \"content units\" refer to?", "71087.HwH71AMu++": "What do \"content\" and \"structure\" refer to?"}}
{"idx": "72224", "paper_id": "201666897", "title": "Adversarial Domain Adaptation for Machine Reading Comprehension", "abstract": "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.", "context_section_header": "", "context_paragraph": "Specifically, our proposed method first generates synthetic question-answer pairs given pas-sages in the target domain. Different from Golub et al. (2017), which only used pseudo questionanswer pairs to fine-tune pre-trained MRC models, our AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passagequestion pairs in the source domain, to train an additional domain classifier as a discriminator. The passage-question encoder and the domain classifier are jointly trained via adversarial learning. In this way, the encoder is enforced to learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span.", "sentence": "Different from Golub et al. (2017), which only used pseudo questionanswer pairs to fine-tune pre-trained MRC models, our AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passagequestion pairs in the source domain, to train an additional domain classifier as a discriminator.", "cited_ids": [{"paper_id": "20298653", "citation": "Golub et al. (2017)"}], "y": "Different from Golub et al. (2017), which only used pseudo question-answer pairs to fine-tune pre-trained MRC [Machine Reading Comprehension] models, the authors' AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passage-question pairs in the source domain, to train [via adversarial training] an additional [Question Answer Module] domain classifier as a discriminator.", "snippet_surface": "Different from Golub et al. (2017), which only used pseudo question-answer pairs to fine-tune pre-trained MRC models, the authors' AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passage-question pairs in the source domain, to train an additional domain classifier as a discriminator.", "questions": {"72224.zOLIUU1dp0": "What is the difference between Golub et al. (2017) and AdaMRC?", "72224.xpm3AS2ax6": "What are pseudo question-answer pairs?", "72224.NaJwTeskmo": "What is the additional domain classifier used in AdaMRC?"}}
{"idx": "72451", "paper_id": "52938038", "title": "Improving the Transformer Translation Model with Document-Level Context", "abstract": "Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.", "context_section_header": "", "context_paragraph": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01). The gain on the concatenated test set (i.e., \"All\") is 1.96 BLEU points. It also outperforms the cache-based method  adapted for Transformer significantly (p < 0.01), which also uses the two-step training strategy. Table 4 shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task.  Table 3: Comparison with previous works on Chinese-English translation task. The evaluation metric is caseinsensitive BLEU score.  use a hierarchical RNN to incorporate document-level context into RNNsearch.  use a cache to exploit document-level context for RNNsearch. * is an adapted version of the cache-based method for Transformer. Note that \"MT06\" is not included in \"All\".   Table 5: Subjective evaluation of the comparison between the original Transformer model and our model. \">\" means that Transformer is better than our model, \"=\" means equal, and \"<\" means worse.", "sentence": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01).", "cited_ids": [{"paper_id": "13756489", "citation": "(Vaswani et al., 2017)"}], "y": "The authors' results show that their approach [which extends the Transformer model to use the document-level context (which the authors compute and incorporate into the encoder/decoder using multihead self-attention)] achieves significant improvements over the original Transformer model [the state-of-the-art NMT model that does not exploit document-level context] (p < 0.01).", "snippet_surface": "The authors' results show that their approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01), as seen in Table 3.", "questions": {"72451.TCgiPkgGj3": "What are the significant improvements?", "72451.Ed2jMd26Lh": "What is the original Transformer model?", "72451.6NN0TG4kYY": "What is the p-value?"}}
{"idx": "75108", "paper_id": "12847491", "title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis", "abstract": "We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.", "context_section_header": "", "context_paragraph": "Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions on tree structures, which has been used for syntactic analysis (Socher et al., 2010) and sentence sentiment analysis (Socher et al., 2013). (Dong et al., 2014;Nguyen and Shirai, 2015) adopted Rec-NN for aspect sentiment classification, by converting the opinion target as the tree root and propagating the sentiment of targets depending on the context and syntactic relationships between them. However, Rec-NN needs dependency parsing which is likely ineffective on nonstandard texts such as news comments and tweets. (Chen et al., 2016) employed Convolution NNs to identify the senti-  ment of a clause which is then used to infer the sentiment of the target. The method has an assumption that an opinion word and its target lie in the same clause. TD-LSTM (Tang et al., 2015) utilizes LSTM to model the context information of a target by placing the target in the middle and propagating the state word by word from the beginning and the tail to the target respectively to capture the information before and after it. Nevertheless, TD-LSTM might not work well when the opinion word is far from the target, because the captured feature is likely to be lost (  reported similar problems of LSTM-based models in machine translation). (Graves et al., 2014) introduced the concept of memory for NNs and proposed a differentiable process to read and write memory, which is called Neural Turing Machine (NTM). Attention mechanism, which has been used successfully in many areas Rush et al., 2015), can be treated as a simplified version of NTM because the size of memory is unlimited and we only need to read from it. Single attention or multiple attentions were applied in aspect sentiment classification in some previous works (Wang et al., 2016;Tang et al., 2016). One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\"). More importantly, we combine the results of attentions in a nonlinear way. (Wang et al., 2016) only uses one attention, while our model uses multiple attentions. The effectiveness of multiple attentions was also investigated in QA task (Kumar et al., 2015), which shows that multiple attentions allow a model to attend different parts of the input during each pass. (Kumar et al., 2015) assigns attention scores to memory slices independently and their attention process is more complex, while we produce a normalized attention distribution to attend information from the memory.", "sentence": "One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "cited_ids": [{"paper_id": "359042", "citation": "(Tang et al., 2016)"}], "y": "One difference between the authors' method [a sentiment analysis model similar to the target dependent LSTM] and (Tang et al., 2016) is that the authors introduce a memory module between the attention module and the input module, thus their method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "snippet_surface": "One difference between the authors' method and (Tang et al., 2016) is that the authors introduce a memory module between the attention module and the input module, thus allowing the method to synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "questions": {"75108.ivJrg+y4va": "What is the purpose of the memory module?", "75108.7QDl4fYETC": "What is an example of a sentiment phrase?", "75108.EoFYwmLKg2": "How does the memory module affect feature synthesis?"}}
{"idx": "8580", "paper_id": "218613822", "title": "Smart To-Do: Automatic Generation of To-Do Items from Emails", "abstract": "Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.", "context_section_header": "", "context_paragraph": "Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004;Carenini et al., 2007;Dredze et al., 2008). There has also been considerable research on identifying speech acts or tasks in emails (Carvalho and Cohen, 2005;Lampert et al., 2010;Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019). Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo. Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset. (Lin et al., 2018) and identifying intents in email conversations . However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004). The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.", "sentence": "The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.", "cited_ids": [{"paper_id": "182953152", "citation": "(Zhang and Tetreault, 2019)"}], "y": "The closest work to the authors' is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas their method [generating to-do items from emails through a two step process; content selection and subsequent text generation] relies on identifying the task-related context.", "snippet_surface": "The closest work to the authors' is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas their method relies on identifying the task-related context.", "questions": {"8580.FnKPBBK72I": "What is the focus of Zhang and Tetreault's work?", "8580.ABi34RBuWH": "What is the difference between their approach and the authors' approach?"}}
{"idx": "90656", "paper_id": "9300324", "title": "A hybrid text classification approach for analysis of student essays", "abstract": "We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Rose et al., 2002a). CarmelTC learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a Naive Bayes classification of that text. We explore the tradeoffs between symbolic and \"bag of words\" approaches. Our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two \"bag of words\" approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach.", "context_section_header": "", "context_paragraph": "In contrast to many previous approaches to automated essay grading (Burstein et al., 1998;Larkey, 1998), our goal is not to assign a letter grade to student essays. Instead, our purpose is to tally which set of \"correct answer aspects\" are present in student essays. For example, we expect satisfactory answers to the example question above to include a detailed explanation of how Newton's first law applies to this scenario. From Newton's first law, the student should infer that the pumpkin and the man will continue at the same constant horizontal velocity that they both had before the release. Thus, they will always have the same displacement from the point of release. Therefore, after the pumpkin rises and falls, it will land back in the man's hands. Our goal is to coach students through the process of constructing good physics explanations. Thus, our focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "sentence": "Thus, our focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "cited_ids": [{"paper_id": "2242666", "citation": "(Burstein et al., 2001)"}], "y": "The authors [are trying to build a hybrid text classification approach for analysing essays, called CarmelTC, and their] focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001)", "snippet_surface": "Thus, the authors' focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "questions": {"90656.lScihlLJVV": "What is the focus of Burstein et al.'s (2001) approach?"}}
{"idx": "91100", "paper_id": "17942720", "title": "Urdu and Hindi: Translation and sharing of linguistic resources", "abstract": "Hindi and Urdu share a common phonology, morphology and grammar but are written in different scripts. In addition, the vocabularies have also diverged significantly especially in the written form. In this paper we show that we can get reasonable quality translations (we estimated the Translation Error rate at 18%) between the two languages even in absence of a parallel corpus. Linguistic resources such as treebanks, part of speech tagged data and parallel corpora with English are limited for both these languages. We use the translation system to share linguistic resources between the two languages. We demonstrate improvements on three tasks and show: statistical machine translation from Urdu to English is improved (0.8 in BLEU score) by using a Hindi-English parallel corpus, Hindi part of speech tagging is improved (upto 6% absolute) by using an Urdu part of speech corpus and a Hindi-English word aligner is improved by using a manually word aligned Urdu-English corpus (upto 9% absolute in F-Measure).", "context_section_header": "", "context_paragraph": "Converting between the scripts of Hindi and Urdu is non-trivial and has been a recent focus (Malik et al., 2008;Malik et al., 2009). (Malik et al., 2008) uses hand designed rules encoded using finite state transducers to transliterate between Hindi and Urdu. As reported in (Malik et al., 2009) these hand designed rules achieve accuracies of only about 50% in the absence of diacritical marks. (Malik et al., 2009) improves Urdu\u2192Urdu transliteration performance to 79% by post processing the output of the transducer with a statistical language model. In contrast to (Malik et al., 2009) we use a statistical model for character transliteration. As discussed in Section 1.1, due to the divergence of vocabularies in written Hindi and Urdu, transliteration is not sufficient to convert from written Urdu to written Hindi. We also use a more flexible model that allows for more natural translations by allowing Urdu words to translate into Hindi words that do not sound the same. (Sinha, 2009) builds an English-Urdu machine translation system using an English-Hindi machine translation system and a Hindi-Urdu word mapping table, suitably adjusted for part of speech and gender. Their system is not statistical, and is largely based on manual creation of a large database of Hindi-Urdu correspondences. Additionally, as mentioned in the conclusion, their system cannot be used for direct translation from Hindi to Urdu, since a grammatical analysis of the English provides information necessary for the Hindi to Urdu mapping. In contrast to this work, our techniques are largely statistical, require minimal manual effort and can directly translate between Hindi and Urdu without the associated English.", "sentence": "In contrast to (Malik et al., 2009) we use a statistical model for character transliteration.", "cited_ids": [{"paper_id": "7957798", "citation": "(Malik et al., 2009)"}], "y": "In contrast to (Malik et al., 2009), [who uses a a finite-state model,] the authors use a [model that can be used to solve some problems mentioned in Malik's work] for character transliteration.", "snippet_surface": "In contrast to (Malik et al., 2009), the authors use a statistical model for character transliteration.", "questions": {"91100.7lfYP65hVm": "What approach did Malik et al. use?", "91100.Wjf5OxSg/V": "What is a statistical model for character transliteration?"}}
{"idx": "91184", "paper_id": "1832412", "title": "Automatic prediction of aspectual class of verbs in context", "abstract": "This paper describes a new approach to predicting the aspectual class of verbs in context, i.e., whether a verb is used in a stative or dynamic sense. We identify two challenging cases of this problem: when the verb is unseen in training data, and when the verb is ambiguous for aspectual class. A semi-supervised approach using linguistically-motivated features and a novel set of distributional features based on representative verb types allows us to predict classes accurately, even for unseen verbs. Many frequent verbs can be either stative or dynamic in different contexts, which has not been modeled by previous work; we use contextual features to resolve this ambiguity. In addition, we introduce two new datasets of clauses marked for aspectual class.", "context_section_header": "", "context_paragraph": "In contrast to Siegel and McKeown (2000), we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs. Instead we predict the aspectual class of verbs in the context of their arguments and modifiers. We show that this method works better than using only type-based features, especially for verbs with ambiguous aspectual class. In addition, we show that type-based features, including novel distributional features based on representative verbs, accurately predict predominant aspectual class for unseen verb types. Our work differs from prior work in that we treat the problem as a three-way classification task, predicting DYNAMIC, STATIVE or BOTH as the aspectual class of a verb in context.", "sentence": "In contrast to Siegel and McKeown (2000), we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs.", "cited_ids": [{"paper_id": "6433813", "citation": "Siegel and McKeown (2000)"}], "y": "In contrast to Siegel and McKeown (2000), the authors do not conduct the task of predicting aspectual class, [a property of verbs related to how they are used in context] solely at the type level. This approach ignores the minority class of ambiguous verbs.", "snippet_surface": "In contrast to Siegel and McKeown (2000), the authors do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs.", "questions": {"91184.RhqwaNd0if": "What does Siegel and McKeown (2000) propose?", "91184.UcNLChw4/B": "What is the task of predicting aspectual class?", "91184.TJhNJwTXTT": "What is the minority class of ambiguous verbs?"}}
{"idx": "91729", "paper_id": "235683036", "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction", "abstract": "Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.", "context_section_header": "", "context_paragraph": "NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014;Gupta et al., 2016;Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and conse-quently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE.  and , on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on , Lin et al. (2020) introduced ONEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from ONEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. While several Seq2Seq-based models Zeng et al., 2018Wei et al., 2019;Zhang et al., 2019) have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in that: (1) it is generating a BFS/DFS traversal of the target graph, which captures dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information.", "sentence": "Note that our model differs from ONEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities.", "cited_ids": [{"paper_id": "220048375", "citation": "(Lin et al., 2020)"}], "y": "The authors' model [Hybrid Span Generator (HySPA)] differs from ONEIE (Lin et al., 2020) in that their model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while the author\u2019s method efficiently generates existing relations [like PHYS, and ORG-AFF] and entities.\"", "snippet_surface": "The authors' model differs from ONEIE (Lin et al., 2020) in that their model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while their method efficiently generates existing relations and entities.", "questions": {"91729.6KWu5ltskI": "What is ONEIE?", "91729.pet+Nd0fAx": "How does ONEIE use feature engineered templates?", "91729.yMaxUQudUx": "How does ONEIE do pairwise classification for relation extraction?"}}
{"idx": "92677", "paper_id": "18127129", "title": "Automatically Building a Corpus for Sentiment Analysis on Indonesian Tweets", "abstract": "The popularity of the user generated content, such as Twitter, has made it a rich source for the sentiment analysis and opinion mining tasks. This paper presents our study in automatically building a training corpus for the sentiment analysis on Indonesian tweets. We start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the Expectation and Maximization (EM) framework. We apply our automatically built corpus to perform two tasks, namely opinion tweet extraction and tweet polarity classification using various machine learning approaches. Experiment result shows that a classifier model trained on our data, which is automatically constructed using our proposed method, outperforms the baseline system in terms of opinion tweet extraction and tweet polarity classification.", "context_section_header": "", "context_paragraph": "First, we use different techniques to automatically collect training data. Second, we perform two-level sentiment analysis, namely, opinion tweet extraction and tweet polarity classification. Moreover, in the experiment section, we show that our method to collect training data is better than the one proposed by Pak and Paroubek (2010). Our method also produces much larger data since we do not rely on sheer emoticon-containing tweets to collect training data.", "sentence": "Moreover, in the experiment section, we show that our method to collect training data is better than the one proposed by Pak and Paroubek (2010).", "cited_ids": [{"paper_id": "550498", "citation": "Pak and Paroubek (2010)"}], "y": "The authors show in the experiment section that [starting with a small set of labeled seed corpus and expanding it using a classifier model] to collect training data [outperforms the baseline] proposed by Pak and Paroubek (2010).", "snippet_surface": "Moreover, in the experiment section, the authors show that their method to collect training data is better than the one proposed by Pak and Paroubek (2010).", "questions": {"92677.X1XDj8bG4w": "What is the method proposed by Pak and Paroubek (2010)?", "92677.su6g5jVaEN": "How did the authors measure the effectiveness of their method?"}}
{"idx": "93538", "paper_id": "34272708", "title": "Bootstrapping Arabic-Italian SMT through Comparable Texts and Pivot Translation", "abstract": "This paper describes efforts towards the development of an Arabic to Italian SMT system for the news domain. Since only very little parallel data are available for this language pair, we investigated both the exploitation of comparable corpora and pivot translation. Experimental evaluation was conducted on a new benchmark developed by extending two Arabic-to-English NIST evaluation sets. Preliminary results show potentials of both approaches with respect to performance achieved by a popular state-of-the-art Web-based translation service.", "context_section_header": "", "context_paragraph": "In Section 4.1 we propose three different document alignment methods and experimentally evaluate them on the basis of precision and recall. All methods share the initial translation of the document title, which is then used to detect comparable documents. In such respect, also our methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "sentence": "In such respect, also our methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "cited_ids": [{"paper_id": "541460", "citation": "(Uszkoreit et al., 2010)"}], "y": "The authors' methods [three different document alignment methods] only rely on the textual content of the documents [news report texts expressing the same content in different languages] and do not require any meta-data, much alike but more affordable than Uszkoreit et al., (2010).", "snippet_surface": "The authors' methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "questions": {"93538.kXbADg4R82": "What methods are the authors referring to?", "93538.uIx/8S8BSZ": "What does \"meta-data\" refer to?", "93538.6x4o5Q6Obx": "What is more affordable than Uszkoreit et al. (2010)?"}}
{"idx": "93720", "paper_id": "10702193", "title": "Composing Simple Image Descriptions using Web-scale N-grams", "abstract": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches.", "context_section_header": "", "context_paragraph": "Our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et al. (2010)), or summarizing existing text fragments associated with an image (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010a)). Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text (Feng and Lapata (2010a)) or encyclopedic text (Aker and Gaizauskas (2010)) that is contextually relevant to the image, but not closely pertinent to the specific content of the image. Third, we aim to build a general image description method as compared to work that requires domain specific hand-written grammar rules (Yao et al. (2010)). Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "sentence": "Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "cited_ids": [{"paper_id": "53307035", "citation": "(Kulkarni et al., 2011)"}], "y": "The authors allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach [as proposed in (Kulkarni et al., 2011)] that drives annotation more directly from computer vision inputs [e.g. images and videos.]", "snippet_surface": "Lastly, the authors allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "questions": {"93720.qPPt76rvsS": "What is the very recent approach?", "93720.RwGvfau0xP": "How does it drive annotation from computer vision inputs?", "93720.wl0LHyKE5m": "What is the difference between it and the authors' approach?"}}
{"idx": "9500", "paper_id": "20667722", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "abstract": "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.", "context_section_header": "", "context_paragraph": "In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL). In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph. The agent takes incremental steps by sampling a relation to extend its path. To better guide the RL agent for learning relational paths, we use policy gradient training (Mnih et al., 2015) with a novel reward function that jointly encourages accuracy, diversity, and efficiency. Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset.", "sentence": "In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph.", "cited_ids": [{"paper_id": "14941970", "citation": "(Bordes et al., 2013)"}], "y": "In contrast to PRA, the authors use a translation-based knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of their RL agent [which is used in a path learning process], which reasons in the vector space environment of the knowledge graph [which contains information on abstract or concrete entities].", "snippet_surface": "In contrast to PRA, the authors use a translation-based knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of their RL agent, which reasons in the vector space environment of the knowledge graph.", "questions": {"9500.pTJ0it3qkY": "What is PRA?", "9500.6MZWN1ugpP": "What is a translation-based knowledge based embedding method?", "9500.MOnNYnR9Ef": "What is the vector space environment of the knowledge graph?"}}
