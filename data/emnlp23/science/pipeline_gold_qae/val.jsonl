{"idx": "103595", "paper_id": "4957206", "title": "Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource", "abstract": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "context_section_header": "", "context_paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "sentence": "Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "cited_ids": [{"paper_id": "13475624", "citation": "Jiang et al. (2016)"}], "y": "[There are three differences between Jiang et al's and the authors work on relation extraction. First, a difference in scale. Second, a difference in granularity.] Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations [which look like semantic frames] from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "snippet_surface": "Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "questions": {"RFUs0cLj5x": "What is the first difference?", "2lFRRKXRjC": "What is the second difference?", "QyGJfTBT7D": "What does the event in the knowledge base look like?", "ewn9IQmUgT": "What type of relations are extracted?"}, "answers": {"RFUs0cLj5x": "The scale difference.", "2lFRRKXRjC": "The granularity difference.", "QyGJfTBT7D": "Like semantic frames.", "ewn9IQmUgT": "Temporal relations are extracted."}, "evidence": {"RFUs0cLj5x": [{"section": "Related Work", "paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "selected": "First, scale difference:", "paper_id": "4957206"}], "2lFRRKXRjC": [{"section": "Related Work", "paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "selected": "Second, granularity difference:", "paper_id": "4957206"}], "QyGJfTBT7D": [{"section": "Learning", "paragraph": "Due to the slight event annotation difference in TBDense, we collect our training data as follows. We first extract all the verb semantic frames from the raw text of TBDense. Then we only keep those semantic frames that are matched to an event in TBDense (about 85% semantic frames are kept in this stage). By doing so, we can simply use the TempRel annotations provided in TBDense. Hereafter the TBDense dataset used in this paper refers to this version unless otherwise specified.", "selected": "Then we only keep those semantic frames that are matched to an event in TBDense", "paper_id": "4957206"}], "ewn9IQmUgT": [{"section": "Abstract", "paragraph": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "selected": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language.", "paper_id": "4957206"}]}}
{"idx": "104377", "paper_id": "15193840", "title": "Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers", "abstract": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "context_section_header": "", "context_paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "sentence": "The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "cited_ids": [{"paper_id": "2462277", "citation": "(Komiya and Okumura, 2011)"}], "y": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD [(Word sense disambiguation)], source data, and target data, whereas the authors determined the method for each instance [the individual data contained in the triple].", "snippet_surface": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, whereas the authors determined the method for each instance.", "questions": {"xKLVA/UqO6": "What is a \"triple\" of a target word type?", "5WWPAq8lni": "What does \"WSD\" stand for?", "e0KsKMSGtK": "What does \"each instance\" refer to?"}, "answers": {"xKLVA/UqO6": "It is a group of data containing exactly 3 piece of information: target type of WSD, source data, target data.", "5WWPAq8lni": "WSD: Word sense disambiguation", "e0KsKMSGtK": "\"Each instance\" refers to the individual data contained in the triple. Instead of considering them altogether, they are investigated individually."}, "evidence": {"xKLVA/UqO6": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "for each triple of the target word type of WSD, source data, and target data,", "paper_id": "15193840"}, {"section": "Conclusion", "paragraph": "We described how the optimal method of DA could be determined depending on the properties of the source and target data using decision tree learning and found what properties affected the determination of the best method when Japanese WSD was performed. We defined a case as a triple of the target word type of WSD, the source data, and the target data, all of which were classified into two labels (TO and RS) or three labels (TO, RS, and SA). Here, the case with TO should only be trained with a small amount of target data, the case with RS should be trained with source data and a small amount of target data, and SA represents a case with no difference between the accuracies for the two methods. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively. We automatically generated a decision tree in eight ways, the most accurate of which was with SA label when the WSD accuracies of the two methods were totally equal, performed binary classification without SA, and classified cases without weighted word tokens. The top node in the tree that was generated indicated that simulation using ten manually labeled word tokens of the target data was an important clue enabling the optimal DA method to be predicted.", "selected": "We defined a case as a triple of the target word type of WSD, the source data, and the target data,", "paper_id": "2462277"}], "5WWPAq8lni": [{"section": "Abstract", "paragraph": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}, {"section": "Introduction", "paragraph": "However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}], "e0KsKMSGtK": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "target word type of WSD, source data, and target data", "paper_id": "15193840"}]}}
{"idx": "112709", "paper_id": "227230401", "title": "Formality Style Transfer with Shared Latent Space", "abstract": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "context_section_header": "", "context_paragraph": "To verify the effectiveness and generalization of our method, we conduct experiments in three different settings: Data Limited, Data Augmentation, and Pre-training. In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset. When we use large-scale non-parallel data to enhance our method in the data augmentation and pre-training settings, our method still consistently outperforms the baselines by 1 BLEU score. The ablation test further studies the effectiveness of the joint training, the auto-encoding training, and the auxiliary losses in different scenarios, showing the robustness of our method.", "sentence": "In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "cited_ids": [{"paper_id": "4859003", "citation": "(Rao and Tetreault, 2018)"}], "y": "In the data-limited scenario, experimental results show that the authors' method [Sequence-to-Sequence with Shared Latent Space trained with two auxiliary losses with joint training of bi-directional transfer and auto-encoding] is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains [categories of informal sentences] (namely, F&R [family and relationships] and E&M [entertainment and music]) of the GYAFC dataset.", "snippet_surface": "In the data-limited scenario, experimental results show that the authors' method is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "questions": {"QJveBARNkz": "What does F&R stand for?", "pk1ETlyGVv": "What does E&M stand for?", "VxCqlAwvTe": "What is the authors' method?"}, "answers": {"QJveBARNkz": "F&R stands for Family & Relationships category of informal sentences.", "pk1ETlyGVv": "E&M stands for Entertainment & Music category of informal sentences.", "VxCqlAwvTe": "The authors' method is called Sequence-to-Sequence with Shared Latent Space, and it includes two auxiliary losses with joint training of bi-directional transfer and auto-encoding."}, "evidence": {"QJveBARNkz": [{"section": "Related Work", "paragraph": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk. The workers are presented with detailed instructions, as well  as examples. To ensure quality control, four experts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required standards. They also provided the workers with reasons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly performed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions.", "selected": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk.", "paper_id": "4859003"}], "pk1ETlyGVv": [{"section": "Related Work", "paragraph": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk. The workers are presented with detailed instructions, as well  as examples. To ensure quality control, four experts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required standards. They also provided the workers with reasons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly performed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions.", "selected": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories", "paper_id": "4859003"}], "VxCqlAwvTe": [{"section": "Abstract", "paragraph": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "selected": "In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding.", "paper_id": "227230401"}, {"section": "Our Approach", "paragraph": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS). Figure 1 depicts the overall framework of S2S-SLS. It has a shared encoder f enc , but two decoders f i dec and f f dec for the informal and formal styles, respectively. We will then describe our matching losses in detail. They ensure that the shared latent space of formal and informal styles does capture semantic information while eliminating the style.", "selected": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS).", "paper_id": "227230401"}]}}
{"idx": "113060", "paper_id": "202538019", "title": "Mixture Content Selection for Diverse Sequence Generation", "abstract": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "context_section_header": "", "context_paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "sentence": "While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation.", "cited_ids": [{"paper_id": "67787922", "citation": "Shen et al. (2019)"}], "y": "Shen et al. (2019) makes a RNN decoder as a MOE [deep Mixture of Experts], whereas the authors make a SELECTOR [a general plug-and-play module specialized for diversification that wraps around and guides an existing encoder-decoder model] as a MoE to diversify content selection [selection of individual tokens from the source to be fed into the encoder-decoder model] and enable the encoder-decoder model's one-to-one generation.", "snippet_surface": "Shen et al. (2019) makes a RNN decoder as a MoE, whereas the authors make a SELECTOR as a MoE to diversify content selection and let the encoder-decoder models one-to-one generation.", "questions": {"qcZ7zMU/Ij": "What does \"MoE\" stand for?", "0nuJozFz9/": "What is \"a SELECTOR?\"", "uW85CSDZaZ": "What is \"one-to-one generation?\"", "KA0t7+Mu76": "What is the content that is being selected?"}, "answers": {"qcZ7zMU/Ij": "Mixture of Experts", "0nuJozFz9/": "SELECTOR is a general plug-and-play module that wraps around and guides an existing encoder-decoder model. This is a generic module that is specialized for increasing generation diversity.", "uW85CSDZaZ": "This is the generation method that employes a standard encoder-decoder model to generate a target sequence given each selected content from the source.", "KA0t7+Mu76": "Individual tokens are selected from the source to be fed into the encoder-decoder model."}, "evidence": {"qcZ7zMU/Ij": [{"section": "Related Work", "paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "selected": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps.", "paper_id": "202538019"}], "0nuJozFz9/": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": "We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model", "paper_id": "202538019"}], "uW85CSDZaZ": [{"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}], "KA0t7+Mu76": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": ". The generation stage uses a standard encoder-decoder model given each selected content from the source sequence.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "selected": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}]}}
{"idx": "114795", "paper_id": "196471395", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning", "abstract": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "context_section_header": "", "context_paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "sentence": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "cited_ids": [{"paper_id": "1294169", "citation": "(Henderson et al., 2014)"}], "y": "The authors' system [consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty]. The system operates in the well-known DSTC2 domain [seed data that contains the dialogues between human asking for information and the machine providing it] which concerns information about restaurants in Cambridge. However, the authors' multi-agent system [a system with multiple agent create novel languages by each playing a different role and having their own objective]. Supports any slot filling / information-seeking domain.", "snippet_surface": "The authors' system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, their multi-agent system supports any slotfilling / information-seeking domain.", "questions": {"d+L547/CZx": "What is the authors' system?", "N0apey0EKr": "What is \"dstc2\" domain?", "GU316DPPTi": "What is their multi-agent system?"}, "answers": {"d+L547/CZx": "The author's system consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty.", "N0apey0EKr": "DSTC2 is not a domain, but the seed data which contains the dialogues between humans asking for information and the machine providing it.", "GU316DPPTi": "The multi-agent system is a type of system that has each agent playing a different role and has their own objective. These agents create languages not present in the original data."}, "evidence": {"d+L547/CZx": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language.", "paper_id": "196471395"}], "N0apey0EKr": [{"section": "Introduction", "paragraph": "Employing a user simulator is an established method for dialogue policy learning (Schatzmann et al., 2007, among others) and end-to-end dialogue training (Asri et al., 2016;Liu and Lane, 2018b). Training two conversational agents concurrently has been proposed by Georgila et al. (2014); training them via natural language communication was partially realized by Liu and Lane (2017), as they train agents that receive text input but generate dialogue acts. However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator. Inspired by Hakkani-T\u00fcr (2018), each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal. This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents. The architecture of each agent is mirrored as shown in Figure 1, so the effort of adding agents with new roles is minimal. As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information. Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "selected": "As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "paper_id": "196471395"}], "GU316DPPTi": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time).", "paper_id": "196471395"}]}}
{"idx": "116565", "paper_id": "234324760", "title": "Modeling German Word Order Acquisition via Bayesian Inference", "abstract": "The question of how children acquire the grammatical principles of their native language has been debated in cognitive science and linguistics for over 50 years (e.g., Chomsky, 1965; Xu and Tenenbaum, 2007). Perfors et al. (2011) suggest that children compare various syntactic hypotheses in order to determine which is most compatible with the socalled primary linguistic data (cf. Chomsky, 1965; Wexler and Culicover, 1980; Clark and Lappin, 2013; among many others), and that a Bayesian model selection approach might be able to do some of this work. More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG) that used hierarchical phrase structure over a regular grammar representing linear phrase structure, without any initial prior bias towards one grammar type or another. Therefore, they argued that correct linguistic generalizations can be achieved by a learner equipped with domain-general Bayesian inference capacities but without language-specific innate knowledge. In their work, the various syntactic hypotheses were represented by grammars that were all able to parse the entirety of a pre-processed English childdirected speech corpus. However, a computational model without the ability to compare competing hypotheses that are supported by different subsets of the data is severely limited due to the inevitability of noise. Children are also very likely to hear errors, actual or only perceived (Krentz and Corina, 2008; Friederici et al., 2011). Furthermore, hypotheses concerning typological tendencies might not warrant comparison over fully congruent data sets. The premise of this work is therefore to extend the Bayesian system designed by Perfors et al. (2011) to allow for comparing models supported by different data subsets. We evaluated this system in the context of word order acquisition in German. To this end, we designed PCFGs representing different word order hypotheses a child might entertain. Many linguists agree that German word order is Subject-Object-Verb (SOV) Verb-Second (V2) (henceforth: SOV+V2) (e.g., Bierwisch, 1963). Yet many simple German sentences also maintain Subject-Verb-Object (SVO) order. Because of the SVO word order of many simple German sentences, a child might initially consider a left-branching grammar only able to account for SVO sentences, only to reject it in order to account for embedded sentences and sentences with auxiliaries. Note that we do not commit to a specific theoretical position on the plausibility of humans performing verb movement operations, and merely lean on this theory to specify conceivable word order hypotheses, assuming that a PCFG representing SVO word order should be able to parse a decent number of German sentences, but fewer than the SOV+V2 PCFG. Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011). Our PCFGs can thus serve as snapshots of different hypotheses comparable in a priori complexity that a child may at some point compare in order to determine the word order of their language, though they by no means exhaust the space of hypotheses that a child may consider.", "context_section_header": "", "context_paragraph": "Note that we do not commit to a specific theoretical position on the plausibility of humans performing verb movement operations, and merely lean on this theory to specify conceivable word order hypotheses, assuming that a PCFG representing SVO word order should be able to parse a decent number of German sentences, but fewer than the SOV+V2 PCFG. Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011). Our PCFGs can thus serve as snapshots of different hypotheses comparable in a priori complexity that a child may at some point compare in order to determine the word order of their language, though they by no means exhaust the space of hypotheses that a child may consider.", "sentence": "Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "cited_ids": [{"paper_id": "17269147", "citation": "Perfors et al. (2011)"}], "y": "Additionally, the authors' PCFGs [probabilistic context free grammars] are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "snippet_surface": "Additionally, the authors' PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011).", "questions": {"1uquVhbWQk": "What are pcfgs?"}, "answers": {"1uquVhbWQk": "probabilistic context free grammar"}, "evidence": {"1uquVhbWQk": [{"section": "Abstract", "paragraph": "The question of how children acquire the grammatical principles of their native language has been debated in cognitive science and linguistics for over 50 years (e.g., Chomsky, 1965; Xu and Tenenbaum, 2007). Perfors et al. (2011) suggest that children compare various syntactic hypotheses in order to determine which is most compatible with the socalled primary linguistic data (cf. Chomsky, 1965; Wexler and Culicover, 1980; Clark and Lappin, 2013; among many others), and that a Bayesian model selection approach might be able to do some of this work. More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG) that used hierarchical phrase structure over a regular grammar representing linear phrase structure, without any initial prior bias towards one grammar type or another. Therefore, they argued that correct linguistic generalizations can be achieved by a learner equipped with domain-general Bayesian inference capacities but without language-specific innate knowledge. In their work, the various syntactic hypotheses were represented by grammars that were all able to parse the entirety of a pre-processed English childdirected speech corpus. However, a computational model without the ability to compare competing hypotheses that are supported by different subsets of the data is severely limited due to the inevitability of noise. Children are also very likely to hear errors, actual or only perceived (Krentz and Corina, 2008; Friederici et al., 2011). Furthermore, hypotheses concerning typological tendencies might not warrant comparison over fully congruent data sets. The premise of this work is therefore to extend the Bayesian system designed by Perfors et al. (2011) to allow for comparing models supported by different data subsets. We evaluated this system in the context of word order acquisition in German. To this end, we designed PCFGs representing different word order hypotheses a child might entertain. Many linguists agree that German word order is Subject-Object-Verb (SOV) Verb-Second (V2) (henceforth: SOV+V2) (e.g., Bierwisch, 1963). Yet many simple German sentences also maintain Subject-Verb-Object (SVO) order. Because of the SVO word order of many simple German sentences, a child might initially consider a left-branching grammar only able to account for SVO sentences, only to reject it in order to account for embedded sentences and sentences with auxiliaries. Note that we do not commit to a specific theoretical position on the plausibility of humans performing verb movement operations, and merely lean on this theory to specify conceivable word order hypotheses, assuming that a PCFG representing SVO word order should be able to parse a decent number of German sentences, but fewer than the SOV+V2 PCFG. Additionally, our PCFGs are comparable in complexity in contrast to complexities differing by orders of magnitude for the grammars used by Perfors et al. (2011). Our PCFGs can thus serve as snapshots of different hypotheses comparable in a priori complexity that a child may at some point compare in order to determine the word order of their language, though they by no means exhaust the space of hypotheses that a child may consider.", "selected": "More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG)", "paper_id": "234324760"}, {"section": "Introduction", "paragraph": "The question of how children acquire the grammatical principles of their native language has been debated in cognitive science and linguistics for over 50 years (e.g., Chomsky, 1965;Xu and Tenenbaum, 2007). Perfors et al. (2011) suggest that children compare various syntactic hypotheses in order to determine which is most compatible with the socalled primary linguistic data (cf. Chomsky, 1965;Wexler and Culicover, 1980;Clark and Lappin, 2013; among many others), and that a Bayesian model selection approach might be able to do some of this work. More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG) that used hierarchical phrase structure over a regular grammar representing linear phrase structure, without any initial prior bias towards one grammar type or another. Therefore, they argued that correct linguistic generalizations can be achieved by a learner equipped with domain-general Bayesian inference capacities but without language-specific innate knowledge.", "selected": "More specifically, they demonstrate that a Bayesian inference system presented with child-directed input would prefer a probabilistic context free grammar (PCFG)", "paper_id": "234324760"}]}}
{"idx": "119638", "paper_id": "17786494", "title": "Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University Joint Learning for Coreference Resolution with Markov Logic Joint Learning for Coreference Resolution with Markov Logic", "abstract": "Pairwise coreference resolution models must merge pairwise coreference decisions to generate \ufb01nal outputs. Traditional merging methods adopt different strategies such as the best-\ufb01rst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classi\ufb01cation and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance.", "context_section_header": "", "context_paragraph": "In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009;Yoshikawa et al., 2009;Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. Compared with it, our methods are more applicable for real dataset. Huang et al. (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results. Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework.", "sentence": "Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches.", "cited_ids": [{"paper_id": "7124715", "citation": "Poon and Domingos (2008)"}], "y": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches. [To be more specific, the entity-mention model is an unsupervised machine learning method which performs joint inference across mentions and uses Markov logic as a representation language. The mention-pair model is a supervised method which splits the task into mention detection, pairwise classification and mention clustering.]", "snippet_surface": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches.", "questions": {"fgUyACF4RQ": "How are the both approaches different?"}, "answers": {"fgUyACF4RQ": "The entity-mention model is an unsupervised machine learning method which performs joint inference across mentions and uses Markov logic as a representation language. The mention-pair model is a supervised method which splits the task into mention detection, pairwise classification and mention clustering."}, "evidence": {"fgUyACF4RQ": [{"section": "Introduction", "paragraph": "The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "selected": "Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "paper_id": "17786494"}, {"section": "Introduction", "paragraph": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "selected": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "paper_id": "17786494"}, {"section": "Abstract", "paragraph": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "selected": "In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "paper_id": "7124715"}]}}
{"idx": "123037", "paper_id": "9453296", "title": "Building Specialized Bilingual Lexicons Using Word Sense Disambiguation", "abstract": "This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach.", "context_section_header": "", "context_paragraph": "Recent improvements of the standard approach are based on the assumption that the more the context vectors are representative, the better the bilingual lexicon extraction is. Prochasson et al. (2009) used transliterated words and scientific compound words as 'anchor points'. Giving these words higher priority when comparing target vectors improved bilingual lexicon extraction. In addition to transliteration, Rubino and Linar\u00e8s (2011) combined the contextual representation within a thematic one. The basic intuition of their work is that a term and its translation share thematic similarities. Hazem and Morin (2012) recently proposed a method that filters the entries of the bilingual dictionary based upon POS-tagging and domain relevance criteria, but no improvements was demonstrated. Gaussier et al. (2004) attempted to solve the problem of different word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with a very small improvement were reported for a mixed method. One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors.", "sentence": "One important difference with Gaussier et al. (2004) is that they focus on words ambiguities on source and target languages, whereas we consider that it is sufficient to disambiguate only translated source context vectors.", "cited_ids": [{"paper_id": "459519", "citation": "Gaussier et al. (2004)"}], "y": "One important difference between Gaussier et al. (2004) and the authors' work is Gaussier et al. focus on words ambiguities on source and target languages [iin machine translation], whereas the authors consider that it is sufficient to disambiguate only translated source context vectors.", "snippet_surface": "One important difference between the authors and Gaussier et al. (2004) is that the latter focus on word ambiguities on source and target languages, whereas the authors consider it sufficient to disambiguate only translated source context vectors.", "questions": {"V/RIx+ylq2": "What are the source and target languages?"}, "answers": {"V/RIx+ylq2": "Source is the language a word comes from and target the language that it's translated into."}, "evidence": {"V/RIx+ylq2": [{"section": "Introduction", "paragraph": "Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998;Rapp, 1995;Chiao and Zweigenbaum, 2003). The main work in this research area could be seen as an extension of Harris's distributional hypothesis (Harris, 1954). It is based on the simple observation that a word and its translation are likely to appear in similar contexts across languages (Rapp, 1995). Based on this assumption, the alignment method, known as the standard approach builds and compares context vectors for each word of the source and target languages.", "selected": "Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998;Rapp, 1995;Chiao and Zweigenbaum, 2003). The main work in this research area could be seen as an extension of Harris's distributional hypothesis (Harris, 1954). It is based on the simple observation that a word and its translation are likely to appear in similar contexts across languages (Rapp, 1995). Based on this assumption, the alignment method, known as the standard approach builds and compares context vectors for each word of the source and target languages.", "paper_id": "9453296"}]}}
{"idx": "135732", "paper_id": "206467", "title": "Modeling Syntactic Context Improves Morphological Segmentation", "abstract": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "context_section_header": "", "context_paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "sentence": "Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "cited_ids": [{"paper_id": "9519654", "citation": "(Poon et al., 2009"}], "y": "The author's full model [which is to learn words with common affixes and use learned syntactic categories to refine morphological segmentation] yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5% [by integrating POS categorization and grammatical agreement realization].", "snippet_surface": "The authors' full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "questions": {"6umauXyFX1": "What is the authors' full model?", "R9xqYKYHzl": "How does the authors' model outperforms the best published results?"}, "answers": {"6umauXyFX1": "The author's full model is to learn words with common affixes and use learned syntactic categories to refine morphological segmentation.", "R9xqYKYHzl": "The author's model outperforms Poon et al., by integrating POS categorization and realization of grammatical agreement which demonstrate that incorporating syntactic information improve morphological analysis."}, "evidence": {"6umauXyFX1": [{"section": "Abstract", "paragraph": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "selected": "Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "selected": "We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.", "paper_id": "206467"}], "R9xqYKYHzl": [{"section": "Introduction", "paragraph": "\u2022 Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "selected": "Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological realization of grammatical agreement. In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers. For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes. Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "selected": "Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological realization of grammatical agreement. In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers. For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes. Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "selected": "Morphological realization of grammatical agreement.", "paper_id": "206467"}, {"section": "Linguistic Intuition", "paragraph": "Morphological realization of grammatical agreement. In morphologically rich languages, agreement is commonly realized using matching suffices. In many cases, members of a dependent pair such as adjective and noun have the exact same suffix. A common example in the Arabic Treebank is the bigram \"Al-Df-p Al-grby-p\" (which is translated word-for-word as \"the-bank the-west\") where the last morpheme \"p\" is a feminine singular noun suffix.", "selected": "Morphological realization of grammatical agreement.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "selected": "Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "paper_id": "206467"}]}}
{"idx": "137696", "paper_id": "12186762", "title": "Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis", "abstract": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "context_section_header": "", "context_paragraph": "Several differences between this work and previous approaches make direct comparisons challenging and possibly not very informative. (Socher et al., 2013) use syntactic trees, as opposed to discourse trees, as recursive structures for training. Thus we cannot compare with his \"All\"-level results. For \"Root\"-level, (Socher et al., 2013) reports 45.7% fine-grained sentiment accuracy compared to 44.82% of our Multi-tasking. This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "sentence": "This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "cited_ids": [{"paper_id": "12252194", "citation": "(Bhatia et al., 2015)"}], "y": "The authors suggest that [root-level report difference between their work and other approaches] is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU [elementary discourse unit] level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach [of creating a vector, applying three different recursive neural net models and then combining these models in two joint models].", "snippet_surface": "The authors suggest that this difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "OIgzN1GRK4": "What is \"EDU\" level?"}, "answers": {"6gKwRw0I/Q": "they create a vector and apply three different recursive neural net models. They then combine these models in two joint models", "OIgzN1GRK4": "elementary discourse units level"}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "selected": "In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training.", "paper_id": "12186762"}], "OIgzN1GRK4": [{"section": "Introduction", "paragraph": "This paper focuses on studying two fundamental NLP tasks, Discourse Parsing and Sentiment Analysis. The importance of these tasks and their wide applications (e.g., (Gerani et al., 2014), (Rosenthal et al., 2014)) has initiated much interest in studying both, but no method yet exists that can come close to human performance in solving them. Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1: The Discourse Tree of a sentence from Sentiment Treebank dataset trees' text spans 1 . Nodes also have labels identifying discourse relationships (\"contrast\", \"evidence\", etc.) between their corresponding subtrees. The relation also specifies nucliearity of the children. Nuclei are the core parts of the relation and Satellites are the supportive ones.", "selected": "Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1", "paper_id": "12186762"}]}}
{"idx": "138193", "paper_id": "661892", "title": "Regularizing Mono- and Bi-Word Models for Word Alignment", "abstract": "Conditional probabilistic models for word alignment are popular due to the elegant way of handling them in the training stage. However, they have weaknesses such as garbage collection and scale poorly beyond single word based models (DeNero et al., 2006): not all parameters should actually be used. To alleviate the problem, in this paper we explore regularity terms that penalize the used parameters. They share the advantages of the standard training in that iterative schemes decompose over the sentence pairs. We explore the models IBM-1 and HMM, then generalize to models we term Bi-word models, where each target word can be aligned to up to two source words. We give two optimization strategies for the arising tasks, using EM and projected gradient descent. While both are well-known, to our knowledge they have never been compared experimentally for the task of word alignment. As a side-effect, we show that, against common belief, for parametric HMMs the M-step is not solved by renormalizing expectations. We demonstrate that the regularity terms improve on the f-measures of the standard HMMs and that they improve translation quality.", "context_section_header": "", "context_paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "sentence": "In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "cited_ids": [{"paper_id": "8518269", "citation": "(Schoenemann, 2011)"}], "y": "In contrast to the authors' recent work [on machine translation] (Schoenemann, 2011) (where they used an L 0 -norm [for single-word based alignment of bilingual sentence pairs]) they do not use the maximum approximation and also address Bi-word models [, where each target word is allowed to align to up to two source words].", "snippet_surface": "In contrast to the authors' recent work (Schoenemann, 2011) (where they used an L 0 -norm) they do not use the maximum approximation and also address Bi-word models.", "questions": {"d2gzArachg": "What bi-word models are being referred ?", "dH+2p4ZAjS": "What is the authors' recent work?"}, "answers": {"d2gzArachg": "In Bi-word models, each target word is allowed to align to up to two source words.", "dH+2p4ZAjS": "The authors' recent work on machine translation contributes to single-word based alignment of bilingual sentence pairs by combining probabilistic and non-probabilistic approaches (using the L0-norm). Unlike the current work, they do not address alignments of more than a single word."}, "evidence": {"d2gzArachg": [{"section": "Bi-Word Models", "paragraph": "In this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words. The alignment of target word j is expressed as the tuple (a j,1 , a j,2 ), where the allowed set of values is a subset of {0, . . . , I} \u00d7 {0, . . . , I}. The value (0, 0) will denote unaligned words. In any other case we require that a j,2 > a j,1 . If a j,1 is 0 the word is aligned only once. If a j,1 > 0 it is aligned twice. We further forbid the case where a j,1 > 0 and a j,2 = I since at the sentence end the considered data usually contain a punctuation mark which aligns only once. Note that otherwise there are no restrictions, in particular we do not require that the two aligned words are at consecutive positions (although such knowledge could be enforced in our framework).", "selected": "n this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words.", "paper_id": "661892"}], "dH+2p4ZAjS": [{"section": "Introduction", "paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "selected": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "paper_id": "661892"}, {"section": "Introduction", "paragraph": "Traditionally (Brown et al., 1993b;Al-Onaizan et al., 1999) single word based models are trained by the EM-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences. Refinements that also allow symmetrized models are based on bipartite graph matching (Matusov et al., 2004;Taskar et al., 2005) or quadratic assignment problems (Lacoste-Julien et al., 2006). Recently, Bodrumlu et al. (2009) proposed the first method that treats a nondecomposable problem by handling all sentence pairs at once and via integer linear programming. Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "selected": "Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "paper_id": "8518269"}, {"section": "Introduction", "paragraph": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "selected": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "paper_id": "8518269"}]}}
{"idx": "145596", "paper_id": "102351751", "title": "Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models", "abstract": "We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems.", "context_section_header": "", "context_paragraph": "We evaluate the ability of our model in generating graph paths for previously unseen textual definitions on seven ontologies (Section 3). We show that our technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings. The code and resources for the paper can be found at https://github.com/VictorProkhorov/ Text2Path.", "sentence": "We show that our technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings.", "cited_ids": [{"paper_id": "52076072", "citation": "(Kartsaklis et al., 2018)"}], "y": "[The authors present a new technique that maps a textual description of a concept to a hierarchical sequence of the concepts' hypernyms to find other senses of the concept word] on par or outperforming the competitive multi-sense LSTM model thanks to a better use of external information in the form of word embedding.", "snippet_surface": "The authors show that their technique either outperforms or performs on a par with a competitive multi-sense LSTM model (Kartsaklis et al., 2018) by better utilising external information in the form of word embeddings.", "questions": {"vXj4ywMoad": "What is the authors' technique?"}, "answers": {"vXj4ywMoad": "The authors developed a novel technique that maps a textual description of a concept to a hierarchical sequence of the concept's hypernyms to find other senses of the concept word."}, "evidence": {"vXj4ywMoad": [{"section": "Introduction", "paragraph": "To address these issues we propose a novel technique which first represents an ontology concept as a sequence of its ancestors in the ontology (hypernyms) and then maps the corresponding textual description to this unique representation. For example, given the textual description of the concept swift (\"small bird that resembles a swallow and is noted for its rapid flight\"), we map it to the hierarchical sequence of entities in a lexical ontology: animal \u2192 chordate \u2192 vertebrate \u2192 bird \u2192 apodiform bird. This sequence of nodes constitutes a path. 1 Our model is based on a sequence-to-sequence neural network (Sutskever et al., 2014) coupled with an attention mechanism (Bahdanau et al., 2014). Specifically, we use an LSTM (Hochreiter and Schmidhuber, 1997) encoder to project the textual description into a vector space and an LSTM decoder to predict the sequence of entities that are relevant to this definition. With this framework we do not need to rely on the pre-existing vector space of the entities, since the decoder explicitly learns topological dependencies between the entities of the ontology. Furthermore, the proposed model is more interpretable for two reasons. First, instead of the closest points in a vector space, it outputs paths; therefore, we can trace all predictions the model makes. Second, the attention mechanism allows to visualise which words in a textual description the model selects while predicting a specific concept in the path. In this paper, we consider rooted tree graphs 2 only and leave the extension of the algorithm for more generic graphs to future work.", "selected": "To address these issues we propose a novel technique which first represents an ontology concept as a sequence of its ancestors in the ontology (hypernyms) and then maps the corresponding textual description to this unique representation.", "paper_id": "102351751"}, {"section": "Introduction", "paragraph": "To address these issues we propose a novel technique which first represents an ontology concept as a sequence of its ancestors in the ontology (hypernyms) and then maps the corresponding textual description to this unique representation. For example, given the textual description of the concept swift (\"small bird that resembles a swallow and is noted for its rapid flight\"), we map it to the hierarchical sequence of entities in a lexical ontology: animal \u2192 chordate \u2192 vertebrate \u2192 bird \u2192 apodiform bird. This sequence of nodes constitutes a path. 1 Our model is based on a sequence-to-sequence neural network (Sutskever et al., 2014) coupled with an attention mechanism (Bahdanau et al., 2014). Specifically, we use an LSTM (Hochreiter and Schmidhuber, 1997) encoder to project the textual description into a vector space and an LSTM decoder to predict the sequence of entities that are relevant to this definition. With this framework we do not need to rely on the pre-existing vector space of the entities, since the decoder explicitly learns topological dependencies between the entities of the ontology. Furthermore, the proposed model is more interpretable for two reasons. First, instead of the closest points in a vector space, it outputs paths; therefore, we can trace all predictions the model makes. Second, the attention mechanism allows to visualise which words in a textual description the model selects while predicting a specific concept in the path. In this paper, we consider rooted tree graphs 2 only and leave the extension of the algorithm for more generic graphs to future work.", "selected": "we use an LSTM (Hochreiter and Schmidhuber, 1997) encoder to project the textual description into a vector space and an LSTM decoder to predict the sequence of entities that are relevant to this definition.", "paper_id": "102351751"}, {"section": "Introduction", "paragraph": "To address these issues we propose a novel technique which first represents an ontology concept as a sequence of its ancestors in the ontology (hypernyms) and then maps the corresponding textual description to this unique representation. For example, given the textual description of the concept swift (\"small bird that resembles a swallow and is noted for its rapid flight\"), we map it to the hierarchical sequence of entities in a lexical ontology: animal \u2192 chordate \u2192 vertebrate \u2192 bird \u2192 apodiform bird. This sequence of nodes constitutes a path. 1 Our model is based on a sequence-to-sequence neural network (Sutskever et al., 2014) coupled with an attention mechanism (Bahdanau et al., 2014). Specifically, we use an LSTM (Hochreiter and Schmidhuber, 1997) encoder to project the textual description into a vector space and an LSTM decoder to predict the sequence of entities that are relevant to this definition. With this framework we do not need to rely on the pre-existing vector space of the entities, since the decoder explicitly learns topological dependencies between the entities of the ontology. Furthermore, the proposed model is more interpretable for two reasons. First, instead of the closest points in a vector space, it outputs paths; therefore, we can trace all predictions the model makes. Second, the attention mechanism allows to visualise which words in a textual description the model selects while predicting a specific concept in the path. In this paper, we consider rooted tree graphs 2 only and leave the extension of the algorithm for more generic graphs to future work.", "selected": "For example, given the textual description of the concept swift (\"small bird that resembles a swallow and is noted for its rapid flight\"), we map it to the hierarchical sequence of entities in a lexical ontology: animal \u2192 chordate \u2192 vertebrate \u2192 bird \u2192 apodiform bird.", "paper_id": "102351751"}]}}
{"idx": "146809", "paper_id": "52183735", "title": "Multi-view Models for Political Ideology Detection of News Articles", "abstract": "A news article\u2019s title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.", "context_section_header": "", "context_paragraph": "Several works study the detection of political ideology through the lens of computational linguistics and natural language processing (Laver et al., 2003;Monroe and Maeda, 2004;Thomas et al., 2006;Lin et al., 2008;Carroll et al., 2009;Ahmed and Xing, 2010;Gentzkow and Shapiro, 2010;Gerrish and Blei, 2011;Sim et al., 2013). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called \"slant index\" which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei (2011) predict the voting patterns of Congress members based on supervised topic models. Other works use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010;Lin et al., 2008). Sim et al. (2013) propose a novel HMM-based model to infer the ideological proportions of the rhetoric used by political candidates in their campaign speeches which relies on a fixed lexicon of bigrams associated with ideologies. The work that is most closely related to our work is that of Iyyer et al. (2014);Preo\u0163iuc-Pietro et al. (2017). Iyyer et al. (2014) use recurrent neural networks to predict political ideology of congressional debates and articles in the ideological book corpus (IBC) and demonstrate the importance of compositionality in predicting ideology where modifier phrases and punctuality affect the political ideological position. Preo\u0163iuc-Pietro et al. (2017) propose models to infer political ideology of Twitter users based on their everyday language. Most crucially, they also show how to effectively use the relationship between user groups to improve prediction accuracy. Our work draws inspiration from both of these works but differentiates itself from these in the following aspects: We leverage the structure of a news article by noting that an article is just not free-form text, but has a rich structure to it. In particular, we model cues from the title, the inferred network, and the content in a joint generic neural variational inference framework to yield improved models for this task. Furthermore, differing from Iyyer et al. (2014), we also incorporate attention mechanisms in our model which enables us to inspect which sentences (or words) have the most predictive power as captured by our model. Finally, since we work with news articles (which also contain hyperlinks), naturally our setting is different from all other previous works in general (which mostly focus on congressional debates) and in particular from Iyyer et al. (2014) where only textual content is modeled or Preo\u0163iuc-Pietro et al. (2017) which focuses on social media users.", "sentence": "Furthermore, differing from Iyyer et al. (2014), we also incorporate attention mechanisms in our model which enables us to inspect which sentences (or words) have the most predictive power as captured by our model.", "cited_ids": [{"paper_id": "216636598", "citation": "Iyyer et al. (2014)"}], "y": "The authors' model [a recurrent neural network] also incorporates attention mechanisms [word level and sentence level], unlike the one from Iyyer et al. (2014), which enables them to inspect which sentences (or words) have the most predictive power as captured by the model.", "snippet_surface": "Furthermore, the authors' model also incorporates attention mechanisms, unlike the one from Iyyer et al. (2014), which enables them to inspect which sentences (or words) have the most predictive power as captured by the model.", "questions": {"ZCvSqcibLE": "What model is being referenced?"}, "answers": {"ZCvSqcibLE": "The referenced model is a recurrent neural network"}, "evidence": {"ZCvSqcibLE": [{"section": "Related Work", "paragraph": "Several works study the detection of political ideology through the lens of computational linguistics and natural language processing (Laver et al., 2003;Monroe and Maeda, 2004;Thomas et al., 2006;Lin et al., 2008;Carroll et al., 2009;Ahmed and Xing, 2010;Gentzkow and Shapiro, 2010;Gerrish and Blei, 2011;Sim et al., 2013). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called \"slant index\" which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei (2011) predict the voting patterns of Congress members based on supervised topic models. Other works use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010;Lin et al., 2008). Sim et al. (2013) propose a novel HMM-based model to infer the ideological proportions of the rhetoric used by political candidates in their campaign speeches which relies on a fixed lexicon of bigrams associated with ideologies. The work that is most closely related to our work is that of Iyyer et al. (2014);Preo\u0163iuc-Pietro et al. (2017). Iyyer et al. (2014) use recurrent neural networks to predict political ideology of congressional debates and articles in the ideological book corpus (IBC) and demonstrate the importance of compositionality in predicting ideology where modifier phrases and punctuality affect the political ideological position. Preo\u0163iuc-Pietro et al. (2017) propose models to infer political ideology of Twitter users based on their everyday language. Most crucially, they also show how to effectively use the relationship between user groups to improve prediction accuracy. Our work draws inspiration from both of these works but differentiates itself from these in the following aspects: We leverage the structure of a news article by noting that an article is just not free-form text, but has a rich structure to it. In particular, we model cues from the title, the inferred network, and the content in a joint generic neural variational inference framework to yield improved models for this task. Furthermore, differing from Iyyer et al. (2014), we also incorporate attention mechanisms in our model which enables us to inspect which sentences (or words) have the most predictive power as captured by our model. Finally, since we work with news articles (which also contain hyperlinks), naturally our setting is different from all other previous works in general (which mostly focus on congressional debates) and in particular from Iyyer et al. (2014) where only textual content is modeled or Preo\u0163iuc-Pietro et al. (2017) which focuses on social media users.", "selected": "Iyyer et al. (2014) use recurrent neural networks to predict political ideology of congressional debates and articles in the ideological book corpus (IBC) and demonstrate the importance of compositionality in predicting ideology where modifier phrases and punctuality affect the political ideological position.", "paper_id": "52183735"}]}}
{"idx": "147259", "paper_id": "18492550", "title": "Experiments on Active Learning for Croatian Word Sense Disambiguation", "abstract": "Supervised word sense disambiguation (WSD) has been shown to achieve state-ofthe-art results but at high annotation costs. Active learning can ameliorate that problem by allowing the model to dynamically choose the most informative word contexts for manual labeling. In this paper we investigate the use of active learning for Croatian WSD. We adopt a lexical sample approach and compile a corresponding senseannotated dataset on which we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance.", "context_section_header": "", "context_paragraph": "All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences. We study three sampling methods in this work, but leave the issues of stopping criterion and class imbalance for future work.", "sentence": "In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences.", "cited_ids": [{"paper_id": "403206", "citation": "Chen et al. (2006)"}], "y": "In contrast to Chen et al. (2006), the authors opt for [two features: 1. A simple binary bag-of-words vector (BoW) 2. The skip-gram model proposed by Mikolov et al. (2013),] derived from [given a sentence in which a polysemous word occurs, the context vector is calculated based on the words it co-occurs within the sentence].", "snippet_surface": "In contrast to Chen et al. (2006), the authors opt for simple, readily available features derived from cooccurrences.", "questions": {"p0t7dHlccT": "What do the simple, readily available features look like?"}, "answers": {"p0t7dHlccT": "Two features:\n1. A simple binary bag-of-words vector (BoW)\n2. The skip-gram model proposed by Mikolov et al. (2013)"}, "evidence": {"p0t7dHlccT": [{"section": "Classifier and Features", "paragraph": "We opt for a simple model with readily available features. The simplest features are word-based context representations: given a sentence in which a polysemous word occurs, we compute its context vector by considering the words it co-occurs with in the sentence. We consider two context representations. First is a simple binary bag-of-words vector (BoW). In our case, the average dimension of a BoW vector is approximately 7000.", "selected": "We opt for a simple model with readily available features. The simplest features are word-based context representations: given a sentence in which a polysemous word occurs, we compute its context vector by considering the words it co-occurs with in the sentence. We consider two context representations. First is a simple binary bag-of-words vector (BoW). In our case, the average dimension of a BoW vector is approximately 7000.", "paper_id": "18492550"}, {"section": "Classifier and Features", "paragraph": "The second representation we use is the recently proposed skip-gram model, a neural word embedding method of Mikolov et al. (2013), which has shown to be useful on a series of NLP tasks. To obtain a context vector, we simply add up the skipgram vectors of all the context words. The advantage of skip-gram representation over BoW is that it generates compact, continuous, and distributed vectors representations such that semantically related words tend to have similar vectors. This not only results in more effective context representations, but also allows for a better generalization, as context vectors of words unseen during training will be similar to vectors of semantically related context words used for training. We build the vectors from hrWaC using the word2vec 5 tool. We use 300 dimensions, negative sampling parameter set to 5, minimum frequency set to 100, and no hierarchical softmax.", "selected": "The second representation we use is the recently proposed skip-gram model, a neural word embedding method of Mikolov et al. (2013), which has shown to be useful on a series of NLP tasks. To obtain a context vector, we simply add up the skipgram vectors of all the context words.", "paper_id": "18492550"}]}}
{"idx": "154425", "paper_id": "248780060", "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression", "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "context_section_header": "", "context_paragraph": "Knowledge Distillation. Knowledge distillation (Hinton et al., 2015) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015) first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019); Sun et al. (2019);Liang et al. (2020) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019) proposed a contrastive distillation framework where the teacher's representations were treated as positives to the corresponding student's representations. Sun et al. (2020); Fu et al. (2021) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019); Tung and Mori (2019); Park et al. (2019) pointed out that the relations of the image representations of the teacher should be preserved in the student's feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020); Wang et al. ( , 2021 used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "sentence": "Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "cited_ids": [{"paper_id": "237091399", "citation": "Shao and Chen (2021)"}], "y": "The authors jointly transfer the token-level, span-level and sample-level structural knowledge [hierarchically across layers], which is different from previous works that only considered a single [level or granularity] of representations. Compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality and presents a different definition of granularity [as a distillation mechanism].\"", "snippet_surface": "Different from previous works that only considered a single granularity of representations, the authors jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality, presents a different definition of granularity.", "questions": {"rEPElQHLo6": "How do the authors jointly transfer the structural knowledge?", "nGd4JG9F8T": "What is the granularity of representation?"}, "answers": {"rEPElQHLo6": "Hierarchically across layers.", "nGd4JG9F8T": "The granularities they refer to are at the semantic level: token-level, span-level, and sample-level."}, "evidence": {"rEPElQHLo6": [{"section": "Abstract", "paragraph": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "selected": "Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers.", "paper_id": "248780060"}], "nGd4JG9F8T": [{"section": "Method", "paragraph": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1.", "selected": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level", "paper_id": "248780060"}]}}
{"idx": "1603.09631.1.1.1", "paper_id": "1603.09631", "title": "Data Collection for Interactive Learning through the Dialog", "abstract": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "context_section_header": "Dataset Collection Process", "context_paragraph": "However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "sentence": "Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "cited_ids": [], "y": "The authors used the crowdsourcing platform CrowdFlower (CF) for their data collection [which consisted of natural dialogs].", "snippet_surface": "Therefore, the authors used the crowdsourcing platform CrowdFlower (CF) for their data collection.", "questions": {"jev62F+g9i": "Where did \"their data collection\" come from?"}, "answers": {"jev62F+g9i": "Their data collection consisted of natural dialogs that were acquired from a crowdsourcing platform CrowdFlower (CF)."}, "evidence": {"jev62F+g9i": [{"section": "Abstract", "paragraph": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "selected": "The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "paper_id": "1603.09631"}, {"section": "Dataset Collection Process", "paragraph": "A perfect data collection scenario for our dataset would use real running dialog system providing general information from the knowledge base to real users. This system could then ask for explanations and answers for questions which it is not able to answer.", "selected": "A perfect data collection scenario for our dataset would use real running dialog system providing general information from the knowledge base to real users.", "paper_id": "1603.09631"}, {"section": "Dataset Collection Process", "paragraph": "However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "selected": "Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection.", "paper_id": "1603.09631"}]}}
{"idx": "1604.00400.1.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Data", "context_paragraph": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "sentence": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "cited_ids": [], "y": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera) [which assess the content relevance between system generated summaries and human written summaries], they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "snippet_surface": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera), they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "questions": {"0hh9M7Z32H": "What does \"rouge\" mean?"}, "answers": {"0hh9M7Z32H": "Rouge is a publicly available evaluation metric widely used in summarisation evaluation, i.e. the content relevance between system generated summaries and human written summaries."}, "evidence": {"0hh9M7Z32H": [{"section": "Introduction", "paragraph": "Automatic text summarization has been an active research area in natural language processing for several decades. To compare and evaluate the performance of different summarization systems, the most intuitive approach is assessing the quality of the summaries by human evaluators. However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce BIBREF0 . To address these problems, automatic evaluation measures for summarization have been proposed. Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries. It is inspired by the success of a similar metric Bleu BIBREF2 which is being used in Machine Translation (MT) evaluation. The main success of Rouge is due to its high correlation with human assessment scores on standard benchmarks BIBREF1 . Rouge has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC[1] BIBREF3 .", "selected": "Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries.", "paper_id": "1604.00400"}]}}
{"idx": "1604.00400.5.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Introduction", "context_paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "sentence": "Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "cited_ids": [], "y": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. The authors also show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "snippet_surface": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, they show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "questions": {"ICq7mtjXWU": "What does \"the authors' results\" refer to?"}, "answers": {"ICq7mtjXWU": "The \"results\" refer to the authors comparing Rouge scores with semi-manual evaluation score on the TAC dataset (i.e., the main contribution of the rest of the paper)."}, "evidence": {"ICq7mtjXWU": [{"section": "Introduction", "paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "selected": "comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset", "paper_id": "1604.00400"}]}}
{"idx": "1604.00727.1.1.1", "paper_id": "1604.00727", "title": "Character-Level Question Answering with Attention", "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.", "context_section_header": "Dataset and Experimental Settings", "context_paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "sentence": "In contrast, our models are trained only on the 76K questions in the training set.", "cited_ids": [], "y": "The authors' models are trained only on the 76K questions in the training set [which comprises the random extract of 76k single-relation questions and their corresponding triples from the SimpleQuestions dataset].", "snippet_surface": "In contrast, the authors' models are trained only on the 76K questions in the training set.", "questions": {"KlXeu/+jcE": "What dataset does \"the training set\" refer to?"}, "answers": {"KlXeu/+jcE": "The \"training dataset\" refer to the random extract of 76k ssingle-relation questions and their corresponding triples from the SimpleQuestions dataset. This is opposed to the additional data used for training the MemNNs model."}, "evidence": {"KlXeu/+jcE": [{"section": "Dataset and Experimental Settings", "paragraph": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "selected": "The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train,", "paper_id": "1604.00727"}, {"section": "Dataset and Experimental Settings", "paragraph": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "selected": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0", "paper_id": "1604.00727"}, {"section": "Dataset and Experimental Settings", "paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "selected": "76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively.", "paper_id": "1604.00727"}]}}
{"idx": "1606.02891.1.1.1", "paper_id": "1606.02891", "title": "Edinburgh Neural Machine Translation Systems for WMT 16", "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English<->Czech, English<->German, English<->Romanian and English<->Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.", "context_section_header": "Baseline System", "context_paragraph": "Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", "sentence": "Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", "cited_ids": [], "y": "The authors' systems are attentional encoder-decoder networks. The authors based implementation on the dl4mt-tutorial [which consists of using minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024], which the authors enhanced with new features such as ensemble decoding and pervasive dropout.", "snippet_surface": "The authors' systems are attentional encoder-decoder networks BIBREF0. They based their implementation on the dl4mt-tutorial, which they enhanced with new features such as ensemble decoding and pervasive dropout.", "questions": {"J6Hw3kzUTT": "What does \"dl4mt-tutorial\" consist of?"}, "answers": {"J6Hw3kzUTT": "The author's dl4mt-tutorial consists of using minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. The gradient norm was clipped to 1.0."}, "evidence": {"J6Hw3kzUTT": [{"section": "Baseline System", "paragraph": "We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 BIBREF4 . We train the models with Adadelta BIBREF5 , reshuffling the training corpus between epochs. We validate the model every 10000 minibatches via Bleu on a validation set (newstest2013, newstest2014, or half of newsdev2016 for EN INLINEFORM0 RO). We perform early stopping for single models, and use the 4 last saved models (with models saved every 30000 minibatches) for the ensemble results. Note that ensemble scores are the result of a single training run. Due to resource limitations, we did not train ensemble components independently, which could result in more diverse models and better ensembles.", "selected": "We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 BIBREF4 . We train the models with Adadelta BIBREF5 , reshuffling the training corpus between epochs. We validate the model every 10000 minibatches via Bleu on a validation set (newstest2013, newstest2014, or half of newsdev2016 for EN INLINEFORM0 RO).", "paper_id": "1606.02891"}]}}
{"idx": "1606.03676.1.1.1", "paper_id": "1606.03676", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "abstract": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "context_section_header": "Corpora", "context_paragraph": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43 .", "sentence": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "cited_ids": [], "y": "The authors carried out their experiments [comparing the performance of systems on datasets covering 16 languages] on the Universal Dependencies v1.2 treebanks BIBREF21, (UD1.2), from which morphosyntactically annotated corpora can be trivially extracted.", "snippet_surface": "The authors carried out their experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "questions": {"5hENcascs9": "What does \"their experiments\" refer to?"}, "answers": {"5hENcascs9": "Their experiments refers to the comparison between performance of our systems on datasets covering 16 languages."}, "evidence": {"5hENcascs9": [{"section": "Abstract", "paragraph": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "selected": "Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs).", "paper_id": "1606.03676"}]}}
{"idx": "1609.00425.1.1.1", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism data", "context_paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "sentence": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "cited_ids": [], "y": "To collect a diverse training dataset [for training their classifier to predict dogmatism across different subreddits], the authors randomly sampled 1000 posts from each of the subreddits: \"politics\", \"business\", \"science\", \"AskReddit\", and 1000 additional posts from the Reddit front page.", "snippet_surface": "To collect a diverse training dataset, the authors have randomly sampled 1000 posts from each of the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "questions": {"l33yiX02oV": "What task is the training dataset for?"}, "answers": {"l33yiX02oV": "The training dataset is for predicting dogmatism and to test domain-independence of the classification strategies using curated datasets from different subreddits."}, "evidence": {"l33yiX02oV": [{"section": "Abstract", "paragraph": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "selected": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "We now show how we can use the linguistic feature sets we have described to build a classifier that predicts dogmatism in comments. A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "selected": "A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "Prediction task. Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies. First, we test the performance of our model under cross-validation within the Reddit comment dataset. We then evaluate the Reddit-based model on a held out corpus of New York Times comments annotated using the technique in Section 2. We did not refer to this second dataset during feature construction.", "selected": "Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies.", "paper_id": "1609.00425"}]}}
{"idx": "1609.00425.1.2.2", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism in the Reddit Community ", "context_paragraph": "We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions.", "sentence": "Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "cited_ids": [], "y": "The authors apply the [bag-of-words and linguistic features] model trained on [different subreddit representing different topics, such as politics, business, science and other other posts in the Reddit home page] to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "snippet_surface": "The authors apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "questions": {"wufYdpWojb": "What does \"BOW+LING\" stand for?", "YUmLGBJKGO": "What does \"the full Reddit dataset\" consist of?"}, "answers": {"wufYdpWojb": "BOW stand for bag-of-words, whereas LING stands for linguistic features from the authors' earlier analyses", "YUmLGBJKGO": "The full Reddit dataset consists of different subreddits representing different topics, such as politics, business, science and other posts in the Reddit home page."}, "evidence": {"wufYdpWojb": [{"section": "Predicting dogmatism", "paragraph": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).", "selected": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.", "paper_id": "1609.00425"}], "YUmLGBJKGO": [{"section": "Dogmatism data", "paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "selected": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "paper_id": "1609.00425"}]}}
{"idx": "1611.03599.1.2.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "sentence": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "cited_ids": [], "y": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns), taking into account only the post content. Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "snippet_surface": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns), taking into account only the post content. Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant.", "questions": {"b9GV7o0hWD": "What does \"chinese facebook fan groups\" refer to?"}, "answers": {"b9GV7o0hWD": "This refers to a single-topic, chinese, social media dataset from a Facebook group devoted to being anti-nuclear-power."}, "evidence": {"b9GV7o0hWD": [{"section": "Dataset", "paragraph": "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.", "selected": "FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset", "paper_id": "1611.03599"}, {"section": "Dataset", "paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "selected": "anti-nuclear-power Chinese Facebook fan groups", "paper_id": "1611.03599"}]}}
{"idx": "1611.03599.3.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "sentence": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "cited_ids": [], "y": "The authors collected the CreateDebate dataset from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR) [to develop a deep learning model for stance classification].", "snippet_surface": "The authors collected the CreateDebate dataset from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "questions": {"x8fyBC/IDt": "What are the authors doing with this data?"}, "answers": {"x8fyBC/IDt": "The authors are trying to develop a deep learning model for  stance classification."}, "evidence": {"x8fyBC/IDt": [{"section": "Abstract", "paragraph": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "selected": "In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts.", "paper_id": "1611.03599"}]}}
{"idx": "1611.03599.4.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "To test whether the assumption of this paper \u2013 posts attract users who hold the same stance to like them \u2013 is reliable, we examine the likes from authors of different stances. Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. As the numbers of authors in the Sup, Neu and Uns stances are largely imbalanced, these numbers are normalized by the number of users of each stance. Table TABREF13 shows the results. Posts with stances (i.e., not neutral) attract users of the same stance. Neutral posts also attract both supportive and neutral users, like what we observe in supportive posts, but just the neutral posts can attract even more neutral likers. These results do suggest that users prefer posts of the same stance, or at least posts of no obvious stance which might cause annoyance when reading, and hence support the user modeling in our approach.", "sentence": "Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts.", "cited_ids": [], "y": "For this analysis, the authors use posts [from FBFans, a single-topic Chinese unbalanced social media dataset obtained from Facebook]. They calculate the like statistics of each distinct author from these 32,595 posts.", "snippet_surface": "The authors use posts in FBFans dataset for this analysis. They calculate the like statistics of each distinct author from these 32,595 posts.", "questions": {"Zbh8C31sut": "What does \"fbfans\" consist of?"}, "answers": {"Zbh8C31sut": "FBFans consists of a single-topic Chinese unbalanced social media dataset obtained from Facebook."}, "evidence": {"Zbh8C31sut": [{"section": "Dataset", "paragraph": "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.", "selected": "FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset,", "paper_id": "1611.03599"}, {"section": "Dataset", "paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "selected": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups", "paper_id": "1611.03599"}]}}
{"idx": "1611.03599.5.2.2", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "sentence": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "cited_ids": [], "y": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9, BIBREF5.", "snippet_surface": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of", "questions": {"wQbDAHMW9c": "What does \"labeling results\" refer to?"}, "answers": {"wQbDAHMW9c": "The stance labelling results, either for (F) or against (A), of the proposed method on the CreateDebate dataset."}, "evidence": {"wQbDAHMW9c": [{"section": "Dataset", "paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "selected": "The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12", "paper_id": "1611.03599"}]}}
{"idx": "1611.03599.6.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Baselines", "context_paragraph": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. All these models were trained on the training set, and parameters as well as the SVM kernel selections (linear or RBF) were fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.", "sentence": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information;", "cited_ids": [], "y": "\"The authors pit their model [UTCNN, \"user-topic-comment neural network\"] against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings, where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the\"", "snippet_surface": "The authors pit their model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the", "questions": {"rIL6sckp0h": "What does \"their model\" refer to?"}, "answers": {"rIL6sckp0h": "Their model name UTCCN stands for \"user-topic-comment neural network \""}, "evidence": {"rIL6sckp0h": [{"section": "Abstract", "paragraph": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "selected": "and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards", "paper_id": "1611.03599"}, {"section": "Introduction", "paragraph": "In this paper, we propose the user-topic-comment neural network (UTCNN), a deep learning model that utilizes user, topic, and comment information. We attempt to learn user and topic representations which encode user interactions and topic influences to further enhance text classification, and we also incorporate comment information. We evaluate this model on a post stance classification task on forum-style social media platforms. The contributions of this paper are as follows: 1. We propose UTCNN, a neural network for text in modern social media channels as well as legacy social media, forums, and message boards \u2014 anywhere that reveals users, their tastes, as well as their replies to posts. 2. When classifying social media post stances, we leverage users, including authors and likers. User embeddings can be generated even for users who have never posted anything. 3. We incorporate a topic model to automatically assign topics to each post in a single topic dataset. 4. We show that overall, the proposed method achieves the highest performance in all instances, and that all of the information extracted, whether users, topics, or comments, still has its contributions.", "selected": "user-topic-comment neural network (UTCNN), a deep learning model that utilizes user, topic, and comment information. We attempt to learn user and topic representations which encode user interactions and topic influences to further enhance text classification, and we also incorporate comment information.", "paper_id": "1611.03599"}]}}
{"idx": "1612.03226.1.1.1", "paper_id": "1612.03226", "title": "Active Learning for Speech Recognition: the Power of Gradients", "abstract": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "context_section_header": "Expected Gradient Length", "context_paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "sentence": "Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "cited_ids": [], "y": "Since labels [annotations of the training data with correct transcriptions for use in training automatic speech recognition systems] are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL [expected gradient length, which is an approach in active learning for end-to-end speech recognition] as \u201cexpected model change\u201d. The authors then formalize the intuition for for EGL and show that it follows naturally from reducing the variance of an estimator.", "snippet_surface": "Since labels are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. The following section formalizes the intuition for EGL and shows that it follows naturally from reducing the variance of an estimator.", "questions": {"nI47Fs85Is": "What does EGL mean?", "MvnmzllGHo": "What do \"labels\" refer to?"}, "answers": {"nI47Fs85Is": "Expected Gradient Length is an approach in active learning for end-to-end speech recognition.", "MvnmzllGHo": "Annotations of the training data with correct transcriptions for use in training automatic speech recognition systems"}, "evidence": {"nI47Fs85Is": [{"section": "Abstract", "paragraph": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "selected": "This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition.", "paper_id": "1612.03226"}, {"section": "Expected Gradient Length", "paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "selected": "Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length", "paper_id": "1612.03226"}], "MvnmzllGHo": [{"section": "Introduction", "paragraph": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize. Labeling thousands of hours of audio, however, is expensive and time-consuming. A natural question to ask is how to achieve better generalization with fewer training examples. Active learning studies this problem by identifying and labeling only the most informative data, potentially reducing sample complexity. How much active learning can help in large-scale, end-to-end ASR systems, however, is still an open question.", "selected": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize.", "paper_id": "1612.03226"}]}}
{"idx": "1612.06685.1.1.1", "paper_id": "1612.06685", "title": "Stateology: State-Level Interactive Charting of Language, Feelings, and Values", "abstract": "People's personality and motivations are manifest in their everyday language usage. With the emergence of social media, ample examples of such usage are procurable. In this paper, we aim to analyze the vocabulary used by close to 200,000 Blogger users in the U.S. with the purpose of geographically portraying various demographic, linguistic, and psychological dimensions at the state level. We give a description of a web-based tool for viewing maps that depict various characteristics of the social media users as derived from this large blog dataset of over two billion words.", "context_section_header": "Psycholinguistic and Semantic Maps", "context_paragraph": "LIWC. In addition to individual words, we can also create maps for word categories that reflect a certain psycholinguistic or semantic property. Several lexical resources, such as Roget or Linguistic Inquiry and Word Count BIBREF9 , group words into categories. Examples of such categories are Money, which includes words such as remuneration, dollar, and payment; or Positive feelings with words such as happy, cheerful, and celebration. Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories. For instance, figure FIGREF8 shows the maps created for two categories: Positive Feelings and Money. The maps are not surprising, and interestingly they also reflect an inverse correlation between Money and Positive Feelings .", "sentence": "Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories.", "cited_ids": [], "y": "The authors can use the distribution of the individual words [from carefully selected blogs from across the 50 U.S states that have the highest frequency of appearance in the blogs] in a category to compile distributions for the entire category, and therefore generate maps for these word categories.", "snippet_surface": "The authors can use the distribution of the individual words in a category to compile distributions for the entire category, and therefore generate maps for these word categories.", "questions": {"+wnSBAR48b": "Where do the \"individual words\" come from?"}, "answers": {"+wnSBAR48b": "Individual words come from carefully selected blogs from across the 50 U.S states. These words have the highest frequency of appearance in the blogs."}, "evidence": {"+wnSBAR48b": [{"section": "Linguistic Maps", "paragraph": "Another use of the information found in our dataset is to build linguistic maps, which reflect the geographic lexical variation across the 50 states BIBREF7 . We generate maps that represent the relative frequency by which a word occurs in the different states. Figure FIGREF3 shows sample maps created for two different words. The figure shows the map generated for one location specific word, Maui, which unsurprisingly is found predominantly in Hawaii, and a map for a more common word, lake, which has a high occurrence rate in Minnesota (Land of 10,000 Lakes) and Utah (home of the Great Salt Lake). Our demo described in Section SECREF4 , can also be used to generate maps for function words, which can be very telling regarding people's personality BIBREF8 .", "selected": "We generate maps that represent the relative frequency by which a word occurs in the different states. Figure FIGREF3 shows sample maps created for two different words. The figure shows the map generated for one location specific word, Maui, which unsurprisingly is found predominantly in Hawaii, and a map for a more common word, lake, which has a high occurrence rate in Minnesota (Land of 10,000 Lakes) and Utah (home of the Great Salt Lake)", "paper_id": "1612.06685"}, {"section": "Linguistic Maps", "paragraph": "Another use of the information found in our dataset is to build linguistic maps, which reflect the geographic lexical variation across the 50 states BIBREF7 . We generate maps that represent the relative frequency by which a word occurs in the different states. Figure FIGREF3 shows sample maps created for two different words. The figure shows the map generated for one location specific word, Maui, which unsurprisingly is found predominantly in Hawaii, and a map for a more common word, lake, which has a high occurrence rate in Minnesota (Land of 10,000 Lakes) and Utah (home of the Great Salt Lake). Our demo described in Section SECREF4 , can also be used to generate maps for function words, which can be very telling regarding people's personality BIBREF8 .", "selected": "We generate maps that represent the relative frequency by which a word occurs in the different states.", "paper_id": "1612.06685"}]}}
{"idx": "164331", "paper_id": "14151217", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "context_section_header": "", "context_paragraph": "LSTMs have proven to be very effective language models (Sundermeyer et al., 2010). Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "sentence": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "cited_ids": [{"paper_id": "5923323", "citation": "Gulcehre et al. (2015)"}], "y": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach [for video description generation, Deep Fusion, which combines a frozen high quality LSTM LM with a trained captioning model] is that the output of the monolingual language model is used as an input when training the full underlying video description network [an RNN that automatically describes videos in natural language].", "snippet_surface": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach is that the output of the monolingual language model is used as an input when training the full underlying video description network.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "pCwojJ4Fa1": "What is a \"video description network?\""}, "answers": {"6gKwRw0I/Q": "In their approach (Deep Fusion) unlike the other researchers on the same topic, they avoid tunning the LSTM LM parameters to prevent overwriting the already learned weights of a strong language model. However, they train the full video caption model to incorporate the LM outputs while training the caption domain.", "pCwojJ4Fa1": "Video desription network is a machine learning tool that uses recurrent neural networks method which helps automatically describe videos in natural language."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "selected": "we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Early Fusion. Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus. An LSTM model learns to estimate the probability of an output sequence given an input sequence. To learn a language model, we train the LSTM layer to predict the next word given the previous words. Following the S2VT architecture, we embed one-hot encoded words in lower dimensional vectors. The network is trained on web-scale text corpora and the parameters are learned through backpropagation using stochastic gradient descent. 1 The weights from this network are then used to initialize the embedding and weights of the LSTM layers of S2VT, which is then trained on video-text data. This trained LM is also used as the LSTM LM in the late and deep fusion models.", "selected": "Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus.", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Late Fusion. Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM). More specifically, if y t denotes the output at time step t, and if p V M and p LM denote the proposal distributions of the video captioning model, and the language models respectively, then for all words y \u2208 V in the vocabulary we can recompute the score of each new word, p(y t = y ) as:", "selected": "Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM).", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Deep Fusion. In the deep fusion approach (Fig. 2), we integrate the LM a step deeper in the generation process by concatenating the hidden state of the language model LSTM (h LM t ) with the hidden state of the S2VT video description model (h V M t ) and use the combined latent vector to predict the output word. This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation. However, our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:", "selected": "our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "where x is the visual feature input, W is the weight matrix, and b the biases. We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "selected": "We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "paper_id": "14151217"}], "pCwojJ4Fa1": [{"section": "Introduction", "paragraph": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNs need large training corpora; however, there is a lack of highquality paired video-sentence data. In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description. Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual target language data as well as a translation model trained on more limited parallel bilingual data. This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description. This paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNNs which have shown state-of-the-art performance on the task. Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010). Our first approach (early fusion) is to pre-train the network on plain text before training on parallel video-text corpora. Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model. Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.", "selected": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language.", "paper_id": "14151217"}]}}
{"idx": "168296", "paper_id": "5673257", "title": "Using word alignments to assist computer-aided translation users by marking which target-side words to change or keep unedited", "abstract": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "context_section_header": "", "context_paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "sentence": "In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "cited_ids": [{"paper_id": "1079699", "citation": "Kranias and Samiotou (2004)"}], "y": "In addition, the authors' system is independent of any extermanl resources such as MT systems [Machine Translation] or dictionaries as opposed to the work by Kranias and Samiotou (2004).", "snippet_surface": "In addition, the authors' system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "questions": {"d+L547/CZx": "What is the authors' system?", "J26LlQoCuy": "How is the authors' system independant?", "5ZLWN/eISk": "What are \"mt systems\"?"}, "answers": {"d+L547/CZx": "The author's system uses various sets of pre-computed word alignments in a translation memory (TM)-based computer-aided translation (CAT) system. Word alignments are used only to recommend the words to be kept unedited or altered, without suggesting a translation, in order for the user to focus on words that require editing.", "J26LlQoCuy": "Compared to other authored works, this particular author's system is independent because it does not rely on external resources, e.g. MT systems, dictionaries.", "5ZLWN/eISk": "Machine Translation (MT) systems."}, "evidence": {"d+L547/CZx": [{"section": "Abstract", "paragraph": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "selected": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM.", "paper_id": "5673257"}, {"section": "Abstract", "paragraph": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "selected": "In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%.", "paper_id": "5673257"}, {"section": "Introduction", "paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "selected": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed.", "paper_id": "5673257"}], "J26LlQoCuy": [{"section": "Introduction", "paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "selected": "our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "paper_id": "5673257"}], "5ZLWN/eISk": [{"section": "Abstract", "paragraph": "An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional \"cascade\" integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.", "selected": "Machine Translation (MT)", "paper_id": "1079699"}, {"section": "Abstract", "paragraph": "An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional \"cascade\" integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.", "selected": "The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches.", "paper_id": "1079699"}, {"section": "Automatic Translation Memory Fuzzy Match Post-Editing", "paragraph": "According to the standard TM paradigm (Nagao, 1984), an input text unit (usually a sentence) to be translated is matched against the source language part of translation pairs stored in the TM. If an identical (full) or similar (fuzzy) match is located, then the system suggests its target language equivalent as the translation of the original text unit and lets the user accept/edit this suggestion in order to correspond accurately to the translation of the input text unit. When no full/fuzzy match can be located, the option is usually offered to invoke MT processing to translate the input text unit. The method proposed in this paper, can be classified as an Example-Based Machine Translation application (Somers, 1999), taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "selected": "taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "paper_id": "1079699"}]}}
{"idx": "168810", "paper_id": "17519578", "title": "Question classification using head words and their hypernyms", "abstract": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "context_section_header": "", "context_paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "sentence": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set.", "cited_ids": [{"paper_id": "16137770", "citation": "Li and Roth (2006)"}], "y": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set [a head word feature and two approaches to augment semantic features of head words using WordNet], the authors propose to use a compact yet effective feature set [which includes five binary feature sets: question whword, head word, WordNet semantic feature for head word, word grams and word shape feature].", "snippet_surface": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, the authors propose to use a compact yet effective feature set.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "cEA5A1V1j/": "What compact feature set is the author referring to?"}, "answers": {"6gKwRw0I/Q": "The authors propose a head word feature and two approaches to augment semantic features of head words using WordNet.", "cEA5A1V1j/": "Five binary feature sets (question whword, head word, WordNet semantic feature for head word, word grams and word shape feature)."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "selected": "we propose head word feature and present two approaches to augment semantic features of such head words using WordNet.", "paper_id": "17519578"}, {"section": "Introduction", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "paper_id": "17519578"}], "cEA5A1V1j/": [{"section": "Features", "paragraph": "Each question is represented as a bag of features and is feeded into classifiers in training stage. We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "selected": "We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "paper_id": "17519578"}, {"section": "Conclusion", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of very rich feature space, we proposed a compact yet effective feature set. In particular, we proposed head word feature and presented two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation algorithm was adapted and the depth of hypernym feature was optimized through cross validation, which was to introduce useful information while not bringing too much noise. With further augment of wh-word, unigram feature, and word shape feature, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "we proposed a compact yet effective feature set", "paper_id": "17519578"}]}}
{"idx": "1702.03856.1.2.1", "paper_id": "1702.03856", "title": "Towards speech-to-text translation without speech recognition", "abstract": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.", "context_section_header": "Introduction", "context_paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "sentence": "We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ).", "cited_ids": [], "y": "The authors [created a system which builds on unsupervised speech processing, using unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech. They ] test this system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects.", "snippet_surface": "The authors test their system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects.", "questions": {"SQXMwav7Rp": "What does \"their system\" refer to?"}, "answers": {"SQXMwav7Rp": "Their system builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9"}, "evidence": {"SQXMwav7Rp": [{"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 .", "paper_id": "1702.03856"}, {"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9", "paper_id": "1702.03856"}, {"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ).", "paper_id": "1702.03856"}]}}
{"idx": "1703.10344.1.1.2", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Introduction", "context_paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "sentence": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha.", "cited_ids": [], "y": "While [cyclone] Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone, which has 5 times more human casualties, is not mentioned in the [wikipedia] page for Odisha.", "snippet_surface": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties is not mentioned in the page for Odisha.", "questions": {"hhMQSTf4zI": "What is the \"page for odisha\"?"}, "answers": {"hhMQSTf4zI": "The page for Odisha is the Wikipedia entity page for that Cyclone."}, "evidence": {"hhMQSTf4zI": [{"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive", "paper_id": "1703.10344"}]}}
{"idx": "1703.10344.4.1.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Section Placement", "context_paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "sentence": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "cited_ids": [], "y": "The authors model the ASP [Article-Sectionp placement] task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to [match news sections to] the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "snippet_surface": "The authors model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "questions": {"7Vgm31ZuAG": "What does asp stand for?", "0skFKrMyRx": "What does \"the task\" refer to?"}, "answers": {"7Vgm31ZuAG": "It is one of two decompositions of news. ASP = \"Article-Section placement\".", "0skFKrMyRx": "It refers to the matching of news sections to their correct entity sections."}, "evidence": {"7Vgm31ZuAG": [{"section": "Approach Overview", "paragraph": "We approach the news suggestion problem by decomposing it into two tasks:", "selected": "We approach the news suggestion problem by decomposing it into two tasks:", "paper_id": "1703.10344"}, {"section": "Approach Overview", "paragraph": "ASP: Article\u2013Section placement", "selected": "ASP: Article\u2013Section placement", "paper_id": "1703.10344"}], "0skFKrMyRx": [{"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay.", "paper_id": "1703.10344"}, {"section": "Article\u2013Section Placement", "paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "selected": "or all the `relevant' news entity pairs, the task is to determine the correct entity section.", "paper_id": "1703.10344"}]}}
{"idx": "1703.10344.6.2.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Entity Placement", "context_paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "sentence": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "cited_ids": [], "y": "The authors reimplemented baseline features. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.", "snippet_surface": "The authors reimplemented baseline features discussed in Section SECREF2. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "questions": {"BVqHJB4480": "What does \"baseline features\" refer to?", "fFVa3w+Wmx": "How many features is a \"variety of features\"?"}, "answers": {"BVqHJB4480": "The baseline features refer to measure salience of an entity in text.", "fFVa3w+Wmx": "There are three features based on the text."}, "evidence": {"BVqHJB4480": [{"section": "Article\u2013Entity Placement", "paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "selected": "a variety of features that measure salience of an entity in text are available from the NLP community.", "paper_id": "1703.10344"}], "fFVa3w+Wmx": [{"section": "Article\u2013Entity Placement", "paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "selected": "This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "paper_id": "1703.10344"}]}}
{"idx": "1704.06194.1.1.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "KBQA End-Task Results", "context_paragraph": "Finally, like STAGG, which uses multiple relation detectors (see yih2015semantic for the three models used), we also try to use the top-3 relation detectors from Section \"Relation Detection Results\" . As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP.", "sentence": "As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP", "cited_ids": [], "y": "[Using the top 3 relation detectors] gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP.", "snippet_surface": "This gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP.", "questions": {"7K+Ttn8kaU": "What gives a significant performance boost?"}, "answers": {"7K+Ttn8kaU": "Using the top 3 relation detectors"}, "evidence": {"7K+Ttn8kaU": []}}
{"idx": "1704.06194.1.2.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "Relation Detection Results", "context_paragraph": "Table 2 shows the results on two relation detection tasks. The AMPCNN result is from BIBREF20 , which yielded state-of-the-art scores by outperforming several attention-based methods. We re-implemented the BiCNN model from BIBREF4 , where both questions and relations are represented with the word hash trick on character tri-grams. The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "sentence": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "cited_ids": [], "y": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP [(WQ; a multi-relation KBQA task used for analysis purposes)] and is close to the previous best result of AMPCNN on SimpleQuestions [(SQ; a single-relation KBQA task)]. The authors' proposed HR-BiLSTM [(Hierarchical Residual BiLSTM)] outperformed the best baselines on both tasks by margins of 2-3% (p < 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "snippet_surface": "The baseline BiLSTM with relation word sequence appears to be the best baseline on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. The authors' proposed HR-BiLSTM outperformed the best baselines on both tasks by margins of 2-3% (p $<$ 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively).", "questions": {"K25HfzWFDz": "What is WebQSP?", "INAkzrrNNK": "What is SimpleQuestions?", "+LDELPxYBs": "Is SQ SimpleQuestions and WQ WebQSP?"}, "answers": {"K25HfzWFDz": "WebQSP is a multi-relation KBQA task used for analysis purposes.", "INAkzrrNNK": "SimpleQuestions is a single-relation KBQA task.", "+LDELPxYBs": "Yes, SQ is simple question and WQ is WebQSP"}, "evidence": {"K25HfzWFDz": [{"section": "Task Introduction & Settings", "paragraph": "WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following yih-EtAl:2016:P16-2, we use S-MART BIBREF26 entity-linking outputs. In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set. For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length $\\le $ 2) connected to the topic entity, and set the core-chain labeled in the parse as the positive label and all the others as the negative examples.", "selected": "WebQSP (WQ): A multi-relation KBQA task.", "paper_id": "1704.06194"}, {"section": "Relation Detection Results", "paragraph": "Next, we present empirical evidences, which show why our HR-BiLSTM model achieves the best scores. We use WebQSP for the analysis purposes. First, we have the hypothesis that training of the weighted-sum model usually falls to local optima, since deep BiLSTMs do not guarantee that the two-levels of question hidden representations are comparable. This is evidenced by that during training one layer usually gets a weight close to 0 thus is ignored. For example, one run gives us weights of -75.39/0.14 for the two layers (we take exponential for the final weighted sum). It also gives much lower training accuracy (91.94%) compared to HR-BiLSTM (95.67%), suffering from training difficulty.", "selected": "We use WebQSP for the analysis purposes.", "paper_id": "1704.06194"}], "INAkzrrNNK": [{"section": "Task Introduction & Settings", "paragraph": "SimpleQuestions (SQ): It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) BIBREF2 , in order to compare with previous research. yin2016simple also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results. Therefore, our results can be compared with their reported results on both tasks.", "selected": "SimpleQuestions (SQ): It is a single-relation KBQA task.", "paper_id": "1704.06194"}], "+LDELPxYBs": [{"section": "Task Introduction & Settings", "paragraph": "WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following yih-EtAl:2016:P16-2, we use S-MART BIBREF26 entity-linking outputs. In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set. For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length $\\le $ 2) connected to the topic entity, and set the core-chain labeled in the parse as the positive label and all the others as the negative examples.", "selected": "WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following yih-EtAl:2016:P16-2, we use S-MART BIBREF26 entity-linking outputs. In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set. For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length $\\le $ 2) connected to the topic entity, and set the core-chain labeled in the parse as the positive label and all the others as the negative examples.", "paper_id": "1704.06194"}]}}
{"idx": "1704.06194.2.1.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "Introduction", "context_paragraph": "This paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", "sentence": "First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", "cited_ids": [], "y": "The authors first propose to deal with the unseen relations by breaking the relation names into word sequences for question-relation matching. The authors also propose to build both relation-level and word-level relation representations, as original relation names can sometimes help to match longer question contexts. The authors also use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Lastly, the authors propose a residual learning method, [such as HR-BiLSTM,] for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improving hierarchical matching.", "snippet_surface": "The authors propose to deal with the unseen relations by breaking the relation names into word sequences for question-relation matching. Additionally, they propose to build both relation-level and word-level relation representations, as original relation names can sometimes help to match longer question contexts. The authors also use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, they propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improving hierarchical matching.", "questions": {"uAcGR6C9vi": "What does \"hierarchical\" mean?"}, "answers": {"uAcGR6C9vi": "Hierarchical means that there are are two different layers in the network, and the second one fits the residues of the first."}, "evidence": {"uAcGR6C9vi": [{"section": "Hierarchical Matching between Relation and Question", "paragraph": "Intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. This also ensures the vector spaces of two layers are comparable and makes the second-layer training easier.", "selected": "Intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. This also ensures the vector spaces of two layers are comparable and makes the second-layer training easier.", "paper_id": "1704.06194"}, {"section": "Hierarchical Matching between Relation and Question", "paragraph": "Another way of hierarchical matching consists in relying on attention mechanism, e.g. BIBREF24 , to find the correspondence between different levels of representations. This performs below the HR-BiLSTM (see Table 2 ).", "selected": "Another way of hierarchical matching consists in relying on attention mechanism, e.g. BIBREF24 , to find the correspondence between different levels of representations. This performs below the HR-BiLSTM (see Table 2 )", "paper_id": "1704.06194"}, {"section": "Relation Detection Results", "paragraph": "Finally, we hypothesize that HR-BiLSTM is more than combination of two BiLSTMs with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction. To verify this, we replace the deep BiLSTM question encoder with two single-layer BiLSTMs (both on words) with shortcut connections between their hidden states. This decreases test accuracy to 76.11%. It gives similar training accuracy compared to HR-BiLSTM, indicating a more serious over-fitting problem. This proves that the residual and deep structures both contribute to the good performance of HR-BiLSTM.", "selected": "Finally, we hypothesize that HR-BiLSTM is more than combination of two BiLSTMs with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction", "paper_id": "1704.06194"}]}}
{"idx": "1706.01678.4.1.1", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Baselines", "context_paragraph": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally. The Lead-3 model simply produces the leading three sentences of the document as its summary.", "sentence": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "cited_ids": [], "y": "For the CNN-Dailymail dataset [300k document summary pairs with stories], the Lead-3 model [the leading three sentences of the document] is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "snippet_surface": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally.", "questions": {"vrp0sm4fPD": "What does \"the cnn-dailymail dataset\" consist of?", "rf6XtjU8v7": "What does \"the lead-3 model\" consist of?"}, "answers": {"vrp0sm4fPD": "It consists of 300k document summary pairs with stories having 39 sentences on average.", "rf6XtjU8v7": "It consists of the 3 leading sentences of the document it is used in"}, "evidence": {"vrp0sm4fPD": [{"section": "Datasets", "paragraph": "CNN-Dailymail corpus is better suited for summarization as the average summary size is around 3 or 4 sentences. This dataset has around 300k document summary pairs with stories having 39 sentences on average. The dataset comes in 2 versions, one is the anonymized version, which has been preprocessed to replace named entities, e.g., The Times of India, with a unique identifier for example @entity1. Second is the non-anonymized which has the original text. We use the non-anonymized version of the dataset as it is more suitable for AMR parsing as most of the parsers have been trained on non-anonymized text. The dataset does not have gold-standard AMR graphs. We use automatic parsers to get the AMR graphs but they are not gold-standard and will effect the quality of final summary. To get an idea of the error introduced by using automatic parsers, we compare the results after using gold-standard and automatically generated AMR graphs on the gold-standard dataset.", "selected": "CNN-Dailymail corpus is better suited for summarization as the average summary size is around 3 or 4 sentences. This dataset has around 300k document summary pairs with stories having 39 sentences on average.", "paper_id": "1706.01678"}], "rf6XtjU8v7": [{"section": "Baselines", "paragraph": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally. The Lead-3 model simply produces the leading three sentences of the document as its summary.", "selected": "The Lead-3 model simply produces the leading three sentences of the document as its summary.", "paper_id": "1706.01678"}]}}
{"idx": "1706.01678.4.1.2", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Baselines", "context_paragraph": "For the proxy report section of the AMR bank, we consider the Lead-1-AMR model as the baseline. For this dataset we already have the gold-standard AMR graphs of the sentences. Therefore, we only need to nullify the error introduced by the generator.", "sentence": "For the proxy report section of the AMR bank, we consider the Lead-1-AMR model as the baseline.", "cited_ids": [], "y": "For the proxy report section of the AMR bank, the authors consider the Lead-1-AMR model as the baseline. [It combines a model which produces the leading sentence of the document as a summary, with an abstract meaning representation step which produces an AMR graph of a story, extracts a summary graph and then generates summary sentences.]", "snippet_surface": "For the proxy report section of the AMR bank, the authors consider the Lead-1-AMR model as the baseline.", "questions": {"gblOX2qFAL": "What does \"lead-1-amr\" consist of?"}, "answers": {"gblOX2qFAL": "Lead-1-AMR combines a model which produces the leading sentence of the document as a summary, with an abstract meaning representation step which produces an AMR graph of a story, extracts a summary graph and then generates summary sentences."}, "evidence": {"gblOX2qFAL": [{"section": "Abstract", "paragraph": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "selected": "Abstract Meaning Representation (AMR).", "paper_id": "1706.01678"}, {"section": "Step 2: Story AMR to Summary AMR", "paragraph": "Using this idea of picking important sentences from the beginning, we propose two methods, first is to simply pick initial few sentences, we call this first-n method where n stands for the number of sentences. We pick initial 3 sentences for the CNN-Dailymail corpus i.e. first-3 and only the first sentence for the proxy report section (AMR Bank) i.e. first-1 as they produce the best scores on the ROGUE metric compared to any other first-n. Second, we try to capture the relation between the two most important entities (we define importance by the number of occurrences of the entity in the story) of the document. For this we simply find the first sentence which contains both these entities. We call this the first co-occurrence based sentence selection. We also select the first sentence along with first co-occurrence based sentence selection as the important sentences. We call this the first co-occurrence+first based sentence selection.", "selected": "only the first sentence for the proxy report section (AMR Bank) i.e. first-1", "paper_id": "1706.01678"}, {"section": "Baselines", "paragraph": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally. The Lead-3 model simply produces the leading three sentences of the document as its summary.", "selected": "The Lead-3 model simply produces the leading three sentences of the document as its summary.", "paper_id": "1706.01678"}]}}
{"idx": "1706.01678.5.1.1", "paper_id": "1706.01678", "title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.", "context_section_header": "Step 2: Story AMR to Summary AMR", "context_paragraph": "After parsing (Step 1) we have the AMR graphs for the story sentences. In this step we extract the AMR graphs of the summary sentences using story sentence AMRs. We divide this task in two parts. First is finding the important sentences from the story and then extracting the key information from those sentences using their AMR graphs.", "sentence": "In this step we extract the AMR graphs of the summary sentences using story sentence AMRs. We divide this task in two parts. First is finding the important sentences from the story and then extracting the key information from those sentences using their AMR graphs.", "cited_ids": [], "y": "AMR graphs [that visualize \"abstract meaning representation\" in rooted, directed, edge and vertex labeled form] of the summary sentences are extracted by the authors using story sentence AMRs. The authors divide this task into two parts. First, important sentences from a [news] story are found and then key information is extracted from the sentences found using their AMR graphs.", "snippet_surface": "The authors extract the AMR graphs of the summary sentences using story sentence AMRs. They divide this task into two parts. First, they find the important sentences from the story and then extract the key information from those sentences using their AMR graphs.", "questions": {"NulrsnsZlk": "What do the \"amr graphs\" refer to?"}, "answers": {"NulrsnsZlk": "AMR graphs represent the visualization of \"abstract meaning representation\" in the form of rooted, directed, edge and vertex labeled graphs."}, "evidence": {"NulrsnsZlk": [{"section": "Background: AMR Parsing and Generation", "paragraph": "AMR was introduced by BIBREF1 with the aim to induce work on statistical Natural Language Understanding and Generation. AMR represents meaning using graphs. AMR graphs are rooted, directed, edge and vertex labeled graphs. Figure FIGREF4 shows the graphical representation of the AMR graph of the sentence \"I looked carefully all around me\" generated by JAMR parser ( BIBREF2 ). The graphical representation was produced using AMRICA BIBREF3 . The nodes in the AMR are labeled with concepts as in Figure FIGREF4 around represents a concept. Edges contains the information regarding the relations between the concepts. In Figure FIGREF4 direction is the relation between the concepts look-01 and around. AMR relies on Propbank for semantic relations (edge labels). Concepts can also be of the form run-01 where the index 01 represents the first sense of the word run. Further details about the AMR can be found in the AMR guidelines BIBREF4 .", "selected": "AMR represents meaning using graphs. AMR graphs are rooted, directed, edge and vertex labeled graphs.", "paper_id": "1706.01678"}]}}
{"idx": "1707.06806.3.1.1", "paper_id": "1707.06806", "title": "Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "abstract": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.", "context_section_header": "Evaluation", "context_paragraph": "In this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", "sentence": "In this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", "cited_ids": [], "y": "The authors evaluate their method [online content popularity prediction using on bidirectional recurrent neural network] and compare its performance against the competitive approaches, [such as Bag-of-Words and Convectional Neural Network]. They use an evaluation protocol with random dataset split. The authors measure [the accuracy of the predictions of their proposed model] using standard accuracy metric which they define as a ratio between correctly classified data samples from test dataset and all test samples.", "snippet_surface": "The authors evaluate their method and compare its performance against the competitive approaches. They use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. They measure the performance using standard accuracy metric which they define as a ratio between correctly classified data samples from test dataset and all test samples.", "questions": {"/Eahnu2DVL": "What does \"competitive approaches\" refer to?"}, "answers": {"/Eahnu2DVL": "Competetive approaches refer to the methods that are previously used in online content popularity prediction. These approaches are named as Bag-of-Words and Convectional Neural Network."}, "evidence": {"/Eahnu2DVL": [{"section": "Abstract", "paragraph": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.", "selected": "our approach improves the performance over traditional shallow approaches by a margin of 15%", "paper_id": "1707.06806"}, {"section": "Baselines", "paragraph": "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", "selected": "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM", "paper_id": "1707.06806"}, {"section": "Baselines", "paragraph": "Our second baseline is a deep Convectional Neural Network applied on word embeddings. This baseline represents state-of-the-art method presented in BIBREF4 with minor adjustments to the binary classification task. The architecture of the CNN benchmark we use is the following: the embedding layer transforms one-hot encoded words to their dense vector representations, followed by the convolution layer of 256 filters with width equal to 5 followed by max pooling layer (repeated three times), fully-connected layer with dropout and INLINEFORM0 regularization and finally, sigmoid activation layer. For fair comparison, both baselines were trained using the same training procedure as our method.", "selected": "Our second baseline is a deep Convectional Neural Network applied on word embeddings. This baseline represents state-of-the-art method presented in BIBREF4 with minor adjustments to the binary classification task.", "paper_id": "1707.06806"}]}}
{"idx": "1707.06806.3.2.1", "paper_id": "1707.06806", "title": "Shallow reading with Deep Learning: Predicting popularity of online content using only its title", "abstract": "With the ever decreasing attention span of contemporary Internet users, the title of online content (such as a news article or video) can be a major factor in determining its popularity. To take advantage of this phenomenon, we propose a new method based on a bidirectional Long Short-Term Memory (LSTM) neural network designed to predict the popularity of online content using only its title. We evaluate the proposed architecture on two distinct datasets of news articles and news videos distributed in social media that contain over 40,000 samples in total. On those datasets, our approach improves the performance over traditional shallow approaches by a margin of 15%. Additionally, we show that using pre-trained word vectors in the embedding layer improves the results of LSTM models, especially when the training set is small. To our knowledge, this is the first attempt of applying popularity prediction using only textual information from the title.", "context_section_header": "Introduction", "context_paragraph": "In this paper we propose a method for online content popularity prediction based on a bidirectional recurrent neural network called BiLSTM. This work is inspired by recent successful applications of deep neural networks in many natural language processing problems BIBREF5 , BIBREF6 . Our method attempts to model complex relationships between the title of an article and its popularity using novel deep network architecture that, in contrast to the previous approaches, gives highly interpretable results. Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "sentence": "Last but not least, the proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "cited_ids": [], "y": "The proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy [(greater than 15%)] over the standard shallow approach [(a classification method called as Support Vector Machine with linear kernel)]. It also outperforms the current state-of-the-art on two distinct datasets with over 40,000 samples.", "snippet_surface": "The proposed BiLSTM method provides a significant performance boost in terms of prediction accuracy over the standard shallow approach, while outperforming the current state-of-the-art on two distinct datasets with over 40,000 samples.", "questions": {"5B5D4/lDng": "How much better is a \"significant performance boost\"?", "b451Iz1chk": "What does the \"standard shallow approach\" refer to?"}, "answers": {"5B5D4/lDng": "The improvent in accuracy reaches a level of greater than 15% compared to the shallow architecture.", "b451Iz1chk": "Standard shallow approach is referred to a classification  method called as Support Vector Machine with linear kernel."}, "evidence": {"5B5D4/lDng": [{"section": "Results", "paragraph": "The results of our experiments can be seen in Tab. TABREF21 and TABREF22 . Our proposed BiLSTM approach outperforms the competing methods consistently across both datasets. The performance improvement is especially visible for The NowThisNews dataset and reaches over 15% with respect to the shallow architecture in terms of of accuracy. Although the improvement with respect to the other methods based on deep neural network is less evident, the recurrent nature of our method provides much more intuitive interpretation of the results and allow for parsing the contribution of each single word to the overall score.", "selected": "The performance improvement is especially visible for The NowThisNews dataset and reaches over 15% with respect to the shallow architecture in terms of of accuracy.", "paper_id": "1707.06806"}], "b451Iz1chk": [{"section": "Baselines", "paragraph": "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", "selected": "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM", "paper_id": "1707.06806"}]}}
{"idx": "1708.01464.1.1.1", "paper_id": "1708.01464", "title": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion", "abstract": "Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and automatic speech recognition systems. Most g2p systems are monolingual: they require language-specific data or handcrafting of rules. Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available. As an alternative, we present a neural sequence-to-sequence approach to g2p which is trained on spelling--pronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems. We show an 11% improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages. Our model is also much more compact relative to previous approaches.", "context_section_header": "High Resource Results", "context_paragraph": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "sentence": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "cited_ids": [], "y": "LangID-High does not present a more accurate result [than wFST (weighted Finite State Transducer)], although it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource [grapheme-to-phoneme conversion models] are 197.5 MB.", "snippet_surface": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB.", "questions": {"/LaXZiwzUP": "What does \"wFST\" stand for?", "Bd6pmN7fKZ": "What does \"high resource models\" refer to?"}, "answers": {"/LaXZiwzUP": "A weighted finite state transducer.", "Bd6pmN7fKZ": "It refers to grapheme-to-phoneme conversion models where this is a lot of training data available."}, "evidence": {"/LaXZiwzUP": [{"section": "Low Resource g2p", "paragraph": "Our approach is similar in goal to deri2016grapheme's model for adapting high resource g2p models for low resource languages. They trained weighted finite state transducer (wFST) models on a variety of high resource languages, then transferred those models to low resource languages, using a language distance metric to choose which high resource models to use and a phoneme distance metric to map the high resource language's phonemes to the low resource language's phoneme inventory. These distance metrics are computed based on data from Phoible BIBREF4 and URIEL BIBREF5 .", "selected": "weighted finite state transducer", "paper_id": "1708.01464"}], "Bd6pmN7fKZ": [{"section": "Introduction", "paragraph": "We attempt to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem. The source sequences for our system are words in the standard orthography in any language. The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA). Our results show that the parameters learned by the shared encoder\u2013decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.", "selected": "We attempt to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem. The source sequences for our system are words in the standard orthography in any language. The target sequences are the corresponding representation in the International Phonetic Alphabet (IPA). Our results show that the parameters learned by the shared encoder\u2013decoder are able to exploit the orthographic and phonemic similarities between the various languages in our data.", "paper_id": "1708.01464"}, {"section": "Low Resource g2p", "paragraph": "Our approach is similar in goal to deri2016grapheme's model for adapting high resource g2p models for low resource languages. They trained weighted finite state transducer (wFST) models on a variety of high resource languages, then transferred those models to low resource languages, using a language distance metric to choose which high resource models to use and a phoneme distance metric to map the high resource language's phonemes to the low resource language's phoneme inventory. These distance metrics are computed based on data from Phoible BIBREF4 and URIEL BIBREF5 .", "selected": "Our approach is similar in goal to deri2016grapheme's model for adapting high resource g2p models for low resource languages. They trained weighted finite state transducer (wFST) models on a variety of high resource languages, then transferred those models to low resource languages, using a language distance metric to choose which high resource models to use and a phoneme distance metric to map the high resource language's phonemes to the low resource language's phoneme inventory. These distance metrics are computed based on data from Phoible BIBREF4 and URIEL BIBREF5 .", "paper_id": "1708.01464"}]}}
{"idx": "1709.07814.1.1.1", "paper_id": "1709.07814", "title": "Attention-based Wav2Text with Feature Transfer Learning", "abstract": "Conventional automatic speech recognition (ASR) typically performs multi-level pattern recognition tasks that map the acoustic speech waveform into a hierarchy of speech units. But, it is widely known that information loss in the earlier stage can propagate through the later stages. After the resurgence of deep learning, interest has emerged in the possibility of developing a purely end-to-end ASR system from the raw waveform to the transcription without any predefined alignments and hand-engineered models. However, the successful attempts in end-to-end architecture still used spectral-based features, while the successful attempts in using raw waveform were still based on the hybrid deep neural network - Hidden Markov model (DNN-HMM) framework. In this paper, we construct the first end-to-end attention-based encoder-decoder model to process directly from raw speech waveform to the text transcription. We called the model as\"Attention-based Wav2Text\". To assist the training process of the end-to-end model, we propose to utilize a feature transfer learning. Experimental results also reveal that the proposed Attention-based Wav2Text model directly with raw waveform could achieve a better result in comparison with the attentional encoder-decoder model trained on standard front-end filterbank features.", "context_section_header": "Attention-based Encoder Decoder for Raw Speech Recognition", "context_paragraph": "In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 .", "sentence": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part.", "cited_ids": [], "y": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, the authors construct an encoder with several convolutional layers followed by [network-in-network (NIN)] layers as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) at the higher part.", "snippet_surface": "In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, the authors construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part.", "questions": {"GKVd9z9mL2": "What are NIN layers?"}, "answers": {"GKVd9z9mL2": "NIN layers (which stands for \"network-in-network\") represent complex structures on top of the convolutional layers. They are at the lower part in the encoder."}, "evidence": {"GKVd9z9mL2": [{"section": "Introduction", "paragraph": "Deep learning algorithms have produced many state-of-the-art performances in various tasks that have revitalized the use of neural networks for ASR. One of the important factors behind the popularity of deep learning is the possibility of simplifying many complicated hand-engineered models by letting DNNs find their way to map from input to output spaces. Interest has emerged recently in the possibility of learning DNN-based acoustic models directly from the raw speech waveform without any predefined alignments and hand-engineered models. In this way, the feature extractor and acoustic model can be integrated into a single architecture. Palaz et al. BIBREF1 , BIBREF2 proposed a convolutional neural network (CNN) to directly train an acoustic model from the raw speech waveform. Sainath et al. BIBREF3 used time-convolutional layers over raw speech and trained them jointly with the long short-term memory deep neural network (CLDNN) acoustic model. The results showed that raw waveform CLDNNs matched the performance of log-mel CLDNNs on a voice search task. Ghahremani et al. BIBREF4 recently proposed a CNN time-delay neural network (CNN-TDNN) with network-in-network (NIN) architecture, and also showed that their model outperformed MFCC-based TDNN on the Wall Street Journal (WSJ) BIBREF5 task. But despite significant progress that has been made, the successful models were mostly demonstrated only within the hybrid DNN-HMM speech recognition frameworks.", "selected": "The results showed that raw waveform CLDNNs matched the performance of log-mel CLDNNs on a voice search task. Ghahremani et al. BIBREF4 recently proposed a CNN time-delay neural network (CNN-TDNN) with network-in-network (NIN) architecture", "paper_id": "1709.07814"}, {"section": "Attention-based Encoder Decoder for Raw Speech Recognition", "paragraph": "In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 .", "selected": "the NIN layer represents more complex structures on the top of the convolutional layers.", "paper_id": "1709.07814"}, {"section": "Attention-based Encoder Decoder for Raw Speech Recognition", "paragraph": "In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 .", "selected": "NIN layers BIBREF15 as the lower part in the encoder", "paper_id": "1709.07814"}]}}
{"idx": "1710.03348.1.1.1", "paper_id": "1710.03348", "title": "What does Attention in Neural Machine Translation Pay Attention to?", "abstract": "Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.", "context_section_header": "Introduction", "context_paragraph": "Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "sentence": "Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "cited_ids": [], "y": "[The authors investigated the differences between attention and alignment which are both used to encode the relevant parts of the sources sentence during the translation process. They investigate if the attention model can model alignment and how similar attention is to alignment in different syntactic phenomena]. The authors' analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information ratehr than only the translational equivalent in the case of verbs [because the correct translation of verbs requires more distributed attention compared to nouns].", "snippet_surface": "The authors' analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For example, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs.", "questions": {"n0YlhjeWIN": "What does \"analysis\" refer to?", "cLawMkLlxY": "Why does the model not capture verbs properly?"}, "answers": {"n0YlhjeWIN": "Authors' analysis refers to the investigating the differences between attention and alingment which are both used to encode the relevant parts of the sources sentence during the translation process. They perform these investigations to tackle their research questions including 1) whether the attention model only capable of modelling alignment, 2) how similar attention to alignment is in different syntactic phenomena", "cLawMkLlxY": "The correct translation of verbs requires the systems to concentrate on different parts of the sources sentence. The model needs to pay more distributed attention across the text is necessary in the case of translating verbs compared to the nouns."}, "evidence": {"n0YlhjeWIN": [{"section": "Introduction", "paragraph": "In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?", "selected": "we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general.", "paper_id": "1710.03348"}, {"section": "Introduction", "paragraph": "In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?", "selected": "The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in different syntactic phenomena?", "paper_id": "1710.03348"}], "cLawMkLlxY": [{"section": "Attention Concentration", "paragraph": "The higher attention entropy for verbs, in Figure FIGREF28 , shows that the attention is more distributed compared to nouns. The low correlation between attention entropy and word prediction loss (see Figure FIGREF32 ) shows that attention concentration is not required when translating into verbs. This also confirms that the correct translation of verbs requires the systems to pay attention to different parts of the source sentence.", "selected": "The low correlation between attention entropy and word prediction loss (see Figure FIGREF32 ) shows that attention concentration is not required when translating into verbs. This also confirms that the correct translation of verbs requires the systems to pay attention to different parts of the source sentence.", "paper_id": "1710.03348"}, {"section": "Attention Concentration", "paragraph": "The higher attention entropy for verbs, in Figure FIGREF28 , shows that the attention is more distributed compared to nouns. The low correlation between attention entropy and word prediction loss (see Figure FIGREF32 ) shows that attention concentration is not required when translating into verbs. This also confirms that the correct translation of verbs requires the systems to pay attention to different parts of the source sentence.", "selected": "The higher attention entropy for verbs, in Figure FIGREF28 , shows that the attention is more distributed compared to nouns.", "paper_id": "1710.03348"}]}}
{"idx": "171837", "paper_id": "5679499", "title": "Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues", "abstract": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "context_section_header": "", "context_paragraph": "The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "sentence": "Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "cited_ids": [{"paper_id": "2687019", "citation": "(Riedel et al., 2013)"}], "y": "Key differences between the previous approach and the authors' [approach which is to use latent edge labels in addition to surface-level labels] include their use of syntactic information [expressive lexicalized labels] as opposed to surface-level patterns, and also the ability of the proposed PRA-based method [Path Ranking Algorithm that performs inference over a knowledge base using syntactic information from parsed text] to generate useful inference rules [used for increasing coverage of facts in Knowledge Bases] which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "snippet_surface": "Key differences between the previous approach and the authors' include their use of syntactic information as opposed to surface-level patterns, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "zpDVW0r7TY": "What type of syntatic information is being refered to?", "Wu82qV7lCn": "What is a \"pra-based method\"?"}, "answers": {"6gKwRw0I/Q": "The author's approach is to use latent edge labels in addition to surface-level labels.", "zpDVW0r7TY": "The type of syntactic information refers to expressive lexicalized labels.", "Wu82qV7lCn": "PRA is a Path Ranking Algorithm, that performs inference over a knowledge base using syntactic information from parsed text."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "selected": "For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task.", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen-cies). These additional edges, e.g., (Alex Rodriguez, \"plays for\", NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012). In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach.", "selected": "we derive edge labels by learning latent embeddings of the lexicalized edges.", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "paper_id": "5679499"}], "zpDVW0r7TY": [{"section": "Introduction", "paragraph": "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen-cies). These additional edges, e.g., (Alex Rodriguez, \"plays for\", NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012). In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach.", "selected": "lexicalized syntactic labels (where the labels are words instead of dependen-cies)", "paper_id": "5679499"}], "Wu82qV7lCn": [{"section": "Introduction", "paragraph": "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "selected": "recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011)", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "selected": "PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011)", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "PRA is extended to perform inference over a KB using syntactic information from parsed text.", "paper_id": "5679499"}]}}
{"idx": "173414", "paper_id": "229363636", "title": "Learning Dense Representations of Phrases at Scale", "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "context_section_header": "", "context_paragraph": "tically improves phrase retrieval performance and allows for transfer learning to new domains, without re-building billions of phrase representations. As a result, all these improvements lead to a much stronger phrase retrieval model, without the use of any sparse representations (Table 1). We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets. Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020;. Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.", "sentence": "We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "cited_ids": [{"paper_id": "189762341", "citation": "(Seo et al., 2019;"}, {"paper_id": "189762341", "citation": "(Seo et al., 2019;"}], "y": "The authors evaluate their model, DensePhrases [which is designed to learn phrase representations for retrieving phrase-level knowledge from a large text corpus], on five standard open-domain QA datasets [Natural Questions, SQuAD, TQA, Web Questions, CuratedTREC] and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019), with 15%-25% absolute improvement on most datasets.", "snippet_surface": "The authors evaluate their model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "questions": {"4T9JAsYA/w": "What is the authors' model?", "RQuZVRnVKw": "What do \"most datasets\" refer to?"}, "answers": {"4T9JAsYA/w": "Their model is called DensePhrases and is designed to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods, and aims to provide a neural interface for retrieving phrase-level knowledge from a large text corpus", "RQuZVRnVKw": "all datasets except SQuAD"}, "evidence": {"4T9JAsYA/w": [{"section": "Abstract", "paragraph": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "selected": "learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods.", "paper_id": "229363636"}, {"section": "Introduction", "paragraph": "Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.", "selected": "a neural interface for retrieving phrase-level knowledge from a large text corpus.", "paper_id": "229363636"}, {"section": "Introduction", "paragraph": "Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.", "selected": "a neural interface for retrieving phrase-level knowledge from a large text corpus.", "paper_id": "229363636"}], "RQuZVRnVKw": [{"section": "Open-domain QA.", "paragraph": "Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%-25% absolute improvement on all datasets except SQuAD. Training the model of  on C phrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models , while running much faster during inference (Table 1).  (Guu et al., 2020) {Wiki., CC-News} \u2020 40.4 40.7 42.9 --DPR-multi    Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ). * : no supervision using target training data (zero-shot). \u2020 : unlabeled data used for extra pre-training.   Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples.", "selected": "all datasets except SQuAD", "paper_id": "229363636"}, {"section": "Open-domain QA.", "paragraph": "Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%-25% absolute improvement on all datasets except SQuAD. Training the model of  on C phrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models , while running much faster during inference (Table 1).  (Guu et al., 2020) {Wiki., CC-News} \u2020 40.4 40.7 42.9 --DPR-multi    Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ). * : no supervision using target training data (zero-shot). \u2020 : unlabeled data used for extra pre-training.   Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples.", "selected": "all datasets except SQuAD", "paper_id": "229363636"}]}}
{"idx": "176206", "paper_id": "216642069", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube", "abstract": "Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos, a domain that, a priori, we expect to be relatively\"easy:\"speakers in instructional videos will often reference the literal objects/actions being depicted. Because instructional videos make up only a fraction of the web's diverse video content, we ask: can similar models be trained on broader corpora? And, if so, what types of videos are\"grounded\"and what types are not? We examine the diverse YouTube8M corpus, first verifying that it contains many non-instructional videos via crowd labeling. We pretrain a representative model on YouTube8M and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set still results in representations that generalize to both non-instructional and instructional domains.", "context_section_header": "", "context_paragraph": "porally corresponding (clip, ASR caption) pairs are sampled (\"Positive\" cases). For each positive case, a set of mismatched \"N egative\" cases is also sampled both from other videos and from the same video in equal proportion. In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change. 3 The following hinge loss is minimized for margin \u03b4:", "sentence": "In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change.", "cited_ids": [{"paper_id": "182952863", "citation": "Miech et al. (2019)"}], "y": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this makes their [simple] error analysis significantly more straightforward, and results in [a minimal positive] performance change.", "snippet_surface": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this simplifying choice makes their error analysis significantly more straightforward, and results in minimal performance change.", "questions": {"VYfhk0y6eP": "Is there a positive or a negative performance change?"}, "answers": {"VYfhk0y6eP": "A positive performance change is noted"}, "evidence": {"VYfhk0y6eP": [{"section": "Domain", "paragraph": "We next ask: are instructional videos indeed easier to ground? While human judgements of instructional-ness and intra-video AUC are positively correlated \u03c1 = .20 (p 0), the low magnitude of this correlation provides additional empirical confirmation that other types of videos are 6 To make sure that the model is not succeeding simply because a category happened to be frequent in the dataset, we note the correlation between category AUC and category frequency is essentially zero (\u03c1 = .02, p > .58).", "selected": "positively correlated \u03c1 = .20 (p 0),", "paper_id": "216642069"}, {"section": "Domain", "paragraph": "Additionally, we train OLS regression models to predict segment AUC from lexical unigram features, while controlling for timing/length features. Lexical features add predictive capacity (p .01, F-test). While we find some patterns, e.g., intro/outro-language (e.g., \"hey\", \"welcome\", \"peace\") predictive of segment AUC for both categories, we also observe topical patterns, e.g., several unigrams associated with specific action figure body parts (\"knee\", \"shoulder\", \"joint\", etc.) are positively associated with segment AUC.", "selected": "positively associated with segment AUC.", "paper_id": "216642069"}]}}
{"idx": "176260", "paper_id": "42957292", "title": "Effect Functors for Opinion Inference", "abstract": "Sentiment analysis has so far focused on the detection of explicit opinions. However, of late implicit opinions have received broader attention, the key idea being that the evaluation of an event type by a speaker depends on how the participants in the event are valued and how the event itself affects the participants. We present an annotation scheme for adding relevant information, couched in terms of so-called effect functors, to German lexical items. Our scheme synthesizes and extends previous proposals. We report on an inter-annotator agreement study. We also present results of a crowdsourcing experiment to test the utility of some known and some new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation.", "context_section_header": "", "context_paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "sentence": "Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations.", "cited_ids": [{"paper_id": "193721", "citation": "Ruppenhofer and Brandes (2015)"}], "y": "[Additional functors proposed for verbs embedding states of possibility] are added by the authors to functor types [such as possession, possibility, sentiment, and scalarity], unlike alternative approaches.", "snippet_surface": "The authors add explicit annotations of functor types to the annotations, unlike Ruppenhofer and Brandes (2015).", "questions": {"CY2yAlX3ch": "What \"functor types\" are considered?", "CfJJriGHG8": "What do the explicit annotations refer to?"}, "answers": {"CY2yAlX3ch": "Possession, Possibility, Sentiment, Scalarity", "CfJJriGHG8": "The explicit annotations refer to the additional functors proposed for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity."}, "evidence": {"CY2yAlX3ch": [{"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5.", "paper_id": "42957292"}, {"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "embedding states of possibility, predicates expressing location, and predicates expressing similarity.", "paper_id": "42957292"}], "CfJJriGHG8": [{"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity.", "paper_id": "42957292"}]}}
{"idx": "178903", "paper_id": "53082875", "title": "A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images", "abstract": "Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely, on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.", "context_section_header": "", "context_paragraph": "All the aforementioned methods rely on independently pre-trained linguistic embeddings and visual features. In this work, we propose a different strategy, which consists in adapting those representations so that the information can be fused in earlier stages. In this respect, the closest work to ours is (Lazaridou et al., 2015), which proposes to augment the SKIP-GRAM objective function with a term mapping the textual embeddings to the visual features. Crudely, the linguistic embeddings must therefore predict both the text co-occurrences and (pre-trained) visual features. We emphasize two key differences with our approach. First, instead of performing a regression or mapping from the textual embeddings to the visual features, our model learns to infer perceptual latent factors to retain only the portion of visual information that can supplement the linguistic embeddings in representing words. Second, while Lazaridou et al. (2015) combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way. Specifically, our model seeks latent factors that are good at explaining the word-context co-occurrences. For instance, a visual feature of (an image of) OCEAN often contains information about SKY and BLUE -such visual information could be beneficial to predict cooccurrence of tokens in the context of OCEAN. This desiderata further strengthens the learned embeddings to be visually grounded. In our experiments, we show that our approach tends to group concrete visually similar concepts together.", "sentence": "Second, while Lazaridou et al. (2015) combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way.", "cited_ids": [{"paper_id": "6618571", "citation": "Lazaridou et al. (2015)"}], "y": "While Lazaridou et al. (2015) [predicts both the text co-occurrences and pre-trained visual features], the authors use a joint probabilistic model integrating both visual and text information in a principled way", "snippet_surface": "While Lazaridou et al. (2015) combines two objectives, the authors use a joint probabilistic model integrating both visual and text information in a principled way.", "questions": {"7nXp3q1cmq": "What two objectives do Lazaridou et al. combine?"}, "answers": {"7nXp3q1cmq": "Lazaridou et al's two objectives are to predict both the text co-occurrences and pre-trained visual features."}, "evidence": {"7nXp3q1cmq": [{"section": "Related Work", "paragraph": "All the aforementioned methods rely on independently pre-trained linguistic embeddings and visual features. In this work, we propose a different strategy, which consists in adapting those representations so that the information can be fused in earlier stages. In this respect, the closest work to ours is (Lazaridou et al., 2015), which proposes to augment the SKIP-GRAM objective function with a term mapping the textual embeddings to the visual features. Crudely, the linguistic embeddings must therefore predict both the text co-occurrences and (pre-trained) visual features. We emphasize two key differences with our approach. First, instead of performing a regression or mapping from the textual embeddings to the visual features, our model learns to infer perceptual latent factors to retain only the portion of visual information that can supplement the linguistic embeddings in representing words. Second, while Lazaridou et al. (2015) combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way. Specifically, our model seeks latent factors that are good at explaining the word-context co-occurrences. For instance, a visual feature of (an image of) OCEAN often contains information about SKY and BLUE -such visual information could be beneficial to predict cooccurrence of tokens in the context of OCEAN. This desiderata further strengthens the learned embeddings to be visually grounded. In our experiments, we show that our approach tends to group concrete visually similar concepts together.", "selected": "In this respect, the closest work to ours is (Lazaridou et al., 2015), which proposes to augment the SKIP-GRAM objective function with a term mapping the textual embeddings to the visual features. Crudely, the linguistic embeddings must therefore predict both the text co-occurrences and (pre-trained) visual features.", "paper_id": "53082875"}]}}
{"idx": "1801.05147.2.2.1", "paper_id": "1801.05147", "title": "Adversarial Learning for Chinese NER from Crowd Annotations", "abstract": "To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.", "context_section_header": "Data Sets", "context_paragraph": "With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two domains: Dialog and E-commerce domain. We hire undergraduate students to annotate the sentences. They are required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators are educated some tips in fifteen minutes and also provided with 20 exemplifying sentences.", "sentence": "With the purpose of obtaining evaluation datasets from crowd annotators, we collect the sentences from two domains: Dialog and E-commerce domain. We hire undergraduate students to annotate the sentences. They are required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators are educated some tips in fifteen minutes and also provided with 20 exemplifying sentences.", "cited_ids": [], "y": "The authors collected sentences from two domains, Dialog and E-commerce, [to show the results of crowdsourcing NER labels from non-experts]. They hired undergraduate students to annotate the sentences. The students were required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators were taught some tips in fifteen minutes and also provided with 20 example sentences.", "snippet_surface": "With the purpose of obtaining evaluation datasets from crowd annotators, the authors collected the sentences from two domains: Dialog and E-commerce. They hired undergraduate students to annotate the sentences. The students were required to identify the predefined types of entities in the sentences. Together with the guideline document, the annotators were taught some tips in fifteen minutes and also provided with 20 example sentences.", "questions": {"QeFkbuMjhJ": "What task was the dataset collected for?"}, "answers": {"QeFkbuMjhJ": "It was collected to show the results of crowdsourcing NER labels from non-experts."}, "evidence": {"QeFkbuMjhJ": [{"section": "Abstract", "paragraph": "To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.", "selected": ". In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER)", "paper_id": "1801.05147"}, {"section": "Abstract", "paragraph": "To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.", "selected": "crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts.", "paper_id": "1801.05147"}]}}
{"idx": "1804.05918.1.1.1", "paper_id": "1804.05918", "title": "Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph", "abstract": "We argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph, or independently from all discourse relations and the overall paragraph-level discourse structure. With the goal of improving implicit discourse relation classification, we introduce a paragraph-level neural networks that model inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predict a sequence of discourse relations in a paragraph. Experimental results show that our model outperforms the previous state-of-the-art systems on the benchmark corpus of PDTB.", "context_section_header": "Experimental Results", "context_paragraph": "The second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph. Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations. Especially on the contingency relation, the classification performance was improved by another 1.42 percents. Moreover, the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).", "sentence": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).", "cited_ids": [], "y": "The basic [discourse-level neural network] model yields good performance for recognizing explicit discourse relations as well. This is comparable with the previous best result of a 92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11.", "snippet_surface": "The basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with the previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11).", "questions": {"do2PB9e8rD": "What is \"the basic model\" refer to?", "S8PC1YvaKT": "How good is a \"good performance\"?"}, "answers": {"do2PB9e8rD": "The discourse level neural network model.", "S8PC1YvaKT": "Better than 92.05%."}, "evidence": {"do2PB9e8rD": [{"section": "The Basic Model Architecture", "paragraph": "Figure 1 illustrates the overall architecture of the discourse-level neural network model that consists of two Bi-LSTM layers, one max-pooling layer in between and one softmax prediction layer. The input of the neural network model is a paragraph containing a sequence of discourse units, while the output is a sequence of discourse relations with one relation between each pair of adjacent discourse units.", "selected": "the discourse-level neural network model that consists of two Bi-LSTM layers, one max-pooling layer in between and one softmax prediction laye", "paper_id": "1804.05918"}], "S8PC1YvaKT": [{"section": "Experimental Results", "paragraph": "The second row shows the performance of our basic paragraph-level model which predicts both implicit and explicit discourse relations in a paragraph. Compared to the variant system (the first row), the basic model further improved the classification performance on the first three implicit relations. Especially on the contingency relation, the classification performance was improved by another 1.42 percents. Moreover, the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).", "selected": "92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11", "paper_id": "1804.05918"}]}}
{"idx": "1805.08241.1.1.1", "paper_id": "1805.08241", "title": "Sparse and Constrained Attention for Neural Machine Translation", "abstract": "In NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs.", "context_section_header": "Experiments", "context_paragraph": "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit BIBREF21 with the default parameters . We used a validation set to tune hyperparameters introduced by our model. Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.", "sentence": "We evaluated our attention transformations on three language pairs.", "cited_ids": [], "y": "The authors evaluated their attention transformations on three language pairs [German-English, Japanese-English, and Romanian-English].", "snippet_surface": "The authors evaluated their attention transformations on three language pairs.", "questions": {"ihG8NDFJFS": "What language pairs are the attention transformations tested on?"}, "answers": {"ihG8NDFJFS": "German-English\n Japanese-English\n Romanian-English"}, "evidence": {"ihG8NDFJFS": [{"section": "Experiments", "paragraph": "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units BIBREF20 with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit BIBREF21 with the default parameters . We used a validation set to tune hyperparameters introduced by our model. Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.", "selected": "IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.", "paper_id": "1805.08241"}]}}
{"idx": "1806.04511.1.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Discussion", "context_paragraph": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. Building separate models for each language requires both labeled and unlabeled data. Even though having lots of labeled data in every language is the perfect case, it is unrealistic. Therefore, eliminating the resource requirement in this resource-constrained task is crucial. The fact that machine translation can be used in reusing models from different languages is promising for reducing the data requirements.", "sentence": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.", "cited_ids": [], "y": "The authors' results show that their RNN model achieved significant improvements over the majority baseline for both non-English (on the average 22.76% improvement [relative to the RNN-based framework]; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement). This demonstrates that the model is robust to handle multiple languages.", "snippet_surface": "The authors' results show that their RNN model achieved significant improvements over the majority baseline for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement). This demonstrates that the model is robust to handle multiple languages.", "questions": {"evbHYEIfXe": "What is relative improvement here? Relative to what?"}, "answers": {"evbHYEIfXe": "The improvement is relative to the RNN-based framework"}, "evidence": {"evbHYEIfXe": [{"section": "Discussion", "paragraph": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. Building separate models for each language requires both labeled and unlabeled data. Even though having lots of labeled data in every language is the perfect case, it is unrealistic. Therefore, eliminating the resource requirement in this resource-constrained task is crucial. The fact that machine translation can be used in reusing models from different languages is promising for reducing the data requirements.", "selected": "Considering the improvements over the majority baseline achieved by the RNN model", "paper_id": "1806.04511"}]}}
{"idx": "1806.04511.3.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Corpora", "context_paragraph": "For evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus.", "sentence": "These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus.", "cited_ids": [], "y": "Datasets [comprising restaurant reviews in four different languages (Spanish, Turkish, Dutch, Russian)] are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. A table shows the number of reviews in each test corpus alongside whether it is a positive or negative review.", "snippet_surface": "These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. Table TABREF7 shows the number of observations in each test corpus.", "questions": {"dIt5i30Bjt": "What does \"these datasets\" refer to?"}, "answers": {"dIt5i30Bjt": "These datasets are restaurant reviews in four different languages (Spanish, Turkish, Dutch, Russian)."}, "evidence": {"dIt5i30Bjt": [{"section": "Corpora", "paragraph": "Two sets of corpora are used in this study, both are publicly available. The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian). We focus on polarity detection in reviews, therefore all datasets in this study have two class values (positive, negative).", "selected": "The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).", "paper_id": "1806.04511"}, {"section": "Corpora", "paragraph": "Two sets of corpora are used in this study, both are publicly available. The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian). We focus on polarity detection in reviews, therefore all datasets in this study have two class values (positive, negative).", "selected": "contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)", "paper_id": "1806.04511"}, {"section": "Corpora", "paragraph": "For evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus.", "selected": "or evaluation of the multilingual approach, we use four languages. These datasets", "paper_id": "1806.04511"}]}}
{"idx": "1806.04511.4.1.1", "paper_id": "1806.04511", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "abstract": "Sentiment analysis is a widely studied NLP task where the goal is to determine opinions, emotions, and evaluations of users towards a product, an entity or a service that they are reviewing. One of the biggest challenges for sentiment analysis is that it is highly language dependent. Word embeddings, sentiment lexicons, and even annotated data are language specific. Further, optimizing models for each language is very time consuming and labor intensive especially for recurrent neural network models. From a resource perspective, it is very challenging to collect data for different languages. In this paper, we look for an answer to the following research question: can a sentiment analysis model trained on a language be reused for sentiment analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the data is more limited? Our goal is to build a single model in the language with the largest dataset available for the task, and reuse it for languages that have limited resources. For this purpose, we train a sentiment analysis model using recurrent neural networks with reviews in English. We then translate reviews in other languages and reuse this model to evaluate the sentiments. Experimental results show that our robust approach of single model trained on English reviews statistically significantly outperforms the baselines in several different languages.", "context_section_header": "Experimental Results", "context_paragraph": "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", "sentence": "In addition to the majority baseline, we also compare our results with a lexicon-based approach.", "cited_ids": [], "y": "In addition to the majority baseline, [which is defined as the language model's accuracy if it always predict the majority class in the dataset] the authors also compare their results with a lexicon-based approach.", "snippet_surface": "In addition to the majority baseline, the authors also compare their results with a lexicon-based approach.", "questions": {"AW2U7VJ+oP": "What does the \"majority baseline\" refer to?"}, "answers": {"AW2U7VJ+oP": "Is the language model's accuracy if it always predict the majority class in the dataset."}, "evidence": {"AW2U7VJ+oP": [{"section": "Experimental Results", "paragraph": "For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset. For example, if the dataset has 60% of all reviews positive and 40% negative, majority baseline would be 60% because a model that always predicts \u201cpositive\u201d will be 60% accurate and will make mistakes 40% of the time.", "selected": "For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset. For example, if the dataset has 60% of all reviews positive and 40% negative, majority baseline would be 60% because a model that always predicts \u201cpositive\u201d will be 60% accurate and will make mistakes 40% of the time.", "paper_id": "1806.04511"}]}}
{"idx": "1808.09111.1.1.1", "paper_id": "1808.09111", "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections", "abstract": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "context_section_header": "Data", "context_paragraph": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. To create the observed data embeddings, we train skip-gram word embeddings BIBREF7 that are found to capture syntactic properties well when trained with small context window BIBREF8 , BIBREF9 . Following BIBREF9 , the dimensionality INLINEFORM0 is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language modeling benchmark dataset BIBREF21 in addition to the WSJ corpus.", "sentence": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "cited_ids": [], "y": "[The authors evaluate their approach of using Markov and tree-structured priors on the part-of-speech (POS) induction and unsupervised dependency parsing]. These experiments were run on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "snippet_surface": "The authors run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank for both POS tagging and dependency parsing.", "questions": {"0LWW4U6eIN": "What does \"dependency parsing\" mean?", "rEUjbO7Pyk": "What does \"experiments\" refer to?"}, "answers": {"0LWW4U6eIN": "A dependency parse is a latent syntactic representation.", "rEUjbO7Pyk": "The authors experiment uses Markov and tree-structured priors to illustrate their approach of evaluating two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation."}, "evidence": {"0LWW4U6eIN": [{"section": "Introduction", "paragraph": "In our approach, we propose to learn a new latent embedding space as a projection of pre-trained embeddings (depicted in Figure SECREF5 ), while jointly learning latent syntactic structure \u2013 for example, POS categories or syntactic dependencies. To this end, we introduce a new generative model (shown in Figure FIGREF6 ) that first generates a latent syntactic representation (e.g. a dependency parse) from a discrete structured prior (which we also call the \u201csyntax model\u201d), then, conditioned on this representation, generates a sequence of latent embedding random variables corresponding to each word, and finally produces the observed (pre-trained) word embeddings by projecting these latent vectors through a parameterized non-linear function. The latent embeddings can be jointly learned with the structured syntax model in a completely unsupervised fashion.", "selected": "we introduce a new generative model (shown in Figure FIGREF6 ) that first generates a latent syntactic representation (e.g. a dependency parse) from a discrete structured prior (which we also call the \u201csyntax model\u201d)", "paper_id": "1808.09111"}], "rEUjbO7Pyk": [{"section": "Abstract", "paragraph": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "selected": "In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation.", "paper_id": "1808.09111"}, {"section": "Abstract", "paragraph": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "selected": "a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior", "paper_id": "1808.09111"}, {"section": "Abstract", "paragraph": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.", "selected": "we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior.", "paper_id": "1808.09111"}]}}
{"idx": "1809.01541.1.2.1", "paper_id": "1809.01541", "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding", "abstract": "This paper documents the Team Copenhagen system which placed first in the CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the\"inflection in context\"task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.", "context_section_header": "Our system", "context_paragraph": "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", "sentence": "Multilingual training is performed by randomly alternating between languages for every new minibatch.", "cited_ids": [], "y": "The authors perform multilingual training by randomly alternating between languages for every new [randomly selected] minibatch.", "snippet_surface": "The authors perform multilingual training by randomly alternating between languages for every new minibatch.", "questions": {"0CgbUmuw8X": "Where do they get the new minibatch?"}, "answers": {"0CgbUmuw8X": "Minibatches are randomy selected"}, "evidence": {"0CgbUmuw8X": [{"section": "Our system", "paragraph": "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", "selected": "we experiment with random groupings of two to three languages", "paper_id": "1809.01541"}]}}
{"idx": "1809.09194.1.1.1", "paper_id": "1809.09194", "title": "Stochastic Answer Networks for SQuAD 2.0", "abstract": "This paper presents an extension of the Stochastic Answer Network (SAN), one of the state-of-the-art machine reading comprehension models, to be able to judge whether a question is unanswerable or not. The extended SAN contains two components: a span detector and a binary classifier for judging whether the question is unanswerable, and both components are jointly optimized. Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. To facilitate the research on this field, we release our code: https://github.com/kevinduh/san_mrc.", "context_section_header": "Model", "context_paragraph": "Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2", "sentence": "The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2", "cited_ids": [], "y": "The authors use the attention function BIBREF11 to compute the similarity score between passages and questions as: INLINEFORM2.", "snippet_surface": "The authors use the attention function BIBREF11 to compute the similarity score between passages and questions as: INLINEFORM2.", "questions": {"5tWRrLLkw/": "What does \"attention function\" refer to?"}, "answers": {"5tWRrLLkw/": "The attention function refers to an MRC system called BIBREF11 for calculating a similarity score."}, "evidence": {"5tWRrLLkw/": [{"section": "Background", "paragraph": "In comparison with many existing MRC systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , which extract answers by finding a sub-string in the passages/paragraphs, we propose a model that not only extracts answers but also predicts whether such an answer should exist. Using a multi-task learning approach (c.f. BIBREF5 ), we extend the Stochastic Answer Network (SAN) BIBREF1 for MRC answer span detector to include a classifier that whether the question is unanswerable. The unanswerable classifier is a pair-wise classification model BIBREF6 which predicts a label indicating whether the given pair of a passage and a question is unanswerable. The two models share the same lower layer to save the number of parameters, and separate the top layers for different tasks (the span detector and binary classifier).", "selected": "In comparison with many existing MRC systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 ,", "paper_id": "1809.09194"}, {"section": "Model", "paragraph": "Lexicon Encoding Layer. We map the symbolic/surface feature of INLINEFORM0 and INLINEFORM1 into neural space via word embeddings , 16-dim part-of-speech (POS) tagging embeddings, 8-dim named-entity embeddings and 4-dim hard-rule features. Note that we use small embedding size of POS and NER to reduce model size and they mainly serve the role of coarse-grained word clusters. Additionally, we use question enhanced passages word embeddings which can viewwed as soft matching between questions and passages. At last, we use two separate two-layer position-wise Feed-Forward Networks (FFN) BIBREF11 , BIBREF1 to map both question and passage encodings into the same dimension. As results, we obtain the final lexicon embeddings for the tokens for INLINEFORM2 as a matrix INLINEFORM3 , and tokens in INLINEFORM4 as INLINEFORM5 .", "selected": "At last, we use two separate two-layer position-wise Feed-Forward Networks (FFN) BIBREF11", "paper_id": "1809.09194"}]}}
{"idx": "1810.08699.1.1.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Experiments", "context_paragraph": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .", "sentence": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .", "cited_ids": [], "y": "The authors describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on their data. They trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model that uses bidirectional LSTM cells for character-based feature extraction and CRF [Conditional Random Field], described in Guillaume Genthial's \"Sequence Tagging with Tensorflow\" blog post.", "snippet_surface": "The authors describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on their data. They trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", "questions": {"X6oIipI1g/": "What is \"CRF\"?"}, "answers": {"X6oIipI1g/": "CRF stands for Conditional Random Field"}, "evidence": {"X6oIipI1g/": [{"section": "Experiments", "paragraph": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 .", "selected": "described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15", "paper_id": "1810.08699"}, {"section": "Models", "paragraph": "Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .", "selected": "conditional random fields (CRF)", "paper_id": "1810.08699"}, {"section": "Models", "paragraph": "The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.", "selected": "a CRF layer labels the sequence of these contextual representations", "paper_id": "1810.08699"}]}}
{"idx": "1810.08699.1.1.3", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Models", "context_paragraph": "spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.", "sentence": "spaCy 2.0 uses a CNN-based transition system for named entity recognition.", "cited_ids": [], "y": "SpaCy 2.0 [one of the NER models evaluated] uses a CNN-based transition system for named entity recognition [NER].", "snippet_surface": "SpaCy 2.0 uses a CNN-based transition system for named entity recognition.", "questions": {"qJLBG7Bvo0": "Is SpaCy 2.0 one of the ner models evaluated?"}, "answers": {"qJLBG7Bvo0": "Yes, spaCy 2.0 was one of the NER models used."}, "evidence": {"qJLBG7Bvo0": [{"section": "Discussion", "paragraph": "For spaCy 2.0 named entity recognizer, the same word embedding models were tested. However, in this case the performance of 200-dimensional embeddings was highest (Table TABREF21 ). Unsurprisingly, both deep learning models outperformed the feature-based Stanford recognizer in recall, the latter however demonstrated noticeably higher precision.", "selected": "For spaCy 2.0 named entity recognizer, the same word embedding models were tested", "paper_id": "1810.08699"}, {"section": "Discussion", "paragraph": "It is clear that the development set of automatically generated examples was not an ideal indicator of models' performance on gold-standard test set. Higher development set scores often led to lower test scores as seen in the evaluation results for spaCy 2.0 and Char-biLSTM+biLSTM+CRF (Tables TABREF21 and TABREF20 ). Analysis of errors on the development set revealed that many were caused by the incompleteness of annotations, when named entity recognizers correctly predicted entities that were absent from annotations (e.g. [\u053d\u054d\u0540\u0544-\u056b LOC] (USSR's), [\u0534\u056b\u0576\u0561\u0574\u0578\u0576 ORG] (the_Dinamo), [\u054a\u056b\u0580\u0565\u0576\u0565\u0575\u0561\u0576 \u0569\u0565\u0580\u0561\u056f\u0572\u0566\u0578\u0582 LOC] (Iberian Peninsula's) etc). Similarly, the recognizers often correctly ignored non-entities that are incorrectly labeled in data (e.g. [\u0585\u057d\u0574\u0561\u0576\u0576\u0565\u0580\u056b PER], [\u056f\u0578\u0576\u057d\u0565\u0580\u057e\u0561\u057f\u0578\u0580\u056b\u0561\u0576 ORG] etc).", "selected": "Higher development set scores often led to lower test scores as seen in the evaluation results for spaCy 2.0 and Char-biLSTM+biLSTM+CRF (Tables TABREF21 and TABREF20 ).", "paper_id": "1810.08699"}, {"section": "Conclusion", "paragraph": "Additionally, to establish the applicability of Wikipedia-based approaches for the Armenian language, we provide evaluation results for 3 different named entity recognition systems trained and tested on our datasets. The results reinforce the ability of deep learning approaches in achieving relatively high recall values for this specific task, as well as the power of using character-extracted embeddings alongside conventional word embeddings.", "selected": "evaluation results for 3 different named entity recognition systems", "paper_id": "1810.08699"}]}}
{"idx": "1810.08699.1.1.4", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Models", "context_paragraph": "The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines. The distinctive feature of this approach is the way contextual word embeddings are formed. For each token separately, to capture its word shape features, character-based representation is extracted using a bidirectional LSTM BIBREF18 . This representation gets concatenated with a distributional word vector such as GloVe, forming an intermediate word embedding. Using another bidirectional LSTM cell on these intermediate word embeddings, the contextual representation of tokens is obtained (Figure FIGREF17 ). Finally, a CRF layer labels the sequence of these contextual representations. In our experiments, we used Guillaume Genthial's implementation of the algorithm. We set the size of character-based biLSTM to 100 and the size of second biLSTM network to 300.", "sentence": "The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.", "cited_ids": [], "y": "The main model that authors focused on was the recurrent model with a CRF [Conditional Random Field] top layer, and the above-mentioned methods[, the Stanford NER and spaCy 2.0,] served mostly as baselines.", "snippet_surface": "The main model that authors focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.", "questions": {"/oeWZlm6iy": "What are the aboved-mentioned methods?"}, "answers": {"/oeWZlm6iy": "The \"above-mentioned\" methods refer to the two NER models used as comparisons for the proposed contribution model.  They are the Stanford NER and spaCy 2.0"}, "evidence": {"/oeWZlm6iy": [{"section": "Models", "paragraph": "Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 .", "selected": "Stanford NER is conditional random fields (CRF)", "paper_id": "1810.08699"}, {"section": "Models", "paragraph": "spaCy 2.0 uses a CNN-based transition system for named entity recognition. For each token, a Bloom embedding is calculated based on its lowercase form, prefix, suffix and shape, then using residual CNNs, a contextual representation of that token is extracted that potentially draws information from up to 4 tokens from each side BIBREF17 . Each update of the transition system's configuration is a classification task that uses the contextual representation of the top token on the stack, preceding and succeeding tokens, first two tokens of the buffer, and their leftmost, second leftmost, rightmost, second rightmost children. The valid transition with the highest score is applied to the system. This approach reportedly performs within 1% of the current state-of-the-art for English . In our experiments, we tried out 50-, 100-, 200- and 300-dimensional pre-trained GloVe embeddings. Due to time constraints, we did not tune the rest of hyperparameters and used their default values.", "selected": "paCy 2.0 uses a CNN-based transition system for named entity recognition.", "paper_id": "1810.08699"}]}}
{"idx": "1810.08699.2.1.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Test dataset", "context_paragraph": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .", "sentence": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "cited_ids": [], "y": "In order to evaluate the models trained on the generated data [consisting of annotated sentences and tokens from the Armenian Wikipedia and manually curated datasets], the authors manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "snippet_surface": "In order to evaluate the models trained on generated data, the authors manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "questions": {"+R7deKv5BC": "What does the generated data consist of?"}, "answers": {"+R7deKv5BC": "The generated data consists of annotated sentences and tokens from the Armenian Wikipedia and manually curated datasets."}, "evidence": {"+R7deKv5BC": [{"section": "Generated data", "paragraph": "Using the algorithm described above, we generated 7455 annotated sentences with 163247 tokens based on 20 February 2018 dump of Armenian Wikipedia.", "selected": "Using the algorithm described above, we generated 7455 annotated sentences with 163247 tokens based on 20 February 2018 dump of Armenian Wikipedia.", "paper_id": "1810.08699"}, {"section": "Generated data", "paragraph": "The generated data is still significantly smaller than the manually annotated corpora from CoNLL 2002 and 2003. For comparison, the train set of English CoNLL 2003 corpus contains 203621 tokens and the German one 206931, while the Spanish and Dutch corpora from CoNLL 2002 respectively 273037 and 218737 lines. The smaller size of our generated data can be attributed to the strict selection of candidate sentences as well as simply to the relatively small size of Armenian Wikipedia.", "selected": "The smaller size of our generated data can be attributed to the strict selection of candidate sentences as well as simply to the relatively small size of Armenian Wikipedia.", "paper_id": "1810.08699"}]}}
{"idx": "1810.08699.2.2.1", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Automated training corpus generation", "context_paragraph": "We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "sentence": "This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "cited_ids": [], "y": "The approach using [Sysoev and Andrianov's modification of Nothman et al. to automatically generate data for the Russian language] uses [extracted text and outgoing] links from Wikipedia articles to generate sequences of named-entity annotated tokens.", "snippet_surface": "This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "questions": {"hWMxR+6E1/": "What does this approach refer to?"}, "answers": {"hWMxR+6E1/": "The approach refers to using Sysoev and Andrianov's modification of the Nothman et al. to automatically generate data. This approach was used for the Russian lanugage. This approach works by extracting Wikipedia article texts with outgoing link and then convert these links into named entities."}, "evidence": {"hWMxR+6E1/": [{"section": "Introduction", "paragraph": "Nothman et al. generated a silver-standard corpus for 9 languages by extracting Wikipedia article texts with outgoing links and turning those links into named entity annotations based on the target article's type BIBREF5 . Sysoev and Andrianov used a similar approach for the Russian language BIBREF6 BIBREF7 . Based on its success for a wide range of languages, our choice fell on this model to tackle automated data generation and annotation for the Armenian language.", "selected": "Nothman et al. generated a silver-standard corpus for 9 languages by extracting Wikipedia article texts with outgoing links and turning those links into named entity annotations based on the target article's type BIBREF5 .", "paper_id": "1810.08699"}, {"section": "Automated training corpus generation", "paragraph": "We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. This approach uses links between Wikipedia articles to generate sequences of named-entity annotated tokens.", "selected": "We used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer.", "paper_id": "1810.08699"}]}}
{"idx": "1810.08699.3.1.2", "paper_id": "1810.08699", "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition", "abstract": "In this work, we tackle the problem of Armenian named entity recognition, providing silver- and gold-standard datasets as well as establishing baseline results on popular models. We present a 163000-token named entity corpus automatically generated and annotated from Wikipedia, and another 53400-token corpus of news sentences with manual annotation of people, organization and location named entities. The corpora were used to train and evaluate several popular named entity recognition models. Alongside the datasets, we release 50-, 100-, 200-, 300-dimensional GloVe word embeddings trained on a collection of Armenian texts from Wikipedia, news, blogs, and encyclopedia.", "context_section_header": "Test dataset", "context_paragraph": "During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. Only named entities corresponding to BBN's person name category were tagged as PER. Those include proper names of people, including fictional people, first and last names, family names, unique nicknames. Similarly, organization name categories, including company names, government agencies, educational and academic institutions, sports clubs, musical ensembles and other groups, hospitals, museums, newspaper names, were marked as ORG. However, unlike BBN, we did not mark adjectival forms of organization names as named entities. BBN's gpe name, facility name, location name categories were combined and annotated as LOC.", "sentence": "During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "cited_ids": [], "y": "During annotation [of named entity datasets using three different classes: PER, ORG, LOC], the authors generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "snippet_surface": "During annotation, the authors generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "questions": {"Oa+vp7mM9L": "What was being annotated?"}, "answers": {"Oa+vp7mM9L": "Named entities datasets were annotated using three different classes: PER, ORG, LOC."}, "evidence": {"Oa+vp7mM9L": [{"section": "Test dataset", "paragraph": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .", "selected": "we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "paper_id": "1810.08699"}, {"section": "Test dataset", "paragraph": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8 , FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme. Tokens and sentences were segmented according to the UD standards for the Armenian language BIBREF11 .", "selected": "The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC),", "paper_id": "1810.08699"}]}}
{"idx": "1810.12196.2.1.1", "paper_id": "1810.12196", "title": "ReviewQA: a relational aspect-based opinion reading dataset", "abstract": "Deep reading models for question-answering have demonstrated promising performance over the last couple of years. However current systems tend to learn how to cleverly extract a span of the source document, based on its similarity with the question, instead of seeking for the appropriate answer. Indeed, a reading machine should be able to detect relevant passages in a document regarding a question, but more importantly, it should be able to reason over the important pieces of the document in order to produce an answer when it is required. To motivate this purpose, we present ReviewQA, a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that we expect a model to master. Indeed, each question comes with an associated type that characterizes the required competency. With this framework, it is possible to benchmark the main families of models and to get an overview of what are the strengths and the weaknesses of a given model on the set of tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of the recent datasets, but selected among a set of candidates that contains all the possible answers to the questions of the dataset. Finally, we present several baselines over this dataset.", "context_section_header": "Models", "context_paragraph": "Logistic regression: To produce the representation of the input, we concatenate the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question. It produces an array of size INLINEFORM0 where INLINEFORM1 is the vocabulary size. Then we use a logistic regression to select the most probable answer among the INLINEFORM2 possibilities.", "sentence": "Logistic regression: To produce the representation of the input, we concatenate the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question.", "cited_ids": [], "y": "Logistic regression: To produce the representation of the input [hotel reviews], by concatenating the Bag-Of-Words [obtained from the source document for which deep reading models have been used] for representation of the document and the question.", "snippet_surface": "The authors used logistic regression to produce the representation of the input, by concatenating the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question.", "questions": {"VoNvmM1+yg": "What does \"the document\" refer to?"}, "answers": {"VoNvmM1+yg": "The document is the source on which deep reading models have been used for question and answering; it is a text document. In this paper, the authors used hotel reviews as their source document."}, "evidence": {"VoNvmM1+yg": [{"section": "Abstract", "paragraph": "Deep reading models for question-answering have demonstrated promising performance over the last couple of years. However current systems tend to learn how to cleverly extract a span of the source document, based on its similarity with the question, instead of seeking for the appropriate answer. Indeed, a reading machine should be able to detect relevant passages in a document regarding a question, but more importantly, it should be able to reason over the important pieces of the document in order to produce an answer when it is required. To motivate this purpose, we present ReviewQA, a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that we expect a model to master. Indeed, each question comes with an associated type that characterizes the required competency. With this framework, it is possible to benchmark the main families of models and to get an overview of what are the strengths and the weaknesses of a given model on the set of tasks evaluated in this dataset. Our corpus contains more than 500.000 questions in natural language over 100.000 hotel reviews. Our setup is projective, the answer of a question does not need to be extracted from a document, like in most of the recent datasets, but selected among a set of candidates that contains all the possible answers to the questions of the dataset. Finally, we present several baselines over this dataset.", "selected": "Deep reading models for question-answering have demonstrated promising performance over the last couple of years. However current systems tend to learn how to cleverly extract a span of the source document", "paper_id": "1810.12196"}, {"section": "Introduction", "paragraph": "A large majority of the human knowledge is recorded through text documents. That is why ability for a system to automatically infer information from text without any structured data has become a major challenge. Answering questions about a given document is a relevant proxy task that has been proposed as a way to evaluate the reading ability of a given model. In this configuration, a text document such as a news article, a document from Wikipedia or any type of text is presented to a machine with an associated set of questions. The system is then expected to answer these questions and evaluated by its accuracy on this task. The machine reading framework is very general and we can imagine a large panel of questions that can possibly handle most of the standard natural language processing tasks. For example, the task of named entities recognition can be formulated as a machine reading one where your document is the sentence and the question would be 'What are the named entities mentioned in this sentence?'. These natural language interactions are an important objective for reading systems.", "selected": "In this configuration, a text document such as a news article, a document from Wikipedia or any type of text is presented to a machine with an associated set of questions. The system is then expected to answer these questions and evaluated by its accuracy on this task.", "paper_id": "1810.12196"}, {"section": "ReviewQA dataset", "paragraph": "We think that evaluating the task of sentiment analysis through the setup of question-answering is a relevant playground for machine reading research. Indeed natural language questions about the different aspects of the targeted venues are typical kind of questions we want to be able to ask to a system. In this context, we introduce a set of reasoning questions types over the relationships between aspects. We propose ReviewQA, a dataset of natural language questions over hotel reviews. These questions are divided into 8 groups, regarding the competency required to be answered. In this section, we describe each task and the process followed to generate this dataset.", "selected": "We propose ReviewQA, a dataset of natural language questions over hotel reviews.", "paper_id": "1810.12196"}]}}
{"idx": "1811.00147.1.1.1", "paper_id": "1811.00147", "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "abstract": "We introduce a new method DOLORES for learning knowledge graph embeddings that effectively captures contextual cues and dependencies among entities and relations. First, we note that short paths on knowledge graphs comprising of chains of entities and relations can encode valuable information regarding their contextual usage. We operationalize this notion by representing knowledge graphs not as a collection of triples but as a collection of entity-relation chains, and learn embeddings for entities and relations using deep neural models that capture such contextual usage. In particular, our model is based on Bi-Directional LSTMs and learn deep representations of entities and relations from constructed entity-relation chains. We show that these representations can very easily be incorporated into existing models to significantly advance the state of the art on several knowledge graph prediction tasks like link prediction, triple classification, and missing relation type prediction (in some cases by at least 9.5%).", "context_section_header": "Dolores: Learner", "context_paragraph": "While it is obvious that our embeddings can be used as features for new predictive models, it is also very easy to incorporate our learned Dolores embeddings into existing predictive models on knowledge graphs. The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings. In our evaluation below, we show how to improve several state-of-the-art models on various tasks simply by incorporating Dolores as a drop-in replacement to the original embedding layer.", "sentence": "The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings.", "cited_ids": [], "y": "The only requirement is that the model accepts as input an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), the authors can just use Dolores embeddings [which are created using connections between language models and random walks on knowledge graphs] as a drop-in replacement. The authors just need to initialize the corresponding embedding layer with Dolores embeddings.", "snippet_surface": "The only requirement is that the model accepts as input an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), the authors can just use Dolores embeddings as a drop-in replacement. They just need to initialize the corresponding embedding layer with Dolores embeddings.", "questions": {"3icxdyP+MO": "How do you initialize an embedding layer with Dolores embedding?"}, "answers": {"3icxdyP+MO": "Creating connections between language models and random walks on knowledge graphs."}, "evidence": {"3icxdyP+MO": [{"section": "Introduction", "paragraph": "Unlike most knowledge graph embeddings like TransD, TransE BIBREF2 , BIBREF4 etc. which are typically learned using shallow models, the representations learned by Dolores are deep: dependent on an entire path (rather than just a triple), are functions of internal states of a Bi-Directional LSTM and composed of representations learned at various layers potentially capturing varying degrees of abstractions. Dolores is inspired by recent advances in learning word representations (word embeddings) from deep neural language models using Bi-Directional LSTMs BIBREF8 . In particular, we derive connections between the work of Peters et al. ( BIBREF8 ) who learn deep contextualized word embeddings from sentences using a Bi-Directional LSTM based language model and random walks on knowledge graphs. These connections enable us to propose new \u201cdeep contextualized\u201d knowledge graph embeddings which we call Dolores embeddings.", "selected": "learn deep contextualized word embeddings from sentences using a Bi-Directional LSTM based language model and random walks on knowledge graphs. These connections enable us to propose new \u201cdeep contextualized\u201d knowledge graph embeddings which we call Dolores embeddings.", "paper_id": "1811.00147"}]}}
{"idx": "1811.00383.1.2.1", "paper_id": "1811.00383", "title": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on the assisting-target language pair (parent model) which is later fine-tuned for the source-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, We propose to pre-order the assisting language sentence to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.", "context_section_header": "Network", "context_paragraph": "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 . These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29 .", "sentence": "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 .", "cited_ids": [], "y": "The authors use the pre-existing CFILT-preorder system for reordering English sentences to match the Indian language word order before running translation. The system contains two re-ordering systems: (1) generic rules that apply to all Indian languages, and (2) Hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.", "snippet_surface": "The authors use the prexisting CFILT-preorder system for reordering English sentences to match the Indian language word order before running translation. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 .", "questions": {"gVko2XrzCl": "What is the task the authors are trying to perform?"}, "answers": {"gVko2XrzCl": "To reorder English sentences to match the Indian language word order before translation."}, "evidence": {"gVko2XrzCl": [{"section": "Network", "paragraph": "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17 , and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28 . These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29 .", "selected": "reordering English sentences to match the Indian language word order.", "paper_id": "1811.00383"}]}}
{"idx": "1811.00383.2.1.1", "paper_id": "1811.00383", "title": "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages", "abstract": "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on the assisting-target language pair (parent model) which is later fine-tuned for the source-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, We propose to pre-order the assisting language sentence to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality.", "context_section_header": "Languages", "context_paragraph": "We experimented with English INLINEFORM0 Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order.", "sentence": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.", "cited_ids": [], "y": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks [the target language pair].", "snippet_surface": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.", "questions": {"6d43KC+tNh": "What does \"child tasks\" mean?"}, "answers": {"6d43KC+tNh": "Child task is the target language pair."}, "evidence": {"6d43KC+tNh": [{"section": "Introduction", "paragraph": "Transfer learning has also been explored in the multilingual Neural Machine Translation BIBREF3 , BIBREF9 , BIBREF10 . The goal is to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. The child model can now be fine-tuned on the source-target language pairs, if parallel corpus is available. The divergence between the source and the assisting language can adversely impact the benefits obtained from transfer learning. Multiple studies have shown that transfer learning works best when the languages are related BIBREF3 , BIBREF10 , BIBREF9 . Several studies have tried to address lexical divergence between the source and the target languages BIBREF10 , BIBREF11 , BIBREF12 . However, the effect of word order divergence and its mitigation has not been explored. In a practical setting, it is not uncommon to have source and assisting languages with different word order. For instance, it is possible to find parallel corpora between English and some Indian languages, but very little parallel corpora between Indian languages. Hence, it is natural to use English as an assisting language for inter-Indian language translation.", "selected": "The goal is to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target translation is the parent task).", "paper_id": "1811.00383"}]}}
{"idx": "1811.05711.1.1.1", "paper_id": "1811.05711", "title": "From Free Text to Clusters of Content in Health Records: An Unsupervised Graph Partitioning Approach", "abstract": "Electronic Healthcare records contain large volumes of unstructured data in different forms. Free text constitutes a large portion of such data, yet this source of richly detailed information often remains under-used in practice because of a lack of suitable methodologies to extract interpretable content in a timely manner. Here we apply network-theoretical tools to the analysis of free text in Hospital Patient Incident reports in the English National Health Service, to find clusters of reports in an unsupervised manner and at different levels of resolution based directly on the free text descriptions contained within them. To do so, we combine recently developed deep neural network text-embedding methodologies based on paragraph vectors with multi-scale Markov Stability community detection applied to a similarity graph of documents obtained from sparsified text vector similarities. We showcase the approach with the analysis of incident reports submitted in Imperial College Healthcare NHS Trust, London. The multiscale community structure reveals levels of meaning with different resolution in the topics of the dataset, as shown by relevant descriptive terms extracted from the groups of records, as well as by comparing a posteriori against hand-coded categories assigned by healthcare personnel. Our content communities exhibit good correspondence with well-defined hand-coded categories, yet our results also provide further medical detail in certain areas as well as revealing complementary descriptors of incidents beyond the external classification. We also discuss how the method can be used to monitor reports over time and across different healthcare providers, and to detect emerging trends that fall outside of pre-existing categories.", "context_section_header": "Multiscale graph partitioning for text analysis: description of the framework", "context_paragraph": "Figure 1 shows a summary of our pipeline. First, we pre-process each document to transform text into consecutive word tokens, where words are in their most normalised forms, and some words are removed if they have no distinctive meaning when used out of context BIBREF5 , BIBREF6 . We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. This training step is only done once. This Doc2Vec model is subsequently used to infer high-dimensional vector descriptions for the text of each of the 3229 documents in our target analysis set. We then compute a matrix containing pairwise similarities between any pair of document vectors, as inferred with Doc2Vec. This matrix can be thought of as a full, weighted graph with documents as nodes and edges weighted by their similarity. We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF8 , a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The derived MST-kNN graph is analysed with Markov Stability BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , a multi-resolution dynamics-based graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need for choosing a priori the number of clusters, scale or organisation. To analyse a posteriori the different partitions across levels of resolution, we use both visualisations and quantitative scores. The visualisations include word clouds to summarise the main content, graph layouts, as well as Sankey diagrams and contingency tables that capture the correspondences across levels of resolution and relationships to the hand-coded classifications. The partitions are also evaluated quantitatively to score: (i) their intrinsic topic coherence (using pairwise mutual information BIBREF13 , BIBREF14 ), and (ii) their similarity to the operator hand-coded categories (using normalised mutual information BIBREF15 ). We now expand on the steps of the computational framework.", "sentence": "We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results.", "cited_ids": [], "y": "[After pre-processing documents to transform text into consecutive word tokens,] the authors then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records[, which were obtained by tokenizing their documents, removing punctuation and digits, stemming the tokens, and removing stop-words]. Training on smaller sets (1 million) also produces good results.", "snippet_surface": "The authors then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results.", "questions": {"fDuVb7aaih": "The snippet says, the authors \"then\" train a .... What is done before the training?", "9YnrHRfBRJ": "What is done as part of preprocessing the text records?"}, "answers": {"fDuVb7aaih": "Pre-process documents to transform text into consecutive word tokens", "9YnrHRfBRJ": "They tokenize their documents, remove punctuation and digits. Then they stem the tokens and remove stop-words."}, "evidence": {"fDuVb7aaih": [{"section": "Multiscale graph partitioning for text analysis: description of the framework", "paragraph": "Figure 1 shows a summary of our pipeline. First, we pre-process each document to transform text into consecutive word tokens, where words are in their most normalised forms, and some words are removed if they have no distinctive meaning when used out of context BIBREF5 , BIBREF6 . We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. This training step is only done once. This Doc2Vec model is subsequently used to infer high-dimensional vector descriptions for the text of each of the 3229 documents in our target analysis set. We then compute a matrix containing pairwise similarities between any pair of document vectors, as inferred with Doc2Vec. This matrix can be thought of as a full, weighted graph with documents as nodes and edges weighted by their similarity. We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF8 , a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The derived MST-kNN graph is analysed with Markov Stability BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , a multi-resolution dynamics-based graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need for choosing a priori the number of clusters, scale or organisation. To analyse a posteriori the different partitions across levels of resolution, we use both visualisations and quantitative scores. The visualisations include word clouds to summarise the main content, graph layouts, as well as Sankey diagrams and contingency tables that capture the correspondences across levels of resolution and relationships to the hand-coded classifications. The partitions are also evaluated quantitatively to score: (i) their intrinsic topic coherence (using pairwise mutual information BIBREF13 , BIBREF14 ), and (ii) their similarity to the operator hand-coded categories (using normalised mutual information BIBREF15 ). We now expand on the steps of the computational framework.", "selected": "Figure 1 shows a summary of our pipeline. First, we pre-process each document to transform text into consecutive word tokens, where words are in their most normalised forms, and some words are removed if they have no distinctive meaning when used out of context BIBREF5 , BIBREF6 .", "paper_id": "1811.05711"}], "9YnrHRfBRJ": [{"section": "Text Preprocessing", "paragraph": "Text preprocessing is important to enhance the performance of text embedding. We applied standard preprocessing techniques in natural language processing to the raw text of all 13 million records in our corpus. We normalise words into a single form and remove words that do not carry significant meaning. Specifically, we divide our documents into iterative word tokens using the NLTK library BIBREF5 and remove punctuation and digit-only tokens. We then apply word stemming using the Porter algorithm BIBREF6 , BIBREF16 . If the Porter method cannot find a stemmed version for a token, we apply the Snowball algorithm BIBREF17 . Finally, we remove any stop-words (repeat words with low content) using NLTK's stop-word list. Although some of the syntactic information is reduced due to text preprocessing, this process preserves and consolidates the semantic information of the vocabulary, which is of relevance to our study.", "selected": "Text preprocessing is important to enhance the performance of text embedding. We applied standard preprocessing techniques in natural language processing to the raw text of all 13 million records in our corpus. We normalise words into a single form and remove words that do not carry significant meaning. Specifically, we divide our documents into iterative word tokens using the NLTK library BIBREF5 and remove punctuation and digit-only tokens. We then apply word stemming using the Porter algorithm BIBREF6 , BIBREF16 . If the Porter method cannot find a stemmed version for a token, we apply the Snowball algorithm BIBREF17 . Finally, we remove any stop-words (repeat words with low content) using NLTK's stop-word list. Although some of the syntactic information is reduced due to text preprocessing, this process preserves and consolidates the semantic information of the vocabulary, which is of relevance to our study.", "paper_id": "1811.05711"}]}}
{"idx": "181932", "paper_id": "8241948", "title": "Fast Recursive Multi-class Classification of Pairs of Text Entities for Biomedical Event Extraction", "abstract": "Extracting biomedical events from scientific articles to automatically update dedicated knowledge bases has become a popular research topic with important applications. Most existing approaches are either pipeline models of specific classifiers, usually subject to cascading errors, or joint structured models, more efficient but also more costly and complicated to train. This paper proposes a system based on a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. Such pairs are recursively provided to the classifier, allowing to extract events involving other events as arguments. This model facilitates inference compared to joint models while % relying on a single main classifier compared to being more direct and efficient than usual pipeline approaches. This method yields the best results reported so far on the BioNLP 2011 and 2013 Genia tasks.", "context_section_header": "", "context_paragraph": "Biomedical event extraction is attracting more and more attention, especially thanks to the organization of recurrent dedicated BioNLP challenges (Kim et al., 2009;Kim et al., 2011b;Kim et al., 2013). We propose here a new approach which relies on a single multi-class classifier for recursively detecting events from (trigger, argument) pairs. Compared to standard pipeline approaches based on sequences of classifiers (Bj\u00f6rne and Salakoski, 2013;Hakala et al., 2013), we avoid the intermediate problem of associating isolated triggers to event types, relying on a tricky multi-label classification problem. Instead, we directly extract compounds of events in the form of (trigger, argument) pairs, simply relying on a multi-class problem, whereby (trigger, argument) pairs are associated to event types. Considering pairs of words also allows us to characterize examples by sophisticated joint features such as shortest path in the dependency parse tree, and hence to achieve much accurate trigger detection than pipeline models. Besides, compared to Markov random fields (Riedel and McCallum, 2011a), our discriminant model does not represent the full joint distribution of words and events. We thus have a simpler inference process, which results into drastically reduced training times (15  Figure 1: Part of a sentence and corresponding extracted events for the BioNLP 2013 Genia task.", "sentence": "Besides, compared to Markov random fields (Riedel and McCallum, 2011a), our discriminant model does not represent the full joint distribution of words and events.", "cited_ids": [{"paper_id": "18198203", "citation": "(Riedel and McCallum, 2011a"}], "y": "Compared to Markov random fields (Riedel and McCallum, 2011a), the authors [proposed a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. This] discriminant model does not represent the full joint distribution of words and events.", "snippet_surface": "Additionally, compared to Markov random fields (Riedel and McCallum, 2011a), the authors' discriminant model does not represent the full joint distribution of words and events.", "questions": {"4T9JAsYA/w": "What is the authors' model?"}, "answers": {"4T9JAsYA/w": "The authors propose a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities."}, "evidence": {"4T9JAsYA/w": [{"section": "Abstract", "paragraph": "Extracting biomedical events from scientific articles to automatically update dedicated knowledge bases has become a popular research topic with important applications. Most existing approaches are either pipeline models of specific classifiers, usually subject to cascading errors, or joint structured models, more efficient but also more costly and complicated to train. This paper proposes a system based on a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. Such pairs are recursively provided to the classifier, allowing to extract events involving other events as arguments. This model facilitates inference compared to joint models while % relying on a single main classifier compared to being more direct and efficient than usual pipeline approaches. This method yields the best results reported so far on the BioNLP 2011 and 2013 Genia tasks.", "selected": "This paper proposes a system based on a pairwise model that transforms event extraction into a simple multi-class problem of classifying pairs of text entities. Such pairs are recursively provided to the classifier, allowing to extract events involving other events as arguments. This model facilitates inference compared to joint models while % relying on a single main classifier compared to being more direct and efficient than usual pipeline approaches.", "paper_id": "8241948"}]}}
{"idx": "183543", "paper_id": "2281724", "title": "Combining Heterogeneous Models for Measuring Relational Similarity", "abstract": "In this work, we study the problem of measuring relational similarity between two word pairs (e.g., silverware:fork and clothing:shirt). Due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources. Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models. When evaluated in the setting of a recently proposed SemEval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman\u2019s rank correlation.", "context_section_header": "", "context_paragraph": "In this paper, we explore the problem of measuring relational similarity in the same task setting. We argue that due to the large number of possible relations, building an ensemble of relational simi-larity models based on heterogeneous information sources is the key to advance the state-of-the-art on this problem. By combining two general-purpose relational similarity models with three specific wordrelation models covering relations like IsA and synonymy/antonymy, we improve the previous stateof-the-art substantially -having a relative gain of 54.1% in Spearman's rank correlation and 14.7% in the MaxDiff accuracy! Our main contributions are threefold. First, we propose a novel directional similarity method based on the vector representation of words learned from a recurrent neural network language model. The relation of two words is captured by their vector offset in the latent semantic space. Similarity of relations can then be naturally measured by a distance function in the vector space. This method alone already performs better than all existing systems. Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well. Third, we demonstrate that by augmenting existing word-relation models, which cover only a small number of relations, the overall system can be further improved.", "sentence": "Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well.", "cited_ids": [{"paper_id": "15637201", "citation": "(Rink and Harabagiu, 2012)"}], "y": "Unlike the previous finding, where SVMs learn a much poorer model than naive Bayes  (Rink and Harabagiu, 2012), the authors show that using a highly regularized log-linear model [for assessing relational similarity] on simple contextual pattern features collected from a document collection of 20 GB,  a discriminative approach can learn a strong model as well.", "snippet_surface": "Secondly, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), the authors show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well.", "questions": {"0ZvBhVdISx": "What log-linear model is being referred to?"}, "answers": {"0ZvBhVdISx": "A model for assessing relational similarity."}, "evidence": {"0ZvBhVdISx": [{"section": "Introduction", "paragraph": "In this paper, we explore the problem of measuring relational similarity in the same task setting. We argue that due to the large number of possible relations, building an ensemble of relational simi-larity models based on heterogeneous information sources is the key to advance the state-of-the-art on this problem. By combining two general-purpose relational similarity models with three specific wordrelation models covering relations like IsA and synonymy/antonymy, we improve the previous stateof-the-art substantially -having a relative gain of 54.1% in Spearman's rank correlation and 14.7% in the MaxDiff accuracy! Our main contributions are threefold. First, we propose a novel directional similarity method based on the vector representation of words learned from a recurrent neural network language model. The relation of two words is captured by their vector offset in the latent semantic space. Similarity of relations can then be naturally measured by a distance function in the vector space. This method alone already performs better than all existing systems. Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well. Third, we demonstrate that by augmenting existing word-relation models, which cover only a small number of relations, the overall system can be further improved.", "selected": "we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well.", "paper_id": "2281724"}, {"section": "Models for Relational Similarity", "paragraph": "We investigate three types of models for relational similarity. Operating in a word vector space, the directional similarity model compares the vector differences of target and prototypical word pairs to estimate their relational similarity. The lexical pattern method collects contextual information of pairs of words when they co-occur in large corpora, and learns a highly regularized log-linear model. Finally, the word relation models incorporate existing, specific word relation measures for general relational similarity.", "selected": "he lexical pattern method collects contextual information of pairs of words when they co-occur in large corpora, and learns a highly regularized log-linear model.", "paper_id": "2281724"}]}}
{"idx": "183813", "paper_id": "10910870", "title": "Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora", "abstract": "We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available.", "context_section_header": "", "context_paragraph": "2 In (Deng et al., 2006), p. 5, the p(a k ) = p(x, y) which determines the prior probability of having an alignment containing x source and y target sentences, is equal to 0 if x < 1 or y < 1. As p(a k ) is a multiplicative factor of the model, the probability of having an insertion or a deletion is always equal to 0. finds a model-optimal alignment composed of the smallest possible correspondences, namely 1-to-0/0-to-1 and 1-to-1, and then merges those correspondences into larger alignments. This allows the finding of 1-to-0/0-to-1 alignments as well as high quality 1-to-many/many-to-1 alignments, leading to high accuracy on parallel texts but also on corpora containing large blocs of inserted or deleted text. Furthermore, our approach keeps the computational costs of the alignment procedure low: our aligner is, on average, about 550 times faster than our implementation 3 of (Deng et al., 2006).", "sentence": "Furthermore, our approach keeps the computational costs of the alignment procedure low: our aligner is, on average, about 550 times faster than our implementation 3 of (Deng et al., 2006).", "cited_ids": [{"paper_id": "11879506", "citation": "(Deng et al., 2006)"}], "y": "The authorss' approach [use a two-step clustering approach for sentence alignment] that keeps the computational costs of the alignment procedure [finding a large amount of sentences with a dynamic programming search] low: their aligner is, on average, about 550 times faster than their implementation 3 of (Deng et al., 2006).", "snippet_surface": "Furthermore, the authors' approach keeps the computational costs of the alignment procedure low: their aligner is, on average, about 550 times faster than their implementation 3 of (Deng et al., 2006).", "questions": {"TcYeL2nk5F": "What is the \"alignment procedure\"?", "6gKwRw0I/Q": "What is the authors' approach?"}, "answers": {"TcYeL2nk5F": "Alignement procedure is a dynamic programming search that finds large amount of matching sentences", "6gKwRw0I/Q": "A two-step clustering approach for unsupervised, language-pair independent sentence alignment."}, "evidence": {"TcYeL2nk5F": [{"section": "Related Work", "paragraph": "Among approaches that are unsupervised and language independent, (Brown et al., 1991) and (Gale and Church, 1993) use sentence-length statistics in order to model the relationship between groups of sentences that are translations of each other. As shown in (Chen, 1993) the accuracy of sentencelength based methods decreases drastically when aligning texts containing small deletions or free translations. In contrast, our approach augments a sentence-length based model with lexical statistics and hence constantly provides high quality alignments. (Moore, 2002) proposes a multi-pass search procedure where sentence-length based statistics are used in order to extract the training data for the IBM Model-1 translation tables. The acquired lexical statistics are then combined with the sentence-length based model in order to extract 1-to-1 correspondences with high accuracy 1 . Moore's approach constantly achieves high precision, is robust to sequences of inserted and deleted text, and is fast. However, the obtained recall is at most equal to the proportion of 1-to-1 correspondences contained in the parallel text to align. This point is especially problematic when aligning asymmetrical parallel corpora. In contrast, our approach allows to extract 1-to-many/manyto-1 correspondences. Hence, we achieve high accuracy in terms of precision and recall on both symmetrical and asymmetrical documents. Moreover, because we use, in the last pass of our multipass method, a novel two-stage search procedure, our aligner also requires acceptably low computational resources. (Deng et al., 2006) have developed a multipass method similar to (Moore, 2002) but where the last pass is composed of two alignment procedures: a standard dynamic programming (DP) search that allows one to find many-to-many alignments containing a large amount of sentences in each language and a divisive clustering algorithm that optimally refines those alignments through iterative binary splitting. This alignment method allows one to find, in addition to 1-to-1 correspondences, high quality 1-to-many/manyto-1 alignments. However, 1-to-0 and 0-to-1 correspondences are not modeled in this approach 2 . This leads to poor performance on parallel texts containing that type of correspondence. Furthermore performing an exhaustive DP search in order to find large size many-to-many alignments involves high computational costs. In comparison to (Deng et al., 2006), our approach works in the opposite way. Our two-step search procedure first 1 The used search heuristic is a forward-backward computation with a pruned dynamic programming procedure as the forward pass.", "selected": "where the last pass is composed of two alignment procedures: a standard dynamic programming (DP) search that allows one to find many-to-many alignments containing a large amount of sentences in each language and a divisive clustering algorithm that optimally refines those alignments through iterative binary splitting.", "paper_id": "10910870"}], "6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora. Asymmetrical parallel corpora contain a large proportion of 1-to-0/0-to-1 and 1-to-many/many-to-1 sentence correspondences. We have developed a novel approach which is fast and allows us to achieve high accuracy in terms of F1 for the alignment of both asymmetrical and symmetrical parallel corpora. The source code of our aligner and the test sets are freely available.", "selected": "We address the problem of unsupervised and language-pair independent alignment of symmetrical and asymmetrical parallel corpora.", "paper_id": "10910870"}, {"section": "Two-Step Clustering Approach", "paragraph": "We present here our two-step clustering approach to sentence alignment 4 which is the main contribution of this paper. We begin by giving the main ideas of our approach using an introductory example (section 3.1). Then we show to which extent computational costs are reduced in comparison to a standard DP search (section 3.2) before presenting the theoretical background of our approach (section 3.3). We further discuss a novel pruning strategy used within our approach (section 3.4). This pruning technique is another important contribution of this paper. Next, we present the alignment model (section 3.5) which is a slightly modified version of the alignment model used in (Moore, 2002). Finally, we describe the overall procedure required to align a parallel text with our method (section 3.6).", "selected": "We present here our two-step clustering approach to sentence alignment 4 which is the main contribution of this paper.", "paper_id": "10910870"}]}}
{"idx": "184958", "paper_id": "16919810", "title": "One Tree is not Enough: Cross-lingual Accumulative Structure Transfer for Semantic Indeterminacy", "abstract": "We address the task of parsing semantically indeterminate expressions, for which several correct structures exist that do not lead to differences in meaning. We present a novel non-deterministic structure transfer method that accumulates all structural information based on cross-lingual word distance derived from parallel corpora. Our system\u2019s output is a ranked list of trees. To evaluate our system, we adopted common IR metrics. We show that our system outperforms previous cross-lingual structure transfer methods significantly. In addition, we illustrate that tree accumulation can be used to combine partial evidence across languages to form a single structure, thereby making use of sparse parallel data in an optimal way.", "context_section_header": "", "context_paragraph": "Our system is inspired by Ziering and Van der Plas (2015), who exploit cross-lingual surface variation for bracketing 3NCs. There are various ways of translating English noun compounds. Germanic languages such as Swedish frequently use closed compounds (i.e., single nouns), whereas Romance languages such as French use open compounds (i.e., lexemes composed of several words). Paraphrased translations (e.g., human rights abuse aligned to the partially closed German Verletzung der Menschenrechte (abuse of human rights)) can reveal the internal structure of a compound. While Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, we gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "sentence": "While Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, we gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "cited_ids": [{"paper_id": "10070164", "citation": "Ziering and Van der Plas (2015)"}], "y": "Ziering and Van der Plas (2015) follow the deterministic take [(i.e., the notion that syntactic structures are usually understood that for every structure there exists conditions that can have no other structure)] by producing a single tree output, whereas the authors gather all structural information and produce a ranked list of plausible [system outputs], where similarly-ranked [system outputs] indicate semantic indeterminacy.", "snippet_surface": "Ziering and Van der Plas (2015) follow the deterministic take by producing a single tree output, whereas the authors gather all structural information and produce a ranked list of plausible trees, where similarly-ranked trees indicate semantic indeterminacy.", "questions": {"jNA/5Inlpx": "What does \"the deterministic take\" stand for?"}, "answers": {"jNA/5Inlpx": "The notion that syntactic structures are usually understood that for every structure there exists conditions that can have no other structure"}, "evidence": {"jNA/5Inlpx": [{"section": "Introduction", "paragraph": "In addition to previous work focused on disambiguation, we show that multilingual data can be used to point to semantic indeterminacy. Syntactic structures are usually understood deterministically in that for every structure there exists conditions that can have no other structure. However, previous work in NLP shows that such a deterministic take might not always be suitable. Hindle and Rooth (1993) were the first to discuss the phenomenon of semantic indeterminacy in PP attachment, e.g., in the sentence They mined the roads along the coast, the PP along the coast may be attached to both the verb or the object without changing the meaning. On the NP level, Lauer (1995) observed 12.54% semantically indeterminate three-noun compounds (3NCs) in his dataset, e.g., in 'Most advanced aircraft have precision navigation systems', both precision navigation and navigation system can be bracketed leading to the same meaning. We found more striking evidence from parallel corpora, where the multiple translations found for a given NP reflect large differences in structure. While tobacco advertising ban is translated to German as Werbeverbot f\u00fcr Tabakerzeugnisse (advertising ban for tobacco products), the Danish equivalent is forbuddet mod tobaksreklamer (ban of tobacco advertising). Similarly, animal welfare standards is once translated to Dutch as normen op het gebied van dierenwelzijn (standards in the field of animal welfare) and to German as Wohlfahrtsstandards f\u00fcr Tiere (welfare standards for animals).", "selected": "Syntactic structures are usually understood deterministically in that for every structure there exists conditions that can have no other structure", "paper_id": "16919810"}]}}
{"idx": "188418", "paper_id": "1397387", "title": "Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data", "abstract": "We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets.", "context_section_header": "", "context_paragraph": "Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains.", "sentence": "When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points.", "cited_ids": [{"paper_id": "1916754", "citation": "(Koo et al., 2008)"}], "y": "When comparing to the strong baseline of using Brown-clusters based features [created using a maximum likelihood clustering model] (Koo et al., 2008), we find that our triplets-based method [which involves looking at dependencies between sets of 3 words] outperform them by over 0.27 UAS [unlabeled attachment score] points.", "snippet_surface": "When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), the authors find that their triplets-based method outperforms them by over 0.27 UAS points.", "questions": {"80booZ60Uw": "What are brown-cluster based features?", "YCKQOgbG5+": "What is triplets-based method?", "XnjYPjYHnD": "What does uas stand for?"}, "answers": {"80booZ60Uw": "There are baseline and cluster-based features. They are created using a maximum liklihood model that merges clusters that increase the likelihood of the text the most, which results in a hierarchical structure of merges.", "YCKQOgbG5+": "The triplets based method involves looking at dependencies between sets of 3 words", "XnjYPjYHnD": "Unlabeled attachment score"}, "evidence": {"80booZ60Uw": [{"section": "Brown clustering algorithm", "paragraph": "In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004;Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters. By tracing the pairwise merge operations, one obtains a hierarchical clustering of the words, which can be represented as a binary tree as in Figure 2.", "selected": "The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters. By tracing the pairwise merge operations, one obtains a hierarchical clustering of the words, which can be represented as a binary tree as in Figure 2.", "paper_id": "1916754"}, {"section": "Brown clustering algorithm", "paragraph": "In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004;Liang, 2005). However, we expect that our approach can function with other clustering algorithms (as in, e.g., Li and McCallum (2005)). We briefly describe the Brown algorithm below. The input to the algorithm is a vocabulary of words to be clustered and a corpus of text containing these words. Initially, each word in the vocabulary is considered to be in its own distinct cluster. The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters. By tracing the pairwise merge operations, one obtains a hierarchical clustering of the words, which can be represented as a binary tree as in Figure 2.", "selected": "In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992). W", "paper_id": "1916754"}, {"section": "Brown clustering algorithm", "paragraph": "Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2. In order to obtain a clustering of the words, we select all nodes at a certain depth from the root of the hierarchy. For example, in Figure 2 we might select the four nodes at depth 2 from the root, yielding the clusters {apple,pear}, {Apple,IBM}, {bought,run}, and {of,in}. Note that the same clustering can be obtained by truncating each word's bit-string to a 2-bit prefix. By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al., 2004).", "selected": "Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2. In order to obtain a clustering of the words, we select all nodes at a certain depth from the root of the hierarchy", "paper_id": "1916754"}, {"section": "Baseline features", "paragraph": "Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens. 1 Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions. Examples of baseline features are provided in Table 1. 1 We augment the McDonald et al. (2005a) feature set with backed-off versions of the \"Surrounding Word POS Features\" that include only one neighboring POS tag. We also add binned distance features which indicate whether the number of tokens between the head and modifier of a dependency is greater than 2, 5, 10, 20, 30, or 40 tokens.", "selected": "Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions.", "paper_id": "1916754"}, {"section": "Baseline features", "paragraph": "Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens. 1 Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions. Examples of baseline features are provided in Table 1. 1 We augment the McDonald et al. (2005a) feature set with backed-off versions of the \"Surrounding Word POS Features\" that include only one neighboring POS tag. We also add binned distance features which indicate whether the number of tokens between the head and modifier of a dependency is greater than 2, 5, 10, 20, 30, or 40 tokens.", "selected": "Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens.", "paper_id": "1916754"}], "YCKQOgbG5+": [{"section": "Introduction", "paragraph": "We are concerned with semi-supervised dependency parsing, namely how to leverage large amounts of unannotated data, in addition to annotated Treebank data, to improve dependency parsing accuracy. Our method (Section 2) is based on parsing large amounts of unannotated text using a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features. The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank. Specifically, we attempt to infer a function assoc(head, modif er) measuring the \"goodness\" of head-modifier relations (\"how good is an arc in which black is a modifier of jump\"). A similar approach was taken by Chen et al. (2009) and Van Noord et al. (2007). We depart from their work by extending the scoring to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a;Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3).", "selected": "That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over)", "paper_id": "1397387"}, {"section": "Introduction", "paragraph": "We are concerned with semi-supervised dependency parsing, namely how to leverage large amounts of unannotated data, in addition to annotated Treebank data, to improve dependency parsing accuracy. Our method (Section 2) is based on parsing large amounts of unannotated text using a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features. The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank. Specifically, we attempt to infer a function assoc(head, modif er) measuring the \"goodness\" of head-modifier relations (\"how good is an arc in which black is a modifier of jump\"). A similar approach was taken by Chen et al. (2009) and Van Noord et al. (2007). We depart from their work by extending the scoring to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a;Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3).", "selected": "Learning a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs", "paper_id": "1397387"}], "XnjYPjYHnD": [{"section": "Experiments and Results", "paragraph": "For auto-parsed data, we parse the text of the BLLIP corpus (Charniak, 2000) using our baseline parser. This is the same corpus used for deriving Brown clusters for use as features in (Koo et al., 2008). We use the clusters provided by Terry Koo 6 . Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graphbased parsers in terms of running time in realistic parsing scenarios. Our base model is a reimplementation of a first-order projective Graphbased parser (McDonald et al., 2005), which we extend to support the semi-supervised \u03c6 lex features. The parser is trained for 10 iterations of online-training with passive-aggressive updates (Crammer et al., 2006). For the Brown-cluster features, we use the feature templates described by (Koo et al., 2008;Bansal et al., 2014).", "selected": "Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations.", "paper_id": "1397387"}]}}
{"idx": "188615", "paper_id": "51976920", "title": "Rapid Adaptation of Neural Machine Translation to New Languages", "abstract": "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \u201cseed models\u201d, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \u201csimilar-language regularization\u201d, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.", "context_section_header": "", "context_paragraph": "While adapting MT systems to new languages is a long-standing challenge (Schultz and Black, 2006;Jabaian et al., 2013), multilingual NMT is highly promising in its ability to abstract across language boundaries (Firat et al., 2016;Ha et al., 2016;Johnson et al., 2016). Results on multilingual training for low-resource translation (Gu et al., 2018; further demonstrates this potential, although these works do not consider adaptation to new languages, the main focus of our work. Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined. Finally, unsupervised NMT approaches (Artetxe et al., 2017;Lample et al., 2018Lample et al., , 2017 require no parallel data, but rest on strong assumptions about high-quality comparable monolingual data. As we show, when this assumption breaks down these methods fail to function, while our cold-start methods achieve non-trivial accuracies even with no monolingual data.", "sentence": "Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined.", "cited_ids": [{"paper_id": "16631020", "citation": "(Zoph et al., 2016)"}], "y": "The authors did not examine partial freezing of parameters, a method that has proven useful for cross-lingual adaptation by Zoph et al. (2016); this is orthogonal to their multi-lingual training approach but the two methods [partial freezing of parameters and multilingual training] could potentially be combined.", "snippet_surface": "Notably, the authors did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to their multi-lingual training approach but the two methods could potentially be combined.", "questions": {"wom8hV51ZK": "What do the two methods refer to?", "HkGw55/3v0": "What is \"their multi-lingual training approach\"?"}, "answers": {"wom8hV51ZK": "they refer to \"partial freezing of parameters\" and \"multilingual training approach\".", "HkGw55/3v0": "It is the author's proposed process of training a seed model on a large number of languages."}, "evidence": {"wom8hV51ZK": [{"section": "Related Work", "paragraph": "While adapting MT systems to new languages is a long-standing challenge (Schultz and Black, 2006;Jabaian et al., 2013), multilingual NMT is highly promising in its ability to abstract across language boundaries (Firat et al., 2016;Ha et al., 2016;Johnson et al., 2016). Results on multilingual training for low-resource translation (Gu et al., 2018; further demonstrates this potential, although these works do not consider adaptation to new languages, the main focus of our work. Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined. Finally, unsupervised NMT approaches (Artetxe et al., 2017;Lample et al., 2018Lample et al., , 2017 require no parallel data, but rest on strong assumptions about high-quality comparable monolingual data. As we show, when this assumption breaks down these methods fail to function, while our cold-start methods achieve non-trivial accuracies even with no monolingual data.", "selected": "Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation (Zoph et al., 2016); this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined.", "paper_id": "51976920"}], "HkGw55/3v0": [{"section": "Abstract", "paragraph": "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \u201cseed models\u201d, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \u201csimilar-language regularization\u201d, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.", "selected": "We propose methods based on starting with massively multilingual \u201cseed models\u201d, which can be trained ahead-of-time, and then continuing training on data related to the LRL.", "paper_id": "51976920"}, {"section": "Introduction", "paragraph": "To examine this question we propose NMT methods at the intersection of cross-lingual transfer learning (Zoph et al., 2016) and multilingual training (Johnson et al., 2016), two paradigms that, to our knowledge, have not been used together in previous work. Our methods, laid out in \u00a72 follow the process of training a seed model on a large number of languages, then fine-tuning the model to improve its performance on the language of interest. We propose a novel method of similar-language regularization (SLR) where training data from a second similar languages is used to help prevent over-fitting to the small LRL dataset.", "selected": "We propose a novel method of similar-language regularization (SLR) where training data from a second similar languages is used to help prevent over-fitting to the small LRL dataset.", "paper_id": "51976920"}, {"section": "Introduction", "paragraph": "To examine this question we propose NMT methods at the intersection of cross-lingual transfer learning (Zoph et al., 2016) and multilingual training (Johnson et al., 2016), two paradigms that, to our knowledge, have not been used together in previous work. Our methods, laid out in \u00a72 follow the process of training a seed model on a large number of languages, then fine-tuning the model to improve its performance on the language of interest. We propose a novel method of similar-language regularization (SLR) where training data from a second similar languages is used to help prevent over-fitting to the small LRL dataset.", "selected": "Our methods, laid out in \u00a72 follow the process of training a seed model on a large number of languages, then fine-tuning the model to improve its performance on the language of interest.", "paper_id": "51976920"}]}}
{"idx": "1902.09666.1.2.2", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Experiments and Evaluation", "context_paragraph": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "sentence": "Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "cited_ids": [], "y": "The authors also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs [(FastText embeddings and updatable embeddings learned by the model during training)] as the above BiLSTM [(Bidirectional Long Short-Term-Memory)].", "snippet_surface": "The authors also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", "questions": {"9g9vHprHTH": "What does the \"multi-channel inputs\" refer to?", "uHMPMMG33f": "What does bilstm stand for?"}, "answers": {"9g9vHprHTH": "It refers to two input channels: FastText embeddings and updatable embeddings learned by the model during training", "uHMPMMG33f": "BILSTM refers to bidirectional Long Short-Term-Memory"}, "evidence": {"9g9vHprHTH": [{"section": "Experiments and Evaluation", "paragraph": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "selected": "BiLSTM", "paper_id": "1902.09666"}, {"section": "Categorization of Offensive Language", "paragraph": "The CNN system achieved higher performance in this experiment compared to the BiLSTM, with a macro-F1 score of 0.69. All systems performed better at identifying target and threats (TIN) than untargeted offenses (UNT).", "selected": "BiLSTM", "paper_id": "1902.09666"}, {"section": "Experiments and Evaluation", "paragraph": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "selected": "We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training", "paper_id": "1902.09666"}], "uHMPMMG33f": [{"section": "Experiments and Evaluation", "paragraph": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM.", "selected": "bidirectional Long Short-Term-Memory", "paper_id": "1902.09666"}]}}
{"idx": "1902.09666.5.3.1", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Hierarchically Modelling Offensive Content", "context_paragraph": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10 .", "sentence": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.", "cited_ids": [], "y": "The authors use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language in the OLID dataset, [which contains tweets that have been categorised and annotated according to whether their content is offensive, the type of offensive language, and who the targets of the offensive language are.].", "snippet_surface": "The authors use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language in the OLID dataset.", "questions": {"xRjMk/l5q6": "What does \"the olid dataset\" consist of?"}, "answers": {"xRjMk/l5q6": "The OLID dataset consists of tweets that have been categorised and annotated according to whether their content is offensive, the type of offensive language, and who the targets of the offensive language are."}, "evidence": {"xRjMk/l5q6": [{"section": "Abstract", "paragraph": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "selected": "Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available", "paper_id": "1902.09666"}, {"section": "Hierarchically Modelling Offensive Content", "paragraph": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10 .", "selected": "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language", "paper_id": "1902.09666"}, {"section": "Data Collection", "paragraph": "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'. We carried out a first round of trial annotation of 300 instances with six experts. The goal of the trial annotation was to 1) evaluate the proposed tagset; 2) evaluate the data retrieval method; and 3) create a gold standard with instances that could be used as test questions in the training and test setting annotation which was carried out using crowdsourcing. The breakdown of keywords and their offensive content in the trial data of 300 tweets is shown in Table TABREF14 . We included a left (@NewYorker) and far-right (@BreitBartNews) news accounts because there tends to be political offense in the comments. One of the best offensive keywords was tweets that were flagged as not being safe by the Twitter `safe' filter (the `-' indicates `not safe'). The vast majority of content on Twitter is not offensive so we tried different strategies to keep a reasonable number of tweets in the offensive class amounting to around 30% of the dataset including excluding some keywords that were not high in offensive content such as `they are` and `to:NewYorker`. Although `he is' is lower in offensive content we kept it as a keyword to avoid gender bias. In addition to the keywords in the trial set, we searched for more political keywords which tend to be higher in offensive content, and sampled our dataset such that 50% of the the tweets come from political keywords and 50% come from non-political keywords. In addition to the keywords `gun control', and `to:BreitbartNews', political keywords used to collect these tweets are `MAGA', `antifa', `conservative' and `liberal'. We computed Fliess' INLINEFORM0 on the trial set for the five annotators on 21 of the tweets. INLINEFORM1 is .83 for Layer A (OFF vs NOT) indicating high agreement. As to normalization and anonymization, no user metadata or Twitter IDs have been stored, and URLs and Twitter mentions have been substituted to placeholders. We follow prior work in related areas (burnap2015cyber,davidson2017automated) and annotate our data using crowdsourcing using the platform Figure Eight. We ensure data quality by: 1) we only received annotations from individuals who were experienced in the platform; and 2) we used test questions to discard annotations of individuals who did not reach a certain threshold. Each instance in the dataset was annotated by multiple annotators and inter-annotator agreement has been calculated. We first acquired two annotations for each instance. In case of 100% agreement, we considered these as acceptable annotations, and in case of disagreement, we requested more annotations until the agreement was above 66%. After the crowdsourcing annotation, we used expert adjudication to guarantee the quality of the annotation. The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .", "selected": "The data included in OLID has been collected from Twitter.", "paper_id": "1902.09666"}]}}
{"idx": "1902.09666.5.3.3", "paper_id": "1902.09666", "title": "Predicting the Type and Target of Offensive Posts in Social Media", "abstract": "As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.", "context_section_header": "Level B: Categorization of Offensive Language", "context_paragraph": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "sentence": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "cited_ids": [], "y": "[To predict the type and target of offensive posts in social media, the authors use a trained system on pre-tagged tweets to discriminate between insults/threats and untargeted offensive tweets (e.g., swearing). The authors categorize] Level B to include the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "snippet_surface": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.", "questions": {"IIjHISOCEf": "How does level b accomplish this task?"}, "answers": {"IIjHISOCEf": "The systems were trained on pre-tagged tweets to discriminate between insults/threats and untargeted offensive tweets (e.g., swearing)"}, "evidence": {"IIjHISOCEf": [{"section": "Level B: Categorization of Offensive Language", "paragraph": "Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others (see next layer);", "selected": "Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others (see next layer);", "paper_id": "1902.09666"}, {"section": "Level B: Categorization of Offensive Language", "paragraph": "Untargeted (UNT): Posts containing non-targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language.", "selected": "Untargeted (UNT): Posts containing non-targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language.", "paper_id": "1902.09666"}, {"section": "Data Collection", "paragraph": "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'. We carried out a first round of trial annotation of 300 instances with six experts. The goal of the trial annotation was to 1) evaluate the proposed tagset; 2) evaluate the data retrieval method; and 3) create a gold standard with instances that could be used as test questions in the training and test setting annotation which was carried out using crowdsourcing. The breakdown of keywords and their offensive content in the trial data of 300 tweets is shown in Table TABREF14 . We included a left (@NewYorker) and far-right (@BreitBartNews) news accounts because there tends to be political offense in the comments. One of the best offensive keywords was tweets that were flagged as not being safe by the Twitter `safe' filter (the `-' indicates `not safe'). The vast majority of content on Twitter is not offensive so we tried different strategies to keep a reasonable number of tweets in the offensive class amounting to around 30% of the dataset including excluding some keywords that were not high in offensive content such as `they are` and `to:NewYorker`. Although `he is' is lower in offensive content we kept it as a keyword to avoid gender bias. In addition to the keywords in the trial set, we searched for more political keywords which tend to be higher in offensive content, and sampled our dataset such that 50% of the the tweets come from political keywords and 50% come from non-political keywords. In addition to the keywords `gun control', and `to:BreitbartNews', political keywords used to collect these tweets are `MAGA', `antifa', `conservative' and `liberal'. We computed Fliess' INLINEFORM0 on the trial set for the five annotators on 21 of the tweets. INLINEFORM1 is .83 for Layer A (OFF vs NOT) indicating high agreement. As to normalization and anonymization, no user metadata or Twitter IDs have been stored, and URLs and Twitter mentions have been substituted to placeholders. We follow prior work in related areas (burnap2015cyber,davidson2017automated) and annotate our data using crowdsourcing using the platform Figure Eight. We ensure data quality by: 1) we only received annotations from individuals who were experienced in the platform; and 2) we used test questions to discard annotations of individuals who did not reach a certain threshold. Each instance in the dataset was annotated by multiple annotators and inter-annotator agreement has been calculated. We first acquired two annotations for each instance. In case of 100% agreement, we considered these as acceptable annotations, and in case of disagreement, we requested more annotations until the agreement was above 66%. After the crowdsourcing annotation, we used expert adjudication to guarantee the quality of the annotation. The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .", "selected": "One of the best offensive keywords was tweets that were flagged as not being safe by the Twitter `safe' filter (the `-' indicates `not safe').", "paper_id": "1902.09666"}, {"section": "Categorization of Offensive Language", "paragraph": "In this experiment, the two systems were trained to discriminate between insults and threats (TIN) and untargeted (UNT) offenses, which generally refer to profanity. The results are shown in Table TABREF19 .", "selected": "In this experiment, the two systems were trained to discriminate between insults and threats (TIN) and untargeted (UNT) offenses, which generally refer to profanity. The results are shown in Table TABREF19 .", "paper_id": "1902.09666"}]}}
{"idx": "1904.01548.2.2.1", "paper_id": "1904.01548", "title": "Understanding language-elicited EEG data by predicting it from a fine-tuned language model", "abstract": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.", "context_section_header": "Related Work", "context_paragraph": "This work is most closely related to the paper from which we get the ERP data: BIBREF0 . In that work, the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals we consider here. The authors do not directly train a model to predict ERPs. Instead, models of the probability distribution of each word in context are used to compute a surprisal for each word, which is input into a mixed effects regression along with word frequency, word length, word position in the sentence, and sentence position in the experiment. The effect of the surprisal is assessed using a likelihood-ratio test. In BIBREF7 , the authors take an approach similar to BIBREF0 . The authors compare the explanatory power of surprisal (as computed by an LSTM or a Recurrent Neural Network Grammar (RNNG) language model) to a measure of syntactic complexity they call \u201cdistance\" that counts the number of parser actions in the RNNG language model. The authors find that surprisal (as predicted by the RNNG) and distance are both significant factors in a mixed effects regression which predicts the P600, while the surprisal as computed by an LSTM is not. Unlike BIBREF0 and BIBREF7 , we do not use a linking function (e.g. surprisal) to relate a language model to ERPs. We thus lose the interpretability provided by the linking function, but we are able to predict a significant proportion of the variance for all of the ERP components, where prior work could not. We interpret our results through characterization of the ERPs in terms of how they relate to each other and to eye-tracking data rather than through a linking function. The authors in BIBREF8 also use a recurrent neural network to predict neural activity directly. In that work the authors predict magnetoencephalography (MEG) activity, a close cousin to EEG, recorded while participants read a chapter of Harry Potter and the Sorcerer\u2019s Stone BIBREF9 . Their approach to characterization of processing at each MEG sensor location is to determine whether it is best predicted by the context vector of the recurrent network (prior to the current word being processed), the embedding of the current word, or the probability of the current word given the context. In future work we also intend to add these types of studies to the ERP predictions.", "sentence": "This work is most closely related to the paper from which we get the ERP data: BIBREF0 .", "cited_ids": [], "y": "The author's [new approach to fine-tuning a language model to predict event-related potentials (ERP)] is most closely related to the paper from which they get the ERP data.", "snippet_surface": "This work is most closely related to the paper from which the authors get the ERP data: BIBREF0.", "questions": {"hB24qRXmVe": "What does \"this work\" refer to?", "zVO6f7p4i1": "What does ERP stand for?"}, "answers": {"hB24qRXmVe": "The authros' a new approach to fine-tuning a language model to predict ERPs", "zVO6f7p4i1": "event-related potentials"}, "evidence": {"hB24qRXmVe": [{"section": "Abstract", "paragraph": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.", "selected": "We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable.", "paper_id": "1904.01548"}], "zVO6f7p4i1": [{"section": "Abstract", "paragraph": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.", "selected": "event-related potentials", "paper_id": "1904.01548"}, {"section": "Introduction", "paragraph": "The cognitive processes involved in human language comprehension are complex and only partially identified. According to the dual-stream model of speech comprehension BIBREF1 , sound waves are first converted to phoneme-like features and further processed by a ventral stream that maps those features onto words and semantic structures, and a dorsal stream that (among other things) supports audio-short term memory. The mapping of words onto meaning is thought to be subserved by widely distributed regions of the brain that specialize in particular modalities \u2014 for example visual aspects of the word banana reside in the occipital lobe of the brain and are activated when the word banana is heard BIBREF2 \u2014 and the different representation modalities are thought to be integrated into a single coherent latent representation in the anterior temporal lobe BIBREF3 . While this part of meaning representation in human language comprehension is somewhat understood, much less is known about how the meanings of words are integrated together to form the meaning of sentences and discourses. One tool researchers use to study the integration of meaning across words is electroencephelography (EEG), which measures the electrical activity of large numbers of neurons acting in concert. EEG has the temporal resolution necessary to study the processes involved in meaning integration, and certain stereotyped electrical responses to word presentations, known as event-related potentials (ERPs), have been identified with some of the processes thought to contribute to comprehension.", "selected": "event-related potentials", "paper_id": "1904.01548"}, {"section": "Abstract", "paragraph": "Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.", "selected": "event-related potentials (ERPs)", "paper_id": "1904.01548"}, {"section": "Introduction", "paragraph": "The cognitive processes involved in human language comprehension are complex and only partially identified. According to the dual-stream model of speech comprehension BIBREF1 , sound waves are first converted to phoneme-like features and further processed by a ventral stream that maps those features onto words and semantic structures, and a dorsal stream that (among other things) supports audio-short term memory. The mapping of words onto meaning is thought to be subserved by widely distributed regions of the brain that specialize in particular modalities \u2014 for example visual aspects of the word banana reside in the occipital lobe of the brain and are activated when the word banana is heard BIBREF2 \u2014 and the different representation modalities are thought to be integrated into a single coherent latent representation in the anterior temporal lobe BIBREF3 . While this part of meaning representation in human language comprehension is somewhat understood, much less is known about how the meanings of words are integrated together to form the meaning of sentences and discourses. One tool researchers use to study the integration of meaning across words is electroencephelography (EEG), which measures the electrical activity of large numbers of neurons acting in concert. EEG has the temporal resolution necessary to study the processes involved in meaning integration, and certain stereotyped electrical responses to word presentations, known as event-related potentials (ERPs), have been identified with some of the processes thought to contribute to comprehension.", "selected": "event-related potentials (ERPs)", "paper_id": "1904.01548"}]}}
{"idx": "1904.04358.2.1.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "Dataset", "context_paragraph": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "sentence": "In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "cited_ids": [], "y": "The authors explore [the challenge of decoding intended speech or motor activity from brain signals, specifically electroencephalography (EEG) data] by specifically targeting five binary classification problems addressed in BIBREF17, BIBREF18, i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "snippet_surface": "The authors explore the problem space by specifically targeting five binary classification problems addressed in BIBREF17, BIBREF18, i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "questions": {"h3ZJHZJJlz": "What does the \"problem space\" refer to?"}, "answers": {"h3ZJHZJJlz": "In this paper, the \"problem space\" refers to the challenge of decoding intended speech or motor activity from brain signals, specifically electroencephalography (EEG) data."}, "evidence": {"h3ZJHZJJlz": [{"section": "Introduction", "paragraph": "Among the various brain activity-monitoring modalities in BCI, electroencephalography (EEG) BIBREF5 , BIBREF6 has demonstrated promising potential to differentiate between various brain activities through measurement of related electric fields. EEG is non-invasive, portable, low cost, and provides satisfactory temporal resolution. This makes EEG suitable to realize BCI systems. EEG data, however, is challenging: these data are high dimensional, have poor SNR, and suffer from low spatial resolution and a multitude of artifacts. For these reasons, it is not particularly obvious how to decode the desired information from raw EEG signals. Although the area of BCI based speech intent recognition has received increasing attention among the research community in the past few years, most research has focused on classification of individual speech categories in terms of discrete vowels, phonemes and words BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . This includes categorization of imagined EEG signal into binary vowel categories like /a/, /u/ and rest BIBREF7 , BIBREF8 , BIBREF9 ; binary syllable classes like /ba/ and /ku/ BIBREF1 , BIBREF10 , BIBREF11 , BIBREF12 ; a handful of control words like 'up', 'down', 'left', 'right' and 'select' BIBREF15 or others like 'water', 'help', 'thanks', 'food', 'stop' BIBREF13 , Chinese characters BIBREF14 , etc. Such works mostly involve traditional signal processing or manual feature handcrafting along with linear classifiers (e.g., SVMs). In our recent work BIBREF16 , we introduced deep learning models for classification of vowels and words that achieved 23.45% improvement of accuracy over the baseline.", "selected": "his makes EEG suitable to realize BCI systems. EEG data, however, is challenging: these data are high dimensional, have poor SNR, and suffer from low spatial resolution and a multitude of artifacts", "paper_id": "1904.04358"}, {"section": "Proposed Framework", "paragraph": "Cognitive learning process underlying articulatory speech production involves incorporation of intermediate feedback loops and utilization of past information stored in the form of memory as well as hierarchical combination of several feature extractors. To this end, we develop our mixed neural network architecture composed of three supervised and a single unsupervised learning step, discussed in the next subsections and shown in Fig. FIGREF1 . We formulate the problem of categorizing EEG data based on speech imagery as a non-linear mapping INLINEFORM0 of a multivariate time-series input sequence INLINEFORM1 to fixed output INLINEFORM2 , i.e, mathematically INLINEFORM3 : INLINEFORM4 , where c and t denote the EEG channels and time instants respectively.", "selected": "categorizing EEG data based on speech imagery", "paper_id": "1904.04358"}, {"section": "Dataset", "paragraph": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "selected": "pairwise EEG-phoneme mapping", "paper_id": "1904.04358"}]}}
{"idx": "1904.04358.3.1.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "CNN & LSTM", "context_paragraph": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "sentence": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "cited_ids": [], "y": "The authors decode spatial connections between electrodes [features that are decoded to determine the dimensionality of matrices] between the [parts of channel covariance matrices that are used to determine their dimensionality of these matrices] from the channel covariance matrix by using a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The feature map at a given CNN layer with input INLINEFORM1, weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4. At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, they apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, they exploit an LSTM with two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as a CNN [trained on the channel covariant matrices].", "snippet_surface": "The authors decode spatial connections between the electrodes from the channel covariance matrix by using a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The feature map at a given CNN layer with input INLINEFORM1, weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4. At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, they apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, they exploit an LSTM BIB", "questions": {"AHtdfBf1h2": "What does \"spatial connections\" mean?", "f8L98MO1tU": "How do they \"exploit an lstm bib\"?"}, "answers": {"AHtdfBf1h2": "Spatial connections are features of electrodes that are decoded  to determine the dimensionality of matrices.", "f8L98MO1tU": "An LSTM BIB is a four-layered recurrent neural network on the channel covariant matrices."}, "evidence": {"AHtdfBf1h2": [{"section": "CNN & LSTM", "paragraph": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "selected": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.", "paper_id": "1904.04358"}], "f8L98MO1tU": [{"section": "CNN & LSTM", "paragraph": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "selected": "In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN.", "paper_id": "1904.04358"}]}}
{"idx": "1904.04358.4.2.1", "paper_id": "1904.04358", "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "context_section_header": "Dataset", "context_paragraph": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "sentence": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18 . In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.", "cited_ids": [], "y": "The authors evaluate their model [, a BCI system for classification, ]on a publicly available dataset, KARA ONE, composed of multimodal data [from 14 participants] for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since the authors' intention is to classify the phonological categories from human thoughts [(i.e., he relationship between imagined speech and active thoughts)], they discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is reportedly challenging to attain a pairwise EEG-ph [because of the mixed nature of EEG signals.]", "snippet_surface": "The authors evaluate their model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since the authors' intention is to classify the phonological categories from human thoughts, they discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is reportedly challenging to attain a pairwise EEG-ph", "questions": {"rIL6sckp0h": "What does \"their model\" refer to?"}, "answers": {"rIL6sckp0h": "Their model refers to the BCI system for classification as described."}, "evidence": {"rIL6sckp0h": [{"section": "Abstract", "paragraph": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an alternative vocal communication pathway for people with speaking disabilities. As a step towards full decoding of imagined speech from active thoughts, we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme. To better capture the complex representation of high-dimensional electroencephalography (EEG) data, we compute the joint variability of EEG electrodes into a channel cross-covariance matrix. We then extract the spatio-temporal information encoded within the matrix using a mixed deep neural network strategy. Our model framework is composed of a convolutional neural network (CNN), a long-short term network (LSTM), and a deep autoencoder. We train the individual networks hierarchically, feeding their combined outputs in a final gradient boosting classification step. Our best models achieve an average accuracy of 77.9% across five different binary classification tasks, providing a significant 22.5% improvement over previous methods. As we also show visually, our work demonstrates that the speech imagery EEG possesses significant discriminative information about the intended articulatory movements responsible for natural speech synthesis.", "selected": "we present a BCI system for subject-independent classification of phonological categories exploiting a novel deep learning based hierarchical feature extraction scheme.", "paper_id": "1904.04358"}]}}
{"idx": "190534", "paper_id": "9519654", "title": "Unsupervised Morphological Segmentation with Log-Linear Models", "abstract": "Morphological segmentation breaks words into morphemes (the basic semantic units). It is a key component for natural language processing systems. Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources. However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efficient algorithms for learning and inference by combining contrastive estimation with sampling. Our system, based on monolingual features only, outperforms a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor.", "context_section_header": "", "context_paragraph": "We focus on inflectional morphology and test our approach on datasets in Arabic and Hebrew. Our system, using monolingual features only, outperforms Snyder & Barzilay (2008b) by a large margin, even when their system uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007).", "sentence": "On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007).", "cited_ids": [{"paper_id": "8819802", "citation": "(Creutz and Lagus, 2007)"}], "y": "[The authors present a log-linear model for unsupervised morphological segmentation,] reducing F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007) [which uses information about word frequency] on the Arabic Penn Treebank.", "snippet_surface": "The authors' system reduces F1 error by 11% compared to Morfessor Categories-MAP (Creutz and Lagus, 2007) on the Arabic Penn Treebank.", "questions": {"d+L547/CZx": "What is the authors' system?", "x1Czcjp9qJ": "What is morfessor categories?"}, "answers": {"d+L547/CZx": "The author presents a log-linear model for unsupervised morphological segmentation. The model uses overlapping features, e.g. morphemes and their contexts, and enables easy extension to incorporate additional features and linguistic knowledge.", "x1Czcjp9qJ": "Morfessor segments the input words into units called morphs. \n Morfessor Catergories-ML is a maximum likelihood model that is used for reanalysing a segmentation produced by the Baseline-Length algorithm. it operates on data that consists of words. \n Morfessor Catergories-MAP uses information about word frequency."}, "evidence": {"d+L547/CZx": [{"section": "Abstract", "paragraph": "Morphological segmentation breaks words into morphemes (the basic semantic units). It is a key component for natural language processing systems. Unsupervised morphological segmentation is attractive, because in every language there are virtually unlimited supplies of text, but very few labeled resources. However, most existing model-based systems for unsupervised morphological segmentation use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efficient algorithms for learning and inference by combining contrastive estimation with sampling. Our system, based on monolingual features only, outperforms a state-of-the-art system by a large margin, even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence. On the Arabic Penn Treebank, our system reduces F1 error by 11% compared to Morfessor.", "selected": "we present the first log-linear model for unsupervised morphological segmentation. Our model uses overlapping features such as morphemes and their contexts, and incorporates exponential priors inspired by the minimum description length (MDL) principle. We present efficient algorithms for learning and inference by combining contrastive estimation with sampling.", "paper_id": "9519654"}, {"section": "Introduction", "paragraph": "In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008).", "selected": "We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models", "paper_id": "9519654"}], "x1Czcjp9qJ": [{"section": "Abstract", "paragraph": "We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes. A lexicon of word segments, called morphs, is induced from the data. The lexicon stores information about both the usage and form of the morphs. Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data. Morfessor is shown to perform very well compared to a widely known benchmark algorithm, in particular on Finnish data.", "selected": "a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes.", "paper_id": "8819802"}, {"section": "INTRODUCTION", "paragraph": "In this work, we describe a general probabilistic model family for morphology induction. The model family that we call Morfessor consists of independent components that can be combined in different configurations. We utilize the maximum a posteriori framework for expressing the model optimization criteria.", "selected": "we describe a general probabilistic model family for morphology induction. The model family that we call Morfessor consists of independent components that can be combined in different configurations.", "paper_id": "8819802"}, {"section": "INTRODUCTION", "paragraph": "Morfessor segments the input words into units called morphs. A lexicon of morphs is constructed where information about both the distributional nature (usage) and form of each morph is stored. Usage relates to the distributional nature of the occurrence of morphs in words. Form corresponds to the string of letters that comprise the morph. We experimentally evaluate different instances of the Morfessor model and compare them against a benchmark morphologylearning algorithm [Goldsmith 2001[Goldsmith , 2005.", "selected": "Morfessor segments the input words into units called morphs. A lexicon of morphs is constructed where information about both the distributional nature (usage) and form of each morph is stored.", "paper_id": "8819802"}, {"section": "FORMULATION OF THE MORFESSOR MODEL STRUCTURE", "paragraph": "We present Morfessor, a probabilistic model family for morphology learning. The model family consists of a number of distinct components which can be interpreted to encode both syntactic and semantic aspects of morphs, which are word segments discovered from data. Morfessor is a unifying framework that encompasses the particular models introduced earlier in Creutz and Lagus [2002], Creutz [2003], and Creutz andLagus [2004, 2005a] and also has close connections to models proposed by other researchers. Each of these particular works has brought additional understanding regarding relevant problems and how they can be solved.", "selected": "The model family consists of a number of distinct components which can be interpreted to encode both syntactic and semantic aspects of morphs, which are word segments discovered from data.", "paper_id": "8819802"}, {"section": "Categories-ML", "paragraph": "The Morfessor Categories-ML model has been presented in Creutz and Lagus [2004]. The model is a maximum likelihood (ML) model that is applied for reanalyzing a segmentation produced by the Baseline-Length algorithm. The morphotactics of the full Morfessor model is used in Categories-ML and all four usage features are included. However, the computation of the category membership probabilities (Section 3.5.3) is only utilized for initializing a category tagging of the morph segmentation obtained from Baseline-Length. Emission probabilities (Equation (6)) are then obtained as maximum likelihood estimates from the tagging.", "selected": "The Morfessor Categories-ML model has been presented in Creutz and Lagus [2004]. The model is a maximum likelihood (ML) model that is applied for reanalyzing a segmentation produced by the Baseline-Length algorithm.", "paper_id": "8819802"}, {"section": "Categories-ML", "paragraph": "The size of the morph lexicon is not taken into account directly in the calculation of the overall probability, but some heuristics are applied. If a morph in the lexicon consists of other morphs that are present in the lexicon (e.g., seemed = seem+ed), the most probable split (essentially according to Equation (10b)) is selected and the redundant morph is removed. A split into nonmorphemes is not allowed, however. If, on the contrary, a word has been shattered into many short fragments, these are removed by joining them with their neighboring morphs which hopefully creates a proper morph (e.g., flu+s+ter+ed becomes fluster+ed). This takes place by joining together nonmorphemes with their shortest neighbors until the resulting morph can qualify as a stem which is determined by Equation (17). The Categories-ML algorithm operates on data consisting of word types.", "selected": "The Categories-ML algorithm operates on data consisting of word types.", "paper_id": "8819802"}]}}
{"idx": "1906.08593.2.1.1", "paper_id": "1906.08593", "title": "Conflict as an Inverse of Attention in Sequence Relationship", "abstract": "Attention is a very efficient way to model the relationship between two sequences by comparing how similar two intermediate representations are. Initially demonstrated in NMT, it is a standard in all NLU tasks today when efficient interaction between sequences is considered. However, we show that attention, by virtue of its composition, works best only when it is given that there is a match somewhere between two sequences. It does not very well adapt to cases when there is no similarity between two sequences or if the relationship is contrastive. We propose an Conflict model which is very similar to how attention works but which emphasizes mostly on how well two sequences repel each other and finally empirically show how this method in conjunction with attention can boost the overall performance.", "context_section_header": "The model", "context_paragraph": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", "sentence": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input.", "cited_ids": [], "y": "The authors create two models [for their language understanding task,] both of which constitute of three main parts: encoder, interaction and classifier and take two sequences [(individual words or phrases)] as input.", "snippet_surface": "The authors create two models both of which constitute of three main parts: encoder, interaction and classifier and take two sequences as input.", "questions": {"iXSnDQAJFQ": "What do \"two models\" refer to?"}, "answers": {"iXSnDQAJFQ": "The two models refer to the language understanding tasks conducting by the author. Task 1 being the Quora Duplicate Question Pair Detection, and task 2 being the Ranking questions in Bing's People Also Ask. Both models constitute of three main parts."}, "evidence": {"iXSnDQAJFQ": [{"section": "The model", "paragraph": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", "selected": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input.", "paper_id": "1906.08593"}]}}
{"idx": "1907.03060.1.2.1", "paper_id": "1907.03060", "title": "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation", "abstract": "This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking. Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario.", "context_section_header": "Conclusion", "context_paragraph": "In this paper, we challenged the difficult task of Ja INLINEFORM0 Ru news domain translation in an extremely low-resource setting. We empirically confirmed the limited success of well-established solutions when restricted to in-domain data. Then, to incorporate out-of-domain data, we proposed a multilingual multistage fine-tuning approach and observed that it substantially improves Ja INLINEFORM1 Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53 . This paper contains an empirical comparison of several existing approaches and hence we hope that our paper can act as a guideline to researchers attempting to tackle extremely low-resource translation.", "sentence": "Then, to incorporate out-of-domain data, we proposed a multilingual multistage fine-tuning approach and observed that it substantially improves Ja INLINEFORM1 Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53 .", "cited_ids": [], "y": "The authors proposed a multilingual multistage fine-tuning approach to incorporate out-of-domain data, and observed that it substantially improves Ja\u2192Ru [Japanese to Russian news] translation by over 3.7 BLEU points compared to a strong baseline.", "snippet_surface": "The authors then proposed a multilingual multistage fine-tuning approach to incorporate out-of-domain data, and observed that it substantially improves Ja\u2192Ru translation by over 3.7 BLEU points compared to a strong baseline, as summarized in Table TABREF53.", "questions": {"8tBbn2SGsg": "What is \"Ja-->Ru translation\"?", "W+cIAlukJD": "What are \"BLEU\" points?"}, "answers": {"8tBbn2SGsg": "Japanese to Russian news translation.", "W+cIAlukJD": "BLEU score is a metric that measures the success of the translation."}, "evidence": {"8tBbn2SGsg": [{"section": "Abstract", "paragraph": "This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking. Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data. We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario.", "selected": "This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese--Russian pair for benchmarking", "paper_id": "1907.03060"}, {"section": "Introduction", "paragraph": "In this paper, we work on a linguistically distant and thus challenging language pair Japanese INLINEFORM0 Russian (Ja INLINEFORM1 Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor. Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja INLINEFORM2 En and Ru INLINEFORM3 En, are also small. As we demonstrate in Section SECREF4 , this severely limits the performance of prominent low-resource techniques, such as multilingual modeling, back-translation, and pivot-based PBSMT. To remedy this, we propose a novel multistage fine-tuning method for NMT that combines multilingual modeling BIBREF8 and domain adaptation BIBREF9 .", "selected": "In this paper, we work on a linguistically distant and thus challenging language pair Japanese INLINEFORM0 Russian (Ja INLINEFORM1 Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor.", "paper_id": "1907.03060"}, {"section": "Our Japanese\u2013Russian Setting", "paragraph": "In this paper, we deal with Ja INLINEFORM0 Ru news translation. This language pair is very challenging because the languages involved have completely different writing system, phonology, morphology, grammar, and syntax. Among various domains, we experimented with translations in the news domain, considering the importance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, due to large presence of out-of-vocabulary (OOV) tokens and long sentences. To establish and evaluate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common BIBREF10 .", "selected": "In this paper, we deal with Ja INLINEFORM0 Ru news translation.", "paper_id": "1907.03060"}], "W+cIAlukJD": [{"section": "MT Methods Examined", "paragraph": "Table TABREF22 contains some specific hyper-parameters for our baseline NMT models. The hyper-parameters not mentioned in this table used the default values in tensor2tensor. For M2M systems, we over-sampled Ja INLINEFORM0 Ru and Ja INLINEFORM1 En training data so that their sizes match the largest Ru INLINEFORM2 En data. To reduce the number of unknown words, we used tensor2tensor's internal sub-word segmentation mechanism. Since we work in a low-resource setting, we used shared sub-word vocabularies of size 16k for the uni- and bi-directional models and 32k for the M2M models. The number of training iterations was determined by early-stopping: we evaluated our models on the development set every 1,000 updates, and stopped training if BLEU score for the development set was not improved for 10,000 updates (10 check-points). Note that the development set was created by concatenating those for the individual translation directions without any over-sampling.", "selected": "The number of training iterations was determined by early-stopping: we evaluated our models on the development set every 1,000 updates, and stopped training if BLEU score for the development set was not improved for 10,000 updates (10 check-points).", "paper_id": "1907.03060"}]}}
{"idx": "1907.09369.3.1.2", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "sentence": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "cited_ids": [], "y": "In the [CrowdFlower dataset annotated for emotions], the authors reported results from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. They only reported part of their result for CrowdFlower dataset that can be mapped to one of the seven labels.", "snippet_surface": "In the second one, the authors reported results from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. They only reported part of their result for CrowdFlower dataset that can be mapped to one of the seven labels.", "questions": {"rIsOAtv7Zd": "What does \"the second one\" refer to?"}, "answers": {"rIsOAtv7Zd": "An approach that it is the CrowdFlower dataset annotated for emotions."}, "evidence": {"rIsOAtv7Zd": [{"section": "Baseline Approaches", "paragraph": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", "selected": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", "paper_id": "1907.09369"}]}}
{"idx": "1907.09369.3.2.1", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "sentence": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "cited_ids": [], "y": "The authors' first experiment was conducted by Wang et al. BIBREF21, who downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "snippet_surface": "The authors' first experiment was conducted by Wang et al. BIBREF21, who downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "questions": {"abFyeyrhRw": "What does \"first experiment\" refer to?", "IyNjbmzWn4": "How does parrott detect emotion?"}, "answers": {"abFyeyrhRw": "In that experiment, they downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrot's three level categorization of emotions in seven categories.", "IyNjbmzWn4": "Categorizing the hashtags among joy, sadness, anger, love, fear, thankfulness or surprise."}, "evidence": {"abFyeyrhRw": [{"section": "Baseline Approaches", "paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "selected": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "paper_id": "1907.09369"}], "IyNjbmzWn4": [{"section": "Baseline Approaches", "paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "selected": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "paper_id": "1907.09369"}]}}
{"idx": "1907.09369.4.2.2", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Baseline Approaches", "context_paragraph": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "sentence": "Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "cited_ids": [], "y": "The authors only report part of their result for CrowdFlower dataset [comprising emotional tweets] [whereby emotions are annotated] that can be mapped to one of their seven labels.", "snippet_surface": "The authors only report part of their result for CrowdFlower dataset that can be mapped to one of their seven labels.", "questions": {"hS6CMt/Zu9": "What kind of data is in the \"crowdflower dataset\"?"}, "answers": {"hS6CMt/Zu9": "Emotional tweets."}, "evidence": {"hS6CMt/Zu9": [{"section": "Model Performances on New Dataset", "paragraph": "To asses the performance of these models on a totally unseen data, we tried to classify the CrowdFlower emotional tweets dataset. The CrowdFlower dataset consists of 40k tweets annotated via crowd-sourcing each with a single emotional label. This dataset is considered a hard dataset to classify with a lot of noise. The distribution of the dataset can be seen in Table TABREF18 . The labeling on this dataset is non-standard, so we used the following mapping for labels:", "selected": "To asses the performance of these models on a totally unseen data, we tried to classify the CrowdFlower emotional tweets dataset.", "paper_id": "1907.09369"}]}}
{"idx": "1907.09369.5.1.1", "paper_id": "1907.09369", "title": "Emotion Detection in Text: Focusing on Latent Representation", "abstract": "In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.", "context_section_header": "Model", "context_paragraph": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature. As mentioned in the Introduction, Recurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets.", "sentence": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature.", "cited_ids": [], "y": "The authors' architecture [a bidirectional GRU network used to classify emotions in tweets] was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods [that infer emotion by looking for specific n-grams] commonly used in the literature.", "snippet_surface": "The authors' architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature.", "questions": {"/2lL7pzVbN": "What does the \"authors' architecture\" refer to?", "pUfa6g//r1": "What are examples of \"lexicon-based methods\"?"}, "answers": {"/2lL7pzVbN": "The \"architecture\" refers to the deep neural network the authors used to classify emotions in the tweets dataset. This architecture uses a model based on bidirectional RNN (a bidirectional GRU network). (The paper then further describes the structure and performance of this model)", "pUfa6g//r1": "Lexicon-based methods infer emotion by looking for specific word n-gram features of a text. Two concrete examples presented in this paper as baselines include BIBREF21 and BIBREF33."}, "evidence": {"/2lL7pzVbN": [{"section": "Model", "paragraph": "In this section, we introduce the deep neural network architecture that we used to classify emotions in the tweets dataset. Emotional expressions are more complex and context-dependent even compared to other forms of expressions based mostly on the complexity and ambiguity of human emotions and emotional expressions and the huge impact of context on the understanding of the expressed emotion. These complexities are what led us to believe lexicon-based features like is normally used in conventional machine learning approaches are unable to capture the intricacy of emotional expressions.", "selected": "the deep neural network architecture that we used to classify emotions in the tweets dataset", "paper_id": "1907.09369"}, {"section": "Model", "paragraph": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature. As mentioned in the Introduction, Recurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets.", "selected": "a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets", "paper_id": "1907.09369"}], "pUfa6g//r1": [{"section": "Related Work", "paragraph": "A lot of work has been done on detecting emotion in speech or visual data BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . But detecting emotions in textual data is a relatively new area that demands more research. There have been many attempts to detect emotions in text using conventional machine learning techniques and handcrafted features in which given the dataset, the authors try to find the best feature set that represents the most and the best information about the text, then passing the converted text as feature vectors to the classifier for training BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 . During the process of creating the feature set, in these methods, some of the most important information in the text such as the sequential nature of the data, and the context will be lost.", "selected": ", the authors try to find the best feature set that represents the most and the best information about the text, then passing the converted text as feature vectors to the classifier for training BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26", "paper_id": "1907.09369"}, {"section": "Baseline Approaches", "paragraph": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", "selected": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", "paper_id": "1907.09369"}, {"section": "Baseline Approaches", "paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "selected": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.", "paper_id": "1907.09369"}, {"section": "Baseline Approaches", "paragraph": "In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise. To assess the quality of using hashtags as labels, the sampled 400 tweets randomly and after comparing human annotations by hashtag labels they came up with simple heuristics to increase the quality of labeling by ignoring tweets with quotations and URLs and only keeping tweets with 5 terms or more that have the emotional hashtags at the end of the tweets. Using these rules they extracted around 2.5M tweets. After sampling another 400 random tweets and comparing it to human annotation the saw that hashtags can classify the tweets with 95% precision. They did some pre-processing by making all words lower-case, replaced user mentions with @user, replaced letters/punctuation that is repeated more than twice with the same two letters/punctuation (e.g., ooooh INLINEFORM0 ooh, !!!!! INLINEFORM1 !!); normalized some frequently used informal expressions (e.g., ll \u2192 will, dnt INLINEFORM2 do not); and stripped hash symbols. They used a sub-sample of their dataset to figure out the best approaches for classification, and after trying two different classifiers (multinomial Naive Bayes and LIBLINEAR) and 12 different feature sets, they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "selected": "they got their best results using logistic regression branch for LIBLINEAR classifier and a feature set consist of n-gram(n=1,2), LIWC and MPQA lexicons, WordNet-Affect and POS tags.", "paper_id": "1907.09369"}, {"section": "Baseline Approaches", "paragraph": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "selected": "In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets.", "paper_id": "1907.09369"}]}}
{"idx": "1908.05828.1.1.1", "paper_id": "1908.05828", "title": "Named Entity Recognition for Nepali Language", "abstract": "Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created. We discuss and compare the performance of various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.", "context_section_header": "Conclusion and Future work", "context_paragraph": "We also present a neural architecture BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration. We believe this will not only help Nepali language but also other languages falling under the umbrellas of Devanagari languages. Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively.", "sentence": "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively.", "cited_ids": [], "y": "The authors' model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperform all other models the authors generated in the [Nepali Named Entity Recognition] (OurNepali) and ILPRL datasets, respectively.", "snippet_surface": "The authors' model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other models experimented in OurNepali and ILPRL dataset respectively.", "questions": {"MdUPZmjqO0": "What is OurNepali dataset?"}, "answers": {"MdUPZmjqO0": "It's a Nepali Named Entity Recognition (NER) dataset the authors created."}, "evidence": {"MdUPZmjqO0": [{"section": "Abstract", "paragraph": "Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created. We discuss and compare the performance of various machine learning models on this dataset. We also propose a novel NER scheme for Nepali and show that this scheme, based on grapheme-level representations, outperforms character-level representations when combined with BiLSTM models. Our best models obtain an overall F1 score of 86.89, which is a significant improvement on previously reported performance in literature.", "selected": "Named Entity Recognition (NER) has been studied for many languages like English, German, Spanish, and others but virtually no studies have focused on the Nepali language. One key reason is the lack of an appropriate, annotated dataset. In this paper, we describe a Nepali NER dataset that we created.", "paper_id": "1908.05828"}, {"section": "Dataset Statistics ::: OurNepali dataset", "paragraph": "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25.", "selected": "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25.", "paper_id": "1908.05828"}]}}
{"idx": "1908.06941.1.1.2", "paper_id": "1908.06941", "title": "Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics", "abstract": "In distributional semantics, the pointwise mutual information ($\\mathit{PMI}$) weighting of the cooccurrence matrix performs far better than raw counts. There is, however, an issue with unobserved pair cooccurrences as $\\mathit{PMI}$ goes to negative infinity. This problem is aggravated by unreliable statistics from finite corpora which lead to a large number of such pairs. A common practice is to clip negative $\\mathit{PMI}$ ($\\mathit{\\texttt{-} PMI}$) at $0$, also known as Positive $\\mathit{PMI}$ ($\\mathit{PPMI}$). In this paper, we investigate alternative ways of dealing with $\\mathit{\\texttt{-} PMI}$ and, more importantly, study the role that negative information plays in the performance of a low-rank, weighted factorization of different $\\mathit{PMI}$ matrices. Using various semantic and syntactic tasks as probes into models which use either negative or positive $\\mathit{PMI}$ (or both), we find that most of the encoded semantics and syntax come from positive $\\mathit{PMI}$, in contrast to $\\mathit{\\texttt{-} PMI}$ which contributes almost exclusively syntactic information. Our findings deepen our understanding of distributional semantics, while also introducing novel $PMI$ variants and grounding the popular $PPMI$ measure.", "context_section_header": "PMI & Matrix Factorization", "context_paragraph": "such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "sentence": "In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "cited_ids": [], "y": "The authors find that this [the normalized method used in BIBREF7] works poorly if done symmetrically, so they introduce a variant called NNEGPMI which only normalizes -PMI.", "snippet_surface": "The authors find that this works poorly if done symmetrically, so they introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$:", "questions": {"AJrpo7TRq9": "What does the pronoun \"this\" refer to?"}, "answers": {"AJrpo7TRq9": "The normalised method used in Bibref7."}, "evidence": {"AJrpo7TRq9": [{"section": "PMI & Matrix Factorization", "paragraph": "Normalization: We also experiment with normalized $\\mathit {PMI}$ ($\\mathit {NPMI}$) BIBREF7:", "selected": "normalized $\\mathit {PMI}$ ($\\mathit {NPMI}$) BIBREF7:", "paper_id": "1908.06941"}, {"section": "PMI & Matrix Factorization", "paragraph": "Normalization: We also experiment with normalized $\\mathit {PMI}$ ($\\mathit {NPMI}$) BIBREF7:", "selected": "Normalization", "paper_id": "1908.06941"}]}}
{"idx": "1908.09246.3.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Experimental Setup", "context_paragraph": "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12 , Twitter, and Google datasets) are employed. Details are summarized below:", "sentence": "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12 , Twitter, and Google datasets) are employed.", "cited_ids": [], "y": "The authors validate the effectiveness of AEM [Adversarial-neural Event Mode] for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news) using three datasets.", "snippet_surface": "The authors validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news) using three datasets (FSD BIBREF12, Twitter, and Google datasets).", "questions": {"5TC29YvIbv": "What does \"aem\" stand for?"}, "answers": {"5TC29YvIbv": "AEM  (Adversarial-neural Event Mode) is an event extraction model based on GANs"}, "evidence": {"5TC29YvIbv": [{"section": "Abstract", "paragraph": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "selected": "n event extraction model based on Generative Adversarial Nets,", "paper_id": "1908.09246"}]}}
{"idx": "1908.09246.4.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Generative Adversarial Nets", "context_paragraph": "Although various GAN based approaches have been explored for many applications, none of these approaches tackles open-domain event extraction from online texts. We propose a novel GAN-based event extraction model called AEM. Compared with the previous models, AEM has the following differences: (1) Unlike most GAN-based text generation approaches, a generator network is employed in AEM to learn the projection function between an event distribution and the event-related word distributions (entity, location, keyword, date). The learned generator captures event-related patterns rather than generating text sequence; (2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration; (3) The discriminative features learned by the discriminator of AEM provide a straightforward way to visualize the extracted events.", "sentence": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration;", "cited_ids": [], "y": "Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM [where Gibbs sampler computes the conditional posterior distribution and assigns an event for each document], AEM could extract the events more efficiently due to CUDA acceleration [ie using GPUs].", "snippet_surface": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to CUDA acceleration.", "questions": {"Na/oEPMLGI": "What does \"traditional inference procedure\" refer to?", "Zt1AZoTCs5": "What is \"cuda acceleration\"?"}, "answers": {"Na/oEPMLGI": "Traditional inference procedure refer to Gibbs sampling for parameter inference used in LEM and DPEMM. The Gibbs sampler computes the conditional posterior distribution and assigns an event for each document.", "Zt1AZoTCs5": "CUDA refers to a GPU API developed by NVIDIA than can been used to more quickly run machine learning models on GPUs."}, "evidence": {"Na/oEPMLGI": [{"section": "Abstract", "paragraph": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "selected": "Gibbs sampling for parameter inference", "paper_id": "1908.09246"}, {"section": "Introduction", "paragraph": "To extract structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al. zhou2014simple proposed an unsupervised Latent Event Model (LEM) for open-domain event extraction. To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "selected": "Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document", "paper_id": "1908.09246"}, {"section": "Generative Adversarial Nets", "paragraph": "Although various GAN based approaches have been explored for many applications, none of these approaches tackles open-domain event extraction from online texts. We propose a novel GAN-based event extraction model called AEM. Compared with the previous models, AEM has the following differences: (1) Unlike most GAN-based text generation approaches, a generator network is employed in AEM to learn the projection function between an event distribution and the event-related word distributions (entity, location, keyword, date). The learned generator captures event-related patterns rather than generating text sequence; (2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration; (3) The discriminative features learned by the discriminator of AEM provide a straightforward way to visualize the extracted events.", "selected": "unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM", "paper_id": "1908.09246"}], "Zt1AZoTCs5": []}}
{"idx": "1908.09246.5.1.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "sentence": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns.", "cited_ids": [], "y": "Instead of providing an analytic approximation, [the Adversarial-neural Event Model] uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. The supervision signal provided by the discriminator will help the generator to capture the event-related patterns.", "snippet_surface": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. The supervision signal provided by the discriminator will help the generator to capture the event-related patterns.", "questions": {"iMYgxLDXL1": "What does aem stand for?", "EGPvifVZzJ": "What is an example of a \"discriminator network\"?"}, "answers": {"iMYgxLDXL1": "AEM means Adversarial-neural Event Model", "EGPvifVZzJ": "One example of discriminatory network is INLINEFORM0, which is a generator network and applies transformation from a document-event distribution layer to a dimensional hidden layer."}, "evidence": {"iMYgxLDXL1": [{"section": "Abstract", "paragraph": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "selected": "To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM).", "paper_id": "1908.09246"}], "EGPvifVZzJ": [{"section": "Network Architecture", "paragraph": "Subsequently, INLINEFORM0 transforms INLINEFORM1 into a INLINEFORM2 -dimensional hidden space using a linear layer followed by layer normalization, and the transformation is defined as: DISPLAYFORM0 ", "selected": "Subsequently, INLINEFORM0 transforms INLINEFORM1 into a INLINEFORM2 -dimensional hidden space using a linear layer followed by layer normalization, and the transformation is defined as: DISPLAYFORM0", "paper_id": "1908.09246"}, {"section": "Network Architecture", "paragraph": "Finally, four generated distributions are concatenated to represent the generated document INLINEFORM0 corresponding to the input INLINEFORM1 : DISPLAYFORM0 ", "selected": "four generated distributions are concatenated to represent the generated document INLINEFORM0 corresponding to the input INLINEFORM1 : DISPLAYFORM0", "paper_id": "1908.09246"}, {"section": "Network Architecture", "paragraph": "The discriminator network INLINEFORM0 is designed as a fully-connected network which contains an input layer, a discriminative feature layer (discriminative features are employed for event visualization) and an output layer. In AEM, INLINEFORM1 uses fake document INLINEFORM2 and real document INLINEFORM3 as input and outputs the signal INLINEFORM4 to indicate the source of the input data (lower value denotes that INLINEFORM5 is prone to predict the input data as a fake document and vice versa).", "selected": "The discriminator network INLINEFORM0 is designed as a fully-connected network which contains an input layer, a discriminative feature layer (discriminative features are employed for event visualization) and an output layer. In AEM, INLINEFORM1 uses fake document INLINEFORM2 and real document INLINEFORM3 as input and outputs the signal INLINEFORM4 to indicate the source of the input data (lower value denotes that INLINEFORM5 is prone to predict the input data as a fake document and vice versa).", "paper_id": "1908.09246"}]}}
{"idx": "1908.09246.5.2.1", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To extract structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al. zhou2014simple proposed an unsupervised Latent Event Model (LEM) for open-domain event extraction. To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "sentence": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 .", "cited_ids": [], "y": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document [concatenation of four multinomial distributions of the document] are generated from a single event which can be represented by a quadruple (entity, location, keyword, date).\" [(2) this is a time-consuming process and takes a long time to converge]", "snippet_surface": "However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple (entity, location, keyword, date).", "questions": {"spKSxQWmzD": "What is the second limitation?"}, "answers": {"spKSxQWmzD": "The second limitation is that this is a time-consuming process and takes a long time to converge."}, "evidence": {"spKSxQWmzD": [{"section": "Introduction", "paragraph": "To extract structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al. zhou2014simple proposed an unsupervised Latent Event Model (LEM) for open-domain event extraction. To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "selected": "(2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "paper_id": "1908.09246"}]}}
{"idx": "1908.09246.5.2.2", "paper_id": "1908.09246", "title": "Open Event Extraction from Online Text using a Generative Adversarial Network", "abstract": "To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15\\% is observed in F-measure.", "context_section_header": "Introduction", "context_paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "sentence": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "cited_ids": [], "y": "To deal with the limitations of previous work [assuming words in a document are generated from a single event, and the long time it takes the Gibbs sampler to run], the authors propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "snippet_surface": "To deal with these limitations, the authors propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "questions": {"vnN3LkoKFz": "What do \"these limitations\" refer to?", "f3ju40Znbe": "What is \"adversarial training\"?"}, "answers": {"vnN3LkoKFz": "The Latent Event Model (LEM) and Dirichelt Process Event Mixture Model (DPEMM) are models introduced by Zhou et al., 2014, and Zhou et al., 2017, respectively. The author of this paper suggests two limitations of both of these models: 1) both models assume that all words in a document are generated from a single event; 2) during the inference process of both models, the Gibbs sampler need to compute the conditional posterior distribution and assigns an event for each document--which is time consuming.", "f3ju40Znbe": "The author bases their proposed model on adversarial training. The Adversarial-neural Event Model (AEM) uses a discriminatory network to discriminate between the reconstructed documents form latent events and the original input documents."}, "evidence": {"vnN3LkoKFz": [{"section": "Introduction", "paragraph": "To extract structured representations of events such as who did what, when, where and why, Bayesian approaches have made some progress. Assuming that each document is assigned to a single event, which is modeled as a joint distribution over the named entities, the date and the location of the event, and the event-related keywords, Zhou et al. zhou2014simple proposed an unsupervised Latent Event Model (LEM) for open-domain event extraction. To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "selected": "both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", "paper_id": "1908.09246"}], "f3ju40Znbe": [{"section": "Introduction", "paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "selected": "we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction", "paper_id": "1908.09246"}, {"section": "Introduction", "paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "selected": ", AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents.", "paper_id": "1908.09246"}, {"section": "Introduction", "paragraph": "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", "selected": "The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).", "paper_id": "1908.09246"}]}}
{"idx": "1908.10084.1.3.1", "paper_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.  ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "context_section_header": "Introduction", "context_paragraph": "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", "sentence": "On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder.", "cited_ids": [], "y": "On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points [of cosine-similarity score] compared to InferSent and 5.5 points compared to Universal Sentence Encoder.", "snippet_surface": "The authors find that SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder on seven Semantic Textual Similarity (STS) tasks.", "questions": {"UxjdXRxnoG": "What metric is used to evaluate models on the STS task?"}, "answers": {"UxjdXRxnoG": "cosine-similarity score"}, "evidence": {"UxjdXRxnoG": [{"section": "Evaluation - Semantic Textual Similarity ::: Argument Facet Similarity", "paragraph": "SBERT is fine-tuned using the Regression Objective Function. The similarity score is computed using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation $r$ to make the results comparable to Misra et al. However, we showed BIBREF22 that Pearson correlation has some serious drawbacks and should be avoided for comparing STS systems. The results are depicted in Table TABREF12.", "selected": "The similarity score is computed using cosine-similarity based on the sentence embeddings.", "paper_id": "1908.10084"}]}}
{"idx": "1908.10084.2.1.1", "paper_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.  ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.  ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.", "context_section_header": "Evaluation - Semantic Textual Similarity ::: Unsupervised STS", "context_paragraph": "We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, the STS benchmark BIBREF10, and the SICK-Relatedness dataset BIBREF21. These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine-similarity. The results are depicted in Table TABREF6.", "sentence": "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", "cited_ids": [], "y": "The authors showed in BIBREF22 that Pearson correlation [a well-known correlation measure in statistics] is badly suited for STS. Instead, they compute the Spearman's rank correlation between the cosine-similarity [which is computed by dividing the dot product of the vectors by the product of their magnitudes] of the sentence embeddings and the gold labels", "snippet_surface": "The authors showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, they compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", "questions": {"FgX634iexR": "What is \"pearson correlation\"?", "kqbRlPMopn": "How is the \"cosine-similarity\" determined?"}, "answers": {"FgX634iexR": "Pearson correlation is a well-known correlation measure in statistics.", "kqbRlPMopn": "Cosine-similarity is a popular similarity measure between two vectors computed by dividing the dot product of the vectors by the product of their magnitudes."}, "evidence": {"FgX634iexR": [], "kqbRlPMopn": []}}
{"idx": "1909.00512.1.1.1", "paper_id": "1909.00512", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings", "abstract": "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.", "context_section_header": "Approach ::: Measures of Contextuality", "context_paragraph": "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "sentence": "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "cited_ids": [], "y": "The authors measure how contextual a word representation is using three different metrics: self-similarity [the average cosine similarities between its contextualized representation in a certain context], intra-sentence similarity [the average cosine similarity between its word representations and the sentence vector], and maximum explainable variance [the proportion of variance in contextualized representations for a given layer that can be explained by the first principal component].", "snippet_surface": "The authors measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", "questions": {"XTuqPt2LCP": "What does \"self-similarity\" mean?", "cO/Zt0NxXi": "What does \"intra-sentence similarity\" mean?", "KARSwmjo7p": "What does \"maximum explainable variance\" mean?"}, "answers": {"XTuqPt2LCP": "Self-similarity of a word is the average cosine similarity between its contextualized representation in a certain context.", "cO/Zt0NxXi": "Intra-sentence similarity is the average cosine similarity between its word representations and the sentence vector.", "KARSwmjo7p": "The maximum explainable variance is the proportion of variance in contextualized representations for a given layer that can be explained by the first principal component."}, "evidence": {"XTuqPt2LCP": [{"section": "Approach ::: Measures of Contextuality ::: Definition 1", "paragraph": "where $\\cos $ denotes the cosine similarity. In other words, the self-similarity of a word $w$ in layer $\\ell $ is the average cosine similarity between its contextualized representations across its $n$ unique contexts. If layer $\\ell $ does not contextualize the representations at all, then $\\textit {SelfSim}_\\ell (w) = 1$ (i.e., the representations are identical across all contexts). The more contextualized the representations are for $w$, the lower we would expect its self-similarity to be.", "selected": "In other words, the self-similarity of a word $w$ in layer $\\ell $ is the average cosine similarity between its contextualized representations across its $n$ unique contexts.", "paper_id": "1909.00512"}], "cO/Zt0NxXi": [{"section": "Approach ::: Measures of Contextuality ::: Definition 2", "paragraph": "Put more simply, the intra-sentence similarity of a sentence is the average cosine similarity between its word representations and the sentence vector, which is just the mean of those word vectors. This measure captures how context-specificity manifests in the vector space. For example, if both $\\textit {IntraSim}_\\ell (s)$ and $\\textit {SelfSim}_\\ell (w)$ are low $\\forall \\ w \\in s$, then the model contextualizes words in that layer by giving each one a context-specific representation that is still distinct from all other word representations in the sentence. If $\\textit {IntraSim}_\\ell (s)$ is high but $\\textit {SelfSim}_\\ell (w)$ is low, this suggests a less nuanced contextualization, where words in a sentence are contextualized simply by making their representations converge in vector space.", "selected": "Put more simply, the intra-sentence similarity of a sentence is the average cosine similarity between its word representations and the sentence vector, which is just the mean of those word vectors.", "paper_id": "1909.00512"}, {"section": "Findings ::: Context-Specificity ::: Context-specificity manifests very differently in ELMo, BERT, and GPT-2.", "paragraph": "As noted earlier, contextualized representations are more context-specific in upper layers of ELMo, BERT, and GPT-2. However, how does this increased context-specificity manifest in the vector space? Do word representations in the same sentence converge to a single point, or do they remain distinct from one another while still being distinct from their representations in other contexts? To answer this question, we can measure a sentence's intra-sentence similarity. Recall from Definition 2 that the intra-sentence similarity of a sentence, in a given layer of a given model, is the average cosine similarity between each of its word representations and their mean, adjusted for anisotropy. In Figure FIGREF25, we plot the average intra-sentence similarity of 500 uniformly randomly sampled sentences.", "selected": "Recall from Definition 2 that the intra-sentence similarity of a sentence, in a given layer of a given model, is the average cosine similarity between each of its word representations and their mean, adjusted for anisotropy.", "paper_id": "1909.00512"}], "KARSwmjo7p": [{"section": "Approach ::: Measures of Contextuality ::: Definition 3", "paragraph": "$\\textit {MEV}_\\ell (w)$ is the proportion of variance in $w$'s contextualized representations for a given layer that can be explained by their first principal component. It gives us an upper bound on how well a static embedding could replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\\textit {MEV}_\\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations.", "selected": "$\\textit {MEV}_\\ell (w)$ is the proportion of variance in $w$'s contextualized representations for a given layer that can be explained by their first principal component.", "paper_id": "1909.00512"}, {"section": "Findings ::: Static vs. Contextualized ::: On average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding.", "paragraph": "Recall from Definition 3 that the maximum explainable variance (MEV) of a word, for a given layer of a given model, is the proportion of variance in its contextualized representations that can be explained by their first principal component. This gives us an upper bound on how well a static embedding could replace a word's contextualized representations. Because contextualized representations are anisotropic (see section SECREF21), much of the variation across all words can be explained by a single vector. We adjust for anisotropy by calculating the proportion of variance explained by the first principal component of uniformly randomly sampled word representations and subtracting this proportion from the raw MEV. In Figure FIGREF29, we plot the average anisotropy-adjusted MEV across uniformly randomly sampled words.", "selected": "Recall from Definition 3 that the maximum explainable variance (MEV) of a word, for a given layer of a given model, is the proportion of variance in its contextualized representations that can be explained by their first principal component.", "paper_id": "1909.00512"}]}}
{"idx": "1909.07575.1.2.1", "paper_id": "1909.07575", "title": "Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation", "abstract": "End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model outperforms baselines 2.2 BLEU on a large benchmark dataset.", "context_section_header": "Experiments ::: Baseline Models and Implementation", "context_paragraph": "We compare our method with following baselines.", "sentence": "We compare our method with following baselines.", "cited_ids": [], "y": "The authors compare their method with the baselines [Vanilla Speech Translation, Pre-training, and multi-task]", "snippet_surface": "The authors compare their method with the following baselines.", "questions": {"r2VgLDHcEu": "What does \"their method\" refer to?", "cJ/SgerpsU": "What are the baselines?"}, "answers": {"r2VgLDHcEu": "Their method it to pull $\\mathbf {h^s}$ into the latent space where $\\mathbf {e}^s$ belong. Specifically, we share the weight $W_{ctc}$ in CTC classification layer with the source embedding weights $W_{E^s}$, which means $W_{ctc} = W_{E^s}$. In this way, when predicting the CTC path $\\mathbf {\\pi }$, the probability of observing the particular label $w_i \\in V_{src}\\cup ${`-'} at time step $t$, $p(\\pi _t=w_i|\\mathbf {x})$, is computed by normalizing the product of hidden vector $h_t^s$ and the $i$-th vector in $W_{E^s}$:", "cJ/SgerpsU": "Vanilla Speech Translation baseline, Pre-training baseline and multi-task baseline"}, "evidence": {"r2VgLDHcEu": [{"section": "Our method ::: Subnet-Consistency", "paragraph": "Our model keeps role consistency between pre-training and fine-tuning by connecting two encoders for ST task. However, this leads to some new problems: 1) The text encoder consumes $\\mathbf {e^s}$ during MT training, while it accepts $\\mathbf {h^s}$ during ST training. However, $\\mathbf {e^s}$ and $\\mathbf {h^s}$ may not follow the same distribution, resulting in the semantic inconsistency. 2) Besides, the length of $\\mathbf {h^s}$ is not the same order of magnitude with the length of $\\mathbf {e^s}$, resulting in the length inconsistency.", "selected": "Our model keeps role consistency between pre-training and fine-tuning by connecting two encoders for ST task.", "paper_id": "1909.07575"}, {"section": "Our method ::: Subnet-Consistency", "paragraph": "In response to the above two challenges, we propose two countermeasures: 1) We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging $\\mathbf {e^s}$ and $\\mathbf {h^s}$ in the same space. 2)We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.", "selected": "We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging $\\mathbf {e^s}$ and $\\mathbf {h^s}$ in the same space.", "paper_id": "1909.07575"}, {"section": "Our method ::: Subnet-Consistency", "paragraph": "In response to the above two challenges, we propose two countermeasures: 1) We share weights between CTC classification layer and source-end word embedding layer during training of ASR and MT, encouraging $\\mathbf {e^s}$ and $\\mathbf {h^s}$ in the same space. 2)We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.", "selected": "We feed the text encoder source sentences in the format of CTC path, which are generated from a seq2seq model, making it more robust toward long inputs.", "paper_id": "1909.07575"}, {"section": "Our method ::: Subnet-Consistency ::: Semantic Consistency", "paragraph": "To bridge the space gap, our idea is to pull $\\mathbf {h^s}$ into the latent space where $\\mathbf {e}^s$ belong. Specifically, we share the weight $W_{ctc}$ in CTC classification layer with the source embedding weights $W_{E^s}$, which means $W_{ctc} = W_{E^s}$. In this way, when predicting the CTC path $\\mathbf {\\pi }$, the probability of observing the particular label $w_i \\in V_{src}\\cup ${`-'} at time step $t$, $p(\\pi _t=w_i|\\mathbf {x})$, is computed by normalizing the product of hidden vector $h_t^s$ and the $i$-th vector in $W_{E^s}$:", "selected": "to pull $\\mathbf {h^s}$ into the latent space where $\\mathbf {e}^s$ belong. Specifically, we share the weight $W_{ctc}$ in CTC classification layer with the source embedding weights $W_{E^s}$, which means $W_{ctc} = W_{E^s}$. In this way, when predicting the CTC path $\\mathbf {\\pi }$, the probability of observing the particular label $w_i \\in V_{src}\\cup ${`-'} at time step $t$, $p(\\pi _t=w_i|\\mathbf {x})$, is computed by normalizing the product of hidden vector $h_t^s$ and the $i$-th vector in $W_{E^s}$:", "paper_id": "1909.07575"}], "cJ/SgerpsU": [{"section": "Experiments ::: Baseline Models and Implementation", "paragraph": "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.", "selected": "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.", "paper_id": "1909.07575"}, {"section": "Experiments ::: Baseline Models and Implementation", "paragraph": "Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.", "selected": "Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.", "paper_id": "1909.07575"}, {"section": "Experiments ::: Baseline Models and Implementation", "paragraph": "Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.", "selected": "Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.", "paper_id": "1909.07575"}]}}
{"idx": "1909.08089.1.1.1", "paper_id": "1909.08089", "title": "Extractive Summarization of Long Documents by Combining Global and Local Context", "abstract": "In this paper, we propose a novel neural single document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.", "context_section_header": "Results and Analysis", "context_paragraph": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate randomization as the statistical significance test method BIBREF32 with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries.", "sentence": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively.", "cited_ids": [], "y": "The performance of all models [in the cited previous work in extractive summarisation] on arXiv and Pubmed is shown in Tables TABREF28 and TABREF29, respectively.", "snippet_surface": "The performance of all models on arXiv and Pubmed is shown in Tables TABREF28 and TABREF29, respectively.", "questions": {"9fepq6gzXg": "What does \"all models\" refer to?"}, "answers": {"9fepq6gzXg": "All models refer to the cited previous work in extractive summarisation."}, "evidence": {"9fepq6gzXg": [{"section": "Models for Comparison", "paragraph": "We perform a systematic comparison with previous work in extractive summarization. For completeness, we also compare with recent neural abstractive approaches. In all the experiments, we use the same train/val/test splitting.", "selected": "previous work in extractive summarization.", "paper_id": "1909.08089"}, {"section": "Models for Comparison", "paragraph": "Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).", "selected": "Oracle: uses the Gold Standard extractive labels, generated based on ROUGE (Sec. 4.2).", "paper_id": "1909.08089"}, {"section": "Results and Analysis", "paragraph": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively. Follow the work BIBREF18 , we use the approximate randomization as the statistical significance test method BIBREF32 with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 ( INLINEFORM0 ). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of traditional and neural approaches is left as future work. Similarly, the neural extractive models also dominate the neural abstractive models on ROUGE-1,2, but these abstractive models tend to have the highest ROUGE-L scores, possibly because they are trained directly on gold standard abstract summaries.", "selected": "Table TABREF28 and Table TABREF29", "paper_id": "1909.08089"}]}}
{"idx": "1909.09067.1.1.1", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Introduction", "context_paragraph": "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).", "sentence": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "cited_ids": [], "y": "The focus of this work is on representing information that is valuable for tasks [that around information that is not inherently linguistic (e.g., text structure, typography, position, or dimensions)] but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "snippet_surface": "The focus of this work is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "questions": {"ZQKqaNzKYS": "What are the tasks that have been ignored?"}, "answers": {"ZQKqaNzKYS": "tasks around Information which is not inherently linguistic, e.g. text structure, typography, position or dimensions"}, "evidence": {"ZQKqaNzKYS": [{"section": "Introduction", "paragraph": "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).", "selected": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "paper_id": "1909.09067"}]}}
{"idx": "1909.09067.1.1.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "sentence": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "cited_ids": [], "y": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element [consisting on information about text structure] in the textstructure layer [which contains information about physical pages including paragraphs and lines].", "snippet_surface": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer.", "questions": {"cDffdIlE1y": "What is the \"textspan\" element?", "S42MJj1YhI": "What is the \"textstructure\" layer?"}, "answers": {"cDffdIlE1y": "Element consisting on information about text structure.", "S42MJj1YhI": "Textstructure is a layer that contains information about physical pages including paragraphs and lines."}, "evidence": {"cDffdIlE1y": [{"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Primary Data", "paragraph": "For the webpages, a static dump of all documents was created. Following this, the documents were manually checked to verify the language. The main content was subsequently extracted, i.e., HTML markup and boilerplate removed using the Beautiful Soup library for Python. Information on text structure (e.g., paragraphs, lines) and typography (e.g., boldface, italics) was retained. Similarly, image information (content, position, and dimensions of an image) was preserved.", "selected": "Information on text structure (e.g., paragraphs, lines) and typography (e.g., boldface, italics) was retained. Similarly, image information (content, position, and dimensions of an image) was preserved.", "paper_id": "1909.09067"}], "S42MJj1YhI": [{"section": "Introduction", "paragraph": "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).", "selected": "pecifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.", "paper_id": "1909.09067"}, {"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "paragraph": "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "selected": "nformation on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer", "paper_id": "1909.09067"}]}}
{"idx": "1909.09067.2.1.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "sentence": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "cited_ids": [], "y": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes of the token layer [which contained information about the tokens and the linking to other annotation layers].", "snippet_surface": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes of the token elements of the tokens layer.", "questions": {"evwWtoQS1N": "What is the role of the \"tokens layer\" or \"token elements\" in the larger model?"}, "answers": {"evwWtoQS1N": "To contain information about the tokens and to be linked to other annotation layers"}, "evidence": {"evwWtoQS1N": [{"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "paragraph": "The following types of annotations were added: text structure, fonts, images, tokens, parts of speech, morphological units, lemmas, sentences, and dependency parses. TCF does not readily accommodate the incorporation of all of these types of information. We therefore extended the format in the following ways:", "selected": "The following types of annotations were added: text structure, fonts, images, tokens, parts of speech, morphological units, lemmas, sentences, and dependency parses. TCF does not readily accommodate the incorporation of all of these types of information.", "paper_id": "1909.09067"}, {"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "paragraph": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "selected": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "paper_id": "1909.09067"}]}}
{"idx": "1909.09067.2.2.2", "paper_id": "1909.09067", "title": "A Corpus for Automatic Readability Assessment and Text Simplification of German", "abstract": "In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification of German. The corpus is compiled from web sources and consists of approximately 211,000 sentences. As a novel contribution, it contains information on text structure, typography, and images, which can be exploited as part of machine learning approaches to readability assessment and text simplification. The focus of this publication is on representing such information as an extension to an existing corpus standard.", "context_section_header": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "context_paragraph": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "sentence": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "cited_ids": [], "y": "A separate fonts layer [for preserving information about font configurations referenced in the tokens layer] was introduced to preserve detailed information on the font configurations referenced in the tokens layer.", "snippet_surface": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer.", "questions": {"rUWJdwRpwY": "How does the \"fonts layer\" fit into the larger system?"}, "answers": {"rUWJdwRpwY": "It is used to preserve information about font configurations referenced in the tokens layer"}, "evidence": {"rUWJdwRpwY": [{"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "paragraph": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "selected": "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", "paper_id": "1909.09067"}, {"section": "Building a Corpus for Automatic Readability Assessment and Automatic Text Simplification of German ::: Secondary Data", "paragraph": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "selected": "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer", "paper_id": "1909.09067"}]}}
{"idx": "1909.09551.1.1.1", "paper_id": "1909.09551", "title": "Natural Language Processing via LDA Topic Model in Recommendation Systems", "abstract": "Today, Internet is one of the widest available media worldwide. Recommendation systems are increasingly being used in various applications such as movie recommendation, mobile recommendation, article recommendation and etc. Collaborative Filtering (CF) and Content-Based (CB) are Well-known techniques for building recommendation systems. Topic modeling based on LDA, is a powerful technique for semantic mining and perform topic extraction. In the past few years, many articles have been published based on LDA technique for building recommendation systems. In this paper, we present taxonomy of recommendation systems and applications based on LDA. In addition, we utilize LDA and Gibbs sampling algorithms to evaluate ISWC and WWW conference publications in computer science. Our study suggest that the recommendation systems based on LDA could be effective in building smart recommendation system in online communities.", "context_section_header": "Discussion, Open Issues and Future Directions", "context_paragraph": "In this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. We succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field. According to our studies, some issues require further research, which can be very effective and attractive for the future.", "sentence": "In this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. We succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field.", "cited_ids": [], "y": "The authors focused on the LDA approaches to recommendation systems and given the importance of research, they have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. They evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. They succeeded in discovering the relationship between LDA topics and paper features [titles and abstracts of the papers] and also obtained the researchers' interest in research field.", "snippet_surface": "The authors focused on the LDA approaches to recommendation systems and given the importance of research, studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. They evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. They succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field.", "questions": {"NvlG54dMhW": "What are paper features?"}, "answers": {"NvlG54dMhW": "Paper features are the titles and abstracts of the papers."}, "evidence": {"NvlG54dMhW": [{"section": "Experiment: Semantic Discovery and researchers behavior analysis ::: Dataset", "paragraph": "We extracted ISWC and WWW conferences publications from DBLP website by only considering conferences for which data was available for years 2013-2017. In total, It should be noted that in these experiments, we considered abstracts and titles from each article. In this paper, we used MALLET (http://mallet.cs.umass.edu/) to implement the inference and obtain the topic models. In addition, our full dataset is available at https://github.com/JeloH/Dataset_DBLP. The most important goal of this experiment is discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags.", "selected": "In total, It should be noted that in these experiments, we considered abstracts and titles from each article", "paper_id": "1909.09551"}]}}
{"idx": "1909.11687.1.1.1", "paper_id": "1909.11687", "title": "Extreme Language Model Compression with Optimal Subwords and Shared Projections", "abstract": "Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT_BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.", "context_section_header": "Introduction", "context_paragraph": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "sentence": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "cited_ids": [], "y": "A significant bottleneck [where a large proportion of the model's memory is used] that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix [an embedding matrix consists of teacher and student WordPiece's which can be used as context to predict a word tokenised by either student/teacher vocabulary], often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over 21% of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these since they require the student and teacher models to share the same vocabulary and output space, which profoundly limits their potential to further reduce model sizes.", "snippet_surface": "A significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these since they require the student and teacher models to share the same vocabulary and output space, which profoundly limits their potential to further reduce model sizes.", "questions": {"cg+I+l/dVT": "How significant is \"a significant bottleneck\"?", "XPoiuvAkcM": "What is an \"embedding matrix\"?"}, "answers": {"cg+I+l/dVT": "Whilst pre-trained deep neural network language models have significant performance; the size of the models make them impractical, particularly on mobile and edge devices. Due to their large input vocabulary and embedding dimensions, the input word embedding matrix uses a large proportion of the model's memory. This bottleneck is significant because there are not many known ways of reducing the model sizes without reducing the effectiveness of knowledge distillation.", "XPoiuvAkcM": "An embedding matrix consists of teacher and student WordPiece's which can be used as context to predict a word tokenised by either student/teacher vocabulary."}, "evidence": {"cg+I+l/dVT": [{"section": "Abstract", "paragraph": "Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT_BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.", "selected": "their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models.", "paper_id": "1909.11687"}, {"section": "Introduction", "paragraph": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "selected": "For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "paper_id": "1909.11687"}, {"section": "Introduction", "paragraph": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "selected": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters.", "paper_id": "1909.11687"}, {"section": "Conclusion", "paragraph": "We proposed two novel ideas to improve the effectiveness of knowledge distillation for BERT, focusing on using a significantly smaller vocabulary, as well as smaller embedding and hidden dimensions for the student BERT language models. Our dual training mechanism encourages implicit alignment of the teacher and student WordPiece embeddings, and shared variable projection allows for the faster and direct layer-wise transfer of knowledge to the student BERT model. Combining the two techniques, we trained a series of highly-compressed 12-layer student BERT models. Experiments on these models, to evaluate both generalized language perspective and four standardized downstream tasks, demonstrate the effectiveness of our proposed methods on both model accuracy and compression efficiency.", "selected": "We proposed two novel ideas to improve the effectiveness of knowledge distillation for BERT, focusing on using a significantly smaller vocabulary, as well as smaller embedding and hidden dimensions for the student BERT language models. Our dual training mechanism encourages implicit alignment of the teacher and student WordPiece embeddings, and shared variable projection allows for the faster and direct layer-wise transfer of knowledge to the student BERT model.", "paper_id": "1909.11687"}], "XPoiuvAkcM": [{"section": "Introduction", "paragraph": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "selected": "For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size.", "paper_id": "1909.11687"}, {"section": "Introduction", "paragraph": "Dual Training: Our teacher and student models have different vocabularies and incompatible tokenizations for the same sequence. To address this during distillation, we feed the teacher model a mix of teacher vocabulary-tokenized and student vocabulary-tokenized words within a single sequence. Coupled with the masked language modeling task, this encourages an implicit alignment of the teacher and student WordPiece embeddings, since the student vocabulary embedding may be used as context to predict a word tokenized by the teacher vocabulary and vice versa.", "selected": "Our teacher and student models have different vocabularies and incompatible tokenizations for the same sequence.", "paper_id": "1909.11687"}, {"section": "Introduction", "paragraph": "Dual Training: Our teacher and student models have different vocabularies and incompatible tokenizations for the same sequence. To address this during distillation, we feed the teacher model a mix of teacher vocabulary-tokenized and student vocabulary-tokenized words within a single sequence. Coupled with the masked language modeling task, this encourages an implicit alignment of the teacher and student WordPiece embeddings, since the student vocabulary embedding may be used as context to predict a word tokenized by the teacher vocabulary and vice versa.", "selected": "Coupled with the masked language modeling task, this encourages an implicit alignment of the teacher and student WordPiece embeddings, since the student vocabulary embedding may be used as context to predict a word tokenized by the teacher vocabulary and vice versa.", "paper_id": "1909.11687"}]}}
{"idx": "1909.13104.3.1.1", "paper_id": "1909.13104", "title": "Attention-based method for categorizing different types of online harassment language", "abstract": "In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter, because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual or racist harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. Previous studies have been focused on collecting data about sexism and racism in very broad terms. However, there is not much study focusing on different types of online harassment alone attracting natural language processing techniques. In this work, we present an attention-based approach for the detection of harassment in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classification specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach.", "context_section_header": "Experiments ::: Evaluation and Results", "context_paragraph": "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "sentence": "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "cited_ids": [], "y": "The authors have [compared eight models which vary in terms of their attention-based approach. The models include: LateStateRNN, AvgRNN, Attention RNN, and MultiAttentionRNN--which includes four additional attention heads] and evaluated their models using the F1 Score, which is the harmonic mean of precision and recall. They have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro, the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "snippet_surface": "The authors have evaluated their models considering the F1 Score, which is the harmonic mean of precision and recall. They have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro, the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", "questions": {"0gja9kbjXo": "What does \"their models\" refer to?"}, "answers": {"0gja9kbjXo": "The author has compared eight models which vary in terms of their attention-based approach (i.e. multi-attention or single attention). The models include: LateStateRNN, AvgRNN, Attention RNN, and MultiAttentionRNN--which includes four additional attention heads."}, "evidence": {"0gja9kbjXo": [{"section": "Experiments ::: Evaluation and Results", "paragraph": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "selected": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "paper_id": "1909.13104"}, {"section": "Experiments ::: Evaluation and Results", "paragraph": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "selected": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models.", "paper_id": "1909.13104"}]}}
{"idx": "1909.13104.7.1.1", "paper_id": "1909.13104", "title": "Attention-based method for categorizing different types of online harassment language", "abstract": "In the era of social media and networking platforms, Twitter has been doomed for abuse and harassment toward users specifically women. Monitoring the contents including sexism and sexual harassment in traditional media is easier than monitoring on the online social media platforms like Twitter, because of the large amount of user generated content in these media. So, the research about the automated detection of content containing sexual or racist harassment is an important issue and could be the basis for removing that content or flagging it for human evaluation. Previous studies have been focused on collecting data about sexism and racism in very broad terms. However, there is not much study focusing on different types of online harassment alone attracting natural language processing techniques. In this work, we present an attention-based approach for the detection of harassment in tweets and the detection of different types of harassment as well. Our approach is based on the Recurrent Neural Networks and particularly we are using a deep, classification specific attention mechanism. Moreover, we present a comparison between different variations of this attention-based approach.", "context_section_header": "Experiments ::: Evaluation and Results", "context_paragraph": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "sentence": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9.", "cited_ids": [], "y": "[For the task of classifying tweets as harassing,] LastStateRNN is presented by the authors as the classic RNN model, in which the last state is passed through an MLP [that obtains the state, $h_{*}$, as a result of the passing of the weighted sum ($h_{sum}$)]. Then the LR Layer evaluates the corresponding probability. In contrast, in the AvgRNN model the authors examine the average vector of all states that come out of the [GRU] cells. The AttentionRNN model is the one provided in BIBREF9.", "snippet_surface": "The authors present LastStateRNN as the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model the authors consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9.", "questions": {"rwcarlvRVQ": "What does the mlp do?", "iEMqPQ/mNY": "What does \"the cells\" refer to?"}, "answers": {"rwcarlvRVQ": "The MLP provides the author with a state: $h_{*}$, following the passing of the weighted sum ($h_{sum}$) through an MLP.", "iEMqPQ/mNY": "The RNN model used by the author is a chain of GRU cells that transform the tokens of a social media tweet to a hidden state in order to help classify a tweet as harassing or not."}, "evidence": {"rwcarlvRVQ": [{"section": "Proposed methodology ::: RNN Model and Attention Mechanism", "paragraph": "We would like to add an attention mechanism similar to the one presented in BIBREF9, so that the LR Layer will consider the weighted sum $h_{sum}$ of all the hidden states instead of $h_{k}$:", "selected": "$h_{sum}$", "paper_id": "1909.13104"}, {"section": "Proposed methodology ::: RNN Model and Attention Mechanism", "paragraph": "Alternatively, we could pass $h_{sum}$ through an MLP with k layers and then the LR layer will estimate the corresponding probability. More formally,", "selected": "$h_{sum}$", "paper_id": "1909.13104"}, {"section": "Proposed methodology ::: RNN Model and Attention Mechanism", "paragraph": "where $h_{*}$ is the state that comes out from the MLP. The weights $\\alpha _{t}$ are produced by an attention mechanism presented in BIBREF9 (see Fig. FIGREF7), which is an MLP with l layers. This attention mechanism differs from most previous ones BIBREF16, BIBREF17, because it is used in a classification setting, where there is no previously generated output sub-sequence to drive the attention. It assigns larger weights $\\alpha _{t}$ to hidden states $h_{t}$ corresponding to positions, where there is more evidence that the tweet should be harassment (or any other specific type of harassment) or not. In our work we are using four attention mechanisms instead of one that is presented in BIBREF9. Particularly, we are using one attention mechanism per category. Another element that differentiates our approach from Pavlopoulos et al. BIBREF9 is that we are using a projection layer for the word embeddings (see Fig. FIGREF2). In the next subsection we describe the Model Architecture of our approach.", "selected": "$h_{*}$", "paper_id": "1909.13104"}, {"section": "Proposed methodology ::: RNN Model and Attention Mechanism", "paragraph": "where $h_{*}$ is the state that comes out from the MLP. The weights $\\alpha _{t}$ are produced by an attention mechanism presented in BIBREF9 (see Fig. FIGREF7), which is an MLP with l layers. This attention mechanism differs from most previous ones BIBREF16, BIBREF17, because it is used in a classification setting, where there is no previously generated output sub-sequence to drive the attention. It assigns larger weights $\\alpha _{t}$ to hidden states $h_{t}$ corresponding to positions, where there is more evidence that the tweet should be harassment (or any other specific type of harassment) or not. In our work we are using four attention mechanisms instead of one that is presented in BIBREF9. Particularly, we are using one attention mechanism per category. Another element that differentiates our approach from Pavlopoulos et al. BIBREF9 is that we are using a projection layer for the word embeddings (see Fig. FIGREF2). In the next subsection we describe the Model Architecture of our approach.", "selected": "where $h_{*}$ is the state that comes out from the MLP.", "paper_id": "1909.13104"}], "iEMqPQ/mNY": [{"section": "Proposed methodology ::: RNN Model and Attention Mechanism", "paragraph": "We are presenting an attention-based approach for the problem of the harassment detection in tweets. In this section, we describe the basic approach of our work. We are using RNN models because of their ability to deal with sequence information. The RNN model is a chain of GRU cells BIBREF15 that transforms the tokens $w_{1}, w_{2},..., w_{k}$ of each tweet to the hidden states $h_{1}, h_{2},..., h_{k}$, followed by an LR Layer that uses $h_{k}$ to classify the tweet as harassment or non-harassment (similarly for the other categories). Given the vocabulary V and a matrix E $\\in $ $R^{d \\times \\vert V \\vert }$ containing d-dimensional word embeddings, an initial $h_{0}$ and a tweet $w = <w_{1},.., w_{k}>$, the RNN computes $h_{1}, h_{2},..., h_{k}$, with $h_{t} \\in R^{m}$, as follows:", "selected": "The RNN model is a chain of GRU cells BIBREF15 that transforms the tokens $w_{1}, w_{2},..., w_{k}$ of each tweet to the hidden states $h_{1}, h_{2},..., h_{k}$, followed by an LR Layer that uses $h_{k}$ to classify the tweet as harassment or non-harassment (similarly for the other categories).", "paper_id": "1909.13104"}, {"section": "Experiments ::: Evaluation and Results", "paragraph": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "selected": "in the AvgRNN model we consider the average vector of all states that come out of the cells", "paper_id": "1909.13104"}]}}
{"idx": "1910.01863.1.1.1", "paper_id": "1910.01863", "title": "Template-free Data-to-Text Generation of Finnish Sports News", "abstract": "News articles such as sports game reports are often thought to closely follow the underlying game statistics, but in practice they contain a notable amount of background knowledge, interpretation, insight into the game, and quotes that are not present in the official statistics. This poses a challenge for automated data-to-text news generation with real-world news corpora as training data. We report on the development of a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product. The new dataset and system source code are available for research purposes at this https URL.", "context_section_header": "Text Generation ::: Baseline Experiments on the E2E Dataset", "context_paragraph": "In Table TABREF15 we measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers. Our generation system is compared to the official shared task baseline system, TGen BIBREF24, as well as to the top performing participant system on each score (ST top). Our system outperforms the TGen baseline on 3 out of 5 metrics (BLEU, METEOR and ROUGE-L), which is on par with the official shared task results, where not a single one participant system was able to surpass the baseline on all five metrics. On two metrics, BLEU and METEOR, our system outperforms the best shared task participants.", "sentence": "In Table TABREF15 we measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers.", "cited_ids": [], "y": "The authors measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data [(i.e., the published benchmark of end-to-end language generation in spoken dialogue systems)] using the evaluation script provided by the organizers.", "snippet_surface": "The authors measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers.", "questions": {"Eb1hJ2wy8P": "What is the \"2018 E2E NLG Challenge\"?"}, "answers": {"Eb1hJ2wy8P": "The 2018 E2E NLG challenge is a known date with published baselines of end-to-end language generation in spoken dialogue systems."}, "evidence": {"Eb1hJ2wy8P": [{"section": "Text Generation ::: Baseline Experiments on the E2E Dataset", "paragraph": "To demonstrate the performance of our generation model architecture, we report results on a known dataset with published baselines, namely the E2E NLG Challenge BIBREF2 on end-to-end natural language generation in spoken dialogue systems. The task is to produce a natural language description of a restaurant based on a given meaning representation (MR)\u2014an unordered set of attributes and their values. The attributes included, among others, the restaurant name, area, food type and rating. We represent the given MR as a sequence of tokens where each attribute value is embedded into XML-style beginning and end attribute markers, and the order of attributes is kept fixed across the whole dataset. The target output is a sequence of tokens. We do not apply any explicit delexicalization steps.", "selected": "To demonstrate the performance of our generation model architecture, we report results on a known dataset with published baselines, namely the E2E NLG Challenge BIBREF2 on end-to-end natural language generation in spoken dialogue systems.", "paper_id": "1910.01863"}]}}
{"idx": "1910.06592.1.1.1", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Experiments and Results", "context_paragraph": "Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "sentence": "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1.", "cited_ids": [], "y": "The authors build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts [which have been labelled as containing one of the main fake news types (clickbait, propaganda, satire and hoax)], they rely on a list of 180 Twitter accounts from BIBREF1.", "snippet_surface": "The authors build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, they rely on a list of 180 Twitter accounts from BIBREF1.", "questions": {"E2HE26bpkL": "What is a \"non-factual\" account?"}, "answers": {"E2HE26bpkL": "These are twitter accounts that were labelled as containing one of the main fake news types (clickbait, propaganda, satire and hoax). "}, "evidence": {"E2HE26bpkL": [{"section": "Experiments and Results", "paragraph": "Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "selected": "This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax).", "paper_id": "1910.06592"}]}}
{"idx": "1910.06592.11.2.2", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Experiments and Results", "context_paragraph": "LR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here, we do not aggregate over tweets and thus view each tweet independently.", "sentence": "LR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier.", "cited_ids": [], "y": "The authors extract all the features from each tweet [collected from the factual accounts being studied] and feed them into a LR classifier.", "snippet_surface": "The authors extract all the features from each tweet and feed them into a LR classifier.", "questions": {"D6/K1/32H8": "What does \"each tweet\" refer to?"}, "answers": {"D6/K1/32H8": "This refers to the tweets collected from the factual accounts being studied."}, "evidence": {"D6/K1/32H8": [{"section": "Experiments and Results", "paragraph": "Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "selected": "For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "paper_id": "1910.06592"}, {"section": "Experiments and Results", "paragraph": "Data. We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "selected": "On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", "paper_id": "1910.06592"}]}}
{"idx": "1910.06592.4.1.1", "paper_id": "1910.06592", "title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.", "context_section_header": "Methodology", "context_paragraph": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", "sentence": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "cited_ids": [], "y": "Given a news Twitter account, the authors read its tweets. They then sort the tweets by the posting date ascending and split them into $N$ [approximately equally size] chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "snippet_surface": "The authors read the tweets from the timeline of a given news Twitter account. Then, they sorted the tweets by the posting date in ascending way and split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account.", "questions": {"eXjmLyFHFR": "What does \"the timeline\" refer to?", "80dD3Ex7Uk": "How long is each chunk?"}, "answers": {"eXjmLyFHFR": "The timeline of a twitter account. Timeline is a twitter term and in this context, it means tweets posted by the given news account.", "80dD3Ex7Uk": "The size of each chunk depends on the total amount of tweets in the user's timeline. It's going to be the number of tweets divided by N, assuming divisible by N, since there are N chunks. It's not specified how non-divisible case is handled but they can probably just ignore the remainder tweets."}, "evidence": {"eXjmLyFHFR": [{"section": "Methodology", "paragraph": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", "selected": "Given a news Twitter account, we read its tweets from the account's timeline", "paper_id": "1910.06592"}], "80dD3Ex7Uk": [{"section": "Methodology", "paragraph": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", "selected": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks.", "paper_id": "1910.06592"}, {"section": "Methodology", "paragraph": "Input Representation. Let $t$ be a Twitter account that contains $m$ tweets. These tweets are sorted by date and split into a sequence of chunks $ck = \\langle ck_1, \\ldots , ck_n \\rangle $, where each $ck_i$ contains $s$ tweets. Each tweet in $ck_i$ is represented by a vector $v \\in {\\rm I\\!R}^d$ , where $v$ is the concatenation of a set of features' vectors, that is $v = \\langle f_1, \\ldots , f_n \\rangle $. Each feature vector $f_i$ is built by counting the presence of tweet's words in a set of lexical lists. The final representation of the tweet is built by averaging the single word vectors.", "selected": "Let $t$ be a Twitter account that contains $m$ tweets. These tweets are sorted by date and split into a sequence of chunks $ck = \\langle ck_1, \\ldots , ck_n \\rangle $, where each $ck_i$ contains $s$ tweets.", "paper_id": "1910.06592"}]}}
{"idx": "1910.09982.2.1.1", "paper_id": "1910.09982", "title": "Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection", "abstract": "We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at this http URL.", "context_section_header": "Data", "context_paragraph": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators. More details about the data collection and the annotation, as well as statistics about the corpus can be found in BIBREF10, where an earlier version of the corpus is described, which includes 450 news articles. We further annotated 47 additional articles for the purpose of the shared task using the same protocol and the same annotators.", "sentence": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "cited_ids": [], "y": "The input for [the Fragment level classification task, FLC, and the Sentence level classification task, SLC,] consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "snippet_surface": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.", "questions": {"os3PvXRvBE": "What does \"both tasks\" refer to?"}, "answers": {"os3PvXRvBE": "Both tasks refer to the Fragment level classification task, FLC, and the Sentence level classification task, SLC."}, "evidence": {"os3PvXRvBE": [{"section": "Abstract", "paragraph": "We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at this http URL.", "selected": "FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task).", "paper_id": "1910.09982"}, {"section": "Abstract", "paragraph": "We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at this http URL.", "selected": "SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda.", "paper_id": "1910.09982"}]}}
{"idx": "1911.03059.1.1.1", "paper_id": "1911.03059", "title": "A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification", "abstract": "QA classification system maps questions asked by humans to an appropriate answer category. A sound question classification (QC) system model is the pre-requisite of a sound QA system. This work demonstrates phases of assembling a QA type classification model. We present a comprehensive comparison (performance and computational complexity) among some machine learning based approaches used in QC for Bengali language.", "context_section_header": "Question Collection and Categories", "context_paragraph": "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "sentence": "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "cited_ids": [], "y": "The authors have collected a total of 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. [The questions were classified into a two-layer taxonomy with six coarse classes and fifty finer classes.] The corpus contains the questions and the classes each question belongs to.", "snippet_surface": "The authors have collected a total of 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "questions": {"ojRePy1Xpx": "How are questions classified?"}, "answers": {"ojRePy1Xpx": "The questions are classified into a two-layer taxonomy with six coarse classes (numeric, location, entity, description, human, and abbreviation), and fifty finer classes."}, "evidence": {"ojRePy1Xpx": [{"section": "Question Collection and Categories", "paragraph": "The set of question categories is known as question taxonomy BIBREF0. We have used two layer taxonomy which was proposed by Xin Li, Dan Roth BIBREF24. This two layer taxonomy is made up of two classes which are Coarse Class and Finer Class. There are six coarse classes such as Numeric, Location, Entity, Description, Human and Abbreviation and fifty finer classes such as city, state, mountain, distance, count, definition, group, expression, substance, creative, vehicle etc as shown in the Table I BIBREF0. A coarse-grained description of a system denotes large components while a fine-grained description denotes smaller sub-components of which the larger ones are composed of.", "selected": "The set of question categories is known as question taxonomy BIBREF0. We have used two layer taxonomy which was proposed by Xin Li, Dan Roth BIBREF24. This two layer taxonomy is made up of two classes which are Coarse Class and Finer Class. There are six coarse classes such as Numeric, Location, Entity, Description, Human and Abbreviation and fifty finer classes such as city, state, mountain, distance, count, definition, group, expression, substance, creative, vehicle etc as shown in the Table I BIBREF0. A coarse-grained description of a system denotes large components while a fine-grained description denotes smaller sub-components of which the larger ones are composed of.", "paper_id": "1911.03059"}]}}
{"idx": "1911.03343.1.1.1", "paper_id": "1911.03343", "title": "Negated LAMA: Birds cannot fly", "abstract": "Pretrained language models have achieved remarkable improvements in a broad range of natural language processing tasks, including question answering (QA). To analyze pretrained language model performance on QA, we extend the LAMA (Petroni et al., 2019) evaluation framework by a component that is focused on negation. We find that pretrained language models are equally prone to generate facts (\"birds can fly\") and their negation (\"birds cannot fly\"). This casts doubt on the claim that pretrained language models have adequately learned factual knowledge.", "context_section_header": "Introduction", "context_paragraph": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d). In our experiments, we query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions in terms of rank correlation and overlap of top predictions. We find that the predicted filler words often have high overlap. Thus, negating a cloze statement does not change the predictions in many cases \u2013 but of course it should as our example \u201cbirds can fly\u201d vs. \u201cbirds cannot fly\u201d shows. We identify and analyze a subset of cloze statements where predictions are different. We find that BERT handles negation best among pretrained language models, but it still fails badly on most negated statements.", "sentence": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "cited_ids": [], "y": "The authors analyze the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, they introduce the negated LAMA [LAnguage Model Analysis] dataset. They construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statements (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "snippet_surface": "The authors analyze the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, they introduce the negated LAMA dataset. They construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statements (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "questions": {"bRqiBbyQmU": "What does LAMA stand for?"}, "answers": {"bRqiBbyQmU": "LAnguage Model Analysis"}, "evidence": {"bRqiBbyQmU": [{"section": "Introduction", "paragraph": "Recently, BIBREF0 introduced LAMA (LAnguage Model Analysis) to investigate to what extent pretrained language models have the capacity to recall factual knowledge without the use of fine-tuning. The training objective of pretrained language models is to predict masked tokens in a sequence. With this \u201cfill-in-the-blank\u201d scheme, question answering tasks can be reformulated as cloze statements. For example, \u201cWho developed the theory of relativity?\u201d is reformulated as \u201cThe theory of relativity was developed by [MASK].\u201d. This setup allows for unsupervised open domain question answering. BIBREF0 find that, on this task, pretrained language models outperform supervised baselines using traditional knowledge bases with access to oracle knowledge.", "selected": "Recently, BIBREF0 introduced LAMA (LAnguage Model Analysis)", "paper_id": "1911.03343"}]}}
{"idx": "1911.06964.1.1.1", "paper_id": "1911.06964", "title": "Learning Autocomplete Systems as a Communication Game", "abstract": "We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (interpretability). We propose an unsupervised approach which tackles all three desiderata by constraining the communication scheme to keywords extracted from a source sentence for interpretability and optimizing the efficiency-accuracy tradeoff. Our experiments show that this approach results in an autocomplete system that is 52% more accurate at a given efficiency level compared to baselines, is robust to user variations, and saves time by nearly 50% compared to typing full sentences.", "context_section_header": "Experiments", "context_paragraph": "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "sentence": "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "cited_ids": [], "y": "The authors evaluate their approach [learning encoder-decoder pairs] by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6. They quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "snippet_surface": "The authors evaluate their approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). They quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", "questions": {"eYs6eZLMg3": "What does \"their approach\" refer to?"}, "answers": {"eYs6eZLMg3": "Their approach refers to learning encoder-decoder pairs"}, "evidence": {"eYs6eZLMg3": [{"section": "Introduction", "paragraph": "In this work, we propose a simple, unsupervised approach to an autocomplete system that is efficient, accurate, and interpretable. For interpretability, we restrict keywords to be subsequences of their source sentences based on the intuition that humans can infer most of the original meaning from a few keywords. We then apply multi-objective optimization approaches to directly control and achieve desirable tradeoffs between efficiency and accuracy.", "selected": "n this work, we propose a simple, unsupervised approach to an autocomplete system that is efficient, accurate, and interpretable. For interpretability, we restrict keywords to be subsequences of their source sentences based on the intuition that humans can infer most of the original meaning from a few keywords. We then apply multi-objective optimization approaches to directly control and achieve desirable tradeoffs between efficiency and accuracy.", "paper_id": "1911.06964"}, {"section": "Approach ::: Modeling with autoencoders.", "paragraph": "To learn communication schemes without supervision, we model the cooperative communication between a user and system through an encoder-decoder framework. Concretely, we model the user's encoding strategy $q_{\\alpha }(z\\mid x)$ with an encoder which encodes the target sentence $x$ into the keywords $z$ by keeping a subset of the tokens. This stochastic encoder $q_{\\alpha }(z\\mid x)$ is defined by a model which returns the probability of each token retained in the final subsequence $z$. Then, we sample from Bernoulli distributions according to these probabilities to either keep or drop the tokens independently (see Appendix for an example).", "selected": "To learn communication schemes without supervision, we model the cooperative communication between a user and system through an encoder-decoder framework.", "paper_id": "1911.06964"}, {"section": "Approach ::: Multi-objective optimization.", "paragraph": "Our goal now is to learn encoder-decoder pairs which optimally balance the communication cost and reconstruction loss. The simplest approach to balancing efficiency and accuracy is to weight $\\mathrm {cost}(x, \\alpha )$ and $\\mathrm {loss}(x, \\alpha , \\beta )$ linearly using a weight $\\lambda $ as follows,", "selected": "Our goal now is to learn encoder-decoder pairs which optimally balance the communication cost and reconstruction loss. T", "paper_id": "1911.06964"}]}}
{"idx": "1911.07555.1.2.1", "paper_id": "1911.07555", "title": "Short Text Language Identification for Under Resourced Languages", "abstract": "The paper presents a hierarchical naive Bayesian and lexicon based classifier for short text language identification (LID) useful for under resourced languages. The algorithm is evaluated on short pieces of text for the 11 official South African languages some of which are similar languages. The algorithm is compared to recent approaches using test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) shared tasks' datasets. Remaining research opportunities and pressing concerns in evaluating and comparing LID approaches are also discussed.", "context_section_header": "Related Works", "context_paragraph": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "sentence": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "cited_ids": [], "y": "Existing NLP datasets, models and services are available for South African languages. These include a [language identification (LID) algorithm that identifies the language a particular document is written in] and uses a character level n-gram language model. Multiple [(6)] papers have shown that 'shallow' naive Bayes classifiers and similar models work very well for doing LID. The DSL 2017 paper gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach used an SVM with character n-gram, parts of speech tag features and some other [features beyond character n-grams and parts of speech tags]. The winning approach for", "snippet_surface": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1 gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for", "questions": {"GM2Wz1AWkc": "What is a \"lid algorithm\"?", "DA/srhS9Rd": "How many papers is \"multiple papers\"?"}, "answers": {"GM2Wz1AWkc": "LID stands for \"language identification\", and LID algorithms identify the language a particular document is written in.", "DA/srhS9Rd": "In this case, multiple papers refers to 6 papers (citations 1, 8, 12, 13, 14, 16,  and 17). (However, this is probably more a set of examples than a complete listing --- other papers may also show that these models work for LID)."}, "evidence": {"GM2Wz1AWkc": [{"section": "Introduction", "paragraph": "Accurate language identification (LID) is the first step in many natural language processing and machine comprehension pipelines. If the language of a piece of text is known then the appropriate downstream models like parts of speech taggers and language models can be applied as required.", "selected": "language identification (LID) is the first step in many natural language processing and machine comprehension pipelines", "paper_id": "1911.07555"}, {"section": "Related Works", "paragraph": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "selected": "LID algorithm BIBREF11 that uses a character level n-gram language model", "paper_id": "1911.07555"}], "DA/srhS9Rd": [{"section": "Related Works", "paragraph": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "selected": "BIBREF12, BIBREF8, BIBREF13, BIBREF14", "paper_id": "1911.07555"}, {"section": "Related Works", "paragraph": "Existing NLP datasets, models and services BIBREF10 are available for South African languages. These include an LID algorithm BIBREF11 that uses a character level n-gram language model. Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID .", "selected": "BIBREF16", "paper_id": "1911.07555"}, {"section": "Methodology", "paragraph": "The stacked classifier is also tested against the results reported for four other algorithms BIBREF16, BIBREF26, BIBREF24, BIBREF15. All the comparisons are done using the NCHLT BIBREF7, DSL 2015 BIBREF19 and DSL 2017 BIBREF1 datasets discussed in Section SECREF2.", "selected": "BIBREF16", "paper_id": "1911.07555"}]}}
{"idx": "1911.07555.1.2.3", "paper_id": "1911.07555", "title": "Short Text Language Identification for Under Resourced Languages", "abstract": "The paper presents a hierarchical naive Bayesian and lexicon based classifier for short text language identification (LID) useful for under resourced languages. The algorithm is evaluated on short pieces of text for the 11 official South African languages some of which are similar languages. The algorithm is compared to recent approaches using test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) shared tasks' datasets. Remaining research opportunities and pressing concerns in evaluating and comparing LID approaches are also discussed.", "context_section_header": "Related Works", "context_paragraph": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", "sentence": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task.", "cited_ids": [], "y": "Researchers have investigated deeper LID [language identification] models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% accuracy in the DSL 2015 shared task.", "snippet_surface": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% accuracy in the DSL 2015 shared task.", "questions": {"t/ylqSknIk": "What does \"deeper lid models\" refer to?"}, "answers": {"t/ylqSknIk": "Deeper language identification (LID) models refers to two networks: 1) bidirectional recurrent neural networks; 2) ensembles of recurrent neural networks. Deeper LID models' features include character and word n-grams."}, "evidence": {"t/ylqSknIk": [{"section": "Related Works", "paragraph": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", "selected": "deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks", "paper_id": "1911.07555"}, {"section": "Related Works", "paragraph": "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", "selected": "In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data.", "paper_id": "1911.07555"}]}}
{"idx": "1912.00667.2.2.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Experiments and Results ::: Experimental Setup", "context_paragraph": "Comparison Methods. To demonstrate the generality of our approach on different event detection models, we consider Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models. As the goal of our experiments is to demonstrate the effectiveness of our approach as a new model training technique, we use these widely used models. Also, we note that in our case other neural network models with more complex network architectures for event detection, such as the bi-directional LSTM BIBREF17, turn out to be less effective than a simple feedforward network. For both LR and MLP, we evaluate our proposed human-AI loop approach for keyword discovery and expectation estimation by comparing against the weakly supervised learning method proposed by BIBREF1 (BIBREF1) and BIBREF17 (BIBREF17) where only one initial keyword is used with an expectation estimated by an individual expert.", "sentence": "To demonstrate the generality of our approach on different event detection models, we consider Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "cited_ids": [], "y": "The authors demonstrate the generality of a [ human-AI loop] approach [to discover informative keywords and estimate their expectations ] by considering Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "snippet_surface": "The authors demonstrate the generality of their approach on different event detection models by considering Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2 as the target models.", "questions": {"eYs6eZLMg3": "What does \"their approach\" refer to?"}, "answers": {"eYs6eZLMg3": "The authors showcase a human-AI loop approach that discovers informative keywords and estimates their expectations reliably. This is achieved by multiple iterations to obtain keyword specific expectations (obtain more information)."}, "evidence": {"eYs6eZLMg3": [{"section": "Abstract", "paragraph": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "selected": "This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "paper_id": "1912.00667"}, {"section": "Introduction", "paragraph": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10. An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks. Such an advantage is particularly useful for event detection using complex models, e.g., deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12. An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13. In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training. To address this problem, we introduce a unified probabilistic model that seamlessly integrates expectation inference and model training, thereby allowing the former to benefit from the latter while resolving the inter-dependency between the two.", "selected": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance.", "paper_id": "1912.00667"}]}}
{"idx": "1912.00667.4.1.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Related Work", "context_paragraph": "Human-in-the-Loop Approaches. Our work extends weakly supervised learning methods by involving humans in the loop BIBREF13. Existing human-in-the-loop approaches mainly leverage crowds to label individual data instances BIBREF9, BIBREF10 or to debug the training data BIBREF30, BIBREF31 or components BIBREF32, BIBREF33, BIBREF34 of a machine learning system. Unlike these works, we leverage crowd workers to label sampled microposts in order to obtain keyword-specific expectations, which can then be generalized to help classify microposts containing the same keyword, thus amplifying the utility of the crowd. Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability. In contrast, our approach directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model.", "sentence": "Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "cited_ids": [], "y": "The authors' work [presents a new approach to event detection on microblogging platform using a human-AI loop to find and evaluate keywords.] This work is further connected to the topic of interpretability and transparency of machine learning models for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "snippet_surface": "The authors' work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.", "questions": {"x866fuZvxF": "What does \"authors' work\" refer to?"}, "answers": {"x866fuZvxF": "The author's work refers to a new approach to event detection on microblogging platforms, such as Twitter. The authors introduce a Human-AI loop approach that jointly discovers informative keywords for model training while estimating their expectation. The approach iteratively leverages the crowd to estimate both keyword-specific expectation and the disagreement between the crowd and the model to discover new keywords that are most beneficial for model training."}, "evidence": {"x866fuZvxF": [{"section": "Introduction", "paragraph": "Event detection on microblogging platforms such as Twitter aims to detect events preemptively. A main task in event detection is detecting events of predetermined types BIBREF0, such as concerts or controversial events based on microposts matching specific event descriptions. This task has extensive applications ranging from cyber security BIBREF1, BIBREF2 to political elections BIBREF3 or public health BIBREF4, BIBREF5. Due to the high ambiguity and inconsistency of the terms used in microposts, event detection is generally performed though statistical machine learning models, which require a labeled dataset for model training. Data labeling is, however, a long, laborious, and usually costly process. For the case of micropost classification, though positive labels can be collected (e.g., using specific hashtags, or event-related date-time information), there is no straightforward way to generate negative labels useful for model training. To tackle this lack of negative labels and the significant manual efforts in data labeling, BIBREF1 (BIBREF1, BIBREF3) introduced a weak supervision based learning approach, which uses only positively labeled data, accompanied by unlabeled examples by filtering microposts that contain a certain keyword indicative of the event type under consideration (e.g., `hack' for cyber security). Another key technique in this context is expectation regularization BIBREF6, BIBREF7, BIBREF1. Here, the estimated proportion of relevant microposts in an unlabeled dataset containing a keyword is given as a keyword-specific expectation. This expectation is used in the regularization term of the model's objective function to constrain the posterior distribution of the model predictions. By doing so, the model is trained with an expectation on its prediction for microposts that contain the keyword. Such a method, however, suffers from two key problems:", "selected": "Event detection on microblogging platforms such as Twitter aims to detect events preemptively.", "paper_id": "1912.00667"}, {"section": "Introduction", "paragraph": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10. An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks. Such an advantage is particularly useful for event detection using complex models, e.g., deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12. An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13. In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training. To address this problem, we introduce a unified probabilistic model that seamlessly integrates expectation inference and model training, thereby allowing the former to benefit from the latter while resolving the inter-dependency between the two.", "selected": "More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement.", "paper_id": "1912.00667"}, {"section": "Introduction", "paragraph": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10. An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks. Such an advantage is particularly useful for event detection using complex models, e.g., deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12. An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13. In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training. To address this problem, we introduce a unified probabilistic model that seamlessly integrates expectation inference and model training, thereby allowing the former to benefit from the latter while resolving the inter-dependency between the two.", "selected": "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords.", "paper_id": "1912.00667"}, {"section": "Introduction", "paragraph": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10. An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks. Such an advantage is particularly useful for event detection using complex models, e.g., deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12. An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13. In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training. To address this problem, we introduce a unified probabilistic model that seamlessly integrates expectation inference and model training, thereby allowing the former to benefit from the latter while resolving the inter-dependency between the two.", "selected": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably.", "paper_id": "1912.00667"}]}}
{"idx": "1912.00667.5.2.1", "paper_id": "1912.00667", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "context_section_header": "Experiments and Results ::: Experimental Setup", "context_paragraph": "Evaluation. Following BIBREF1 (BIBREF1) and BIBREF3 (BIBREF3), we use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of our proposed approach. We note that due to the imbalance in our datasets (20% positive microposts in CyberAttack and 27% in PoliticianDeath), accuracy is dominated by negative examples; AUC, in comparison, better characterizes the discriminative power of the model.", "sentence": "Following BIBREF1 (BIBREF1) and BIBREF3 (BIBREF3), we use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of our proposed approach.", "cited_ids": [], "y": "[The authors propose a Human-AI loop to reliably discover informative keywords]. Following BIBREF1 and BIBREF3, the authors use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of their approach.", "snippet_surface": "Following BIBREF1 and BIBREF3, the authors use accuracy and area under the precision-recall curve (AUC) metrics to measure the performance of their proposed approach.", "questions": {"rZs0Wib4vC": "What does \"their proposed approach\" refer to?"}, "answers": {"rZs0Wib4vC": "The authors propose a Human-AI loop to reliably discover informative keywords whilst also estimating their expectations."}, "evidence": {"rZs0Wib4vC": [{"section": "Abstract", "paragraph": "Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant micro-posts containing the keyword -- referred to as the expectation of the distribution -- and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.", "selected": "This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation.", "paper_id": "1912.00667"}, {"section": "Introduction", "paragraph": "To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10. An additional advantage of our approach is that by obtaining new keywords that improve model performance over time, we are able to gain insight into how the model learns for specific event detection tasks. Such an advantage is particularly useful for event detection using complex models, e.g., deep neural networks, which are intrinsically hard to understand BIBREF11, BIBREF12. An additional challenge in involving crowd workers is that their contributions are not fully reliable BIBREF13. In the crowdsourcing literature, this problem is usually tackled with probabilistic latent variable models BIBREF14, BIBREF15, BIBREF16, which are used to perform truth inference by aggregating a redundant set of crowd contributions. Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training. To address this problem, we introduce a unified probabilistic model that seamlessly integrates expectation inference and model training, thereby allowing the former to benefit from the latter while resolving the inter-dependency between the two.", "selected": "we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably.", "paper_id": "1912.00667"}]}}
{"idx": "1912.01214.1.1.2", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Main Results", "context_paragraph": "Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.", "sentence": "The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.", "cited_ids": [], "y": "The authors' results show that their approaches [using one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages] consistently outperform other approaches across languages and datasets. The authors surpass pivoting [i.e. translating a source language into the pivot language which is then translated to the target language], which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat.", "snippet_surface": "The authors' results show that their approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.", "questions": {"DmNzyngSq7": "What are the authors' approaches?", "lH4sl/DjuH": "What is \"pivoting\"?"}, "answers": {"DmNzyngSq7": "One monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages.", "lH4sl/DjuH": "translating a source language into the pivot language which is then translated to the target language BIBREF4, BIBREF5, BIBREF12,"}, "evidence": {"DmNzyngSq7": [{"section": "Abstract", "paragraph": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "selected": "one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages.", "paper_id": "1912.01214"}, {"section": "Abstract", "paragraph": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "selected": "learning approach based on cross-lingual pre-training", "paper_id": "1912.01214"}, {"section": "Experiments ::: Setup ::: Experimental Details.", "paragraph": "We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\text{attn}\\_\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\text{lr}=0.0001$, $t_{\\text{warm}\\_\\text{up}}=4000$ and $\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\alpha =1.0$.", "selected": "traditional transfer learning, pivot-based method and multilingual NMT as our baselines", "paper_id": "1912.01214"}], "lH4sl/DjuH": [{"section": "Related Work", "paragraph": "Pivot-based Method is a common strategy to obtain a source$\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.", "selected": "translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12,", "paper_id": "1912.01214"}]}}
{"idx": "1912.01214.2.1.2", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Main Results ::: Results on MultiUN Dataset.", "context_paragraph": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\rightarrow $ Ar and Es $\\rightarrow $ Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.", "sentence": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\rightarrow $ Ar and Es $\\rightarrow $ Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "cited_ids": [], "y": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of [the public dataset] MultiUN, MLM+BRLM-SA still achieves better performances on [Spanish (Es)] \u2192[Arabic] and Es\u2192[Russian] than strong pivoting$_{\\rm m}$, which uses MNMT [multilingual neural machine translation] to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "snippet_surface": "Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es\u2192Ar and Es\u2192Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.", "questions": {"CLcGxZ5drb": "What is \"MultiUN\"?", "9zinj5GzdA": "What are \"Es\", \"Ar\" and \"Ru\"?", "RrwYHIY37E": "What does \"MNMT\" mean?"}, "answers": {"CLcGxZ5drb": "a public dataset", "9zinj5GzdA": "They are languages.\n (Ar) = Arabic\n(Es) = Spanish\n(Ru) = Russian", "RrwYHIY37E": "A system consisting of one encoder-decoder model for multilingual neural machine translation"}, "evidence": {"CLcGxZ5drb": [{"section": "Experiments ::: Setup", "paragraph": "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.", "selected": "baselines on two public datatsets, Europarl BIBREF31 and MultiUN", "paper_id": "1912.01214"}], "9zinj5GzdA": [{"section": "Experiments ::: Setup ::: Datasets.", "paragraph": "For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.", "selected": "The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation.", "paper_id": "1912.01214"}], "RrwYHIY37E": [{"section": "Experiments ::: Main Results ::: Results on MultiUN Dataset.", "paragraph": "Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.", "selected": "systems consisting of one encoder-decoder model for all zero-shot translation", "paper_id": "1912.01214"}]}}
{"idx": "1912.01214.4.2.1", "paper_id": "1912.01214", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation", "abstract": "Transfer learning between different language pairs has shown its effectiveness for Neural Machine Translation (NMT) in low-resource scenario. However, existing transfer methods involving a common target language are far from success in the extreme scenario of zero-shot translation, due to the language space mismatch problem between transferor (the parent model) and transferee (the child model) on the source side. To address this challenge, we propose an effective transfer learning approach based on cross-lingual pre-training. Our key idea is to make all source languages share the same feature space and thus enable a smooth transition for zero-shot translation. To this end, we introduce one monolingual pre-training method and two bilingual pre-training methods to obtain a universal encoder for different languages. Once the universal encoder is constructed, the parent model built on such encoder is trained with large-scale annotated data and then directly applied in zero-shot translation scenario. Experiments on two public datasets show that our approach significantly outperforms strong pivot-based baseline and various multilingual NMT approaches.", "context_section_header": "Experiments ::: Setup ::: Datasets.", "context_paragraph": "The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\rightarrow $Es and De$\\rightarrow $Fr. For distant language pair Ro$\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.", "sentence": "For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\rightarrow $Es and De$\\rightarrow $Fr. For distant language pair Ro$\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets.", "cited_ids": [], "y": "For the Europarl corpus, the authors evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language [which facilitates translation between two languages that don't have a lot of training data], its left side is the source language, and its right side is the target language. They remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. They use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For the distant language pair Ro\u2192De, they extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there are no official validation and test sets.", "snippet_surface": "For the Europarl corpus, the authors evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. They remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. They use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For the distant language pair Ro\u2192De, they extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there are no official validation and test sets.", "questions": {"1WQDX2x4W7": "What is the \"pivot language\"?"}, "answers": {"1WQDX2x4W7": "A pivot language facilitates translation between two languages that don't have a lot of training data for source-to-target translation, since more of such data exists for source-to-pivot and pivot-to-target translation."}, "evidence": {"1WQDX2x4W7": [{"section": "Experiments ::: Setup ::: Datasets.", "paragraph": "The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\rightarrow $Es and De$\\rightarrow $Fr. For distant language pair Ro$\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.", "selected": "For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language", "paper_id": "1912.01214"}]}}
{"idx": "1912.03804.1.1.1", "paper_id": "1912.03804", "title": "Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group", "abstract": "Online propaganda is central to the recruitment strategies of extremist groups and in recent years these efforts have increasingly extended to women. To investigate ISIS' approach to targeting women in their online propaganda and uncover implications for counterterrorism, we rely on text mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar", "context_section_header": "Related Work", "context_paragraph": "With their ability to operate freely on social media now curtailed, ISIS recruiters and propagandists increased their attentiveness to another longstanding tool\u2013English language online magazines targeting western audiences. Al Hayat, the media wing of ISIS, published multiple online magazines in different languages including English. The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017. The content of these magazines provides a valuable but underutilized resource for understanding ISIS strategies and how they appeal to recruits, specifically English-speaking audiences. They also provide a way to compare ISIS' approach with other radical groups. Ingram compared Dabiq contents with Inspire (Al Qaeda publication) and suggested that Al Qaeda heavily emphasized identity-choice, while ISIS' messages were more balanced between identity-choice and rational-choice BIBREF7. In another research paper, Wignell et al. BIBREF8 compared Dabiq and Rumiah by examining their style and what both magazine messages emphasized. Despite the volume of research on these magazines, only a few researchers used lexical analysis and mostly relied on experts' opinions. BIBREF9 is one exception to this approach where they used word frequency on 11 issues of Dabiq publications and compared attributes such as anger, anxiety, power, motive, etc.", "sentence": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017.", "cited_ids": [], "y": "The English online magazine of ISIS [the Islamic State of Iraq and Syria] was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah [ISIS's online English language publication] which produced 13 English language issues through September 2017.", "snippet_surface": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web in July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017.", "questions": {"8/TuVwWO3X": "What does ISIS stand for?", "e7/ZRz6lO1": "What does \"Rumiyah\" stand for?"}, "answers": {"8/TuVwWO3X": "the Islamic State of Iraq and Syria (ISIS)", "e7/ZRz6lO1": "Rumiyah is ISIS's online English language publications"}, "evidence": {"8/TuVwWO3X": [{"section": "Introduction", "paragraph": "Since its rise in 2013, the Islamic State of Iraq and Syria (ISIS) has utilized the Internet to spread its ideology, radicalize individuals, and recruit them to their cause. In comparison to other Islamic extremist groups, ISIS' use of technology was more sophisticated, voluminous, and targeted. For example, during ISIS' advance toward Mosul, ISIS related accounts tweeted some 40,000 tweets in one day BIBREF0.However, this heavy engagement forced social media platforms to institute policies to prevent unchecked dissemination of terrorist propaganda to their users, forcing ISIS to adapt to other means to reach their target audience.", "selected": "the Islamic State of Iraq and Syria (ISIS)", "paper_id": "1912.03804"}], "e7/ZRz6lO1": [{"section": "Abstract", "paragraph": "Online propaganda is central to the recruitment strategies of extremist groups and in recent years these efforts have increasingly extended to women. To investigate ISIS' approach to targeting women in their online propaganda and uncover implications for counterterrorism, we rely on text mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar", "selected": "Rumiyah (ISIS's online English language publications)", "paper_id": "1912.03804"}]}}
{"idx": "1912.04961.1.1.1", "paper_id": "1912.04961", "title": "Medication Regimen Extraction From Clinical Conversations", "abstract": "Extracting relevant information from clinical conversations and providing it to doctors and patients might help in addressing doctor burnout and patient forgetfulness. In this paper, we focus on extracting the Medication Regimen (dosage and frequency for medications) discussed in a clinical conversation. We frame the problem as a Question Answering (QA) task and perform comparative analysis over: a QA approach, a new combined QA and Information Extraction approach and other baselines. We use a small corpus of 6,692 annotated doctor-patient conversations for the task. Clinical conversation corpora are costly to create, difficult to handle (because of data privacy concerns), and thus `scarce'. We address this data scarcity challenge through data augmentation methods, using publicly available embeddings and pretrain part of the network on a related task of summarization to improve the model's performance. Compared to the baseline, our best-performing models improve the dosage and frequency extractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94, respectively. Using our best-performing model, we present the first fully automated system that can extract Medication Regimen (MR) tags from spontaneous doctor-patient conversations with about ~71% accuracy.", "context_section_header": "Experiments ::: Model variations", "context_paragraph": "We consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "sentence": "We consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "cited_ids": [], "y": "The authors consider QA PGNet and Multi-decoder QA PGnet with lookup table embedding as baseline models and improve on the baselines with other variants [like 'Nearest number' for dosage extraction and 'Random Top 3' for frequency extraction].", "snippet_surface": "The authors consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below.", "questions": {"FxaNZtdN03": "What does QA PGNet stand for?"}, "answers": {"FxaNZtdN03": "It is a Question-Answer model developed from one of the authors approaches."}, "evidence": {"FxaNZtdN03": [{"section": "Approach ::: QA PGNet", "paragraph": "We first encode both the question - $H_Q = encoder(Q)$, and the input - $H_I = encoder(I)$, separately using encoders (with shared weights). Then, to condition $I$ on $Q$ (and vice versa), we use the coattention encoder BIBREF10 which attends to both the $I$ and $Q$ simultaneously to generates the coattention context - $C_D = coatt(H_I, H_Q)$. Finally, using the pointer-generator decoder we find the probability distribution of the output sequence - $P(w) = decoder_{pg}([H_I; C_D])$, which is then decoded to generate the answer sequence.", "selected": "We first encode both the question - $H_Q = encoder(Q)$, and the input - $H_I = encoder(I)$, separately using encoders (with shared weights). Then, to condition $I$ on $Q$ (and vice versa), we use the coattention encoder BIBREF10 which attends to both the $I$ and $Q$ simultaneously to generates the coattention context - $C_D = coatt(H_I, H_Q)$. Finally, using the pointer-generator decoder we find the probability distribution of the output sequence - $P(w) = decoder_{pg}([H_I; C_D])$, which is then decoded to generate the answer sequence.", "paper_id": "1912.04961"}]}}
{"idx": "1912.06262.1.1.1", "paper_id": "1912.06262", "title": "Extracting clinical concepts from user queries", "abstract": "Clinical concept extraction often begins with clinical Named Entity Recognition (NER). Often trained on annotated clinical notes, clinical NER models tend to struggle with tagging clinical entities in user queries because of the structural differences between clinical notes and user queries. User queries, unlike clinical notes, are often ungrammatical and incoherent. In many cases, user queries are compounded of multiple clinical entities, without comma or conjunction words separating them. By using as dataset a mixture of annotated clinical notes and synthesized user queries, we adapt a clinical NER model based on the BiLSTM-CRF architecture for tagging clinical entities in user queries. Our contribution are the following: 1) We found that when trained on a mixture of synthesized user queries and clinical notes, the NER model performs better on both user queries and clinical notes. 2) We provide an end-to-end and easy-to-implement framework for clinical concept extraction from user queries.", "context_section_header": "Experiments ::: Results", "context_paragraph": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", "sentence": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", "cited_ids": [], "y": "[The authors compared two models; the hybrid and ib2b NER models. The main difference between models is their drastic structural differences]. With the [best hyperparameter setting after tuning], the hybrid NER model achieved a F1 score of 0.995 on synthesized queries and 0.948 on clinical notes while the i2b2 NER model achieved a F1 score of 0.441 on synthesized queries and 0.927 on clinical notes.", "snippet_surface": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes.", "questions": {"We82FS+uEs": "What does \"the above hyperparameter setting\" refer to?"}, "answers": {"We82FS+uEs": "This refers to the best combination after tuning: \nembeddings: \u201cELMo on pubmed\u201d,\n\nhidden_size: 256,\n\nlearning_rate: 0.05,\n\nmini_batch_size: 32."}, "evidence": {"We82FS+uEs": [{"section": "Experiments ::: Results", "paragraph": "mini_batch_size: 32.", "selected": "mini_batch_size: 32.", "paper_id": "1912.06262"}]}}
{"idx": "1912.08904.2.2.1", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Introduction", "context_paragraph": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data. Each interaction in Macaw (from both user and system) is a Message object, thus a conversation is a list of Messages. Macaw consists of multiple actions, each action is a module that can satisfy the information needs of users for some requests. For example, search and question answering can be two actions in Macaw. Even multiple search algorithms can be also seen as multiple actions. Each action can produce multiple outputs (e.g., multiple retrieved documents). For every user interaction, Macaw runs all actions in parallel. The actions' outputs produced within a predefined time interval (i.e., an interaction timeout constant) are then post-processed. Macaw can choose one or combine multiple of these outputs and prepare an output Message object as the user's response.", "sentence": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "cited_ids": [], "y": "The authors designed Macaw based on a modular architecture [composed of different parts] to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "snippet_surface": "The authors designed Macaw based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "questions": {"POB4VnCtDC": "What does \"modular architecture\" mean?"}, "answers": {"POB4VnCtDC": "The platform is composed of different parts that can be used alternatively, for example for building personalised user interfaces."}, "evidence": {"POB4VnCtDC": [{"section": "Introduction", "paragraph": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data. Each interaction in Macaw (from both user and system) is a Message object, thus a conversation is a list of Messages. Macaw consists of multiple actions, each action is a module that can satisfy the information needs of users for some requests. For example, search and question answering can be two actions in Macaw. Even multiple search algorithms can be also seen as multiple actions. Each action can produce multiple outputs (e.g., multiple retrieved documents). For every user interaction, Macaw runs all actions in parallel. The actions' outputs produced within a predefined time interval (i.e., an interaction timeout constant) are then post-processed. Macaw can choose one or combine multiple of these outputs and prepare an output Message object as the user's response.", "selected": "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.", "paper_id": "1912.08904"}, {"section": "Introduction", "paragraph": "The modular design of Macaw makes it relatively easy to configure a different user interface or add a new one. The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps. In more detail, Macaw's interface can be a Telegram bot, which supports a wide range of devices and operating systems (see FIGREF4). This allows Macaw to support multi-modal interactions, such as text, speech, image, click, etc. A number of APIs for automatic speech recognition and generation have been employed to support speech interactions. Note that the Macaw's architecture and implementation allows mixed-initiative interactions.", "selected": "he modular design of Macaw makes it relatively easy to configure a different user interface or add a new one.", "paper_id": "1912.08904"}, {"section": "Macaw Architecture", "paragraph": "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup also follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and the interface produces a Message object from the current interaction of user. The interaction can be in multi-modal form, such as text, speech, image, and click. Macaw stores all interactions in an \u201cInteraction Database\u201d. For every interaction, Macaw looks for most recent user-system interactions (including the system's responses) to create a list of Messages, called the conversation list. It is then dispatched to multiple information seeking (and related) actions. The actions run in parallel, and each should respond within a pre-defined time interval. The output selection component selects from (or potentially combines) the outputs generated by different actions and creates a Message object as the system's response. This message is logged into the interaction database and is sent to the interface to be presented to the user. Again, the response message can be multi-modal and include text, speech, link, list of options, etc.", "selected": "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module.", "paper_id": "1912.08904"}]}}
{"idx": "1912.08904.3.2.1", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Macaw Architecture", "context_paragraph": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "sentence": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "cited_ids": [], "y": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. Using Macaw the [information] seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS [conversational information seeking] system and thus is useful for collecting high-quality data from real users for CIS research.", "snippet_surface": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The authors present the architecture of Macaw for such setup in FIGREF16. As shown, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", "questions": {"/Dz6yh8D6x": "What does cis stand for?"}, "answers": {"/Dz6yh8D6x": "Conversational information seeking"}, "evidence": {"/Dz6yh8D6x": [{"section": "Abstract", "paragraph": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "selected": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval", "paper_id": "1912.08904"}]}}
{"idx": "1912.08904.6.1.2", "paper_id": "1912.08904", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "abstract": "Conversational information seeking (CIS) has been recognized as a major emerging research area in information retrieval. Such research will require data and tools, to allow the implementation and study of conversational systems. This paper introduces Macaw, an open-source framework with a modular architecture for CIS research. Macaw supports multi-turn, multi-modal, and mixed-initiative interactions, and enables research for tasks such as document retrieval, question answering, recommendation, and structured data exploration. It has a modular design to encourage the study of new CIS algorithms, which can be evaluated in batch mode. It can also integrate with a user interface, which allows user studies and data collection in an interactive mode, where the back end can be fully algorithmic or a wizard of oz setup. Macaw is distributed under the MIT License.", "context_section_header": "Retrieval and Question Answering in Macaw", "context_paragraph": "Query Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.", "sentence": "Query Generation: This component generates a query based on the past user-system interactions.", "cited_ids": [], "y": "The authors' query generation component [of Macaw, an open-source framework with modular architecture for conversational information seeking], generates a query based on the past user-system interactions.", "snippet_surface": "The authors' query generation component generates a query based on the past user-system interactions.", "questions": {"T+Z6hLV4kC": "What does the \"authors query generation component\" refer to?"}, "answers": {"T+Z6hLV4kC": "Query generation--the component that generates a query based on the past user-system interactions--is one part of the overview of retrieval and question answering actions in Macaw, an open-source framework with modular architecture for conversational information seeking (CIS)."}, "evidence": {"T+Z6hLV4kC": [{"section": "Retrieval and Question Answering in Macaw", "paragraph": "Query Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.", "selected": "Query Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.", "paper_id": "1912.08904"}]}}
{"idx": "1912.08960.1.1.1", "paper_id": "1912.08960", "title": "Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity", "abstract": "Image captioning as a multimodal task has drawn much interest in recent years. However, evaluation for this task remains a challenging problem. Existing evaluation metrics focus on surface similarity between a candidate caption and a set of reference captions, and do not check the actual relation between a caption and the underlying visual content. We introduce a new diagnostic evaluation framework for the task of image captioning, with the goal of directly assessing models for grammaticality, truthfulness and diversity (GTD) of generated captions. We demonstrate the potential of our evaluation framework by evaluating existing image captioning models on a wide ranging set of synthetic datasets that we construct for diagnostic evaluation. We empirically show how the GTD evaluation framework, in combination with diagnostic datasets, can provide insights into model capabilities and limitations to supplement standard evaluations.", "context_section_header": "Introduction", "context_paragraph": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. We empirically demonstrate that the existing metrics BLEU and SPICE do not capture true caption-image agreement in all scenarios, while the GTD framework allows a fine-grained investigation of how well existing models cope with varied visual situations and linguistic constructions.", "sentence": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "cited_ids": [], "y": "The authors conducted practical evaluation of GTD [Gramaticality, truthfulness and diversity] on synthetic data [constructed by the authors themselves]. They constructed a range of datasets designed for image captioning evaluation, called ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). They illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "snippet_surface": "The authors conducted practical evaluation of GTD on synthetic data. They constructed a range of datasets designed for image captioning evaluation, called ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). They illustrate the evaluation of specific image captioning models on ShapeWorldICE.", "questions": {"0GmaYbJY3g": "Where did the authors gather \"synthetic data\"?", "8m1HWhzpet": "What does GTD stand for?"}, "answers": {"0GmaYbJY3g": "The synthetic data used for evaluation was constructed by the authors themselves.", "8m1HWhzpet": "Gramaticality, truthfulness and diversity"}, "evidence": {"0GmaYbJY3g": [{"section": "Introduction", "paragraph": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. We empirically demonstrate that the existing metrics BLEU and SPICE do not capture true caption-image agreement in all scenarios, while the GTD framework allows a fine-grained investigation of how well existing models cope with varied visual situations and linguistic constructions.", "selected": "We construct a range of datasets designed for image captioning evaluation.", "paper_id": "1912.08960"}], "8m1HWhzpet": [{"section": "Introduction", "paragraph": "To address this problem, we propose a set of principled evaluation criteria which evaluate image captioning models for grammaticality, truthfulness and diversity (GTD). These criteria correspond to necessary requirements for image captioning systems: (a) that the output is grammatical, (b) that the output statement is true with respect to the image, and (c) that outputs are diverse and mirror the variability of training captions.", "selected": "To address this problem, we propose a set of principled evaluation criteria which evaluate image captioning models for grammaticality, truthfulness and diversity (GTD).", "paper_id": "1912.08960"}]}}
{"idx": "191502", "paper_id": "258794", "title": "From Natural Language Specifications to Program Input Parsers", "abstract": "We present a method for automatically generating input parsers from English specifications of input file formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then translated into a C++ input parser. We model the problem as a joint dependency parsing and semantic role labeling task. Our method is based on two sources of information: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples. Our results show that our approach achieves 80.0% F-Score accuracy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format specifications from the ACM International Collegiate Programming Contest (which were written in English for humans with no intention of providing support for automated processing). 1", "context_section_header": "", "context_paragraph": "We evaluate our method on a dataset of input specifications from ACM International Collegiate Programming Contests, along with the corresponding input examples. These specifications were written for human programmers with no intention of providing support for automated processing. However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%). The strength of our model in the face of such weak supervision is also highlighted by the fact that it retains an F-Score of 77% even when only one input example is provided for each input Your program is supposed to read the input from the standard input and write its output to the standard output. The first line of the input contains one integer N. N lines follow, the i-th of them contains two real numbers Xi, Yi separated by a single space -the coordinates of the i-th house. Each of the following lines contains four real numbers separated by a single space. These numbers are the coordinates of two different points (X1, Y1) and (X2, Y2), lying on the highway.  Figure 3: An example of generating input parser code from text: (a) a natural language input specification; (b) a specification tree representing the input format structure (we omit the background phrases in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints.", "sentence": "However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "cited_ids": [{"paper_id": "5667590", "citation": "(Clarke et al., 2010)"}], "y": "However, when trained using the noisy supervision, the authors' method [based on joint probabilistic approach using a Bayesian generative model] achieves substantially more accurate translations than a state-of-the-art semantic parser [which uses a model that relies on human supervision] (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "snippet_surface": "However, when trained using the noisy supervision, the authors' method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al., 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%).", "questions": {"VxCqlAwvTe": "What is the authors' method?", "Y1VLKQC/9v": "What is the state-of-the-art semantic parser?"}, "answers": {"VxCqlAwvTe": "The author's method is based on joint probabilistic methods using a Bayesian generative model. It uses two sources of information and translate the language into a specification tree.", "Y1VLKQC/9v": "The state of the art semantic parser is developed by Clarke et al 2010. It uses a model that relies on human supervision and it does not rely on NL syntactic parsing rules."}, "evidence": {"VxCqlAwvTe": [{"section": "Introduction", "paragraph": "We model our problem as a joint dependency parsing and role labeling task, assuming a Bayesian generative process. The distribution over the space of specification trees is informed by two sources of information: (1) the correlation between the text and the corresponding specification tree and (2) the success of the generated parser in reading input examples. Our method uses a joint probability distribution to take both of these sources of information into account, and uses a sampling framework for the inference of specifi-2 During the second step of the process, the specification tree is deterministically translated into code.  Figure 2: Input parser code for reading input files specified in Figure 1. cation trees given text specifications. A specification tree is rejected in the sampling framework if the corresponding code fails to successfully read all of the input examples. The sampling framework also rejects the tree if the text/specification tree pair has low probability.", "selected": "Our method uses a joint probability distribution to take both of these sources of information into account, and uses a sampling framework for the inference of specifi-2", "paper_id": "258794"}, {"section": "Introduction", "paragraph": "We model our problem as a joint dependency parsing and role labeling task, assuming a Bayesian generative process. The distribution over the space of specification trees is informed by two sources of information: (1) the correlation between the text and the corresponding specification tree and (2) the success of the generated parser in reading input examples. Our method uses a joint probability distribution to take both of these sources of information into account, and uses a sampling framework for the inference of specifi-2 During the second step of the process, the specification tree is deterministically translated into code.  Figure 2: Input parser code for reading input files specified in Figure 1. cation trees given text specifications. A specification tree is rejected in the sampling framework if the corresponding code fails to successfully read all of the input examples. The sampling framework also rejects the tree if the text/specification tree pair has low probability.", "selected": "A specification tree is rejected in the sampling framework if the corresponding code fails to successfully read all of the input examples. The sampling framework also rejects the tree if the text/specification tree pair has low probability.", "paper_id": "258794"}, {"section": "Introduction", "paragraph": "We model our problem as a joint dependency parsing and role labeling task, assuming a Bayesian generative process. The distribution over the space of specification trees is informed by two sources of information: (1) the correlation between the text and the corresponding specification tree and (2) the success of the generated parser in reading input examples. Our method uses a joint probability distribution to take both of these sources of information into account, and uses a sampling framework for the inference of specifi-2 During the second step of the process, the specification tree is deterministically translated into code.  Figure 2: Input parser code for reading input files specified in Figure 1. cation trees given text specifications. A specification tree is rejected in the sampling framework if the corresponding code fails to successfully read all of the input examples. The sampling framework also rejects the tree if the text/specification tree pair has low probability.", "selected": "The distribution over the space of specification trees is informed by two sources of information: (1) the correlation between the text and the corresponding specification tree and (2) the success of the generated parser in reading input examples.", "paper_id": "258794"}], "Y1VLKQC/9v": [{"section": "Experimental Setup", "paragraph": "The second baseline Aggressive is a state-ofthe-art semantic parsing framework (Clarke et al., 2010). 8 The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions. Specifically, at each iteration the structure learner predicts the most plausible specification tree for each text document:", "selected": "The second baseline Aggressive is a state-ofthe-art semantic parsing framework (Clarke et al., 2010). 8 The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions. Specifically, at each iteration the structure learner predicts the most plausible specification tree for each text document:", "paper_id": "258794"}, {"section": "Abstract", "paragraph": "Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers.", "selected": "We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world.", "paper_id": "5667590"}, {"section": "Conclusions", "paragraph": "In this paper we tackle one of the key bottlenecks in semantic parsing -providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a new training paradigm for semantic parsing that relies on natural, human level supervision. Second, we suggest a new model for semantic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process. This approach allows the model to generalize better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems.", "selected": "Our solution is two fold, first we present a new training paradigm for semantic parsing that relies on natural, human level supervision. Second, we suggest a new model for semantic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process.", "paper_id": "5667590"}]}}
{"idx": "191560", "paper_id": "236486232", "title": "Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers", "abstract": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.", "context_section_header": "", "context_paragraph": "Offensive Language Detection. Offensive language detection (OLD) has become an active research topic in recent years (Araujo De Souza and Da Costa Abreu, 2020). Nikolov and Radivchev (2019) experimented with a variety of models and observe promising results with BERT and SVC based models. Han et al. (2019) employed a GRU based RNN with 100 dimensional glove word embeddings (Pennington et al., 2014). Additionally, they develop a Modified Sentence Offensiveness Calculation (MSOC) model which makes use of a dictionary of offensive words. Liu et al. (2019) evaluated three models on the OLID dataset, including logistic regression, LSTM and BERT, and results show that BERT achieves the best performance. The concept of transfer learning mentioned in (Liu et al., 2019) is closely related to our work, since the BERT model is also pretrained on external text corpus. However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we propose a new training strategy for domain adaptation.", "sentence": "However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we propose a new training strategy for domain adaptation.", "cited_ids": [{"paper_id": "184482578", "citation": "(Liu et al., 2019)"}], "y": "Different from (Liu et al., 2019), the authors approach [training using the ALBERT loss function for detection of offensive language] exploits external data [by training with different domains of of offensive language] that are closely related to the OLD [Offensive language detection] task, and we propose a new training strategy for domain adaptation [including a predictive layer that should help in aligning the two domains].", "snippet_surface": "However, different from (Liu et al., 2019), the authors' approach exploits external data that are closely related to the OLD task, and they propose a new training strategy for domain adaptation.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "4QGLgjWIJZ": "How does the authors' approach exploit external data?", "KQDZdQ65FP": "What is old task?", "jBEPWMB5LE": "What is the new training strategy?"}, "answers": {"6gKwRw0I/Q": "They introduce training according to an ALBERT loss function including auxiliary text and domain adaptation.", "4QGLgjWIJZ": "By distinguishing between different types of offensive language in a different domain and using that to do well on the target domain", "KQDZdQ65FP": "Offensive language detection.", "jBEPWMB5LE": "The authors include a predictive layer that should help in aligning the two domains"}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.", "selected": "Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain.", "paper_id": "236486232"}, {"section": "Domain Adaptation Approach", "paragraph": "We propose a simple yet effective domain adaptation approach to train an ALBERT model for offensive language detection, which fully exploits auxiliary information from source domain to assist the learning task in target domain. The effectiveness of using auxiliary text for language understanding has been discussed in literature (Rezayi et al., 2021).", "selected": "We propose a simple yet effective domain adaptation approach to train an ALBERT model for offensive language detection, which fully exploits auxiliary information from source domain to assist the learning task in target domain. The effectiveness of using auxiliary text for language understanding has been discussed in literature (Rezayi et al., 2021).", "paper_id": "236486232"}], "4QGLgjWIJZ": [{"section": "Abstract", "paragraph": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.", "selected": "Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain.", "paper_id": "236486232"}, {"section": "Introduction", "paragraph": "In this paper, we propose to tackle the challenging issue of data/label scarcity in offensive language detection, by designing a simple yet effective domain adaptation approach based on bidirectional transformers. Domain adaptation aims to enhance the model capacity for a target domain by exploiting auxiliary information from external data sources (i.e., source domains), especially when the data and labels in the target domain are insufficient (Pan and Yang, 2009;Wang and Deng, 2018;Lai et al., 2018;Li et al., 2017;Li and Fu, 2016;Zhu et al., 2021). In particular, we aim to identify not only if the content is offensive, but also the corresponding type and target. In our work, the offensive language identification dataset (OLID) (Zampieri et al., 2019) is considered as target domain, which contains a hierarchical multi-level structure of offensive contents.", "selected": "Domain adaptation aims to enhance the model capacity for a target domain by exploiting auxiliary information from external data sources (i.e., source domains), especially when the data and labels in the target domain are insufficient (Pan and Yang, 2009;Wang and Deng, 2018;Lai et al., 2018;Li et al., 2017;Li and Fu, 2016;Zhu et al., 2021). In particular, we aim to identify not only if the content is offensive, but also the corresponding type and target. In our work, the offensive language identification dataset (OLID) (Zampieri et al., 2019) is considered as target domain, which contains a hierarchical multi-level structure of offensive contents.", "paper_id": "236486232"}], "KQDZdQ65FP": [{"section": "Abstract", "paragraph": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.", "selected": "Offensive language detection (OLD)", "paper_id": "236486232"}, {"section": "Related Work", "paragraph": "Offensive Language Detection. Offensive language detection (OLD) has become an active research topic in recent years (Araujo De Souza and Da Costa Abreu, 2020). Nikolov and Radivchev (2019) experimented with a variety of models and observe promising results with BERT and SVC based models. Han et al. (2019) employed a GRU based RNN with 100 dimensional glove word embeddings (Pennington et al., 2014). Additionally, they develop a Modified Sentence Offensiveness Calculation (MSOC) model which makes use of a dictionary of offensive words. Liu et al. (2019) evaluated three models on the OLID dataset, including logistic regression, LSTM and BERT, and results show that BERT achieves the best performance. The concept of transfer learning mentioned in (Liu et al., 2019) is closely related to our work, since the BERT model is also pretrained on external text corpus. However, different from (Liu et al., 2019), our approach exploits external data that are closely related to the OLD task, and we propose a new training strategy for domain adaptation.", "selected": "Offensive language detection (OLD)", "paper_id": "236486232"}, {"section": "Introduction", "paragraph": "In today's digital age, the amount of offensive and abusive content found online has reached unprecedented levels. Offensive content online has several detrimental effects on its victims, e.g., victims of cyberbullying are more likely to have lower self-esteem and suicidal thoughts (Vazsonyi et al., 2012). To reduce the impact of offensive online contents, the first step is to detect them in an accurate and timely fashion. Next, it is imperative to identify the type and target of offensive contents. Segregating by type is important, because some types of offensive content are more serious and harmful than other types, e.g., hate speech is illegal in many countries and can attract large fines and even prison sentences, while profanity is not that serious. To this end, offensive language detection (OLD) has been extensively studied in recent years, which is an active topic in natural language understanding.", "selected": "To this end, offensive language detection (OLD) has been extensively studied in recent years, which is an active topic in natural language understanding", "paper_id": "236486232"}], "jBEPWMB5LE": [{"section": "Introduction", "paragraph": "An external large-scale dataset on toxic comment (ToxCom) classification is used as source domain. ALBERT (Lan et al., 2019) is used in our approach owing to its impressive performance on OLD. A set of training procedures are designed to achieve domain adaptation for the OLD task. In particular, as the external dataset is not labelled in the same format as the OLID dataset, we design a separate predictive layer that helps align two domains. Extensive empirical evaluations of our approach and baselines are conducted. The main contributions of our work are summarized as follows:", "selected": "An external large-scale dataset on toxic comment (ToxCom) classification is used as source domain. ALBERT (Lan et al., 2019) is used in our approach owing to its impressive performance on OLD. A set of training procedures are designed to achieve domain adaptation for the OLD task. In particular, as the external dataset is not labelled in the same format as the OLID dataset, we design a separate predictive layer that helps align two domains. Extensive empirical evaluations of our approach and baselines are conducted.", "paper_id": "236486232"}, {"section": "Related Work", "paragraph": "(3) Emoji Substitution. All emojis are converted to text using their corresponding language meaning. This is done using Emoji 2 . (4) Class Weights. The dataset is highly skewed at each level, thus a weighting scheme is used, as follows: Let the classes be {c 1 , c 2 , \u00b7 \u00b7 \u00b7 , c k } and number of samples in each class be {N 1 , N 2 , \u00b7 \u00b7 \u00b7 , N k }, then class c i is assigned a weight of 1 N i . Source Domain. To assist the OLD task in target domain, we employ an external large-scale dataset on toxic comment (ToxCom) classification 3 as source domain. ToxCom consists of 6 different offensive classes. Samples that belong to none of the 6 classes are labelled as clean. The details of ToxCom dataset are shown in Table 2. The number of clean comments is disproportionately high and will lead to considerable training time. Thus, only 16,225 randomly sampled clean comments are employed.", "selected": "The details of ToxCom dataset are shown in Table 2. The number of clean comments is disproportionately high and will lead to considerable training time. Thus, only 16,225 randomly sampled clean comments are employed.", "paper_id": "236486232"}, {"section": "Baselines and Experimental Settings", "paragraph": "In the experiments, four representative models are used as baselines, including the support vector machines (SVM), convolutional neural networks (CNN), BERT and ALBERT. We use the base version of BERT and the large version of ALBERT. The max sequence length is set to 32 and 64 for BERT and ALBERT, respectively. Training samples with length longer than max sequence length are discarded. Moreover, we compare our approach with three state-of-the-art methods (Liu et al., 2019;Han et al., 2019;Nikolov and Radivchev, 2019) on offensive language detection. For domain adaptation, the finetuning and feature extraction approaches, discussed in Section 3.2, are tested. The feature extraction approach gives poor results on all three levels, with scores lower than ALBERT without domain adaptation. The third method is not used as it introduces a new hyperparameter, i.e., the number of trainable layers, which would have to be optimized with considerable computational costs. The finetuning type strategy gives good initial results and is used henceforth. The learning rate is set to 1.5 \u00d7 10 \u22125 and 2 \u00d7 10 \u22125 on the source data and target data, respectively. Following the standard evaluation protocol on the OLID dataset, the 9:1 training versus validation split is used. In each experiment (other than SVM), the models are trained for 3 epochs. The metric used here is macro F1 score, which is calculated by taking the unweighted average for all classes. Best performing models according to validation loss are saved and used for testing. Table 3 shows the results of baselines and our domain adaptation approach, ALBERT (DA). For Task A, deep learning methods, including CNN, BERT and ALBERT, always outperform the classical classification method SVM. ALBERT achieves a macro F1 score of 0.8109, which is the highest score without domain adaptation. Task C is unique as it consists of three labels. All models suffer on the OTH class. This could be because the OTH class consists of very few training samples. Our approach, ALBERT (DA), achieves the state-ofthe-art performance on Task C. Figure 1 further breaks down the classwise scores for analysis. The most notable improvements are on OTH and UNT samples. ALBERT (DA) has an F1 score of 0.46, which is an improvement 43.75% over ALBERT on OTH samples. On UNT samples, ALBERT (DA) improves ALBERT's score of 0.55 to 0.65, which is an improvement of 18%. Conversely, performance on classes on which the ALBERT already has high F1-scores, such as NOT and TIN, do not see major improvements through domain adaptation. On NOT and TIN samples, ALBERT (DA) improves only 1.11% and 1.06% over ALBERT, respectively.", "selected": "Training samples with length longer than max sequence length are discarded.", "paper_id": "236486232"}]}}
{"idx": "194176", "paper_id": "33671165", "title": "Supervised Attention for Sequence-to-Sequence Constituency Parsing", "abstract": "The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.", "context_section_header": "", "context_paragraph": "In this paper, we define several linguisticallymotivated annotations between surface words and nonterminals as \"gold standard alignments\" to enhance the attention mechanism of the constituency parser (Vinyals et al., 2015) by supervised attention. The PTB corpus results showed that our method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "sentence": "The PTB corpus results showed that our method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "cited_ids": [{"paper_id": "14223", "citation": "Vinyals et al. (2015)"}], "y": "The [English Penn Treebank] (PTB) corpus results showed that the authors' method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure. [Their approach defined numerous linguistically motivated annotations between surface words and non-terminals as \"gold standard alignments\" in order to enhance the attention mechanism of the constituency parser via supervised attention--where attentions are learned from given alignments.]", "snippet_surface": "The PTB corpus results showed that the authors' method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "questions": {"YlQF/Jm0n1": "What does PTB stand for?", "VxCqlAwvTe": "What is the authors' method?"}, "answers": {"YlQF/Jm0n1": "\"PTB\" stands for: English Penn Treebank corpus", "VxCqlAwvTe": "The author defined numerous linguistically motivated annotations between surface words and non-terminals as \"gold standard alignments\" in order to enhance the attention mechanism of the constituency parser via supervised attention--where attentions are learned from given alignments."}, "evidence": {"YlQF/Jm0n1": [{"section": "Evaluation Settings", "paragraph": "We experimentally evaluated our methods on the English Penn Treebank corpus (PTB), and split the data into three parts: The Wall Street Journal (WSJ) sections 02-21 for training, section 22 for development and section 23 for testing. In our models, the dimensions of the input word embeddings, the fed label embeddings, the hidden layers, and an attention vector were respectively set to 150, 30, 200, and 200. The LSTM depth was set to 3. Label set L con had a size of 61. The input vocabulary size of PTB was set to 42393. Supervised attention rate \u03bb was set to 1.0. To use entire words as a vocabulary, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate 0.8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) Table 1: Results of parsing evaluation: Seq2Seq indicates the Seq2Seq model on a single model with greedy decoding. +beam shows the beam decoding results. +lex, +left, +right and +span respectively show the results on our proposed lexical head, left word, right word, and span word alignments. +random, +first, and +last respectively show the results on the alignment of baselines random, first word, and last word. +ens(base) shows the ensemble results of five Seq2Seq models without the given alignments. +ens(feat) shows the ensemble results of a Seq2Seq model without a given alignment and Seq2Seq models with lexical head, left word, right word and span word alignments. \u2020 denotes the scores reported in the paper.", "selected": "English Penn Treebank corpus (PTB)", "paper_id": "33671165"}], "VxCqlAwvTe": [{"section": "Abstract", "paragraph": "The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention.", "selected": "we introduce supervised attention to constituency parsing that can be regarded as another translation task.", "paper_id": "33671165"}, {"section": "Introduction", "paragraph": "In this paper, we define several linguisticallymotivated annotations between surface words and nonterminals as \"gold standard alignments\" to enhance the attention mechanism of the constituency parser (Vinyals et al., 2015) by supervised attention. The PTB corpus results showed that our method outperformed Vinyals et al. (2015) by over 1 point in the bracketing F-measure.", "selected": "we define several linguisticallymotivated annotations between surface words and nonterminals as \"gold standard alignments\" to enhance the attention mechanism of the constituency parser (Vinyals et al., 2015) by supervised attention.", "paper_id": "33671165"}, {"section": "Introduction", "paragraph": "In a supervised attention framework, attentions are learned from the given alignments. We denote a link on an alignment between y t and x i as a i t = 1 (a i t = 0 denotes that y t and x i are not linked.). Following a previous work (Liu et al., 2016), we adopt a soft constraint to the objective function:", "selected": "In a supervised attention framework, attentions are learned from the given alignments.", "paper_id": "33671165"}]}}
{"idx": "195310", "paper_id": "202783441", "title": "Identifying Predictive Causal Factors from News Streams", "abstract": "We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years. We then manually validated PCG, finding that 67% of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.", "context_section_header": "", "context_paragraph": "We constructed PCG from news streams of around 700, 000 articles from Google News API and New York Times spread across over 6 years and evaluated it to extract features for stock price predictions. We obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017). The longitudinal PCG provided insights into the variation in importance of the predictive causal factors over time, while consistently maintaining a low prediction error rate between 1.5-5% in predicting 10 stock prices. Using full text of more than 1.5 million articles of Times of India news archives for over 10 years, we performed a fine-grained qualitative analysis of PCG and validated that 67% of the semantic causation arguments found in the news text is connected by a direct edge in PCG while the rest were linked by a path of length 2. In summary, PCG provides a powerful framework for identifying predictive causal factors from news streams to accurately predict and interpret price fluctuations.", "sentence": "We obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017).", "cited_ids": [{"paper_id": "793863", "citation": "(Kang et al., 2017)"}], "y": "The authors obtained two orders lower prediction error compared to a similar semantic causal graph-based method [that uses news outlet information to predict stock price evolution] (Kang et al., 2017).", "snippet_surface": "The authors obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017).", "questions": {"6vhXdhenIo": "Which semantic casual graph-based method is being referred here?"}, "answers": {"6vhXdhenIo": "One that uses news outlet information to predict stock price evolution."}, "evidence": {"6vhXdhenIo": [{"section": "Abstract", "paragraph": "We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years. We then manually validated PCG, finding that 67% of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.", "selected": "We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years.", "paper_id": "202783441"}, {"section": "Introduction", "paragraph": "We constructed PCG from news streams of around 700, 000 articles from Google News API and New York Times spread across over 6 years and evaluated it to extract features for stock price predictions. We obtained two orders lower prediction error compared to a similar semantic causal graph-based method (Kang et al., 2017). The longitudinal PCG provided insights into the variation in importance of the predictive causal factors over time, while consistently maintaining a low prediction error rate between 1.5-5% in predicting 10 stock prices. Using full text of more than 1.5 million articles of Times of India news archives for over 10 years, we performed a fine-grained qualitative analysis of PCG and validated that 67% of the semantic causation arguments found in the news text is connected by a direct edge in PCG while the rest were linked by a path of length 2. In summary, PCG provides a powerful framework for identifying predictive causal factors from news streams to accurately predict and interpret price fluctuations.", "selected": "We constructed PCG from news streams of around 700, 000 articles from Google News API and New York Times spread across over 6 years and evaluated it to extract features for stock price predictions.", "paper_id": "202783441"}]}}
{"idx": "195919", "paper_id": "218974038", "title": "Evaluating Sentence Segmentation in Different Datasets of Neuropsychological Language Tests in Brazilian Portuguese", "abstract": "Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain: the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of parsers used in metrics, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives \u2014 visual and oral \u2014 via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture: (i) the inclusion of a Linear Chain CRF; (ii) the inclusion of a self-attention mechanism; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set.", "context_section_header": "", "context_paragraph": "Since the majority of studies on diagnosing cognitive impairments by NLP methods deal with English-speaking patients (Filiou et al., 2019), in this study we will evaluate the Brazilian Portuguese (BP) language in order to contribute with datasets and studies to develop automatic analysis of connected speech in BP. Our motivation for this study was to explore the performance of a single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets. Therefore, here, we evaluate four datasets used to diagnose cognitive impairments (see Section 3), comprising different stories and two type of stimulus presentation for eliciting narratives: (i) oral stimuli presentation with retelling, where sequencing discourse marks, such as \"e\",\"a\u00ed\", \"da\u00ed\" and \"ent\u00e3o\" (and, then, in English) and confirmatory discourse marks \"n\u00e9\" and \"ok\" (ok, in English) are frequent and (ii) visual stimuli, via both illustrated story book and sequence of scenes of a common event, where deictic expressions (place deixis) are pervasive, such as \"aqui\" and \"a\u00ed\" (here) and \"ali\" and \"l\u00e1\" (there), besides presenting sequencing discourse marks and confirmatory discourse. Figure 1 shows the result of a manual transcription in BP of a narrative of the ABCD story telling task, which presents a story about a woman who is unaware of having lost her wallet while doing the shopping; she then receives a call from a little girl who found the wallet. As we can see in (a) the transcript without punctuation prevents the direct application of NLP methods that rely on sentence segmentation for the correct use of tools as taggers and parsers. These tools are used to implement metrics of syntactic complexity, basic counts of PoS tags and to analyze other levels of language to diagnose cognitive impairments. When the architecture developed in the project Deep-BonDD 1 was trained with The Cinderella Story dataset (a production task elicited via an illustrated story book) and 1 https://github.com/mtreviso/deepbond (a) ahm uma senhora foi fazer compras no me foi no mercado n\u00e3o lembrava o local no me fazer compras e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira a\u00ed ela foi deixou a mercadoria n\u00e3o levou a mercadoria voltou para casa chegando em casa toca o telefone era uma garotinha avisando ela que que tinha achado a carteira \u00e9 isso tem mais coisa n\u00e3o cortei eu resumi o que eu ouvi (b) ahm uma senhora foi fazer compras no me foi no mercado. n\u00e3o lembrava o local . no me fazer compras . e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira . a\u00ed ela foi deixou a mercadoria . n\u00e3o levou a mercadoria . voltou para casa . chegando em casa toca o telefone . era uma garotinha avisando ela que que tinha achado a carteira . \u00e9 isso . tem mais coisa . n\u00e3o cortei . eu resumi o que eu ouvi . Figure 1: (a) Narrative transcribed where there is no punctuation or capitalization, besides presenting several disfluencies, such as unlexicalized filled pauses, restarts and patient's comments, shown in bold. (b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1). The average of F 1 in the three datasets, for all classes, is 0.59. In the original evaluation with training and cross-validation testing in the same dataset, the best F 1 value for Controls was 0.76, for MCIs, 0.74, and for ADs, 0.66. However, Table 1 shows that, in general, for sentence segmentation, more data is beneficial, independently of task and topic of datasets. Given this motivation scenario, where the main goal was to develop a robust and generic segmentation system for narratives of neuropsychological language tests, the present study tries to answer three questions:", "sentence": "(b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1).", "cited_ids": [{"paper_id": "32649543", "citation": "(Treviso et al., 2017a)"}], "y": "The authors evaluated a narrative of a retelling task manually segmented using \"The Wallet Story\" [a story about a woman unaware of having lost her wallet who receives a call from a girl who found it], along with the \"Cinderella\", \"Dog Story\" and \"Lucia Story\" datasets. F 1 values were much lower than the work done by (Treviso et al., 2017a) [which developed a narrative segmentation system based on a Cinderella story transcribed from a picture book].", "snippet_surface": "(b) Narrative manually segmented of a retelling task using The Wallet Story was evaluated with the other three datasets analyzed in this paper. F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1).", "questions": {"uWx4f9HuzT": "What is the wallet story?", "Kuw2u2VFaS": "Which other datasets are being referred here?", "oRBzgzisLI": "What is the original work on cinderella?"}, "answers": {"uWx4f9HuzT": "A story about a woman unaware of having lost her wallet who receives a call from a girl who found it. It is part of the story-telling task.", "Kuw2u2VFaS": "The Cinderella, Dog Story and Lucia Story Datasets.", "oRBzgzisLI": "The development of a narrative segmentation system based on a transcribed Cinderella story from visual sources like picture books."}, "evidence": {"uWx4f9HuzT": [{"section": "Introduction", "paragraph": "Since the majority of studies on diagnosing cognitive impairments by NLP methods deal with English-speaking patients (Filiou et al., 2019), in this study we will evaluate the Brazilian Portuguese (BP) language in order to contribute with datasets and studies to develop automatic analysis of connected speech in BP. Our motivation for this study was to explore the performance of a single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets. Therefore, here, we evaluate four datasets used to diagnose cognitive impairments (see Section 3), comprising different stories and two type of stimulus presentation for eliciting narratives: (i) oral stimuli presentation with retelling, where sequencing discourse marks, such as \"e\",\"a\u00ed\", \"da\u00ed\" and \"ent\u00e3o\" (and, then, in English) and confirmatory discourse marks \"n\u00e9\" and \"ok\" (ok, in English) are frequent and (ii) visual stimuli, via both illustrated story book and sequence of scenes of a common event, where deictic expressions (place deixis) are pervasive, such as \"aqui\" and \"a\u00ed\" (here) and \"ali\" and \"l\u00e1\" (there), besides presenting sequencing discourse marks and confirmatory discourse. Figure 1 shows the result of a manual transcription in BP of a narrative of the ABCD story telling task, which presents a story about a woman who is unaware of having lost her wallet while doing the shopping; she then receives a call from a little girl who found the wallet. As we can see in (a) the transcript without punctuation prevents the direct application of NLP methods that rely on sentence segmentation for the correct use of tools as taggers and parsers. These tools are used to implement metrics of syntactic complexity, basic counts of PoS tags and to analyze other levels of language to diagnose cognitive impairments. When the architecture developed in the project Deep-BonDD 1 was trained with The Cinderella Story dataset (a production task elicited via an illustrated story book) and 1 https://github.com/mtreviso/deepbond (a) ahm uma senhora foi fazer compras no me foi no mercado n\u00e3o lembrava o local no me fazer compras e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira a\u00ed ela foi deixou a mercadoria n\u00e3o levou a mercadoria voltou para casa chegando em casa toca o telefone era uma garotinha avisando ela que que tinha achado a carteira \u00e9 isso tem mais coisa n\u00e3o cortei eu resumi o que eu ouvi (b) ahm uma senhora foi fazer compras no me foi no mercado. n\u00e3o lembrava o local . no me fazer compras . e quando ela foi pagar a conta no caixa percebeu que estava sem a carteira . a\u00ed ela foi deixou a mercadoria . n\u00e3o levou a mercadoria . voltou para casa . chegando em casa toca o telefone . era uma garotinha avisando ela que que tinha achado a carteira . \u00e9 isso . tem mais coisa . n\u00e3o cortei . eu resumi o que eu ouvi . Figure 1: (a) Narrative transcribed where there is no punctuation or capitalization, besides presenting several disfluencies, such as unlexicalized filled pauses, restarts and patient's comments, shown in bold. (b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F 1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1). The average of F 1 in the three datasets, for all classes, is 0.59. In the original evaluation with training and cross-validation testing in the same dataset, the best F 1 value for Controls was 0.76, for MCIs, 0.74, and for ADs, 0.66. However, Table 1 shows that, in general, for sentence segmentation, more data is beneficial, independently of task and topic of datasets. Given this motivation scenario, where the main goal was to develop a robust and generic segmentation system for narratives of neuropsychological language tests, the present study tries to answer three questions:", "selected": "Figure 1 shows the result of a manual transcription in BP of a narrative of the ABCD story telling task, which presents a story about a woman who is unaware of having lost her wallet while doing the shopping; she then receives a call from a little girl who found the wallet.", "paper_id": "218974038"}], "Kuw2u2VFaS": [{"section": "Datasets", "paragraph": "Four datasets were used to train our models (Sections 3.1, 3.2 and 3.3). As a preprocessing step we removed capitalization information and in order to simulate high-quality ASR, we left all speech disfluencies intact. Demographic information of participants and statistics about the narratives of our study are presented in Table 2. Datasets of Neuropsychological Language Tests are typically small, as can be seen in Table 2. Table 2 shows the uniform mean length of sentences of three datasets (The Wallet Story, The Dog Story and The Cinderella Story), with regards to the groups MCI and Controls. This is an interesting feature to train/test a model, using a large dataset which combines several stories. Cinderella's mean length of narratives is very long, while both retellings produce short narratives.", "selected": "Table 2 shows the uniform mean length of sentences of three datasets (The Wallet Story, The Dog Story and The Cinderella Story)", "paper_id": "218974038"}, {"section": "Datasets", "paragraph": "Four datasets were used to train our models (Sections 3.1, 3.2 and 3.3). As a preprocessing step we removed capitalization information and in order to simulate high-quality ASR, we left all speech disfluencies intact. Demographic information of participants and statistics about the narratives of our study are presented in Table 2. Datasets of Neuropsychological Language Tests are typically small, as can be seen in Table 2. Table 2 shows the uniform mean length of sentences of three datasets (The Wallet Story, The Dog Story and The Cinderella Story), with regards to the groups MCI and Controls. This is an interesting feature to train/test a model, using a large dataset which combines several stories. Cinderella's mean length of narratives is very long, while both retellings produce short narratives.", "selected": "Four datasets were used to train our models (Sections 3.1, 3.2 and 3.3).", "paper_id": "218974038"}], "oRBzgzisLI": [{"section": "Abstract", "paragraph": "Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain: the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of parsers used in metrics, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives \u2014 visual and oral \u2014 via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture: (i) the inclusion of a Linear Chain CRF; (ii) the inclusion of a self-attention mechanism; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set.", "selected": "Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments.", "paper_id": "218974038"}, {"section": "Abstract", "paragraph": "Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain: the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of parsers used in metrics, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives \u2014 visual and oral \u2014 via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture: (i) the inclusion of a Linear Chain CRF; (ii) the inclusion of a self-attention mechanism; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set.", "selected": "The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests.", "paper_id": "218974038"}, {"section": "Introduction", "paragraph": "Language assessment has been shown to be an efficient complementary tool for detecting cognitive and neuropsychological disorders, therefore present in most tests, tasks and batteries that evaluate cognitive processes. For example, neuropsychological language tests are an important tool for diagnosing individuals with significant depression in Alzheimer's disease (AD) (Fraser et al., 2016), to differentiate between Mild Cognitive Impairment (MCI) and AD (Drummond et al., 2015), to differentiate between AD and other neurodegenerative dementias (Yancheva et al., 2015;Beltrami et al., 2018) and to differentiate variants of neurodegenerative dementias, such as in Primary Progressive Aphasia (PPA) (Fraser et al., 2014). Language assessment has been performed mainly by using discursive production in which narratives are largely used, since they are a natural form of communication and favor the observation of the patient's functionality in everyday life (Tillas, 2015). The discourse tasks used to assess the narrative productions of elder individuals are often based on: (i) an illustrated story book without a text (e.g. Cinderella), (ii) an immediate and delayed retelling of a story orally presented, or (iii) a single scene or a sequence of scenes, presented on pictures, of a common event that occurs in daily life. With regard to specific batteries used to evaluate language in discourse tasks, we can cite a few (Wechsler, 1997;Bayles and Tomoeda, 1993;Goodglass et al., 1983;H\u00fcbner et al., 2019). Discourse tasks that require some degree of memorization are usually included in verbal memory tests. This is the case of the Logical Memory Subtest task from the Wechsler Memory Scale, used for assessing episodic memory (Wechsler, 1997). In this task, an individual repro-*Work carried out during the master's course at the University of S\u00e3o Paulo. duces a story immediately after listening to it (immediate recall); thirty minutes later, subjects are asked to recall the story again (delayed recall). The retellings are transcribed for further analysis. The higher the number of recalled elements of the narrative, the higher the memory score. This procedure is also used in the Arizona Battery for Communication Disorders of Dementia (ABCD) (Bayles and Tomoeda, 1993). The single-scene description task called \"The Cookie Theft Picture\" is part of the Boston Diagnostic Aphasia Examination (BDAE) (Goodglass et al., 1983). The Cookie-Theft picture has been shown to be clinically relevant in identifying linguistic deficits in Alzheimer's disease patients given the importance of using visual stimuli when evaluating individuals of this group. The Cinderella story is also very widely used in the assessment of aphasia and some types of dementia.  and (Aluisio et al., 2016) based their work on the story of Cinderella. Participants were given a sequenced picture book (without words) to remind them of the story; then they were asked to tell the story in their own words. The narrative samples were then transcribed by trained annotators.", "selected": "The discourse tasks used to assess the narrative productions of elder individuals are often based on: (i) an illustrated story book without a text (e.g. Cinderella),", "paper_id": "218974038"}]}}
{"idx": "197372", "paper_id": "248693452", "title": "Extracting Latent Steering Vectors from Pretrained Language Models", "abstract": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.", "context_section_header": "", "context_paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "sentence": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "cited_ids": [{"paper_id": "4421747", "citation": "Cer et al. (2017)"}, {"paper_id": "4421747", "citation": "Cer et al. (2017)"}], "y": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), the authors' [steering vectors, that are used to generate a target sentence nearly perfectly when they are added to the hidden states of the language model], outperform extractive methods such as averaging language model. The author's approach adds a vector z to steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "snippet_surface": "On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), the authors' steering vectors outperform extractive methods such as averaging language model. Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "questions": {"j+V9+DUay5": "What is \"averaging language model\"?", "STlGczINPb": "What are the authors steering vectors?", "LbqH5+LUAi": "What is \"our approach\"?"}, "answers": {"j+V9+DUay5": "an extractive method to decode a desired target sentence", "STlGczINPb": "The steering vectors are used to generate a target sentence nearly perfectly when they are added to the hidden states of the language model.", "LbqH5+LUAi": "To add a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. The authors reveal that their approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best."}, "evidence": {"j+V9+DUay5": [{"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "extractive methods such as averaging language model", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "extractive methods such as averaging language model", "paper_id": "248693452"}], "STlGczINPb": [{"section": "Abstract", "paragraph": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.", "selected": "added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains.", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "We take a more direct approach and explore whether it is possible to extract latent vectors directly from pretrained language model decoders without fine-tuning. We call these vectors steering vectors and define the latent steering space of a sentence under a language model by the set of extracted steering vectors, which steer the model to generate that sentence exactly. During decoding, we add our steering vector to the hidden states of the language model to generate the target sentence. Rather than training a model to learn steering vectors, we provide several methods to extract fixedlength steering vectors directly from pretrained language model decoders. Experiments show that we can extract steering vectors effectively, achieving nearly perfect recovery for English sentences from a variety of domains without fine-tuning the underlying language model at all.", "selected": "extract latent vectors directly from pretrained language model decoders without fine-tuning.", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "We take a more direct approach and explore whether it is possible to extract latent vectors directly from pretrained language model decoders without fine-tuning. We call these vectors steering vectors and define the latent steering space of a sentence under a language model by the set of extracted steering vectors, which steer the model to generate that sentence exactly. During decoding, we add our steering vector to the hidden states of the language model to generate the target sentence. Rather than training a model to learn steering vectors, we provide several methods to extract fixedlength steering vectors directly from pretrained language model decoders. Experiments show that we can extract steering vectors effectively, achieving nearly perfect recovery for English sentences from a variety of domains without fine-tuning the underlying language model at all.", "selected": "extract latent vectors directly from pretrained language model decoders without fine-tuning.", "paper_id": "248693452"}, {"section": "Recovery effectiveness", "paragraph": "Robustness Now that we have established that steering vector extraction is possible, we explore whether there exist multiple steering vectors which recover the same sentence, and if so, what the relationship is between these vectors. To do this, we take all 64 sentences from the books subset of the main dataset and initialize 8 different steering vectors for each sentence from different seeds. Experiments reveal that for most sentences (63 of 64) all initializations recover the target sentence perfectly, confirming the robustness of our method.", "selected": "8 different steering vectors", "paper_id": "248693452"}, {"section": "Recovery effectiveness", "paragraph": "Robustness Now that we have established that steering vector extraction is possible, we explore whether there exist multiple steering vectors which recover the same sentence, and if so, what the relationship is between these vectors. To do this, we take all 64 sentences from the books subset of the main dataset and initialize 8 different steering vectors for each sentence from different seeds. Experiments reveal that for most sentences (63 of 64) all initializations recover the target sentence perfectly, confirming the robustness of our method.", "selected": "8 different steering vectors", "paper_id": "248693452"}, {"section": "A.3 Sampling", "paragraph": "In order to evaluate whether we can sample steering vectors reliably, we collect 4,000 extracted steering vectors from the Yelp Sentiment test set. To generate, we consider each dimension of the steering vector as an independent random variable that is normally distributed. The dimension means and variances are equal to the mean and variance for that dimension across this set of steering vectors.", "selected": "e collect 4,000 extracted steering vectors from the Yelp Sentiment test set. To generate, we consider each dimension of the steering vector as an independent random variable that is normally distributed. The dimension means and variances are equal to the mean and variance for that dimension across this set of steering vectors.", "paper_id": "248693452"}, {"section": "A.3 Sampling", "paragraph": "In order to evaluate whether we can sample steering vectors reliably, we collect 4,000 extracted steering vectors from the Yelp Sentiment test set. To generate, we consider each dimension of the steering vector as an independent random variable that is normally distributed. The dimension means and variances are equal to the mean and variance for that dimension across this set of steering vectors.", "selected": "e collect 4,000 extracted steering vectors from the Yelp Sentiment test set. To generate, we consider each dimension of the steering vector as an independent random variable that is normally distributed. The dimension means and variances are equal to the mean and variance for that dimension across this set of steering vectors.", "paper_id": "248693452"}], "LbqH5+LUAi": [{"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best.", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best.", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Our approach adds", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Our approach adds", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "paper_id": "248693452"}, {"section": "Introduction", "paragraph": "Next, we take our extracted steering vectors and explore whether they can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark (Zhang et al., 2015). We find that adding an offset vector to extracted steering vectors performs comparably to carefully designed, autoencoderbased models. To see whether steering vectors encode semantics, we explore whether they can be used for unsupervised textual similarity. On the semantic textual similarity benchmark (STS-B, Cer et al. (2017)), our steering vectors outperform extractive methods such as averaging language model Figure 1: Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence. We experiment with adding z steer to different locations inside a GPT-2 model at different timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting the steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual connections inside the transformer block are omitted for clarity. hidden states and GloVe vectors (Pennington et al., 2014) when measuring the cosine similarity between vectors, but fall short of lexical methods tailored to semantic similarity tasks and methods that finetune on natural language inference datasets.", "selected": "Our approach adds a vector z steer to the activations of a pretrained transformer decoder to steer it to decode a desired target sentence.", "paper_id": "248693452"}]}}
{"idx": "197455", "paper_id": "33889881", "title": "Collecting bilingual technical terms from patent families of character-segmented Chinese sentences and morpheme-segmented Japanese sentences", "abstract": "In manual translation of patent documents, a technical term bilingual lexicon is inevitable for a translator to efficiently translate patent documents. Dong et al. (2015) proposed a method of generating bilingual technical term lexicon from morpheme-segmented parallel patent sentences. The proposed method estimates Japanese-Chinese translation of technical terms using the phrase translation table of a statistical machine translation model. The procedure of generating bilingual technical term lexicon consists of the following four steps: (1) extracting Japanese technical terms from Japanese side of parallel patent sentences, (2) collecting all the sentences that contain the extracted Japanese term, (3) generating Chinese translation of the Japanese technical term referring to the phrase translation table of a statistical machine translation model, and (4) applying the Support Vector Machines (SVMs) to the task of identifying bilingual technical terms. In this paper, we segment the Chinese sentences into characters instead of segmenting them into morphemes as in Dong et al. (2015), and represent JapaneseChinese patent families in terms of character-segmented Chinese sentences and morphemesegmented Japanese sentences. Then, to those Japanese-Chinese patent families, we apply the framework (Dong et al., 2015) of identifying bilingual technical terms. As a result, we achieve the performance of over 90% precision with the condition of more than or equal to 60% recall.", "context_section_header": "", "context_paragraph": "Among related works on acquiring bilingual lexicon from text, Itagaki et al. (2007) focused on automatic validation of translation pairs available in the phrase translation table trained by an SMT model. Itagaki et al. (2007) especially studied to apply a Gaussian mixture model based classifier to the task of automatic validation of translation pairs available in the phrase translation table. Yasuda and Sumita (2013) also studied to extract bilingual terms from comparable patents, where, they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences. Yasuda and Sumita (2013) especially studied to exploit kanji character similarity between Japanese and Chinese languages in the task of extracting Japanese-Chinese bilingual term pairs. It is also reported that two types of SMT phrase translation tables are integrated in this task. Haque et al. (2014a) also presented a bilingual terminology extraction method using the phrase translation table trained by a phrase-based SMT. One of the major differences of our approach and those proposed in Itagaki et al. (2007) , Yasuda and Sumita (2013) and Haque et al. (2014b) is that we apply the SVM-based classifier learning framework to the task of identifying bilingual technical term pairs from parallel patent sentences, where we examine various features extracted from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Lu and Tsou (2009) also studied to extract English-Chinese bilingual terms from patent families, where they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences based on an SVM classifier. One of the major differences of our approach and that proposed in Lu and Tsou (2009) is that our features studied in this paper are much finer-grained and cover wider range of information that are available from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Morishita et al. (2008) studied to acquire Japanese-English technical term translation lexicon from the phrase translation tables, which are trained by a phrase-based SMT model with parallel sentences automatically extracted from patent families. The approach taken in Morishita et al. (2008) is based on integrating the phrase translation table and compositional translation generation based on an existing bilingual lexicon for human use. This approach is quite effective in the case of language pairs such as Japanese and English, where an existing bilingual lexicon for human use is widely available. However, this is not always the case in the case of other language pairs such as Japanese and Chinese. Compared with the approach of Morishita et al. (2008), our approach is advantageous in that we concentrate on utilizing information that are available from patent families, but not rely on information source other than patent fami-lies. Also, compared with the features of SVM examined in Morishita et al. (2008), those we employed in this paper cover much wider range of information that are available from patent families. Especially, we concentrate more on utilizing features that are based on statistics of all the parallel sentences of the patent families rather than a single parallel sentence pair. In our proposed framework, we introduce the feature of the number of Chinese translation candidates (f 7 ), which was not examined in Morishita et al. (2008). We also use the rate of parallel sentences where phrase alignment is consistent with word alignments as a feature (f 8 ), while Morishita et al. (2008) used a binary feature which judges for each parallel sentence pair whether a phrase alignment is consistent with word alignments. Finally, we use the feature of the translation probability of compositional translation generation, which, through a preliminary evaluation, is proved to perform better than the binary feature of compositional translation generation employed in Morishita et al. (2008).", "sentence": "Also, compared with the features of SVM examined in Morishita et al. (2008), those we employed in this paper cover much wider range of information that are available from patent families.", "cited_ids": [{"paper_id": "7184744", "citation": "Morishita et al. (2008)"}], "y": "Compared with the features of SVM examined in Morishita et al. (2008), the authors [employed features including term frequency, translation probability, number of translation candidates] in this paper to cover a much wider range of [parallel sentences in two different languages] than is available from patent families.", "snippet_surface": "Additionally, compared with the features of SVM examined in Morishita et al. (2008), the authors' employed features in this paper cover a much wider range of information that is available from patent families.", "questions": {"DpeHJv2qlK": "What are the authors' employed features?", "uTE1JNYpXx": "What kind of information is available from patent families?"}, "answers": {"DpeHJv2qlK": "The features employed including term frequency, translation probability, number of translation candidates.", "uTE1JNYpXx": "Patent families provide an ever increasing dataset of parallel sentences in two different languages."}, "evidence": {"DpeHJv2qlK": [{"section": "Abstract", "paragraph": "In manual translation of patent documents, a technical term bilingual lexicon is inevitable for a translator to efficiently translate patent documents. Dong et al. (2015) proposed a method of generating bilingual technical term lexicon from morpheme-segmented parallel patent sentences. The proposed method estimates Japanese-Chinese translation of technical terms using the phrase translation table of a statistical machine translation model. The procedure of generating bilingual technical term lexicon consists of the following four steps: (1) extracting Japanese technical terms from Japanese side of parallel patent sentences, (2) collecting all the sentences that contain the extracted Japanese term, (3) generating Chinese translation of the Japanese technical term referring to the phrase translation table of a statistical machine translation model, and (4) applying the Support Vector Machines (SVMs) to the task of identifying bilingual technical terms. In this paper, we segment the Chinese sentences into characters instead of segmenting them into morphemes as in Dong et al. (2015), and represent JapaneseChinese patent families in terms of character-segmented Chinese sentences and morphemesegmented Japanese sentences. Then, to those Japanese-Chinese patent families, we apply the framework (Dong et al., 2015) of identifying bilingual technical terms. As a result, we achieve the performance of over 90% precision with the condition of more than or equal to 60% recall.", "selected": "In this paper, we segment the Chinese sentences into characters instead of segmenting them into morphemes as in Dong et al. (2015), and represent JapaneseChinese patent families in terms of character-segmented Chinese sentences and morphemesegmented Japanese sentences. Then, to those Japanese-Chinese patent families, we apply the framework (Dong et al., 2015) of identifying bilingual technical terms.", "paper_id": "33889881"}, {"section": "Features", "paragraph": "Among the bilingual features are the translation probability (f 3 ), rank of the Chinese translation candidates (f 4 ), co-occurrence frequency of the bilingual technical term pairs (f 5 ), the difference of the frequency of Japanese technical term and the co-occurrence frequency of bilingual technical term pairs (f 6 ) 11 , the number of Chinese translation candidates (f 7 ), rate of parallel sentences where phrase alignment is consistent with word alignments (f 8 ), and the translation probability when generating the Chinese translation candidates compositionally from constituents of the Japanese technical term (f 9 ) 12 .", "selected": "Among the bilingual features are the translation probability (f 3 ), rank of the Chinese translation candidates (f 4 ), co-occurrence frequency of the bilingual technical term pairs (f 5 ), the difference of the frequency of Japanese technical term and the co-occurrence frequency of bilingual technical term pairs (f 6 ) 11 , the number of Chinese translation candidates (f 7 ), rate of parallel sentences where phrase alignment is consistent with word alignments (f 8 ), and the translation probability when generating the Chinese translation candidates compositionally from constituents of the Japanese technical term (f 9 ) 12 .", "paper_id": "33889881"}], "uTE1JNYpXx": [{"section": "Features", "paragraph": "The following briefly describes why we employ those features introduced in this section. First, we observed that each term of a bilingual technical term pair tends to be a correct translation of each other when their frequencies are close to each other. Also, since we apply the polynomial (2nd order) kernel as the kernel function of SVMs, we simply introduce primitive features such as frequency of Japanese terms (f 1 ), frequency of Chinese terms (f 2 ), and cooccurrence frequency of bilingual technical term pairs (f 5 ), as well as the difference of those frequencies (f 6 ). In addition to that, they are correct translation of each other if they have a high translation probability and/or are ranked highly in the SMT phrase translation table. Thus, we use those information directly as the features of f 3 and f 4 . Furthermore, we define a feature for the translation probability of compositional translation generation using the phrase translation table (f 9 ). We also employ the number of translation candidates as another feature (f 7 ), since a term tends to be a technical term if the number of its translation candidates is small. Finally, we employ the rate of parallel sentences where phrase alignment is consistent with word alignments as a feature (f 8 ), since this rate tends to be large in the case of correct translation pairs. Table 5 shows the evaluation results for a baseline as well as for SVMs. As the baseline, we simply judge all of the input Japanese-Chinese technical term pairs as correct translation, which is exactly the same procedure as shown in Figure 1. In the tuning of the lower bound of the confidence measure, when maximizing precision, we achieve almost 94% precision while keeping recall almost 60% with the test data. When maximizing F-measure, we achieve almost 84% F-measure with around 80% precision and 87% recall. Table 6 also shows the evaluation results for each pair of the 13 frequency ranges of Japanese technical term frequency (jf ) and Japanese-Chinese co-occurrence frequency (jcf ). As shown in the table, in most pairs of the 13 frequency ranges of Japanese technical term frequency and Japanese-Chinese co-occurrence frequency, we achieve around 90% or higher precision. It is also obvious by comparing those evaluation results with the rates of positive examples for pairs of the 13 frequency ranges of Japanese technical term frequency (jf ) and Japanese-Chinese co-occurrence frequency (jcf ) in Table 1 that, we have lower recalls for certain frequency range pairs when the rates of positive examples are lower for those frequency range pairs 13 . 11 The upper bound 105 shown in Table 4 is used following the result of a preliminary experiment. 12 Patent families are one of the largest parallel sentences resource which contain lots of technical term pairs, and their number grows year by year. As the result of using patent families as knowledge source for solving the task of extracting bilingual technical term pairs, some of the features studied in this paper, such as co-occurrence frequency of the bilingual technical term pairs (f 5 ) and the number of Chinese translation candidates (f 7 ) and so on, happen to be effective only in the case of using patent families as knowledge source. 13 It is also interesting to note that the higher the frequencies of the Japanese technical terms are, the more the variety of translation candidates is, the more the rates of negative examples are, and finally the lower the recall is. On the contrary, the higher the co-occurrence frequencies of the Japanese-Chinese technical term pairs are, the more reliably   Table 7 shows examples of correct and erroneous SVMs' judgments. As shown in Table 7(a), a Japanese-Chinese technical term pair \" \", \" \" are correctly judged by SVM, mainly because its translation probability in the phrase translation table (f 3 ) is high and the rank of the Chinese translation candidate (f 4 ) is 1. In addition, its translation probability of compositional translation generation (f 9 ) are relatively high, and its number of Chinese translation candidates (f 7 ) are relatively small. Compared with this result of correct translation by the framework of this paper based on Chinese sentences segmented by characters, on the other hand, the framework of Dong et al. (2015) based on Chinese sentences segmented by morphemes translates the Japanese technical term \"", "selected": "Patent families are one of the largest parallel sentences resource which contain lots of technical term pairs, and their number grows year by year.", "paper_id": "33889881"}, {"section": "Related Work", "paragraph": "Among related works on acquiring bilingual lexicon from text, Itagaki et al. (2007) focused on automatic validation of translation pairs available in the phrase translation table trained by an SMT model. Itagaki et al. (2007) especially studied to apply a Gaussian mixture model based classifier to the task of automatic validation of translation pairs available in the phrase translation table. Yasuda and Sumita (2013) also studied to extract bilingual terms from comparable patents, where, they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences. Yasuda and Sumita (2013) especially studied to exploit kanji character similarity between Japanese and Chinese languages in the task of extracting Japanese-Chinese bilingual term pairs. It is also reported that two types of SMT phrase translation tables are integrated in this task. Haque et al. (2014a) also presented a bilingual terminology extraction method using the phrase translation table trained by a phrase-based SMT. One of the major differences of our approach and those proposed in Itagaki et al. (2007) , Yasuda and Sumita (2013) and Haque et al. (2014b) is that we apply the SVM-based classifier learning framework to the task of identifying bilingual technical term pairs from parallel patent sentences, where we examine various features extracted from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Lu and Tsou (2009) also studied to extract English-Chinese bilingual terms from patent families, where they first extract parallel sentences from patent families, and then extract bilingual terms from parallel sentences based on an SVM classifier. One of the major differences of our approach and that proposed in Lu and Tsou (2009) is that our features studied in this paper are much finer-grained and cover wider range of information that are available from parallel patent sentences themselves as well as the phrase translation table of a statistical machine translation model trained with those parallel patent sentences. Morishita et al. (2008) studied to acquire Japanese-English technical term translation lexicon from the phrase translation tables, which are trained by a phrase-based SMT model with parallel sentences automatically extracted from patent families. The approach taken in Morishita et al. (2008) is based on integrating the phrase translation table and compositional translation generation based on an existing bilingual lexicon for human use. This approach is quite effective in the case of language pairs such as Japanese and English, where an existing bilingual lexicon for human use is widely available. However, this is not always the case in the case of other language pairs such as Japanese and Chinese. Compared with the approach of Morishita et al. (2008), our approach is advantageous in that we concentrate on utilizing information that are available from patent families, but not rely on information source other than patent fami-lies. Also, compared with the features of SVM examined in Morishita et al. (2008), those we employed in this paper cover much wider range of information that are available from patent families. Especially, we concentrate more on utilizing features that are based on statistics of all the parallel sentences of the patent families rather than a single parallel sentence pair. In our proposed framework, we introduce the feature of the number of Chinese translation candidates (f 7 ), which was not examined in Morishita et al. (2008). We also use the rate of parallel sentences where phrase alignment is consistent with word alignments as a feature (f 8 ), while Morishita et al. (2008) used a binary feature which judges for each parallel sentence pair whether a phrase alignment is consistent with word alignments. Finally, we use the feature of the translation probability of compositional translation generation, which, through a preliminary evaluation, is proved to perform better than the binary feature of compositional translation generation employed in Morishita et al. (2008).", "selected": "Especially, we concentrate more on utilizing features that are based on statistics of all the parallel sentences of the patent families rather than a single parallel sentence pair.", "paper_id": "33889881"}]}}
{"idx": "2001.02284.1.2.1", "paper_id": "2001.02284", "title": "Multipurpose Intelligent Process Automation via Conversational Assistant", "abstract": "Intelligent Process Automation (IPA) is an emerging technology with a primary goal to assist the knowledge worker by taking care of repetitive, routine and low-cognitive tasks. Conversational agents that can interact with users in a natural language are potential application for IPA systems. Such intelligent agents can assist the user by answering specific questions and executing routine tasks that are ordinarily performed in a natural language (i.e., customer support). In this work, we tackle a challenge of implementing an IPA conversational assistant in a real-world industrial setting with a lack of structured training data. Our proposed system brings two significant benefits: First, it reduces repetitive and time-consuming activities and, therefore, allows workers to focus on more intelligent processes. Second, by interacting with users, it augments the resources with structured and to some extent labeled training data. We showcase the usage of the latter by re-implementing several components of our system with Transfer Learning (TL) methods.", "context_section_header": "Model ::: Dialogue Modules", "context_paragraph": "Natural Language Understanding (NLU): We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API. The NLU module contains following functionalities:", "sentence": "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API.", "cited_ids": [], "y": "The authors implemented an NLU unit utilizing handcrafted rules [particular explicit words that are associated with organisational, contextual, or mathematical intent], Regular Expressions (RegEx) and Elasticsearch (ES) API.", "snippet_surface": "The authors implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API.", "questions": {"kfP4YuiA54": "What are the \"handcrafted rules\"?"}, "answers": {"kfP4YuiA54": "The handcrafted rules: \n1) That particular words are explicit\n2) That the explicit words are associated with intent (organisational, contextual, or mathematical). "}, "evidence": {"kfP4YuiA54": [{"section": "Model ::: Dialogue Modules", "paragraph": "Natural Language Understanding (NLU): We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API. The NLU module contains following functionalities:", "selected": "The NLU module contains following functionalities:", "paper_id": "2001.02284"}, {"section": "Model ::: Dialogue Modules", "paragraph": "Natural Language Understanding (NLU): We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API. The NLU module contains following functionalities:", "selected": "We implemented an NLU unit utilizing handcrafted rules", "paper_id": "2001.02284"}, {"section": "Model ::: Dialogue Modules", "paragraph": "Intent classification: As we mentioned above, student questions can be grouped into three main categories: Organizational questions, contextual questions and mathematical questions. To classify the input message by its category or so-called intent, we utilized key-word information predefined by handcrafted rules. We assumed that particular words are explicit and associated with a corresponding intent. If no intent could be classified, then it is assumed that the NLU unit was not capable of understanding and the intent is interpreted as unknown. In this case, the system requests the user to provide an intent manually (by picking one from the mentioned three options). The questions from organizational and theoretical categories are directly delivered to a human tutor, while mathematical questions are processed by the automated system for further analysis.", "selected": "Intent classification: As we mentioned above, student questions can be grouped into three main categories: Organizational questions, contextual questions and mathematical questions.", "paper_id": "2001.02284"}, {"section": "Model ::: Dialogue Modules", "paragraph": "Intent classification: As we mentioned above, student questions can be grouped into three main categories: Organizational questions, contextual questions and mathematical questions. To classify the input message by its category or so-called intent, we utilized key-word information predefined by handcrafted rules. We assumed that particular words are explicit and associated with a corresponding intent. If no intent could be classified, then it is assumed that the NLU unit was not capable of understanding and the intent is interpreted as unknown. In this case, the system requests the user to provide an intent manually (by picking one from the mentioned three options). The questions from organizational and theoretical categories are directly delivered to a human tutor, while mathematical questions are processed by the automated system for further analysis.", "selected": "To classify the input message by its category or so-called intent, we utilized key-word information predefined by handcrafted rules. We assumed that particular words are explicit and associated with a corresponding intent.", "paper_id": "2001.02284"}]}}
{"idx": "2002.01664.1.1.1", "paper_id": "2002.01664", "title": "Identification of Indian Languages using Ghost-VLAD pooling", "abstract": "In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features.", "context_section_header": "INTRODUCTION", "context_paragraph": "In this paper, we explore multiple pooling strategies for language identification task. Mainly we propose Ghost-VLAD based pooling method for language identification. Inspired by the recent work by W. Xie et al. [9] and Y. Zhong et al. [10], we use Ghost-VLAD to improve the accuracy of language identification task for Indian languages. We explore multiple pooling strategies including NetVLAD pooling [11], Average pooling and Statistics pooling( as proposed in X-vectors [7]) and show that Ghost-VLAD pooling is the best pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset. In section 4, we describe the experiments, and in section 5, we describe the results.", "sentence": "We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.", "cited_ids": [], "y": "The authors conducted experiments [to determine a new pooling strategy (GhostVLAD) for language identification in Indian languages] on 635hrs of audio data for 7 Indian languages collected from \"All India Radio\" news channel.", "snippet_surface": "The authors conducted all their experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.", "questions": {"5hENcascs9": "What does \"their experiments\" refer to?"}, "answers": {"5hENcascs9": "The experiment is to determine a new pooling strategy (GhostVLAD) for language identification in Indian languages. The experiment made of use of 635 hours of audio which included 7 Indian languages."}, "evidence": {"5hENcascs9": [{"section": "Abstract", "paragraph": "In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features.", "selected": "a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task.", "paper_id": "2002.01664"}, {"section": "INTRODUCTION", "paragraph": "In this paper, we explore multiple pooling strategies for language identification task. Mainly we propose Ghost-VLAD based pooling method for language identification. Inspired by the recent work by W. Xie et al. [9] and Y. Zhong et al. [10], we use Ghost-VLAD to improve the accuracy of language identification task for Indian languages. We explore multiple pooling strategies including NetVLAD pooling [11], Average pooling and Statistics pooling( as proposed in X-vectors [7]) and show that Ghost-VLAD pooling is the best pooling strategy for language identification. Our model obtains the best accuracy of 98.24%, and it outperforms all the other previously proposed pooling methods. We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel. The paper is organized as follows. In section 2, we explain the proposed pooling method for language identification. In section 3, we explain our dataset. In section 4, we describe the experiments, and in section 5, we describe the results.", "selected": "We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.", "paper_id": "2002.01664"}]}}
{"idx": "2002.02070.1.1.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "sentence": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "cited_ids": [], "y": "The authors train a series of classifiers in order to classify car-speak [an abstract language that relates to the physical specifications of a car]. They train three classifiers on the review vectors that they prepared in Section SECREF8. The classifiers they use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "snippet_surface": "The authors train a series of classifiers in order to classify car-speak. They train three classifiers on the review vectors that they prepared in Section SECREF8. The classifiers they use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "questions": {"baTOJkmfzr": "What does \"car-speak\" mean?"}, "answers": {"baTOJkmfzr": "Car-speak is an abstract language that relates to the physical specifications of a car."}, "evidence": {"baTOJkmfzr": [{"section": "Abstract", "paragraph": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "selected": "Car-speak is abstract language that pertains to a car's physical attributes.", "paper_id": "2002.02070"}]}}
{"idx": "2002.02070.1.2.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak", "context_paragraph": "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.", "sentence": "In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.", "cited_ids": [], "y": "The authors need to filter their dataset [which includes 3,209 reviews about 553 different cars from 49 car manufacturers] so that they have only the most relevant terms in order to accomplish the goal of translating and classifying car-speak [an abstract language related to the physical specifications of a car that is used by potential buyers in order to communicate their requirements]. The authors need to be able to weight each word in each review in order to determine the most relevant ideas in each document for the purpose of classification. The authors train classification models [to understand car-speak data, and aid in finding a path to automate car dealers in dealerships].", "snippet_surface": "The authors need to filter their data set so that they only have the most relevant terms in order to accomplish their goal of translating and classifying car-speak. They then need to be able to weight each word in each review, so that they can determine the most relevant ideas in each document for the purpose of classification. Finally, they need to train various classification models and evaluate them.", "questions": {"zvZmkp1nmU": "What is \"car-speak\"?", "XY7119iolD": "What does \"their data\" refer to?", "V8xQIFs1ju": "Why do they need to train \"various classification models\"?"}, "answers": {"zvZmkp1nmU": "Car-speak is used by potential buyers in order to communicate their requirements; it is an abstract language that relates to the physical specifications of a car.", "XY7119iolD": "The data collected by the author includes 3,209 reviews about 553 different cars from 49 car manufacturers. The data includes car-speak language.", "V8xQIFs1ju": "Various classification models are required to understand car-speak data, and aid in finding a path to automate car dealers in dealerships."}, "evidence": {"zvZmkp1nmU": [{"section": "What is Car-speak?", "paragraph": "When a potential buyer begins to identify their next car-purchase they begin with identifying their needs. These needs often come in the form of an abstract situation, for instance, \u201cI need a car that goes really fast\u201d. This could mean that they need a car with a V8 engine type or a car that has 500 horsepower, but the buyer does not know that, all they know is that they need a \u201cfast\u201d car.", "selected": "When a potential buyer begins to identify their next car-purchase they begin with identifying their needs.", "paper_id": "2002.02070"}, {"section": "What is Car-speak?", "paragraph": "The term \u201cfast\u201d is car-speak. Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to.", "selected": "Car-speak is abstract language that pertains to a car's physical attribute(s).", "paper_id": "2002.02070"}], "XY7119iolD": [{"section": "Abstract", "paragraph": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "selected": "We also aim to curate a reasonable data set of car-speak language.", "paper_id": "2002.02070"}, {"section": "Translating Car-Speak", "paragraph": "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.", "selected": "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms.", "paper_id": "2002.02070"}], "V8xQIFs1ju": [{"section": "Conclusion & Future Work", "paragraph": "In this paper we aim to provide an introductory understanding of car-speak and a way to automate car dealers at dealerships. We first provide a definition of \u201ccar-speak\u201d in Section SECREF3. We explore what constitutes car-speak and how to identify car-speak.", "selected": "In this paper we aim to provide an introductory understanding of car-speak and a way to automate car dealers at dealerships.", "paper_id": "2002.02070"}, {"section": "Conclusion & Future Work", "paragraph": "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", "selected": "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", "paper_id": "2002.02070"}, {"section": "Conclusion & Future Work", "paragraph": "In the future we plan to use more complex models to attempt to understand car-speak. We also would like to test our classifiers on user-provided natural language queries. This would be a more practical evaluation of our classification. It would also satisfy the need for a computer system that understands car-speak.", "selected": "In the future we plan to use more complex models to attempt to understand car-speak.", "paper_id": "2002.02070"}]}}
{"idx": "2002.02070.5.1.1", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "sentence": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "cited_ids": [], "y": "The authors evaluate [four classifiers: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron] by performing 4-fold cross validation on a shuffled data set. The work shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier appear to perform the best across all four metrics. This is likely due to the multi-class nature of the data set.", "snippet_surface": "The authors evaluate their classifiers by performing 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier appear to perform the best across all four metrics. This is likely due to the multi-class nature of the data set.", "questions": {"haqC8n4/0S": "What does \"their classifiers\" refer to?"}, "answers": {"haqC8n4/0S": "It refers to the classifiers: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron, that were trained on the review vectors"}, "evidence": {"haqC8n4/0S": [{"section": "Translating Car-Speak ::: Classification Experiments", "paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "selected": "e K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)", "paper_id": "2002.02070"}]}}
{"idx": "2002.02070.6.1.2", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", "sentence": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers.", "cited_ids": [], "y": "The authors evaluated their classifiers [K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)] by performing 4-fold cross validation on a shuffled data set.", "snippet_surface": "The authors evaluated their classifiers by performing 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers.", "questions": {"haqC8n4/0S": "What does \"their classifiers\" refer to?"}, "answers": {"haqC8n4/0S": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)"}, "evidence": {"haqC8n4/0S": [{"section": "Translating Car-Speak ::: Classification Experiments", "paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "selected": "The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)", "paper_id": "2002.02070"}]}}
{"idx": "2002.02070.6.2.1", "paper_id": "2002.02070", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "abstract": "A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in \"car-speak\". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak.", "context_section_header": "Translating Car-Speak ::: Classification Experiments", "context_paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "sentence": "The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "cited_ids": [], "y": "The authors use K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) [to classify car-speak].", "snippet_surface": "The authors use K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13 as classifiers.", "questions": {"V4+X+DZ4+t": "Why were these classifiers trained?"}, "answers": {"V4+X+DZ4+t": "The classifiers are trained to classify car-speak."}, "evidence": {"V4+X+DZ4+t": [{"section": "Translating Car-Speak ::: Classification Experiments", "paragraph": "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "selected": "We train a series of classifiers in order to classify car-speak.", "paper_id": "2002.02070"}]}}
{"idx": "2003.03106.1.1.1", "paper_id": "2003.03106", "title": "Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT", "abstract": "Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes while preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however, depending on the type of data, the target language or the availability of training documents, the task remains challenging still. The emergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the field of Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018, and the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to conduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The experiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any domain specific feature engineering.", "context_section_header": "Results ::: Experiment A: NUBes-PHI", "context_paragraph": "To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems. Despite the fact that, in general, the systems obtain high values, BERT outperforms them again. BERT's F1-score is 1.9 points higher than the next most competitive result in the comparison. More remarkably, the recall obtained by BERT is about 5 points above.", "sentence": "To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems.", "cited_ids": [], "y": "The authors finish an experiment [that uses data from NUBes-PHI, a corpus of medical reports that classifies clinical text through] strict classification precision, recall and F1-score.", "snippet_surface": "The authors finish this experiment set with a Table showing the strict classification precision, recall and F1-score for the compared systems.", "questions": {"lhZYtX/Vtl": "What does \"this experiment\" refer to?"}, "answers": {"lhZYtX/Vtl": "The experiment that uses the NUBes-PHI one corpus of medical reports annotated with sensitive information."}, "evidence": {"lhZYtX/Vtl": [{"section": "Materials and Methods ::: Experimental design", "paragraph": "We have conducted experiments with BERT in the two datasets of Spanish clinical narrative presented in Section SECREF3 The first experiment set uses NUBes-PHI, a corpus of real medical reports manually annotated with sensitive information. Because this corpus is not publicly available, and in order to compare the BERT-based model to other related published systems, the second set of experiments uses the MEDDOCAN 2019 shared task competition dataset. The following sections provide greater detail about the two experimental setups.", "selected": "The first experiment set uses NUBes-PHI, a corpus of real medical reports manually annotated with sensitive information. Because this corpus is not publicly available, and in order to compare the BERT-based model to other related published systems", "paper_id": "2003.03106"}, {"section": "Materials and Methods ::: Experimental design ::: Experiment A: NUBes-PHI", "paragraph": "In this experiment set, we evaluate all the systems presented in Section SECREF7, namely, the rule-based baseline, the CRF classifier, the spaCy entity tagger, and BERT. The evaluation comprises three scenarios of increasing difficulty:", "selected": "In this experiment set, we evaluate all the systems presented in Section SECREF7, namely, the rule-based baseline, the CRF classifier, the spaCy entity tagger, and BERT.", "paper_id": "2003.03106"}]}}
{"idx": "2003.08380.1.1.1", "paper_id": "2003.08380", "title": "TTTTTackling WinoGrande Schemas", "abstract": "We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the\"entailment\"token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.", "context_section_header": "Results", "context_paragraph": "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. Since T5-3B is larger than RoBERTa, it cannot be ruled out that model size alone explains the performance gain. However, when coupled with the observations of Nogueira et al. BIBREF7, T5's \u201cgenerative capability\u201d, i.e., its ability to generate fluent text, honed through pretraining, seems to play an important role. The fact that the choice of target tokens affects prediction accuracy is consistent with this observation. How and why is the subject of ongoing work.", "sentence": "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture.", "cited_ids": [], "y": "Regarding the WinoGrande leaderboard, the then state-of-the-art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", "snippet_surface": "Looking at the WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture.", "questions": {"oqVmL8PMRQ": "Are there any other standards/leaderboards etc.that rank different architectures for the WinoGrande dataset?"}, "answers": {"oqVmL8PMRQ": "Now, there are no other WinoGrande leaderboards other than the official WinoGrande one."}, "evidence": {"oqVmL8PMRQ": [{"section": "Abstract", "paragraph": "We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the\"entailment\"token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.", "selected": "Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020,", "paper_id": "2003.08380"}]}}
{"idx": "2003.13032.5.1.1", "paper_id": "2003.13032", "title": "Named Entities in Medical Case Reports: Corpus and Experiments", "abstract": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "context_section_header": "A Corpus of Medical Case Reports with Medical Entity Annotation ::: Corpus Overview", "context_paragraph": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. However, as mentioned above, only case presentation sections, headings and abstracts are annotated. The numbers of annotated entities are summarized in Table TABREF24.", "sentence": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "cited_ids": [], "y": "The authors' corpus consists of 53 documents [medical case reports with medical entity annotation which are used for annotating the document for medically relevant categories such as conditions, findings and other things], which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "snippet_surface": "The authors' corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.", "questions": {"8wgkFiOMyE": "What does \"53 documents\" refer to?", "LMv+5VdE0J": "What are the documents used for?"}, "answers": {"8wgkFiOMyE": "Medical case reports with medical entity annotation.", "LMv+5VdE0J": "They annotate the document for medically relevant categories such as conditions, findings, etc..."}, "evidence": {"8wgkFiOMyE": [], "LMv+5VdE0J": [{"section": "Abstract", "paragraph": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "selected": "It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection.", "paper_id": "2003.13032"}, {"section": "Abstract", "paragraph": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "selected": "Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "paper_id": "2003.13032"}, {"section": "Abstract", "paragraph": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "selected": "In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities.", "paper_id": "2003.13032"}, {"section": "Introduction", "paragraph": "The corpus most comparable to ours is the French corpus of clinical case reports by grouin-etal-2019-clinical. Their annotations are based on UMLS semantic types. Even though there is an overlap in annotated entities, semantic classes are not the same. Lab results are subsumed under findings in our corpus and are not annotated as their own class. Factors extend beyond gender and age and describe any kind of risk factor that contributes to a higher probability of having a certain disease. Our corpus includes additional entity types. We annotate conditions, findings (including medical findings such as blood values), factors, and also modifiers which indicate the negation of other entities as well as case entities, i. e., entities specific to one case report. An overview is available in Table TABREF3.", "selected": "We annotate conditions, findings (including medical findings such as blood values), factors, and also modifiers which indicate the negation of other entities as well as case entities, i. e., entities specific to one case report.", "paper_id": "2003.13032"}]}}
{"idx": "2003.13032.6.2.2", "paper_id": "2003.13032", "title": "Named Entities in Medical Case Reports: Corpus and Experiments", "abstract": "We present a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. In the case reports, we annotate cases, conditions, findings, factors and negation modifiers. Moreover, where applicable, we annotate relations between these entities. As such, this is the first corpus of this kind made available to the scientific community in English. It enables the initial investigation of automatic information extraction from case reports through tasks like Named Entity Recognition, Relation Extraction and (sentence/paragraph) relevance detection. Additionally, we present four strong baseline systems for the detection of medical entities made available through the annotated dataset.", "context_section_header": "", "context_paragraph": "Baseline systems for Named Entity Recognition in medical case reports ::: BiLSTM-CRF", "sentence": "Baseline systems for Named Entity Recognition in medical case reports ::: BiLSTM-CRF", "cited_ids": [], "y": "The authors present a baseline system for Named Entity Recognition in medical case reports, BiLSTM-CRF, [which consists of a BioWordVec, followed by a bidirectional LSTM and a CRF layer].", "snippet_surface": "The authors present a baseline system for Named Entity Recognition in medical case reports ::: BiLSTM-CRF.", "questions": {"4VWDbUY84+": "What does \"bilstm-crf\" consist of?"}, "answers": {"4VWDbUY84+": "A BioWordVec, followed by a bidirectional LSTM and a CRF layer."}, "evidence": {"4VWDbUY84+": [{"section": "Baseline systems for Named Entity Recognition in medical case reports ::: BiLSTM-CRF", "paragraph": "Prior to the emergence of deep neural language models, BiLSTM-CRF models BIBREF11 had achieved state-of-the-art results for the task of sequence labeling. We use a BiLSTM-CRF model with both word-level and character-level input. BioWordVec BIBREF12 pre-trained word embeddings are used in the embedding layer for the input representation. A bidirectional LSTM layer is applied to a multiplication of the two input representations. Finally, a CRF layer is applied to predict the sequence of labels. Dropout and L1/L2 regularization is used where applicable. He (uniform) initialization BIBREF13 is used to initialize the kernels of the individual layers. As the loss metric, CRF-based loss is used, while optimizing the model based on the CRF Viterbi accuracy. Additionally, span-based F1 score is used to serialize the best performing model. We train for a maximum of 100 epochs, or until an early stopping criterion is reached (no change in validation loss value grater than 0.01 for ten consecutive epochs). Furthermore, Adam BIBREF14 is used as the optimizer. The learning rate is reduced by a factor of 0.3 in case no significant increase of the optimization metric is achieved in three consecutive epochs.", "selected": ". We use a BiLSTM-CRF model with both word-level and character-level input. BioWordVec BIBREF12 pre-trained word embeddings are used in the embedding layer for the input representation. A bidirectional LSTM layer is applied to a multiplication of the two input representations. Finally, a CRF layer is applied to predict the sequence of labels. Dropout and L1/L2 regularization is used where applicable.", "paper_id": "2003.13032"}]}}
{"idx": "2004.01853.1.1.1", "paper_id": "2004.01853", "title": "STEP: Sequence-to-Sequence Transformer Pre-training for Document Summarization", "abstract": "Abstractive summarization aims to rewrite a long document to its shorter form, which is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Seq2Seq Transformers are powerful models for this problem. Unfortunately, training large Seq2Seq Transformers on limited supervised summarization data is challenging. We, therefore, propose STEP (as shorthand for Sequence-to-Sequence Transformer Pre-training), which can be trained on large scale unlabeled documents. Specifically, STEP is pre-trained using three different tasks, namely sentence reordering, next sentence generation, and masked document generation. Experiments on two summarization datasets show that all three tasks can improve performance upon a heavily tuned large Seq2Seq Transformer which already includes a strong pre-trained encoder by a large margin. By using our best task to pre-train STEP, we outperform the best published abstractive model on CNN/DailyMail by 0.8 ROUGE-2 and New York Times by 2.4 ROUGE-2.", "context_section_header": "Introduction", "context_paragraph": "Based on the above observations, we propose Step (as shorthand for Sequence-to-Sequence TransformEr Pre-training), which can be pre-trained on large scale unlabeled documents. Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG). SR learns to recover a document with randomly shuffled sentences. NSG generates the next segment of a document based on its preceding segment. MDG recovers a masked document to its original form. After pre-trianing Step using the three tasks on unlabeled documents, we fine-tune it on supervised summarization datasets.", "sentence": "Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG).", "cited_ids": [], "y": "The authors create three tasks specifically for the seq2seq model pre-training [of an abstractive document summarisation model which uses an encode and decode transformer]. These tasks are sentence reordering (SR) [for texts having multiple sentences], next sentence generation (NSG) [that assumes correlated sentences], and masked document generation (MDG).", "snippet_surface": "The authors specifically design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG).", "questions": {"3z2KG1bjHX": "Why did the authors specifically design these three tasks?", "J2BytBmAtH": "What does \"seq2seq\" mean?"}, "answers": {"3z2KG1bjHX": "Why in general: to pre trained the encoder and decoder.\nSR: natural approach as a text is made of multiple sentences.\nNSG: assume sentences are correlated.\nMDG: honestly not clear. Make sense conceptually but not detailed in the text.", "J2BytBmAtH": "Seq2seq means sequence to sequence and is an abstractive document summarisation model using an encode and decode transformer."}, "evidence": {"3z2KG1bjHX": [{"section": "Related Work ::: Pre-training", "paragraph": "Pre-training methods draw a lot of attention recently. peters2018deep and radford:2019:arxiv pre-trained LSTM and Transformer encoders using language modeling objectives. To leverage the context in both directions, BIBREF2 proposed BERT, which is trained with the mask language modeling objective. XLNet BIBREF3 is trained with permutation language modeling objective, which removes the independence assumption of masked tokens in BERT. RoBERTa BIBREF4 extends BERT with more training data and better training strategies. All the methods above focus on pre-training an encoder, while we propose methods to pre-train both the encoder and decoder of a seq2seq model.", "selected": "All the methods above focus on pre-training an encoder,", "paper_id": "2004.01853"}, {"section": "Sequence-to-Sequence Transformer Pre-training ::: Pre-training Tasks ::: Sentence Reordering", "paragraph": "A document is typically composed of multiple sentences separated by full stops. In this task, we first shuffle the document by sentences and then recover the original document. There are several reasons why we design this task. First, a summary of a document usually consists of multiple sentences. We expect that Step learns to generate long and coherent summaries (across sentences). The output of the task (i.e., the original document) also contains multiple sentences. Second, sentence reordering (or content reordering) is necessary for summarization. According to the statistics on training sets of our summarization datasets, contents of the original documents are reordered in their summaries for 40% of cases. We define content reordering as follows. For each document-summary pair, we first map each sentence in the summary to one sentence in its paired document by maximizing the ROUGE score. If the sequence of sentences in the summary is different from the sequence of their mapped sentences in the original document, we count this as one content reordering. Thirdly, abstractive summary requires reproducing factual details (e.g., named entities, figures) from source text. We also expect Step to learn to copy tokens. Here is a formal definition of this task. Let us change the notation of a document slightly in this paragraph. Let $X=(S_1, S_2, \\dots , S_m)$ denote a document, where $S_i = (w^i_1, w^i_2, \\dots , w^i_{|S_i|})$ is a sentence in it, $w^i_j$ is a word in $S_i$ and $m$ is the number of sentences. $X$ is still a sequence of tokens (by concatenating tokens in all sentences). Let $A=\\text{\\tt permutation}(m)=(a_1,a_2,\\dots , a_m)$ denote a permuted range of $(1, 2, \\dots , m)$ and therefore $\\hat{X}_S=(S_{a_1}, S_{a_2}, \\dots , S_{a_m})$ is the shuffled document. Note that $\\hat{X}_S$ is a sequence of tokens by concatenating all shuffled sentences. Step can be trained on $\\langle \\hat{X}_S, X \\rangle $ pairs constructed from unlabeled documents, as demonstrated in Figure FIGREF5.", "selected": "There are several reasons why we design this task. First, a summary of a document usually consists of multiple sentences. We expect that Step learns to generate long and coherent summaries (across sentences). The output of the task (i.e., the original document) also contains multiple sentences. Second, sentence reordering (or content reordering) is necessary for summarization.", "paper_id": "2004.01853"}, {"section": "Sequence-to-Sequence Transformer Pre-training ::: Pre-training Tasks ::: Next Sentence Generation", "paragraph": "The second pre-training task leverages the natural order of text. Next Sentence Generation (NSG) uses one span of text in a document to predict its next span of text, as shown in Figure FIGREF5. Specifically, we split a document into two segments (i.e., $G_1$ and $G_2$). Note that each segment might contain multiple sentences, which fits the document summarization task very well, since either a document or its summary usually includes multiple sentences. Intuitively, in a document, sentences are highly correlated with their preceding sentences due to the context dependent nature of documents or language. We intend our model to learn to generate multiple sentences and also learn to focus on preceding context.", "selected": "Intuitively, in a document, sentences are highly correlated with their preceding sentences due to the context dependent nature of documents or language. We intend our model to learn to generate multiple sentences and also learn to focus on preceding context.", "paper_id": "2004.01853"}, {"section": "Sequence-to-Sequence Transformer Pre-training ::: Pre-training Tasks ::: Masked Document Generation", "paragraph": "The third task we consider is Masked Document Generation (MDG) that learns to recover a document with a masked span of tokens (see Figure FIGREF5). For simplicity, a document consisting of a sequence of tokens is denoted as $X=(x_1, x_2, \\cdots , x_{|X|})$. We randomly sample the length of the span $l$ from a discrete uniform distribution $\\mathcal {U}(a, b)$ and the span start position $k$ from another discrete uniform distribution $\\mathcal {U}(1, |X|-l+1)$ (see Section SECREF4 for more details). Thus, $\\mathcal {M}=(x_k, x_{k+1}, \\cdots , x_{k+l-1})$ is the text span to be masked.", "selected": "The third task we consider is Masked Document Generation (MDG) that learns to recover a document with a masked span of tokens", "paper_id": "2004.01853"}], "J2BytBmAtH": [{"section": "Introduction", "paragraph": "Abstractive document summarization aims to rewrite a long document to its shorter form while still retaining its important information. Different from extractive document summarization that extacts important sentences, abstractive document summarization may paraphrase original sentences or delete contents from them. For more details on differences between abstractive and extractive document summary, we refer the interested readers to Nenkova:McKeown:2011 and Section SECREF2. This task is usually framed as a sequence-to-sequence learning problem BIBREF10, BIBREF11. In this paper, we adopt the sequence-to-sequence (seq2seq) Transformer BIBREF9, which has been demonstrated to be the state-of-the-art for seq2seq modeling BIBREF9, BIBREF12. Unfortunately, training large seq2seq Transformers on limited supervised summarization data is challenging BIBREF12 (refer to Section SECREF5). The seq2seq Transformer has an encoder and a decoder Transformer. Abstractive summarization requires both encoding of an input document and generation of a summary usually containing multiple sentences. As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in liu2019text. However, liu2019text leave the decoder randomly initialized. In this paper, we aim to pre-train both the encoder (i.e., the encoding part) and decoder (i.e., the generation part) of a seq2seq Transformer , which is able to improve abstractive summarization performance.", "selected": "The seq2seq Transformer has an encoder and a decoder Transformer.", "paper_id": "2004.01853"}, {"section": "Introduction", "paragraph": "Abstractive document summarization aims to rewrite a long document to its shorter form while still retaining its important information. Different from extractive document summarization that extacts important sentences, abstractive document summarization may paraphrase original sentences or delete contents from them. For more details on differences between abstractive and extractive document summary, we refer the interested readers to Nenkova:McKeown:2011 and Section SECREF2. This task is usually framed as a sequence-to-sequence learning problem BIBREF10, BIBREF11. In this paper, we adopt the sequence-to-sequence (seq2seq) Transformer BIBREF9, which has been demonstrated to be the state-of-the-art for seq2seq modeling BIBREF9, BIBREF12. Unfortunately, training large seq2seq Transformers on limited supervised summarization data is challenging BIBREF12 (refer to Section SECREF5). The seq2seq Transformer has an encoder and a decoder Transformer. Abstractive summarization requires both encoding of an input document and generation of a summary usually containing multiple sentences. As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in liu2019text. However, liu2019text leave the decoder randomly initialized. In this paper, we aim to pre-train both the encoder (i.e., the encoding part) and decoder (i.e., the generation part) of a seq2seq Transformer , which is able to improve abstractive summarization performance.", "selected": "Abstractive document summarization aims to rewrite a long document to its shorter form while still retaining its important information.", "paper_id": "2004.01853"}, {"section": "Introduction", "paragraph": "Abstractive document summarization aims to rewrite a long document to its shorter form while still retaining its important information. Different from extractive document summarization that extacts important sentences, abstractive document summarization may paraphrase original sentences or delete contents from them. For more details on differences between abstractive and extractive document summary, we refer the interested readers to Nenkova:McKeown:2011 and Section SECREF2. This task is usually framed as a sequence-to-sequence learning problem BIBREF10, BIBREF11. In this paper, we adopt the sequence-to-sequence (seq2seq) Transformer BIBREF9, which has been demonstrated to be the state-of-the-art for seq2seq modeling BIBREF9, BIBREF12. Unfortunately, training large seq2seq Transformers on limited supervised summarization data is challenging BIBREF12 (refer to Section SECREF5). The seq2seq Transformer has an encoder and a decoder Transformer. Abstractive summarization requires both encoding of an input document and generation of a summary usually containing multiple sentences. As mentioned earlier, we can take advantage of recent pre-trained Transformer encoders for the document encoding part as in liu2019text. However, liu2019text leave the decoder randomly initialized. In this paper, we aim to pre-train both the encoder (i.e., the encoding part) and decoder (i.e., the generation part) of a seq2seq Transformer , which is able to improve abstractive summarization performance.", "selected": "sequence-to-sequence (seq2seq)", "paper_id": "2004.01853"}]}}
{"idx": "201666", "paper_id": "235313592", "title": "Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning", "abstract": "Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction.", "context_section_header": "", "context_paragraph": "Relation Fact Extraction. In this work, we show that all of the relational fact extraction models can be unified into a graph-oriented output structure analytical framework. From the perspective of graph representation, the prior models can be divided into three categories. Edge List, this type of model usually employs sequence-to-sequence fashion, such as NovelTagging (Zheng et al., 2017), CopyRE (Zeng et al., 2018), CopyRL (Zeng et al.,    2019), and PNDec (Nayak and Ng, 2020). Some models of this category may suffer from the triplet overlapping problem and expensive extraction cost. Adjacency Matrices, many early pipeline approaches (Zelenko et al., 2003;Zhou et al., 2005;Mintz et al., 2009) and recent neural network-based models (Bekoulis et al., 2018;Dai et al., 2019;Fu et al., 2019), can be classified into this category. The main problem for this type of model is the graph representation efficiency. Adjacency List, the recent state-of-the-art model CasRel (Wei et al., 2020) is a partially adjacency list oriented model. In this work, we propose DIRECT that is a fully adjacency list oriented relational fact extraction model. To the best of our knowledge, few previous works analyze this task from the output data structure perspective. GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure. Our work is a pioneer investigation to analyze the output data structure of relational fact extraction.", "sentence": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure.", "cited_ids": [{"paper_id": "196211486", "citation": "(Fu et al., 2019)"}], "y": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while the authors analyze it from the perspective of output structure [the specific type of variable used to encode the output data, e.g., a list or a (sparse) matrix.]", "snippet_surface": "GraphRel (Fu et al., 2019) employs a graph-based approach, but it is utilized from an encoding perspective, while the authors analyze it from the perspective of output structure.", "questions": {"WPPU9oIp2a": "What does output structure mean in this context?"}, "answers": {"WPPU9oIp2a": "The specific type of variable used to encode the output data, e.g., a list or a (sparse) matrix."}, "evidence": {"WPPU9oIp2a": [{"section": "Introduction", "paragraph": "Compared to edge list, adjacency matrices have better relation (edge) searching efficiency (Arifuzzaman and Khan, 2015). Furthermore, adjacency matrices oriented models is able to cover different overlapping cases (Zeng et al., 2018) for relational fact extraction task. But the space cost of this approach can be expensive. For most cases, the output matrices are very sparse. For instance, for a sentence with n tokens, if there are m kinds of relations, the output space is n \u00b7 n \u00b7 m, which can be costly for graph representation efficiency. This phenomenon is also illustrated in Figure 1.", "selected": "But the space cost of this approach can be expensive. For most cases, the output matrices are very sparse. For instance, for a sentence with n tokens, if there are m kinds of relations, the output space is n \u00b7 n \u00b7 m, which can be costly for graph representation efficiency.", "paper_id": "235313592"}, {"section": "The DIRECT Framework", "paragraph": "In this section, we will introduce the framework of the proposed DIRECT model, which includes a shared BERT encoder and three output layers: subject extraction, object extraction, and relation classification. As shown in Figure 2, DIRECT is fully adjacency list oriented. The input sentence is firstly fed into the subject extraction module to extract all subjects. Then each extracted subject is concatenated with the sentence, and fed into the object extraction module to extract all objects, which can form a set of subject-object pairs. Finally, the subject-object pair is concatenated with sentence, and fed into the relation classification module to get the relations between them. For balancing the weights of sub-task losses and to improve the global task performance, three modules share the BERT encoder layer and are trained with an adaptive multi-task learning strategy.", "selected": "In this section, we will introduce the framework of the proposed DIRECT model, which includes a shared BERT encoder and three output layers:", "paper_id": "235313592"}]}}
{"idx": "201707", "paper_id": "1582646", "title": "Stacking for Statistical Machine Translation", "abstract": "We propose the use of stacking, an ensemble learning technique, to the statistical machine translation (SMT) models. A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model.", "context_section_header": "", "context_paragraph": "Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "sentence": "Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "cited_ids": [{"paper_id": "7437692", "citation": "Duan et al. (2010)"}], "y": "The authors' approach [is the same as Duan et al.'s approach, except] in a number of ways: i) the authors use cross-validation-style partitioning for creating training subsets while Duan et al. (2010) do sampling without replacement (80% of the training set); ii) in the authors' approach a number of base models [SMT Log-linear models] are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features [(i.e., offline combinations of phrases in tables)]; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, the authors' method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) the authors' approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "snippet_surface": "The authors' approach differs from this approach in a number of ways: i) they use cross-validation-style partitioning for creating training subsets while Duan et al. (2010) do sampling without replacement (80% of the training set); ii) in their approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, the authors' method not only uses all available data for training, but promotes", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "KPZJQ9USqy": "What base models are trained?", "a2cUnHnT28": "What is ensemble decoding?"}, "answers": {"6gKwRw0I/Q": "Same approach as Duan et al. (2010), plus, using cross-validation style partitioning, models are trained and tuned, promoting diversity of training by allowing models to tune on a different data set and taking advantage of held-out data in the training of base models.", "KPZJQ9USqy": "SMT Log-linear models", "a2cUnHnT28": "A combination of several models dynamically at decoding time."}, "evidence": {"6gKwRw0I/Q": [{"section": "Related Work", "paragraph": "Our approach differs from this approach in a number of ways: i) we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "selected": "we use cross-validation-style partitioning for creating training subsets while they do sampling without replacement (80% of the training set); ii) in our approach a number of base models are trained and tuned and they are combined on-the-fly in the decoder using ensemble decoding which has been shown to be more effective than offline combination of phrase-table-only features; iii) in Duan et al. (2010)'s method, each system gives up 20% of the training data in exchange for more diversity, but in contrast, our method not only uses all available data for training, but promotes diversity through allowing each model to tune on a different data set; iv) our approach takes advantage of held out data (the tuning set) in the training of base models which is beneficial especially when little parallel data is available or tuning/test sets and training sets are from different domains.", "paper_id": "1582646"}], "KPZJQ9USqy": [{"section": "Ensemble Decoding", "paragraph": "SMT Log-linear models (Koehn, 2010) find the most likely target language output e given the source language input f using a vector of feature functions \u03c6:", "selected": "SMT Log-linear models (Koehn, 2010) find the most likely target language output e given the source language input f using a vector of feature functions \u03c6:", "paper_id": "1582646"}], "a2cUnHnT28": [{"section": "Ensemble Decoding", "paragraph": "Ensemble decoding combines several models dynamically at decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation \u2297 over component models.", "selected": "Ensemble decoding combines several models dynamically at decoding time. The", "paper_id": "1582646"}]}}
{"idx": "201907", "paper_id": "202121966", "title": "Aligning Cross-Lingual Entities with Multi-Aspect Information", "abstract": "Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.", "context_section_header": "", "context_paragraph": "In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features.", "sentence": "However, in contrast to Wang et al. (2018), we regard relation features as input to our models.", "cited_ids": [{"paper_id": "53082628", "citation": "Wang et al. (2018)"}], "y": "The authors [utilize] relation features, [i.e. using the number in parentheses after entitiy names] as input to their models [two variants of GCNbased models], in contrast to Wang et al. (2018).", "snippet_surface": "However, in contrast to Wang et al. (2018), the authors regard relation features as input to their models.", "questions": {"35cbQ+dcLT": "What is \"their models\"?"}, "answers": {"35cbQ+dcLT": "Two variants of GCNbased models."}, "evidence": {"35cbQ+dcLT": [{"section": "Descriptions", "paragraph": "In this work, we propose a novel approach to learn cross-lingual entity embeddings by using all aforementioned aspects of information in KGs. To be specific, we propose two variants of GCNbased models, namely MAN and HMAN, that incorporate multi-aspect features, including topological features, relation types, and attributes into cross-lingual entity embeddings. To capture semantic relatedness of literal descriptions, we finetune the pretrained multilingual BERT model (Devlin et al., 2019) to bridge cross-lingual gaps. We design two strategies to combine GCN-based and BERT-based modules to make alignment decisions. Experiments show that our method achieves new state-of-the-art results on two benchmark datasets. Source code for our models is publicly available at https://github.com/ h324yang/HMAN.", "selected": "In this work, we propose a novel approach to learn cross-lingual entity embeddings by using all aforementioned aspects of information in KGs. T", "paper_id": "202121966"}]}}
{"idx": "201966", "paper_id": "3795383", "title": "Grammatical Error Correction Using Feature Selection and Confidence Tuning", "abstract": "This paper proposes a novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors. Most previous machine learning based researches empirically collected features which may bring about noises and increase the computational complexity. Meanwhile, the predicted result is largely affected by the threshold setting of a classifier which can easily lead to low performance but hasn\u2019t been well developed yet. To address these problems, we employ genetic algorithm for feature selection and confidence tuning to reinforce the motivation of correction. Comparative experiments on the NUCLE corpus show that our approach could efficiently reduce feature dimensionality and enhance the final F1 value for the article error correction problem.", "context_section_header": "", "context_paragraph": "In this paper, we focus on the machine learning based approach on error annotated corpus and propose a novel strategy to solve article error correction problem. Primarily, we extract a large number of related syntactic and semantic features from the context. With the help of genetic algorithm, a best feature subset is selected out which could greatly reduce the feature dimensionality. For each testing instance, according to the predicted confidence scores generated by the classifier, our tuning approach measures the trade-off between scores in order to enhance the confidence to a certain category. We didn't include any external corpora as references in our work which is to be further exploited. Experiments on NUCLE corpus show that our approach could efficiently reduce feature dimensionality and take full advantage of predicted scores generated by the classifier. The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "sentence": "The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "cited_ids": [{"paper_id": "14113283", "citation": "(Dahlmeier and Ng, 2011)"}], "y": "The evaluation result shows that the authors\u2019 [machine learning] approach outperforms the state-of-the-art work [based on Alternating Structure Optimization] (Dahlmeier and Ng, 2011) by 2.2% in F 1 on [the NUCLE corpus].", "snippet_surface": "The evaluation result shows that the authors' approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "AdIxkrBHd5": "What is \"this corpus\"?", "q1c3wV+ev0": "What is the state-of-the-art work?"}, "answers": {"6gKwRw0I/Q": "Machine learning based approach to resolve English article error correction problem by extracting larger number of features from the context and selecting best feature subset with a genetic algorithm.", "AdIxkrBHd5": "NUCLE corpus, a NUS Corpus of Learner English including annotated one million words of learner English.", "q1c3wV+ev0": "State-of-the-art work is by Dahlmeier and Ng, 2011, where they use an Alternating Structure Optimization to correct for grammatical errors."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "This paper proposes a novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors. Most previous machine learning based researches empirically collected features which may bring about noises and increase the computational complexity. Meanwhile, the predicted result is largely affected by the threshold setting of a classifier which can easily lead to low performance but hasn\u2019t been well developed yet. To address these problems, we employ genetic algorithm for feature selection and confidence tuning to reinforce the motivation of correction. Comparative experiments on the NUCLE corpus show that our approach could efficiently reduce feature dimensionality and enhance the final F1 value for the article error correction problem.", "selected": "novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors", "paper_id": "3795383"}, {"section": "Introduction", "paragraph": "In this paper, we focus on the machine learning based approach on error annotated corpus and propose a novel strategy to solve article error correction problem. Primarily, we extract a large number of related syntactic and semantic features from the context. With the help of genetic algorithm, a best feature subset is selected out which could greatly reduce the feature dimensionality. For each testing instance, according to the predicted confidence scores generated by the classifier, our tuning approach measures the trade-off between scores in order to enhance the confidence to a certain category. We didn't include any external corpora as references in our work which is to be further exploited. Experiments on NUCLE corpus show that our approach could efficiently reduce feature dimensionality and take full advantage of predicted scores generated by the classifier. The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "selected": "In this paper, we focus on the machine learning based approach on error annotated corpus and propose a novel strategy to solve article error correction problem. Primarily, we extract a large number of related syntactic and semantic features from the context. With the help of genetic algorithm, a best feature subset is selected out which could greatly reduce the feature dimensionality. For each testing instance, according to the predicted confidence scores generated by the classifier, our tuning approach measures the trade-off between scores in order to enhance the confidence to a certain category.", "paper_id": "3795383"}], "AdIxkrBHd5": [{"section": "Introduction", "paragraph": "In this paper, we focus on the machine learning based approach on error annotated corpus and propose a novel strategy to solve article error correction problem. Primarily, we extract a large number of related syntactic and semantic features from the context. With the help of genetic algorithm, a best feature subset is selected out which could greatly reduce the feature dimensionality. For each testing instance, according to the predicted confidence scores generated by the classifier, our tuning approach measures the trade-off between scores in order to enhance the confidence to a certain category. We didn't include any external corpora as references in our work which is to be further exploited. Experiments on NUCLE corpus show that our approach could efficiently reduce feature dimensionality and take full advantage of predicted scores generated by the classifier. The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "selected": "Experiments on NUCLE corpus show that our approach could efficiently reduce feature dimensionality and take full advantage of predicted scores generated by the classifier. The evaluation result shows our approach outperforms the state-of-the-art work (Dahlmeier and Ng, 2011) by 2.2% in F 1 on this corpus.", "paper_id": "3795383"}, {"section": "Abstract", "paragraph": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.", "selected": "NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes.", "paper_id": "14113283"}, {"section": "Introduction", "paragraph": "In this work, we aim to overcome both problems. First, we present a novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and Zhang, 2005). Our approach is able to train models on annotated learner corpora while still taking advantage of large non-learner corpora. Second, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using six different feature sets proposed in previous work. We compare our proposed ASO method with two baselines trained on non-learner text and learner text, respectively. To the best of our knowledge, this is the first extensive comparison of different feature sets on real learner text which is another contribution of our work. Our experiments show that our proposed ASO algorithm significantly improves over both baselines. It also outperforms two commercial grammar checking software packages in a manual evaluation.", "selected": "NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes.", "paper_id": "14113283"}], "q1c3wV+ev0": [{"section": "Abstract", "paragraph": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.", "selected": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization.", "paper_id": "14113283"}, {"section": "Introduction", "paragraph": "In this work, we aim to overcome both problems. First, we present a novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and Zhang, 2005). Our approach is able to train models on annotated learner corpora while still taking advantage of large non-learner corpora. Second, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using six different feature sets proposed in previous work. We compare our proposed ASO method with two baselines trained on non-learner text and learner text, respectively. To the best of our knowledge, this is the first extensive comparison of different feature sets on real learner text which is another contribution of our work. Our experiments show that our proposed ASO algorithm significantly improves over both baselines. It also outperforms two commercial grammar checking software packages in a manual evaluation.", "selected": "novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and Zhang, 2005). Our approach is able to train models on annotated learner corpora while still taking advantage of large non-learner corpora.", "paper_id": "14113283"}]}}
{"idx": "207780", "paper_id": "198622", "title": "SuperTagging and Full Parsing", "abstract": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser.", "context_section_header": "", "context_paragraph": "We are not aware of any other work that directly investigates the extent to which supertagging determines parsing. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree). Rogers (1994) proposes a different context-free variant, \"regular-form TAG\". The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. Our conversion to FSMs is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., reconstructing the derived tree), while we encode  Figure 4: Results (accuracy) for different models using the Gold-Standard supertag on development corpus (Section 00, first 800 sentences) with add-0.001 smoothing, and for the best performing model as well as the baselines on the test corpus (Section 23) the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree (not unlike (Eisner, 2000)). As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in standard TAG parsing using FSMs.", "sentence": "Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "cited_ids": [{"paper_id": "1471139", "citation": "Chiang (2000)"}], "y": "Chiang (2000) parses with an automatically extracted TIG [Tree insertion grammar], but [while the authors use lexical information in the first phase, known as supertagging, in which lexical syntactic properties are determined without building structure], he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "snippet_surface": "Chiang (2000) also parses with an automatically extracted TIG, but unlike the authors' approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "questions": {"pVSFNdP6Ni": "What is \"tig\"?", "6gKwRw0I/Q": "What is the authors' approach?", "brBHfRESlm": "How is the authors' approach different?"}, "answers": {"pVSFNdP6Ni": "Tree insertion grammar", "6gKwRw0I/Q": "The authors investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure", "brBHfRESlm": "The authors use supertagging, which is different from previous work."}, "evidence": {"pVSFNdP6Ni": [{"section": "Representing a TAG as a Set of FSMs", "paragraph": "Note that the treatment of footnodes makes it impossible to deal with trees that have terminal, substitution or active adjunction nodes on both sides of a footnode. It is this situation (iterated, of course) that makes TAG formally more powerful than CFG; in linguistic uses, it is very rare, and no such trees are extracted from the PTB. 2 2 Our construction cannot handle Dutch cross-serial dependencies (not surprisingly), but it can convert the TAG analysis of wh-movement in English and similar languages, because the predicative auxiliary verbal trees do not have terminal or substi-As a result, the grammar is weakly equivalent to a CFG. In fact, the construction treats a TAG as if were a Tree Insertion Grammar (TIG, Schabes and Waters (1995)), or rather, it coerces a TAG to be a TIG: during the traversal, both terminal nodes and nonterminal (i.e., substitution) nodes between the footnode and the root node are ignored (because the traversal stops at the footnode), thus imposing the constraint that the trees may not be wrapping trees and that no further adjunction may occur to the right of the spine in a left auxiliary tree.", "selected": "In fact, the construction treats a TAG as if were a Tree Insertion Grammar (TIG, Schabes and Waters (1995)), or rather, it coerces a TAG to be a TIG:", "paper_id": "198622"}], "6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser.", "selected": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction,", "paper_id": "198622"}], "brBHfRESlm": [{"section": "Abstract", "paragraph": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser.", "selected": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information.", "paper_id": "198622"}, {"section": "Related Work", "paragraph": "We are not aware of any other work that directly investigates the extent to which supertagging determines parsing. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree). Rogers (1994) proposes a different context-free variant, \"regular-form TAG\". The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. Our conversion to FSMs is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., reconstructing the derived tree), while we encode  Figure 4: Results (accuracy) for different models using the Gold-Standard supertag on development corpus (Section 00, first 800 sentences) with add-0.001 smoothing, and for the best performing model as well as the baselines on the test corpus (Section 23) the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree (not unlike (Eisner, 2000)). As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in standard TAG parsing using FSMs.", "selected": "We are not aware of any other work that directly investigates the extent to which supertagging determines parsing.", "paper_id": "198622"}, {"section": "Related Work", "paragraph": "We are not aware of any other work that directly investigates the extent to which supertagging determines parsing. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree). Rogers (1994) proposes a different context-free variant, \"regular-form TAG\". The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. Our conversion to FSMs is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., reconstructing the derived tree), while we encode  Figure 4: Results (accuracy) for different models using the Gold-Standard supertag on development corpus (Section 00, first 800 sentences) with add-0.001 smoothing, and for the best performing model as well as the baselines on the test corpus (Section 23) the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree (not unlike (Eisner, 2000)). As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in standard TAG parsing using FSMs.", "selected": "Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree).", "paper_id": "198622"}]}}
{"idx": "208014", "paper_id": "2765046", "title": "Easy-first Coreference Resolution", "abstract": "We describe an approach to coreference resolution that relies on the intuition that easy decisions should be made early, while harder decisions should be left for later when more information is available. We are inspired by the recent success of the rule-based system of Raghunathan et al. (2010), which relies on the same intuition. Our system, however, automatically learns from training data what constitutes an easy decision. Thus, we can utilize more features, learn more precise weights, and adapt to any dataset for which training data is available. Experiments show that our system outperforms recent state-of-the-art coreference systems including Raghunathan et al.\u2019s system as well as a competitive baseline that uses a pairwise classifier.", "context_section_header": "", "context_paragraph": "We propose a coreference resolution approach that like Raghunathan et al. (2010) aims to consider global consistency while performing fast and deterministic greedy search. Similar to Raghunathan et al. (2010), our algorithm operates by making the easy (most confident) decisions first. It builds up coreference clusters as it goes and uses the information from these clusters in the form of features to make later decisions. However, while Raghunathan et al. (2010) use hand-written rules for their system, we learn feature weights from training data.", "sentence": "However, while Raghunathan et al. (2010) use hand-written rules for their system, we learn feature weights from training data.", "cited_ids": [{"paper_id": "7691746", "citation": "Raghunathan et al. (2010)"}], "y": "However, while Raghunathan et al. (2010) use hand-written rules for their [coreference resolution] system, the authors learn feature weights from training data [with a method that performs supervised perceptron style updates].", "snippet_surface": "However, while Raghunathan et al. (2010) use hand-written rules for their system, the authors learn feature weights from training data.", "questions": {"Lpzb+cp1Bl": "What are feature weights?", "dXILOE/wni": "How do the authors learn feature weights from training data?", "e7P5dxtUMd": "What rules are being referred to?"}, "answers": {"Lpzb+cp1Bl": "Something that tells how to make the merging decisions as it performs greedy agglomerative clustering.", "dXILOE/wni": "Using a learning method that performs supervised perceptron style updates.", "e7P5dxtUMd": "The rules are how Raghunathan et al. (2010)'s system performs coreferene resolution. In the order that they are applied, these rules are: Exact Match, Precise constructions, Strict (then relaxed) head matching, and then Pronouns."}, "evidence": {"Lpzb+cp1Bl": [{"section": "Introduction", "paragraph": "What do the learned weights mean? They tell our system how to make the merging decisions as it performs greedy agglomerative clustering. And how do we learn the weights? Inspired by Goldberg and Elhadad's (2010) approach to easy-first dependency parsing, we utilize a learning method that performs supervised perceptron-style updates as it carries out clustering. Thus, during training, the learner observes partially completed clusterings similar to those that are likely to be encountered during testing.", "selected": "What do the learned weights mean? They tell our system how to make the merging decisions as it performs greedy agglomerative clustering. And how do we learn the weights? Inspired by Goldberg and Elhadad's (2010) approach to easy-first dependency parsing, we utilize a learning method that performs supervised perceptron-style updates as it carries out clustering. Thus, during training, the learner observes partially completed clusterings similar to those that are likely to be encountered during testing.", "paper_id": "2765046"}], "dXILOE/wni": [{"section": "Introduction", "paragraph": "What do the learned weights mean? They tell our system how to make the merging decisions as it performs greedy agglomerative clustering. And how do we learn the weights? Inspired by Goldberg and Elhadad's (2010) approach to easy-first dependency parsing, we utilize a learning method that performs supervised perceptron-style updates as it carries out clustering. Thus, during training, the learner observes partially completed clusterings similar to those that are likely to be encountered during testing.", "selected": ". And how do we learn the weights? Inspired by Goldberg and Elhadad's (2010) approach to easy-first dependency parsing, we utilize a learning method that performs supervised perceptron-style updates as it carries out clustering.", "paper_id": "2765046"}], "e7P5dxtUMd": [{"section": "Abstract", "paragraph": "Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks.", "selected": "To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time.", "paper_id": "7691746"}]}}
{"idx": "208412", "paper_id": "11156254", "title": "New Inflectional Lexicons and Training Corpora for Improved Morphosyntactic Annotation of Croatian and Serbian", "abstract": "In this paper we present newly developed inflectional lexcions and manually annotated corpora of Croatian and Serbian. We introduce hrLex and srLex - two freely available inflectional lexicons of Croatian and Serbian - and describe the process of building these lexicons, supported by supervised machine learning techniques for lemma and paradigm prediction. Furthermore, we introduce hr500k, a manually annotated corpus of Croatian, 500 thousand tokens in size. We showcase the three newly developed resources on the task of morphosyntactic annotation of both languages by using a recently developed CRF tagger. We achieve best results yet reported on the task for both languages, beating the HunPos baseline trained on the same datasets by a wide margin.", "context_section_header": "", "context_paragraph": "For both languages morphological lexicons were developed in the past, but with limited availability. For Croatian the Croatian Morphological Lexicon (Tadi\u0107 and Fulgosi, 2003) was available for search through a web interface since 2005 (Tadi\u0107, 2005). Since 2012 this lexicon is available through Meta-Share, with a size of ca 113,000 lemmas (60% of which are proper names) in version 5.0. However, it is distributed under a non-commercial license in the form of (token, lemma, tag) triples only, and is therefore not useful for expansion or enrichment. \u0160najder et al. (2008) provide another line of work on Croatian inflectional lexica, but the resulting resource is not freely available. For Serbian the SrpMD dictionary (Krstev, 2008), 85,721 lemmas in size, is published under a non-commercial license and indexed on Meta-Share, but is not available for download. The lexicons we present in this paper are freely downloadable, published under the GNU GPL license, organised by lexemes and paired with their inflectional paradigms, thereby enabling a wide range of applications and easy extensibility. Similar to inflectional lexicons, the line of work in annotated corpora of Croatian is reasonably extensive, in contrast to a fairly limited amount of research carried out for Serbian (Vitas et al., 2012), especially considering syntactic annotations. By and large, however, these contributions do not result in freely available resources; for a more detailed overview, see Agi\u0107 et al. (2013b). On top of providing two sizable new inflectional lexicons for the two languages, our hr500k corpus marks a significant new development for Croatian, and by virtue of direct transfer of tagging models, for Serbian as well. While Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, our contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages. With these recent developments, we can safely assume that through our line of work in free-culture resources, Croatian and Serbian are leaving the realm of severely underresourced languages.", "sentence": "While Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, our contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages.", "cited_ids": [{"paper_id": "5139332", "citation": "Agi\u0107 and Ljube\u0161i\u0107 (2015)"}], "y": "Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, and the authors' contribution significantly improves over the previous top scores [80 points LAS] in morphosyntactic tagging for the two languages, [Croatian and Serbian].", "snippet_surface": "Agi\u0107 and Ljube\u0161i\u0107 (2015) document top-level results in dependency parsing, and the authors' contribution significantly improves over the previous top scores in morphosyntactic tagging for the two languages.", "questions": {"PiXM3xR647": "What are the two languages?"}, "answers": {"PiXM3xR647": "Croatian and Serbian."}, "evidence": {"PiXM3xR647": []}}
{"idx": "210070", "paper_id": "9400830", "title": "Neural Sequence-to-sequence Learning of Internal Word Structure", "abstract": "Learning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison. In this paper, we present a neural encoder-decoder model for learning canonical morphological segmentation. Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments. We obtain up to 4% improvement over a strong character-level encoder-decoder baseline for three languages. Our model outperforms the previous state-of-the-art for two languages, while eliminating the need for external resources such as large dictionaries. Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.", "context_section_header": "", "context_paragraph": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data sets.", "sentence": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data sets.", "cited_ids": [{"paper_id": "44472515", "citation": "Gulcehre et al. (2017)"}], "y": "Gulcehre et al. (2017) used a [statistical language model with canonical segmentation and sequence-to-sequence transformation] with recurrent neural networks, while the authors employ [a fused statistical model], which is better adapted to their settings with small data sets.", "snippet_surface": "Last, while Gulcehre et al. (2017) use a language model implemented with recurrent neural networks, the authors employ a statistical language model, which is better adapted to their settings with small data sets.", "questions": {"qtIjSpXGOZ": "What language model is referred/used?"}, "answers": {"qtIjSpXGOZ": "A statistical language model with cannonical segmentation and sequence-to-sequence transformation."}, "evidence": {"qtIjSpXGOZ": [{"section": "Abstract", "paragraph": "Learning internal word structure has recently been recognized as an important step in various multilingual processing tasks and in theoretical language comparison. In this paper, we present a neural encoder-decoder model for learning canonical morphological segmentation. Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments. We obtain up to 4% improvement over a strong character-level encoder-decoder baseline for three languages. Our model outperforms the previous state-of-the-art for two languages, while eliminating the need for external resources such as large dictionaries. Finally, by comparing the performance of encoder-decoder and classical statistical machine translation systems trained with and without corpus counts, we show that including corpus counts is beneficial to both approaches.", "selected": "Our model combines character-level sequence-to-sequence transformation with a language model over canonical segments.", "paper_id": "9400830"}, {"section": "Related Work", "paragraph": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data sets.", "selected": "Last, while Gulcehre et al. (2017) use a lan-guage model implemented with recurrent neural networks, we employ a statistical language model, which is better adapted to our settings with small data set", "paper_id": "9400830"}, {"section": "Model Description", "paragraph": "Given an input sequence, such as the Chintang sentence in Example 1 (line a.), we produce a canonical segmentation (line b.), where we recognize that the sequence kh in the surface form is an instance of the light verb khag.", "selected": "we produce a canonical segmentation (line b.), where we recognize that the sequence kh in the surface form is an instance of the light verb khag.", "paper_id": "9400830"}]}}
{"idx": "210176", "paper_id": "218501192", "title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards", "abstract": "Throughout a conversation, participants make choices that can orient the flow of the interaction. Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. In this work, we develop an unsupervised methodology to quantify how counselors manage this balance. Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range. Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range. By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards. This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis. We also illustrate how our measure can be indicative of a conversation\u2019s progress, as well as its effectiveness.", "context_section_header": "", "context_paragraph": "Prior work has similarly considered how utterances relate to the preceding and subsequent discourse (Webber, 2001). Frameworks like centering theory (Grosz et al., 1995) aim at identify-ing referenced entities, while we aim to more abstractly model interlocutor choices. Past work has also examined how interlocutors mediate a conversation's trajectory through taking or ceding control (Walker and Whittaker, 1990) or shifting topic (Nguyen et al., 2014); Althoff et al. (2016) considers the rate at which counselors in our setting advance across stages of a conversation. While these actions can be construed as forwards-oriented, we focus more on the interplay between forwards-and backwards-oriented actions. A counselor's objectives may also cut across these concepts: for instance, the training stresses the need for empathetic reflecting across all stages and topics.", "sentence": "Frameworks like centering theory (Grosz et al., 1995) aim at identify-ing referenced entities, while we aim to more abstractly model interlocutor choices.", "cited_ids": [{"paper_id": "18229335", "citation": "(Grosz et al., 1995)"}], "y": "Frameworks like centering theory [which says that some utterances sound more coherent because the focus attention (or \"center\") around one entity] (Grosz et al., 1995) aim at identifying referenced entities, while the authors aim to more abstractly model interlocutor choices.", "snippet_surface": "Frameworks like centering theory (Grosz et al., 1995) aim at identifying referenced entities, while the authors aim to more abstractly model interlocutor choices.", "questions": {"Ypz+FdsFOo": "What is centering theory?"}, "answers": {"Ypz+FdsFOo": "It is a theory that tries to explain that some utterances sound more coherent because they focus attention (or \"center\") around one entity."}, "evidence": {"Ypz+FdsFOo": [{"section": "Abstract", "paragraph": "This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences.", "selected": "This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences.", "paper_id": "18229335"}, {"section": "Phenomena To Be Explained", "paragraph": "Discourse (1) is intuitively more coherent than Discourse (2). This difference may be seen to arise from different degrees of continuity in what the discourse is about. Discourse (1) centers around a single individual, describing various actions he took and his reactions to them. In contrast, Discourse (2) seems to flip back and forth among several different entities.", "selected": "Discourse (1) is intuitively more coherent than Discourse (2). This difference may be seen to arise from different degrees of continuity in what the discourse is about. Discourse (1) centers around a single individual, describing various actions he took and his reactions to them. In contrast, Discourse (2) seems to flip back and forth among several different entities.", "paper_id": "18229335"}, {"section": "Basic Center Definitions", "paragraph": "We use the term centers of an utterance to refer to those entities serving to link that utterance to other utterances in the discourse segment that contains it. It is an utterance (i.e., the uttering of a sequence of words at a certain point in the discourse) and not a sentence in isolation that has centers. The same sentence uttered in different discourse situations may have different centers. Centers are thus discourse constructs. Furthermore, centers are semantic objects, not words, phrases, or syntactic forms.", "selected": "We use the term centers of an utterance to refer to those entities serving to link that utterance to other utterances in the discourse segment that contains it. It is an utterance (i.e., the uttering of a sequence of words at a certain point in the discourse) and not a sentence in isolation that has centers. The same sentence uttered in different discourse situations may have different centers. Centers are thus discourse constructs. Furthermore, centers are semantic objects, not words, phrases, or syntactic forms.", "paper_id": "18229335"}]}}
{"idx": "212619", "paper_id": "198977494", "title": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects", "abstract": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "context_section_header": "", "context_paragraph": "2. Given that, DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature are learned, we feed whole n-grams to our model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "sentence": "Given that, DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature are learned, we feed whole n-grams to our model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "cited_ids": [{"paper_id": "886027", "citation": "(Tang et al., 2014"}], "y": "Given that DA [Dialectal Arabic] has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature [the full diversity of Arabic dialects] is learned, the authors feed whole n-grams to their model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "snippet_surface": "Given that DA has a free word order and a varying syntactic nature, therefore, unlike (Tang et al., 2014) whose embeddings were generated using corrupted input ngrams from which the syntactic context nature is learned, the authors feed whole n-grams to their model as the training objective is to capture the semantic and sentiment relations regardless of the order and the syntax of the context words.", "questions": {"WitXM0zd+N": "What does the \"syntatic context nature\" refer to?", "kaCdnJWTMk": "What is \"da\"?"}, "answers": {"WitXM0zd+N": "The syntactic context nature refers to the diversity of the Arabic dialects.", "kaCdnJWTMk": "DA can be inferred to stand for \"Dialectal Arabic\""}, "evidence": {"WitXM0zd+N": [{"section": "Introduction", "paragraph": "Inspired by (Iyyer et al., 2015;Tang et al., 2014), we hypothesize that representing a sentence by its constituent sentiment-specific, unordered and syntax-ignorant n-gram embeddings can handle the diversity of the Arabic dialects and provide better features for the dialectal Arabic SA task. In the current paper, we present a SA framework whose features are n-gram embeddings learned from labeled data (sentiment-specific) and composed via the additive unordered composition function (syntax-ignorant) known as SOWE. The embeddings composition and the sentiment learning processes were conducted within Tw-StAR framework which forms a shallow feed-forward neural network of single hidden layer. The contributions of this study can be briefly described as follows:", "selected": "Inspired by (Iyyer et al., 2015;Tang et al., 2014), we hypothesize that representing a sentence by its constituent sentiment-specific, unordered and syntax-ignorant n-gram embeddings can handle the diversity of the Arabic dialects and provide better features for the dialectal Arabic SA task.", "paper_id": "198977494"}], "kaCdnJWTMk": [{"section": "Introduction", "paragraph": "Thus, to handle such informality of DA, we propose an unordered composition model to construct sentence/phrase embeddings regardless of the order and the syntax of the context's words. Nevertheless, when coming to the sentiment analysis task, sentence embeddings that are merely composed and learned based on the context words do not always infer the sentiment accurately. This is due to the fact that, some words of contradict sentiments might be mentioned within identical contexts which leads to map opposite words close to each other in the embedding space. To clarify that, both sentences in Example 1 and Example 2 contain the same context words organized in the same order; yet the first sentence is of positive polarity while the second has a negative sentiment since the words \" \" and \" \" are antonyms that mean \"interesting\" and \"boring\", respectively.", "selected": "Thus, to handle such informality of DA, we propose an unordered composition model to construct sentence/phrase embeddings regardless of the order and the syntax of the context's words. Nevertheless, when coming to the sentiment analysis task, sentence embeddings that are merely composed and learned based on the context words do not always infer the sentiment accurately. This is due to the fact that, some words of contradict sentiments might be mentioned within identical contexts which leads to map opposite words close to each other in the embedding space. To clarify that, both sentences in Example 1 and Example 2 contain the same context words organized in the same order; yet the first sentence is of positive polarity while the second has a negative sentiment since the words \" \" and \" \" are antonyms that mean \"interesting\" and \"boring\", respectively.", "paper_id": "198977494"}, {"section": "Table 1 :", "paragraph": "1Free word order of dialectal Arabic.Dialect", "selected": "Free word order of dialectal Arabic.Dialect", "paper_id": "198977494"}]}}
{"idx": "212620", "paper_id": "198977494", "title": "Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of Arabic Dialects", "abstract": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "context_section_header": "", "context_paragraph": "3. In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings introduced here are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "sentence": "In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings introduced here are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "cited_ids": [{"paper_id": "216848261", "citation": "(Iyyer et al., 2015)"}], "y": "In contrast to previous studies, that composed unordered embeddings within deep neural models (Iyyer et al., 2015), the embeddings the authors introduced are generated and learned within a shallow feed-forward neural model as we are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "snippet_surface": "In contrast to previous studies, that composed unordered embeddings within deep neural models (BIBREF15), the authors introduced embeddings that are generated and learned within a shallow feed-forward neural model as they are seeking to investigate whether SA of DA can be performed using less complicated neural architectures.", "questions": {"qspIyB/xzb": "What is different about authors introduced embeddings?"}, "answers": {"qspIyB/xzb": "The authors' introduced embeddings were composed of unordered composition as compared to ordered compositions which are usually used"}, "evidence": {"qspIyB/xzb": [{"section": "Abstract", "paragraph": "Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.", "selected": "These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model.", "paper_id": "198977494"}]}}
{"idx": "214438", "paper_id": "248721870", "title": "NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension", "abstract": "NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question per entity) are considered at the same time for a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC) architecture for this formulation. NER-MQMRC architecture considers all entities as input to BERT for learning token embeddings with self-attention and leverages BERT-based entity representation for further improving these token embeddings for NER task. Evaluation on three NER datasets show that our proposed architecture leads to average 2.5 times faster training and 2.3 times faster inference as compared to NER-SQMRC framework based models by considering all entities together in a single pass. Further, we show that our model performance does not degrade compared to single-question based MRC (NER-SQMRC) (Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for AE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this architecture primarily to solve large scale e-commerce attribute (or entity) extraction from unstructured text of a magnitude of 50k+ attributes to be extracted on a scalable production environment with high performance and optimised training and inference runtimes.", "context_section_header": "", "context_paragraph": "\u2022 Using entity information: Leveraging entity information (such as entity name) for learning better representations.  As summarized in Table 1, our proposed NER-MQMRC architecture combines the best of NER-SL and NER-SQMRC. NER-MQMRC considers extraction of multiple entities based on multiple questions on same text, and is novel in three ways -1) Token representations are learnt to incorporate information of all the entities, unlike using single entity as in Mengge et al., 2020). 2) We introduce leveraging BERT-based entity representations for further improving token representations for NER task. 3) Our architecture leads to faster training and inference. E.g. scoring of five entities can be done using a single forward pass with our NER-MQMRC as compared to five passes required earlier with NER-SQMRC based models (Devlin et al., 2019;Mengge et al., 2020). Experiments on three NER datasets establish the effectiveness of NER-MQMRC architecture. NER-MQMRC achieves 2.5x faster training and 2.3x faster inference as compared to single question based MRC (NER-SQMRC) framework based models by considering multiple entities together in training and inference. Further, we show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively. Rest of the paper is organized as follows. We describe our proposed NER-MQMRC architecture in Section 2. We discuss our experimental setup in Section 3 followed by results in Section 4. We discuss the industry impact of our work in Section 5 and summarize the paper in Section 6.", "sentence": "Further, we show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively.", "cited_ids": [{"paper_id": "52967399", "citation": "(Devlin et al., 2019)"}], "y": "The authors show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub [a dataset for E-commerce attribute extraction collected from the AliExpress Sports & Entertainment category], Ecommerce5PT [an extracted set of data from five different product types from Amazon catalogue] and Twitter datasets respectively.", "snippet_surface": "The authors show performance boost over SOTA NER-SQMRC (Devlin et al., 2019), obtaining +0.41%, +0.32% and +0.27% F1 improvements for AE-Pub, Ecommerce5PT and Twitter datasets respectively.", "questions": {"wwvJ48q980": "What is ecommerce5pt dataset?", "NrVeSNUmHs": "What is ae-pub dataset?"}, "answers": {"wwvJ48q980": "Ecommerce5PT dataset is an extracted set of data from five different product types from Amazon catalogue", "NrVeSNUmHs": "The AE-Pub dataset is a dataset for E-commerce attribute extraction collected from the AliExpress Sports & Entertainment category. It poses attribute extraction as a question answering task."}, "evidence": {"wwvJ48q980": [{"section": "Discussion", "paragraph": "Ecommerce5PT is a 33 attributes (size, material, color, etc.) dataset extracted from five different product types from Amazon catalogue. The train data is constructed in a similar way as AE-Pub using distant supervision. The train data quality is improved using automated gazetteer and matching heuristics (refer Appendix A.2). Unlike AE-pub, test data is constructed with manual audit, thus leading to better quality test data.", "selected": "Ecommerce5PT is a 33 attributes (size, material, color, etc.) dataset extracted from five different product types from Amazon catalogue. The train data is constructed in a similar way as AE-Pub using distant supervision. The train data quality is improved using automated gazetteer and matching heuristics (refer Appendix A.2). Unlike AE-pub, test data is constructed with manual audit, thus leading to better quality test data.", "paper_id": "248721870"}], "NrVeSNUmHs": [{"section": "Table 2 :", "paragraph": "contains over 110k triplets (text, attribute, value)", "selected": "contains over 110k triplets (text, attribute, value)", "paper_id": "248721870"}]}}
{"idx": "215684", "paper_id": "7243556", "title": "Edinburgh Research Explorer A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability", "abstract": "We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We de\ufb01ne the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed \ufb01rst, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.", "context_section_header": "", "context_paragraph": "In particular, although the GGJ model achieves high segmentation accuracy on phonemic (nonvariable) input and makes errors that are qualitatively similar to human learners (tending to undersegment the input), its accuracy drops considerably on phonetically noisy data and it tends to oversegment rather than undersegment. Here, we demonstrate that when the model is augmented to account for phonetic variability, it is able to learn common phonetic changes and by doing so, its accuracy improves and its errors return to the more human-like undersegmentation pattern. In addition, we find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012). We analyze the model's phonetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants.", "sentence": "In addition, we find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012).", "cited_ids": [{"paper_id": "6447567", "citation": "(Elsner et al., 2012)"}], "y": "The authors find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning [a bigram language model and a model of phonetic variation are learned simultaneously] (Elsner et al., 2012).\"", "snippet_surface": "The authors find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012).", "questions": {"itWy0KDg22": "What happens in lexical-phonetic learning?"}, "answers": {"itWy0KDg22": "A lexicon, a bigram language model and a model of phonetic variation are learned simultaneously."}, "evidence": {"itWy0KDg22": [{"section": "Lexical-phonetic model", "paragraph": "Our lexical-phonetic model is defined using the standard noisy channel framework: first a sequence of intended word tokens is generated using a language model, and then each token is transformed by a probabilistic finite-state transducer to produce the observed surface sequence. In this section, we present the model in a hierarchical Bayesian framework to emphasize its similarity to existing models, in particular those of Feldman et al. (2009) and Goldwater et al. (2009). In our actual implementation, however, we use approximation and MAP point estimates to make our inference process more tractable; we discuss these simplifications in Section 4.", "selected": "Our lexical-phonetic model is defined using the standard noisy channel framework: first a sequence of intended word tokens is generated using a language model, and then each token is transformed by a probabilistic finite-state transducer to produce the observed surface sequence.", "paper_id": "6447567"}, {"section": "Conclusion", "paragraph": "We have presented a noisy-channel model that simultaneously learns a lexicon, a bigram language model, and a model of phonetic variation, while using only the noisy surface forms as training data. It is the first model of lexical-phonetic acquisition to include word-level context and to be tested on an infant-directed corpus with realistic phonetic variability. Whether trained using gold standard or automatically induced word boundaries, the model recovers lexical items more effectively than a system that assumes no phonetic variability; moreover, the use of word-level context is key to the model's success. Ultimately, we hope to extend the model to jointly infer word boundaries along with lexical-phonetic knowledge, and to work directly from acoustic input. However, we have already shown that lexical-phonetic learning from a broad-coverage corpus is possible, supporting the claim that infants acquire lexical and phonetic knowledge simultaneously.", "selected": "We have presented a noisy-channel model that simultaneously learns a lexicon, a bigram language model, and a model of phonetic variation, while using only the noisy surface forms as training data", "paper_id": "6447567"}]}}
{"idx": "217138", "paper_id": "235421827", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network", "abstract": "We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as\"class prototypes\"as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor's entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks.", "context_section_header": "", "context_paragraph": "We present a new approach (MOLEMAN 1 ) that maintains the dual-encoder architecture, but with the same mention-encoder on both sides. Entity linking is modeled entirely as a mapping between mentions, where inference involves a nearest neighbor search against all known mentions of all entities in the training set. We build MOLEMAN using exactly the same mention-encoder architecture and training data as Model F (Botha et al., 2020). We show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "sentence": "We show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "cited_ids": [{"paper_id": "15156124", "citation": "Tsai and Roth (2016)"}], "y": "The authors show that MOLEMAN significantly outperforms Model F on both Mewsli-9 [a multilingual entity-linking dataset] and the Tsai and Roth (2016) dataset [a dataset in 12 languages based on Wikipedia] particularly for low-coverage languages, and rarer entities.", "snippet_surface": "The authors show that MOLEMAN significantly outperforms Model F on both the Mewsli-9 and Tsai and Roth (2016) datasets, particularly for low-coverage languages, and rarer entities.", "questions": {"9YbC8W2lgG": "What does tsai and roth dataset comprise of?", "0VPWBq2+RS": "What does mewsli-9 dataset comprise of?"}, "answers": {"9YbC8W2lgG": "A multi-lingual text data set based on wikipedia, in 12 languages, for testing the computer model.", "0VPWBq2+RS": "It's a multilingual entity-linking dataset."}, "evidence": {"9YbC8W2lgG": [{"section": "Abstract", "paragraph": "Cross-lingual Wikification is the task of grounding mentions written in non-English documents to entries in the English Wikipedia. This task involves the problem of comparing textual clues across languages, which requires developing a notion of similarity between text snippets across languages. In this paper, we address this problem by jointly training multilingual embeddings for words and Wikipedia titles. The proposed method can be applied to all languages represented in Wikipedia, including those for which no machine translation technology is available. We create a challenging dataset in 12 languages and show that our proposed approach outperforms various baselines. Moreover, our model compares favorably with the best systems on the TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English.", "selected": "We create a challenging dataset in 12 languages and show that our proposed approach outperforms various baselines. Moreover, our model compares favorably with the best systems on the TAC KBP2015 Entity Linking task including those that relied on the availability of translation from the target language to English.", "paper_id": "15156124"}, {"section": "Introduction", "paragraph": "In this paper, we address this problem by using multilingual title and word embeddings. We represent words and Wikipedia titles in both the foreign language and in English in the same continuous vector space, which allows us to compute meaningful similarity between mentions in the foreign language and titles in English. We show that learning these embeddings only requires Wikipedia documents and language links between the titles across different languages, which are quite common in Wikipedia. Therefore, we can learn embeddings for all languages in Wikipedia without any additional annotation or supervision.", "selected": "multilingual title and word embeddings.", "paper_id": "15156124"}, {"section": "Candidate Ranking", "paragraph": "a weighted sum of the features, i , which are based on multilingual title and word embeddings. We represent the mention m by the following contextual clues and use these representation to compute feature values:", "selected": "multilingual title and word embeddings.", "paper_id": "15156124"}, {"section": "Introduction", "paragraph": "We create a challenging Wikipedia dataset for 12 foreign languages and show that the proposed approach, WikiME (Wikification using Multilingual Embeddings), consistently outperforms various baselines. Moreover, the results on the TAC KBP2015 Entity Linking dataset show that our approach compares favorably with the best Spanish system and the best Chinese system despite using significantly weaker resources (no need for translation). We note that the need for translation would have prevented the wikification of 12 languages used in this paper.", "selected": "a challenging Wikipedia dataset for 12 foreign languages and show that the proposed approach, WikiME (Wikification using Multilingual Embeddings), consistently outperforms various baselines.", "paper_id": "15156124"}], "0VPWBq2+RS": []}}
{"idx": "218457", "paper_id": "12186549", "title": "Multimodal Semantic Learning from Child-Directed Input", "abstract": "Children learn the meaning of words by being exposed to perceptually rich situations (linguistic discourse, visual scenes, etc). Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information (visual and social cues), handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure.", "context_section_header": "", "context_paragraph": "While there is work on learning from multimodal data (Roy, 2000;Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007;Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)'s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multimodal features, while in Yu's approach words are simply distributions over categories. It is therefore not clear how Yu's approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words-all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)'s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu's, but it would then have the same limitations.", "sentence": "For example, in comparison to Yu (2005)'s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations.", "cited_ids": [{"paper_id": "15689940", "citation": "Yu (2005)"}], "y": "In comparison to Yu (2005)'s model, the authors\u2019 approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures [(i.e., how visually similar a word is to others)] without positing a categorical level on which to learn word-symbol/category-symbol associations.", "snippet_surface": "The authors' approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations, in comparison to Yu (2005)'s model.", "questions": {"b36JH/zoQR": "What similarity measures are referred here?"}, "answers": {"b36JH/zoQR": "how visually similar a word is to others"}, "evidence": {"b36JH/zoQR": [{"section": "Experiments", "paragraph": "Generalization. Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations. Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category. We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by Silberer and Lapata (2014). Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object (not used in IFC), of a highly visually similar object, of a mildly visually similar object and of a dissimilar one (for duck: duck, chicken, finch and garage, respectively). The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation. Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%). In the majority of cases the top-10 intruders are instances of the most visually related concepts (60% of intruders, vs. 33% expected by chance). For example, the model retrieves pictures of sheep for the word lamb, or bulls for cow. Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition (Rescorla, 1980).", "selected": "mildly visually similar object and of a dissimilar one", "paper_id": "12186549"}, {"section": "Experiments", "paragraph": "Generalization. Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations. Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category. We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by Silberer and Lapata (2014). Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object (not used in IFC), of a highly visually similar object, of a mildly visually similar object and of a dissimilar one (for duck: duck, chicken, finch and garage, respectively). The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation. Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%). In the majority of cases the top-10 intruders are instances of the most visually related concepts (60% of intruders, vs. 33% expected by chance). For example, the model retrieves pictures of sheep for the word lamb, or bulls for cow. Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition (Rescorla, 1980).", "selected": "ighly visually similar", "paper_id": "12186549"}, {"section": "Experiments", "paragraph": "Generalization. Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations. Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category. We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by Silberer and Lapata (2014). Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object (not used in IFC), of a highly visually similar object, of a mildly visually similar object and of a dissimilar one (for duck: duck, chicken, finch and garage, respectively). The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation. Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%). In the majority of cases the top-10 intruders are instances of the most visually related concepts (60% of intruders, vs. 33% expected by chance). For example, the model retrieves pictures of sheep for the word lamb, or bulls for cow. Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition (Rescorla, 1980).", "selected": "3, and were ranked by similarity to a test word AttentiveSocialMSG representation.", "paper_id": "12186549"}, {"section": "Experiments", "paragraph": "Generalization. Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations. Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category. We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by Silberer and Lapata (2014). Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object (not used in IFC), of a highly visually similar object, of a mildly visually similar object and of a dissimilar one (for duck: duck, chicken, finch and garage, respectively). The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation. Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%). In the majority of cases the top-10 intruders are instances of the most visually related concepts (60% of intruders, vs. 33% expected by chance). For example, the model retrieves pictures of sheep for the word lamb, or bulls for cow. Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition (Rescorla, 1980).", "selected": "normed for visual similarity to other objects by Silberer and Lapata", "paper_id": "12186549"}, {"section": "Related Work", "paragraph": "While there is work on learning from multimodal data (Roy, 2000;Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007;Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)'s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multimodal features, while in Yu's approach words are simply distributions over categories. It is therefore not clear how Yu's approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words-all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)'s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu's, but it would then have the same limitations.", "selected": "similarity measures", "paper_id": "12186549"}]}}
{"idx": "221085", "paper_id": "1454594", "title": "The Role of Semantic Roles in Disambiguating Verb Senses", "abstract": "We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicate-argument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles.", "context_section_header": "", "context_paragraph": "(Lee and Ng, 2002) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Vector Machines (SVM) and included local collocations and syntactic relations, and also found that adding syntactic features improved accuracy. Our features are similar to theirs, but we added semantic class features for the verb arguments. We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Gomez, 2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and iden-tify thematic roles and adjuncts; our work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "sentence": "We found that the difference in machine learning algorithms did not play a large role in performance; when we used our features in SVM we obtained almost no difference in performance over using maximum entropy models with Gaussian priors. (Gomez, 2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and iden-tify thematic roles and adjuncts; our work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "cited_ids": [{"paper_id": "7437033", "citation": "(Gomez, 2001)"}], "y": "The authors found that the difference in machine learning algorithms did not play a large role in performance; when they used their features in SVMs they obtained almost no difference in performance over using maximum entropy models with Gaussian priors. Gomez (2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and identify thematic roles [the ties that indicate what part of each argument plays in the predicate's description of the event] and adjuncts [a class of arguments whose labels are derived from the treebank functional tags]; the authors' work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "snippet_surface": "The authors found that the difference in machine learning algorithms did not play a large role in performance; when they used their features in SVM they obtained almost no difference in performance over using maximum entropy models with Gaussian priors. Gomez (2001) described an algorithm using WordNet to simultaneously determine verb senses and attachments of prepositional phrases, and identify thematic roles and adjuncts; their work is different in that it is trained on manually annotated corpora to show the relevance of semantic roles for verb sense disambiguation.", "questions": {"jsdGzotEe3": "What does thematic role refer to?", "TqtifeUzxS": "What is \"verb sense disambiguation\"?"}, "answers": {"jsdGzotEe3": "The semantic ties between nouns and pronouns and the predicate of a sentence are referred to as thematic roles. Thematic roles indicate what part each argument plays in the predicate's description of the event.", "TqtifeUzxS": "Examining verbs in a context and determining which sense of each verb is most suitable for that context."}, "evidence": {"jsdGzotEe3": [{"section": "PropBank semantic annotations", "paragraph": "Our WSD system uses heuristics to attempt to detect predicate arguments from parsed sentences. However, recognition of predicate argument structures is not straightforward, because a natural language will have several different syntactic realizations of the same predicate argument relations.", "selected": "Our WSD system uses heuristics to attempt to detect predicate arguments from parsed sentences.", "paper_id": "1454594"}, {"section": "PropBank semantic annotations", "paragraph": "Our WSD system uses heuristics to attempt to detect predicate arguments from parsed sentences. However, recognition of predicate argument structures is not straightforward, because a natural language will have several different syntactic realizations of the same predicate argument relations.", "selected": "However, recognition of predicate argument structures is not straightforward, because a natural language will have several different syntactic realizations of the same predicate argument relations.", "paper_id": "1454594"}, {"section": "PropBank semantic annotations", "paragraph": "PropBank is a corpus in which verbs are annotated with semantic tags, including coarse-grained sense distinctions and predicate-argument structures. PropBank adds a layer of semantic annotation to the Penn Wall Street Journal Treebank II. An important goal is to provide consistent predicateargument structures across different syntactic realizations of the same verb. Polysemous verbs are also annotated with different framesets. Frameset tags are based on differences in subcategorization frames and correspond to a coarse notion of word senses.", "selected": "PropBank is a corpus in which verbs are annotated with semantic tags, including coarse-grained sense distinctions and predicate-argument structures.", "paper_id": "1454594"}, {"section": "PropBank semantic annotations", "paragraph": "A verb's semantic arguments in PropBank are numbered beginning with 0. Arg0 is roughly equivalent to the thematic role of Agent, and Arg1 usually corresponds to Theme or Patient; however, argument labels are not necessarily consistent across different senses of the same verb, or across different verbs, as thematic roles are usually taken to be. In addition to the core, numbered arguments, verbs can take any of a set of general, adjunct-like arguments (ARGM), whose labels are derived from the Treebank functional tags (DIRection, LOCation, etc.).", "selected": "A verb's semantic arguments in PropBank are numbered beginning with 0. Arg0 is roughly equivalent to the thematic role of Agent, and Arg1 usually corresponds to Theme or Patient; however, argument labels are not necessarily consistent across different senses of the same verb, or across different verbs, as thematic roles are usually taken to be.", "paper_id": "1454594"}], "TqtifeUzxS": [{"section": "Abstract", "paragraph": "We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicate-argument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles.", "selected": "We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.", "paper_id": "1454594"}, {"section": "Introduction", "paragraph": "A word can have different meanings depending on the context in which it is used. Word Sense Disambiguation (WSD) is the task of determining the correct meaning (\"sense\") of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001).", "selected": "Word Sense Disambiguation (WSD) is the task of determining the correct meaning (\"sense\") of a word in context, and several efforts have been made to develop automatic WSD systems.", "paper_id": "1454594"}, {"section": "Introduction", "paragraph": "A word can have different meanings depending on the context in which it is used. Word Sense Disambiguation (WSD) is the task of determining the correct meaning (\"sense\") of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001).", "selected": "have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems", "paper_id": "1454594"}]}}
{"idx": "221159", "paper_id": "9431715", "title": "Learning-based Multi-Sieve Co-reference Resolution with Knowledge", "abstract": "We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or \"grounds\") expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learning-based multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.", "context_section_header": "", "context_paragraph": "(2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009;Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise \"co-reference\" vs. \"non-coreference\" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. In our running example, the decision of whether {[vessel]} m1 refers to {[Kursk]} m2 is made before the decision of whether {[vessel]} m1 refers to {Norwegian [ship]} m 4 since decisions in the same sentence are believed to be easier than cross-sentence ones. We describe our learningbased multi-sieve approach in Sec. 4.", "sentence": "Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available.", "cited_ids": [{"paper_id": "7691746", "citation": "(Raghunathan et al., 2010)"}], "y": "The authors' multi-sieve approach [which splits data into co-reference vs non-coreference for coreference resolution] is different from (Raghunathan et al., 2010) in several respects: (a) their sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing recovery from errors as additional evidence becomes available.", "snippet_surface": "The authors' multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) their sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing recovery from errors as additional evidence becomes available.", "questions": {"3EdvMsf7ba": "What does a sieve mean in this context?", "FxcIGzlHSq": "What do 'earlier' and 'later' sieves represent?"}, "answers": {"3EdvMsf7ba": "Something that splits data in to co-reference vs non-coreference and makes decisions.", "FxcIGzlHSq": "how easy it is to perform the coreference for that sieve"}, "evidence": {"3EdvMsf7ba": [{"section": "Abstract", "paragraph": "We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or \"grounds\") expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learning-based multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset.", "selected": "we deploy a learning-based multi-sieve approach and develop novel entity-based features.", "paper_id": "9431715"}, {"section": "Introduction", "paragraph": "(2) Integrating the strength of rule-based systems such as (Haghighi and Klein, 2009;Raghunathan et al., 2010) into a machine learning framework. We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise \"co-reference\" vs. \"non-coreference\" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). Our multi-sieve approach is different from (Raghunathan et al., 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. In our running example, the decision of whether {[vessel]} m1 refers to {[Kursk]} m2 is made before the decision of whether {[vessel]} m1 refers to {Norwegian [ship]} m 4 since decisions in the same sentence are believed to be easier than cross-sentence ones. We describe our learningbased multi-sieve approach in Sec. 4.", "selected": "We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise \"co-reference\" vs. \"non-coreference\" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010)", "paper_id": "9431715"}, {"section": "Introduction", "paragraph": "(3) A novel approach for entity-based features. As sieves of classifiers are applied, our system attempts to model entities and share the attributes between the mentions belonging to the same entity. Once the decision that {[vessel]} m1 and {[Kursk]} m2 co-refer is made, we want the two mentions to share the Russian nationality. This allows us to avoid erroneously linking {[vessel]} m1 to {Norwegian [ship]} m 4 despite vessel and ship being synonyms in Word-Net. However, in this work we allow the sieves to make conflicting decisions on the same pair of mentions. Hence, obtaining entities and their attributes by straightforward transitive closure of co-reference predictions is impossible. We describe our approach for leveraging possibly contradicting predictions in Sec. 5.", "selected": "However, in this work we allow the sieves to make conflicting decisions on the same pair of mentions. H", "paper_id": "9431715"}, {"section": "Baseline System", "paragraph": "In this work, we are using the state-of-the-art system of (Bengtson and Roth, 2008), which relies on a pairwise scoring function pc to assign an ordered pair of mentions a probability that they are coreferential. It uses a rich set of features including: string edit distance, gender match, whether the mentions appear in the same sentence, whether the heads are synonyms in WordNet etc. The function pc is modeled using regularized averaged perceptron for a tuned number of training rounds, learning rate and margin. For the end system, we keep these parameters intact, our only modifications will be adding knowledge-rich features and adding intermediate classification sieves to the training and the inference, which we will discuss in the following sections.", "selected": "classification sieves t", "paper_id": "9431715"}, {"section": "Learning-based Multi-Sieve Aproach", "paragraph": "In this work, we attempt to integrate the strength of linguistically motivated rule-based systems with the robustness of a machine learning approach. We started with a hypothesis that different types of men- There is a subtle difference between mention pairs (m 1 , m 2 ) and (m 2 , m 3 ). One of the differences is purely structural. The first pair appears in different sentences, while the second pair -in the same sentence. It turns out that string edit distance feature between two named entities has different \"semantics\" depending on whether the two mentions appear in the same sentence. The reason is that to avoid redundancy, humans refer to the same entity differently within the sentence, preferring titles, nicknames and pronouns. Therefore, when a similar-looking named entities appear in the same sentence, they are actually likely to refer to different entities. On the other hand, in the sentence \"Reggie Jackson, nicknamed Mr. October . . . \" we have to rely heavily on sentence structure rather than string edit distance to make the correct co-ref prediction. Our second intuition is that \"easy-first\" inference is necessary to effectively leverage knowledge. For example, in Fig. 3, our goal is to link vessel to Kursk and assign it the Russian/Soviet nationality prior to applying the pairwise co-reference classifier on (vessel, Norwegian ship). Therefore, our goal is to apply the pairwise classifier on pairs in prescribed order and to propagate the knowledge across mentions. The ordering should be such that (a) maximum amount of information is injected at early stages (b) the precision at the early stages is as high as possible (Raghunathan et al., 2010). Hence, we divide the mention pairs as follows: In Tab. 1 we compare the performance at each sieve in two scenarios 11 . First, we train with the entire 525,398 training samples, and then we train on whatever training data is available for the specific sieve 12 . We were surprised to see that the F1 on the nested mentions, when trained on the 5,804 sievespecific samples improves to 79.00 versus 76.11 when trained on the 525,398 top sieve samples.", "selected": "at each sieve in two", "paper_id": "9431715"}, {"section": "Learning-based Multi-Sieve Aproach", "paragraph": "There are several things to note when interpreting the results in Tab 1. First, the sheer ratio of positive to negative samples fluctuates drastically. For example, 208 out of the 992 testing samples at the nested sieve are positive, while there are only 86 positive samples out of 1,746 testing samples in the Same-SenBothNer sieve. It seems unreasonable to use the same model for inference at both sieves. Second, the data for intermediate sieves is not always a subset of the top sieve. The reason is that top sieve extracts a positive instance only for the closest co-referent mentions, while sieves such as AllSentencePairs extract samples for all co-referent pairs which appear in the same sentence. Third, while our division to sieves may resemble witchcraft, it is motivated by the intuition that mentions appearing close to one another are easier instances of co-ref as well as linguistic insights of (Raghunathan et al., 2010).", "selected": "e our division to sieves", "paper_id": "9431715"}, {"section": "Learning-based Multi-Sieve Aproach", "paragraph": "There are several things to note when interpreting the results in Tab 1. First, the sheer ratio of positive to negative samples fluctuates drastically. For example, 208 out of the 992 testing samples at the nested sieve are positive, while there are only 86 positive samples out of 1,746 testing samples in the Same-SenBothNer sieve. It seems unreasonable to use the same model for inference at both sieves. Second, the data for intermediate sieves is not always a subset of the top sieve. The reason is that top sieve extracts a positive instance only for the closest co-referent mentions, while sieves such as AllSentencePairs extract samples for all co-referent pairs which appear in the same sentence. Third, while our division to sieves may resemble witchcraft, it is motivated by the intuition that mentions appearing close to one another are easier instances of co-ref as well as linguistic insights of (Raghunathan et al., 2010).", "selected": "both sieves.", "paper_id": "9431715"}, {"section": "Intermediate Clustering Features (IC)", "paragraph": "Otherwise IC R i stores the pairwise prediction history, thus when classifying a pair (m j , m k ) at sieve i, a classifier can see the predictions of all the previous sieves applicable on that pair. IC E i stores the transitive closures of the sieve-specific predictions. We note that both IC R i and IC E i can have the values +1 and -1 active at the same time if intermediate sieve classifiers generated conflicting predictions. However, a classifier at sieve i will use as features both IC R 1 ,. . . IC R i\u22121 and IC E 1 ,. . . IC E i\u22121 , thus it will know the lowest sieve at which the conflicting evidence occurs. The classifier at sieve i also uses set identity, set containment, set overlap and other set comparison features between E +/\u2212 i\u22121 (m j ) and E +/\u2212 i\u22121 (m k ). We check whether the sets have symmetric difference, whether the size of the intersection between the two sets is at least half the size of the smallest set etc. We also generate subtypes of set comparison features when restricting the elements to NE-mentions and non-pronominal mentions (e.g \"what percent of named entities do the sets have in common?\").", "selected": "sieve-specific predictions.", "paper_id": "9431715"}, {"section": "Surface Form Compatibility (SFC)", "paragraph": "The  Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves. Both knowledge and entity-based features significantly and independently improve the performance for all sieves. The goal of entity-based features is to propagate knowledge effectively, thus it is encouraging that the combination of entity-based and knowledge features performed significantly better than any of the approaches individually at the top sieve.", "selected": "Both knowledge and entity-based features significantly and independently improve the performance for all sieves.", "paper_id": "9431715"}, {"section": "Figure 3 :", "paragraph": "Nested: are pairs such as \"{{[city]m 1 } of [Jerusalem]m 2 }\"where the extent of one of the mentions contains the extent of the other. For some mentions, the extent is the entire clause, so we also added a requirement that mention heads are at most 7 tokens apart. Intuitively, it is the easiest case of co-reference.There are 5,804 training samples and 992 testing samples, out of which 208 are co-referent. SameSenBothNer: are pairs of named entities which appear in the same sentence. We already saw an example for thiscase involving [Mubarak]m2 and [Hosni Mubarak]m3. There are 13,041 training samples and 1,746 testing samples, out of which 86 are co-referent. Adjacent: are pairs of mentions which appear closest to each other on the dependency tree. We note that most of the nested pairs are also adjacent. There are training 5,872 samples and 895 testing samples, out of which 219 are co-referent. SameSentenceOneNer: are pairs which appear in the same sentence and exactly one of the mentions is a named entity, and the other is not a pronoun. Typical pairs are \"Israel-country\", as opposed to \"Bill Clinton -reporter\". This type of pairs is fairly difficult, but our hope is to use encyclopedic knowledge to boost the performance. There are 15,715 training samples and 2,635 testing samples, out of which 207 are co-referent. NerMentionsDiffSent: are pairs of mentions in different sentences, both of which are named entities. There are 189,807 training samples and 24,342 testing samples, out of which 1,628 are co-referent. NonProSameSentence: are pairs in the same sentence, where both mentions are non-pronouns. This sieve includes all the pairs in the SameSentenceOneNer sieve. Typical pairs are \"city-capital\" and \"reporter-celebrity\". There are 33,895 training samples and 5,393 testing samples, out of which 336 are co-referent.. ClosestNonProDiffSent: are pairs of mentions in different sentences with no other mentions between the two. 3,707 training samples and 488 testing samples, out of which 38 are coreferent. AllSentencePairs: All mention pairs within same sentence. There are 49,953 training samples and 7,809 testing samples, out of which 846 are co-referent. TopSieve: The set of mention pairs classified by the baseline system. 525,398 training samples and 85,358 testing samples, out of which 1,387 are co-referent.", "selected": "This sieve inc", "paper_id": "9431715"}, {"section": "Figure 4 :", "paragraph": "We have integrated the strengths of rule-based systems such as(Haghighi and Klein, 2009;Raghunathan et al., 2010)into a multi-sieve machine learning framework. We show that training sieve-specific models significantly increases the performance on most intermediate sievesieves.We develop a novel approach for entity-based inference. Unlike(Rahman and Ng, 2011)who construct entities left-to-right, and similarly to(Raghunathan et al., 2010)we resolve easy instances of coref to reduce error propagation in entity-based features. Unlike(Raghunathan et al., 2010), we allow later stages of inference to change the decisions made at lower stages as additional entity-based evidence becomes available.By adding word-knowledge features and refining the inference, we improve the performance of a state-of-the-art system of(Bengtson and Roth, 2008)by 3 MUC, 2 B 3 and 2 CEAF F1 points on the nontranscript portion of the ACE 2004 dataset.", "selected": "We have integrated the strengths of rule-based systems such as(Haghighi and Klein, 2009;Raghunathan et al., 2010)into a multi-sieve machine learning framework. We show that training sieve-specific models significantly increases the performance on most intermediate sievesieves.We develop a novel approach for entity-based inference. Unlike(Rahman and Ng, 2011)who construct entities left-to-right, and similarly to(Raghunathan et al., 2010)we resolve easy instances of coref to reduce error propagation in entity-based features.", "paper_id": "9431715"}], "FxcIGzlHSq": []}}
{"idx": "221221", "paper_id": "14729876", "title": "Top-Down Predictive Linking and Complex-Feature-Based Formalisms", "abstract": "Automatic compilation of the linking relation employed in certain parsing algorithms for context-free languages is examined. Special problems arise in the extension of these algorithms to the possibly infinite domain of feature structures. A technique is proposed which is designed specifically for left-recursive categories and is based on the generalization of their occurrences in a derivation. Particular attention is drawn to the top-down predictive character of the linking relation and to its significance not only as a filter for increasing the efficiency of syntactic analysis but as a device for the top-down instantiation of information, which then serves as a key to the directed analysis of inflected forms as well as \"unknown\" or \"new\" words.", "context_section_header": "", "context_paragraph": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing. We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991)--but rather as a device for the top-down predictive instantiation of information, as Shieber et al. (1990) have shown for semantic-head-driven generation. In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of \"unknown\" or \"new\" lexical items.", "sentence": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing.", "cited_ids": [{"paper_id": "215762145", "citation": "Shieber et al. (1990)"}], "y": "Whereas Shieber et al. (1990) have discussed similar techniques [syntax-based ones that are top-down and allows for left-recursion] in the context of semantic head-driven generation [which generates language in a manner that is related to the semantic structure of the generated string], the authors are concerned here with parsing.", "snippet_surface": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, the authors are concerned here with parsing.", "questions": {"uDafEfurdH": "What similar techniques are referred here?", "HhuTcM2LI0": "What is semantic head-driven generation?"}, "answers": {"uDafEfurdH": "A technique for linking relation in the context of semantic head-driven generation, which is top-down and allows for left-recursion.", "HhuTcM2LI0": "It is an algorithm that generates language from a grammar in a manner that is related to the semantic structure of the generated string, in contrast with bottom-up generation procedures."}, "evidence": {"uDafEfurdH": [{"section": "Introduction", "paragraph": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing. We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991)--but rather as a device for the top-down predictive instantiation of information, as Shieber et al. (1990) have shown for semantic-head-driven generation. In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of \"unknown\" or \"new\" lexical items.", "selected": "Whereas Shieber et al. (1990) have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing.", "paper_id": "14729876"}, {"section": "Abstract", "paragraph": "We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike a previous bottom-up generator, it allows use of semantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.", "selected": "yet unlike top-down methods, it also permits left-recursion.", "paper_id": "215762145"}], "HhuTcM2LI0": [{"section": "SOURCE OF THE PROBLEMS", "paragraph": "From this point of view, a naive top-down parser or generator performs a depth-first, left-to-right traversal of the tree. Completion steps in Earley's algorithm, whether used for parsing or generation, correspond to a post-order traversal (with prediction acting as a pre-order filter). The left-to-right traversal order of both of these methods is geared towards the given information in a parsing problem, the string, rather than that of a generation problem, the goal logical form. It is exactly this mismatch between structure of the traversal and structure of the problem premise that accounts for the profligacy of these approaches when used for generation.", "selected": "Completion steps in Earley's algorithm, whether used for parsing or generation, correspond to a post-order traversal (with prediction acting as a pre-order filter). The left-to-right traversal order of both of these methods is geared towards the given information in a parsing problem, the string, rather than that of a generation problem, the goal logical form. It is exactly this mismatch between structure of the traversal and structure of the problem premise that accounts for the profligacy of these approaches when used for generation.", "paper_id": "215762145"}, {"section": "SOURCE OF THE PROBLEMS", "paragraph": "Thus, for generation, we want a traversal order geared to the premise of the generation problem, that is, to the semantic structure of the sentence. The new algorithm is designed to reflect such a traversal strategy respecting the semantic structure of the string being generated, rather than the string itself.", "selected": "Thus, for generation, we want a traversal order geared to the premise of the generation problem, that is, to the semantic structure of the sentence. The new algorithm is designed to reflect such a traversal strategy respecting the semantic structure of the string being generated, rather than the string itself.", "paper_id": "215762145"}]}}
{"idx": "221866", "paper_id": "8401287", "title": "Sentiment Flow - A General Model of Web Review Argumentation", "abstract": "Web reviews have been intensively studied in argumentation-related tasks such as sentiment analysis. However, due to their focus on content-based features, many sentiment analysis approaches are effective only for reviews from those domains they have been specifically modeled for. This paper puts its focus on domain independence and asks whether a general model can be found for how people argue in web reviews. Our hypothesis is that people express their global sentiment on a topic with similar sequences of local sentiment independent of the domain. We model such sentiment flow robustly under uncertainty through abstraction. To test our hypothesis, we predict global sentiment based on sentiment flow. In systematic experiments, we improve over the domain independence of strong baselines. Our findings suggest that sentiment flow qualifies as a general model of web review argumentation.", "context_section_header": "", "context_paragraph": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation.", "sentence": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment.", "cited_ids": [{"paper_id": "8715425", "citation": "(Wachsmuth et al., 2014a)"}], "y": "The authors' previous approach (Wachsmuth et al., 2014a) [involved cluster analysis to capture overall argument structure of a review]. [The author\u2019s new approach involve analyzing] the major abstraction steps, [such as content and domain differences,] when modeling sentiment flow to represent global sentiment.", "snippet_surface": "Unlike in the authors' previous approach (Wachsmuth et al., 2014a), they analyze the major abstraction steps when modeling sentiment flow to represent global sentiment.", "questions": {"AXruHBF3iO": "What are the major abstraction steps?", "3ft1n6Gsrb": "What is the authors' previous approach?"}, "answers": {"AXruHBF3iO": "Content and other domain differences", "3ft1n6Gsrb": "The previous approach uses cluster analysis (which they call \"sentiment flow\") to capture overall argument structure of a review."}, "evidence": {"AXruHBF3iO": [{"section": "Introduction", "paragraph": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation.", "selected": "A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it.", "paper_id": "8401287"}, {"section": "Introduction", "paragraph": "Unlike in our previous approach (Wachsmuth et al., 2014a), we analyze the major abstraction steps when modeling sentiment flow to represent global sentiment. A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it. Based on web review corpora with known sentiment flows, we empirically analyze several model variants across three domains. Our results offer clear evidence for the truth of our hypothesis, indicating the generality of sentiment flow as a model of web review argumentation.", "selected": "A general model should abstract from both content and other domain differences, such as a review's length or the density of local sentiment in it.", "paper_id": "8401287"}], "3ft1n6Gsrb": [{"section": "Introduction", "paragraph": "Concretely, here we address the supervised prediction of sentiment scores. To this end, we combine a number of existing argumentation-related features with a novel approach that learns common patterns in sequences of local sentiment through a cluster analysis in order to capture a review's overall argumentation structure. Inspired by explicit semantic analysis (Gabrilovich and Markovitch, 2007), we then compute the similarity of a given review text to each of these sentiment flow patterns and we use these similarities as features for sentiment scoring. To explain a predicted score and, hence, to increase user acceptance, both the underlying model and the sentiment flow patterns can be visualized.", "selected": "Concretely, here we address the supervised prediction of sentiment scores. To this end, we combine a number of existing argumentation-related features with a novel approach that learns common patterns in sequences of local sentiment through a cluster analysis in order to capture a review's overall argumentation structure. Inspired by explicit semantic analysis (Gabrilovich and Markovitch, 2007), we then compute the similarity of a given review text to each of these sentiment flow patterns and we use these similarities as features for sentiment scoring. To explain a predicted score and, hence, to increase user acceptance, both the underlying model and the sentiment flow patterns can be visualized.", "paper_id": "8715425"}]}}
{"idx": "222062", "paper_id": "14365022", "title": "A Syntactified Direct Translation Model with Linear-time Decoding", "abstract": "Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity. \n \nThis paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag-operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system.", "context_section_header": "", "context_paragraph": "Recently, (Shen et al., 2008) introduced an approach for incorporating a dependency-based language model into SMT. They proposed to extract String-to-Dependency trees from the parallel corpus. As the dependency trees are not constituents by nature, they handle non-constituent phrases as well. While this work is in the same general direction as our work, namely aiming at incorporating dependency parsing into SMT, there remain three major differences. Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory. Secondly, their decoder works bottom-up and uses a chart parser with a limited language model capability (3-grams), while we build on the efficient, linear-time decoder commonly used in phrase-based SMT. Thirdly, (Shen et al., 2008) deploys the dependency language model to augment the lexical language model probability be-tween two head words but never seek a full dependency graph. In contrast, our approach integrates an incremental parsing capability, that produces the partial dependency structures incrementally while decoding, and thus provides for better guidance for the search of the decoder for more grammatical output. To the best of our knowledge, our approach is the first to incorporate incremental dependency parsing capabilities into SMT while maintaining the linear-time and -space decoder.", "sentence": "Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory.", "cited_ids": [{"paper_id": "832217", "citation": "(Shen et al., 2008)"}], "y": "Firstly, Shen et al. (2008) resorted to heuristics to extract the String-to-Dependency trees, whereas the authors' approach [of a string-to-dependency algorithm for statistical machine translation] employs the well formalized CCG [combinatory categorial grammar] grammatical theory.", "snippet_surface": "Firstly, Shen et al. (2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas the authors' approach employs the well formalized CCG grammatical theory.", "questions": {"XsAFYq64Qk": "What does ccg stand for?"}, "answers": {"XsAFYq64Qk": "combinatory categorial grammar"}, "evidence": {"XsAFYq64Qk": [{"section": "Abstract", "paragraph": "Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity. \n \nThis paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag-operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system.", "selected": "Combinatory Categorial Grammar (CCG).", "paper_id": "14365022"}]}}
{"idx": "222510", "paper_id": "10148319", "title": "A Multithreaded Conversational Interface for Pedestrian Navigation and Question Answering", "abstract": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "context_section_header": "", "context_paragraph": "While most applications address these two problems independently, some like Google Now, Google Field Trip, etc, mix navigation with exploration. However, such applications present information primarily visually on the screen for the user to read. In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, our system allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1).", "sentence": "In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003).", "cited_ids": [{"paper_id": "5125584", "citation": "(Kray et al., 2003)"}], "y": "The authors' [android mobile app helps with pedestrian navigation in urban environments for tourist information and question answering] and has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003) as [the authors' model doesn't demand the user's attention/visual attention and has a speech-only interface that works in a question and answer way].", "snippet_surface": "In contrast, the authors' system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003).", "questions": {"d+L547/CZx": "What is the authors' system?", "1edukVtlXD": "How does the author achieve the objective of minimizing distraction?"}, "answers": {"d+L547/CZx": "An android mobile app that helps with pedestrian navigation in urban environments for tourist information and question answering.", "1edukVtlXD": "Their model doesn't demand the user's attention/visual attention and has a speech-only interface that works in a question and answer way instead."}, "evidence": {"d+L547/CZx": [{"section": "Abstract", "paragraph": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "selected": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies.", "paper_id": "10148319"}, {"section": "Abstract", "paragraph": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "selected": "mobile applications which treat these problems independently, our Android ag", "paper_id": "10148319"}, {"section": "Abstract", "paragraph": "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "selected": ". In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user\u2019s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer \u201cpopularity\u201d of geographical entities.", "paper_id": "10148319"}, {"section": "Related work", "paragraph": "While most applications address these two problems independently, some like Google Now, Google Field Trip, etc, mix navigation with exploration. However, such applications present information primarily visually on the screen for the user to read. In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, our system allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1).", "selected": "In contrast, our system has the objective of keeping the user's cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, our system allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1).", "paper_id": "10148319"}], "1edukVtlXD": []}}
{"idx": "223314", "paper_id": "219720870", "title": "Iterative Edit-Based Unsupervised Sentence Simplification", "abstract": "We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.", "context_section_header": "", "context_paragraph": "candidate sentence according to the scoring function. Compared with Narayan and Gardent (2016), the order of our simplification operations is not fixed and is decided by the model. Figure 1 illustrates an example in which our model first chooses to delete a sentence fragment, followed by reordering the remaining fragments and replacing a word with a simpler synonym.", "sentence": "Compared with Narayan and Gardent (2016), the order of our simplification operations is not fixed and is decided by the model.", "cited_ids": [{"paper_id": "568368", "citation": "Narayan and Gardent (2016)"}], "y": "The authors' results show that compared with previous work, the order of their simplification operations [lexical substitution, sentence splitting and word/phrase deletion] is not fixed and is decided by the model [the model has iterative edits with no fixed order to achieve the highest score] .", "snippet_surface": "The authors' results show that compared with Narayan and Gardent (2016), the order of their simplification operations is not fixed and is decided by the model.", "questions": {"T0/Sx2glYT": "What simplification operations are referred here?", "58VvGRXmkV": "How does the model impact the order of the simplification operations?"}, "answers": {"T0/Sx2glYT": "lexical substitution, sentence splitting and word/phrase deletion", "58VvGRXmkV": "Iterative edits with no fixed order to achieve the highest score"}, "evidence": {"T0/Sx2glYT": [{"section": "Introduction", "paragraph": "In previous work, researchers have addressed some of the above issues. For example, Alva-Manchego et al. (2017) and Dong et al. (2019) explicitly model simplification operators such as word insertion and deletion. Although these approaches are more controllable and interpretable than standard Seq2Seq models, they still require large volumes of aligned data to learn these operations. To deal with the second issue, Surya et al. (2019) recently proposed an unsupervised neural text simplification approach based on the paradigm of style transfer. However, their model is hard to interpret and control, like other neural network-based models. Narayan and Gardent (2016) attempted to address both issues using a pipeline of lexical substitution, sentence splitting, and word/phrase deletion. However, these operations can only be executed in a fixed order.", "selected": "Narayan and Gardent (2016) attempted to address both issues using a pipeline of lexical substitution, sentence splitting, and word/phrase deletion.", "paper_id": "219720870"}], "58VvGRXmkV": [{"section": "Overview", "paragraph": "Then, a candidate sentence is selected according to certain criteria. This process is repeated until none of the candidates improve the score of the source sentence by a threshold value. The last candidate is returned as the simplified sentence (Section 3.4).", "selected": "Then, a candidate sentence is selected according to certain criteria. This process is repeated until none of the candidates improve the score of the source sentence by a threshold value. The last candidate is returned as the simplified sentence (Section 3.4).", "paper_id": "219720870"}]}}
{"idx": "224246", "paper_id": "5896413", "title": "Word Sense Induction by Community Detection", "abstract": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word. Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses. Our results show competitive performance on the SemEval-2010 WSI Task.", "context_section_header": "", "context_paragraph": "We highlight those related works with connections to community detection. V\u00e9ronis (2004) demon-strated that word co-occurrence graphs follow a small-world network pattern. In his scheme, word senses are discovered by iteratively deleting the more connected portions of the subgraph to reveal the different senses' network structure. Our work capitalizes on this intuition of discovering senserelated subgraphs, but leverages formalized methods for community detection to identify them. Dorow and Widdows (2003) identify senserelated subgraphs in a similar method to community detection for local region of the co-occurrence graph. They use a random walk approach to identify regions of the graph that are sense-specific. Though not identical, we note that the random walk model has been successfully applied to community detection (Rosvall et al., 2009). Furthermore, Dorow and Widdows (2003) performs graph clustering on a perword basis; in contrast, the proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "sentence": "Furthermore, Dorow and Widdows (2003) performs graph clustering on a perword basis; in contrast, the proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "cited_ids": [{"paper_id": "15155425", "citation": "Dorow and Widdows (2003)"}], "y": "[To perform word sense induction (WSI),] Dorow and Widdows (2003) perform graph clustering on a per-word basis; in contrast, the authors' proposed approach identifies communities for the entire [co-occurrence] graph, effectively performing an all-word WSI.", "snippet_surface": "Furthermore, Dorow and Widdows (2003) perform graph clustering on a per-word basis; in contrast, the authors' proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.", "questions": {"oipcWDITSf": "What is the authors' proposed approach?", "UKdGThV9nu": "What is an \"all-word wsi\"?"}, "answers": {"oipcWDITSf": "Using graph-based WSI as community based detection communities of words are identified within a co-occurrence graph to distinguish senses, with the aim of achieving competitive performance and sense quality.", "UKdGThV9nu": "WSI stands for \"word sense induction\".  The phrase \"all-word\" refers to performing WSI for all of the words in the corpus at the same time."}, "evidence": {"oipcWDITSf": [{"section": "Abstract", "paragraph": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word. Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses. Our results show competitive performance on the SemEval-2010 WSI Task.", "selected": "We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses.", "paper_id": "5896413"}, {"section": "Abstract", "paragraph": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word. Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses. Our results show competitive performance on the SemEval-2010 WSI Task.", "selected": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word.", "paper_id": "5896413"}], "UKdGThV9nu": [{"section": "Abstract", "paragraph": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word. Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. We reinterpret graph-based WSI as community detection, a well studied problem in network science. The relations in the co-occurrence graph give rise to word communities, which distinguish senses. Our results show competitive performance on the SemEval-2010 WSI Task.", "selected": "Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word.", "paper_id": "5896413"}]}}
{"idx": "224972", "paper_id": "5389438", "title": "What good are \u2018Nominalkomposita\u2019 for \u2018noun compounds\u2019: Multilingual Extraction and Structure Analysis of Nominal Compositions using Linguistic Restrictors", "abstract": "Finding a definition of compoundhood that is cross-lingually valid is a non-trivial task as shown by linguistic literature. We present an iterative method for defining and extracting English noun compounds in a multilingual setting. We show how linguistic criteria can be used to extract compounds automatically and vice versa how the results of this extraction can shed new lights on linguistic theories about compounding. The extracted compound nouns and their multilingual contexts are a rich source that serves several purposes. In an additional case study we show how the database serves to predict the internal structure of tripartite noun compounds using spelling variations across languages, which leads to a precision of over 91%.", "context_section_header": "", "context_paragraph": "This multilingual perspective on a considerable number of languages has been adopted as well by Macherey et al., (2011), who present a multilingual language-independent approach to compound splitting. Moreover, they learned morphological operations on compounding automatically. Here, Macherey et al., (2011) extract training instances using a method related to Garera and Yarowsky (2008): select a single word f in a language l translated to several English words e i . If there is a translation for each e i to a word g i that shows a (partial) substring match with f , (f ; e 1 , . . . , e n ; g 1 , . . . , g n ) is extracted. While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages. This token-based perspective has the advantage that we can process English NCs for which there is no literal translation to the target language (e.g., health insurance aligned to Krankenversicherung (lit. invalid insurance)).", "sentence": "While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages.", "cited_ids": [{"paper_id": "8497268", "citation": "Macherey et al., (2011)"}], "y": "Macherey et al., (2011) extract training instances type-based in a bilingual setting, whereas the authors directly extract NC [noun compound] instances with a set of four closed compounding languages [languages like German that make one-word compounds].", "snippet_surface": "Macherey et al., (2011) extract training instances type-based in a bilingual setting, whereas the authors directly extract NC instances with a set of four closed compounding languages.", "questions": {"Q1/fQBM1js": "What are \"NC instances?\"", "0ebrTAKgpm": "What are \"closed compounding languages?\""}, "answers": {"Q1/fQBM1js": "NC instances are noun compounds", "0ebrTAKgpm": "Closed compounding languages are languages that make one-word compounds (like German)."}, "evidence": {"Q1/fQBM1js": [{"section": "Introduction", "paragraph": "The first aim of this study is to extract a large database of compounds and their translations in context from a parallel corpus. This database will serve multiple purposes. For example, it will be used to study compounding across different languages, and we will exploit the cross-lingual variation for compound processing. In the second part of this paper, we will show a case study of how the extracted database can be used for analysing the structure of noun phrases, more specifically, we exploit spelling variations across languages for bracketing three-noun compounds (3NCs) such as air traffic control, which could be indicated as LEFT bracketing using the German phrase Kontrolle des Luftverkehrs (control of air traffic).", "selected": "In the second part of this paper, we will show a case study of how the extracted database can be used for analysing the structure of noun phrases, more specifically, we exploit spelling variations across languages for bracketing three-noun compounds (3NCs) such as air traffic control, which could be indicated as LEFT bracketing using the German phrase Kontrolle des Luftverkehrs (control of air traffic).", "paper_id": "5389438"}, {"section": "Introduction", "paragraph": "In Section 2, we discuss the problem of defining noun compounds (NCs) as described by Lieber and Stekauer (2009) and present an iterative method for defining and extracting English NCs starting with an initial definition based on some linguistic tests. In Section 3, we present our method for extracting English NCs and their translations to several languages from a parallel corpus using a set of extraction rules. An experimental setup and results are presented in Section 4. In Section 5 we show in a case study of bracketing three-noun compounds how our database serves for exploiting multilingual spelling variations. Section 6 describes related work and finally Section 7 concludes.", "selected": "In Section 2, we discuss the problem of defining noun compounds (NCs) as described by Lieber", "paper_id": "5389438"}], "0ebrTAKgpm": [{"section": "Introduction", "paragraph": "Being abundant as a phenomenon but scarce in terms of individual examples (the combination of high type frequency and low token frequency) makes the analysis of these compound nouns particularly problematic for statistical techniques that need high token frequencies to make accurate predictions. Data sparsity is expected to lead to low performance. However, the correct analysis of compound nouns is important for a number of NLP tasks, for example in machine translation (Bouillon et al., 1992;Rackow et al., 1992;Johnston and Busa, 1999;Navigli et al., 2003). The accurate translation of compounds is non-trivial, because we find a large amount of variation in the way languages deal with compounding. Some languages such as German use closed compounding (i.e., they create one-word compounds, e.g., Todesstrafe (death penalty)) whereas others do not. In Romance languages, such as French, compounds are not as productive, instead postmodifying prepositional phrases (e.g., peine de mort) and adjectives (peine capitale) are used to construct complex nominals.", "selected": "Some languages such as German use closed compounding (i.e., they create one-word compounds, e.g., Todesstrafe (death penalty)) whereas others do not.", "paper_id": "5389438"}]}}
{"idx": "225325", "paper_id": "52010508", "title": "RNN Simulations of Grammaticality Judgments on Long-distance Dependencies", "abstract": "The paper explores the ability of LSTM networks trained on a language modeling task to detect linguistic structures which are ungrammatical due to extraction violations (extra arguments and subject-relative clause island violations), and considers its implications for the debate on language innatism. The results show that the current RNN model can correctly classify (un)grammatical sentences, in certain conditions, but it is sensitive to linguistic processing factors and probably ultimately unable to induce a more abstract notion of grammaticality, at least in the domain we tested.", "context_section_header": "", "context_paragraph": "This paper is a step in the direction of this research program. We trained two types of RNNs (GRU and LSTM) on a large corpus of English (Wikipedia and parts of UKWAC, Ferraresi et al. 2008), then tested their performances or a range of artificially constructed language structures, contrasting grammatical and ungrammatical examples. Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, we only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap). As a preliminary task (Task A), we check whether the network is sensitive to the difference in processing complexity between subject and object relatives, a much-studied domain in the psycholinguistic literature; next, we turn to two cases of ungrammaticality, one due to a violation of the principle of Full Interpretation (Chomsky, 1986a, p.98-99) (Task B), one to extraction out of \"strong\" syntactic islands (Ross, 1982), specifically, subject and relative clause islands (Task C). Violations of these types have been regarded as sharp both in the theoretical (Szabolcsi and den Dikken, 1999) and the experimental literature (Sorace and Keller, 2005;Cowart, 1997;Sprouse et al., 2013).", "sentence": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, we only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap).", "cited_ids": [{"paper_id": "1056628", "citation": "Lau et al. (2017)"}], "y": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, the authors only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal [a question word that starts with \"wh\" and replaces a noun] or the head of a relative clause and its gap).\"", "snippet_surface": "Unlike Lau et al. (2017), who use LSTM to prove that they can learn the graded nature of human grammaticality, the authors only look at long distance dependencies (the relation between a dislocated element like a Wh-nominal or the head of a relative clause and its gap).", "questions": {"5IK0/8BnqJ": "What is \"wh-nominal\"?"}, "answers": {"5IK0/8BnqJ": "A question word that starts with \"wh\" and replaces a noun (e.g. \"what\", \"which\", or \"who\")."}, "evidence": {"5IK0/8BnqJ": [{"section": "Methodology", "paragraph": "(1) a. Which mouse did the cat catch {? / last night ? / the tail of ? } b. *Which mouse did the cat catch the tail?", "selected": "(1) a. Which mouse did the cat catch {? / last night ? / the tail of ? } b. *Which mouse did the cat catch the tail?", "paper_id": "52010508"}, {"section": "Results and Discussion", "paragraph": "In Figure 2, we presents the effect of different levels of embedding between the wh-element and the gap position in cases where the gap is filled (Ungrammatical cases) or empty (Grammatical). Our findings suggests that in all our scenarios (grammatical and ungrammatical, Wh and affirmatives) an increase in the level of embedding increases the average CEL very significantly. A similar pattern is observed with affirmative sentences. Task C. Subject and Relative Island Violations: So far we have seen that the RNN model was able to distinguish grammatical from ungrammatical pairs with some success, but also to capture a number of interesting effect from the psycholinguistic literature: in Task A, the preference for subject relatives and the effect of intervening pronouns vs. full DPs (see Table 2); in Task B, the almost acceptability of Wh-resumptive pronouns vs. indefinites vs. the fully ungrammatical demonstrative fillers (Table 3), the matching animacy, and the shifting preferences for full stops or question marks (Figure 1). However, in many of these cases the margin of success was small with respective to others; for instance, only 3 ACEL points divide What should Mary discuss from What should Mary discuss it, but the ACEL distance between e.g. What should Mary discuss and What has she said that Mary should discuss is over 20 ACEL points.", "selected": "However, in many of these cases the margin of success was small with respective to others; for instance, only 3 ACEL points divide What should Mary discuss from What should Mary discuss it, but the ACEL distance between e.g. What should Mary discuss and What has she said that Mary should discuss is over 20 ACEL points.", "paper_id": "52010508"}, {"section": "Cases", "paragraph": "(8) a. Who did John see the person that dated ? b. Did John see the person that dated Mary? c. John saw the person that dated Mary. Table 5 shows the ACEL scores for all three types, as well as the ratio between the scores given to the sentence types. One remarkable aspect is that the Wh cases have a much higher (i.e. worse) score than the corresponding affirmative; even more remarkable, however, is the fact that Y/N questions are also very far from assertions, much closer in fact to their Wh counterparts. But Y/N questions have no Wh gap, hence no island effects. Equally remarkably, Y/N-questions and assertions follow a progression which is almost identical to the one of Wh-cases: NO>NS>NSp>RO>RS. This is quite visible from the ratios, which remain very stable under ACEL, and taper down slightly in PPL. This data shows that the increased perplexity with Wh cases has nothing to do with island effects, or we would not find it in Yes/No questions and assertions. Our hypothesis is that it is rather the cumulative effect of increasing syntactic complexity, plus position. Suppose that an NP such as a classmate of John is more complex than John, a classmate and possibly John's classmate, thus potentially more ambiguous. Suppose further that relative clauses are even more complex/ambiguous. Ambiguity leads to uncertainty, so by increasing it we increase perplexity as well, yielding the difference between NO,NS>RO,RS.", "selected": "(8) a. Who did John see the person that dated ? b. Did John see the person that dated Mary? c. John saw the person that dated Mary.", "paper_id": "52010508"}]}}
{"idx": "227076", "paper_id": "52111066", "title": "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction", "abstract": "We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.", "context_section_header": "", "context_paragraph": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "sentence": "Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.", "cited_ids": [{"paper_id": "13335042", "citation": "Artetxe et al. (2017)"}], "y": "The only difference between the authors' approach and Artetxe et al. (2017), the strongest baseline, is that the authors admit both one-to-one alignments [the mapping of each word in the source language to one word in the target language] or one-to-many alignments [the mapping of each word in the source language to many words in the target language] between the words of the languages' respective lexicons, whereas Artexe et al. did not.", "snippet_surface": "Viewed in this light, the difference between the authors' approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "Rga1pxXk6F": "What are one-to-one alignments?", "RR7LLLpiuI": "What are many-to-many alignments?"}, "answers": {"6gKwRw0I/Q": "The approach is the same as the one by Artetxe. However, in this work, they allow both one-to-one and one-to-many alignments between the words of the languages' respective lexicons.", "Rga1pxXk6F": "Each word in the source language is mapped to one word in the target language in the bilingual lexicon.", "RR7LLLpiuI": "Many-to-many alignments allow any number of words in the source language lexicon to map to any number of words in the target language lexicon."}, "evidence": {"6gKwRw0I/Q": [{"section": "Introduction", "paragraph": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "selected": "Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons.", "paper_id": "52111066"}], "Rga1pxXk6F": [], "RR7LLLpiuI": []}}
{"idx": "227077", "paper_id": "52111066", "title": "A Discriminative Latent-Variable Model for Bilingual Lexicon Induction", "abstract": "We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.", "context_section_header": "", "context_paragraph": "Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1-style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "sentence": "Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "cited_ids": [{"paper_id": "13335042", "citation": "Artetxe et al. (2017)"}], "y": "Thus, the authors conclude that their hard constraint on one-to-one alignments [i.e., that there exist a single source for every word in the target lexicon, and that this source cannot be used more than once] is primarily responsible for the improvements [on databases and tasks they tested on] over Artetxe et al. (2017).", "snippet_surface": "Thus, the authors conclude that their hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017).", "questions": {"BUwI/Ldvg8": "What are \"one-to-one alignments\"?"}, "answers": {"BUwI/Ldvg8": "The constraint that there exists a single source for every word in the target lexicon and that source cannot be used more than once."}, "evidence": {"BUwI/Ldvg8": [{"section": "Modeling Assumptions and their Limitations", "paragraph": "In the previous section, we have formulated the induction of a bilingual lexicon as the search for an edge set E, which we treat as a latent variable that we marginalize out in equation (2). Specifically, we assume that E is a partial matching. Thus, for every (i, j) \u2208 m, we have t i \u223c N (\u2126 s j , I), that is, the embedding for v trg (i) is assumed to have been drawn from a Gaussian centered around the embedding for v src (j), after an orthogonal transformation. This gives rise to two modeling assumptions, which we make explicit: (i) There exists a single source for every word in the target lexicon and that source cannot be used more than once. 4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent.", "selected": "This gives rise to two modeling assumptions, which we make explicit: (i) There exists a single source for every word in the target lexicon and that source cannot be used more than once.", "paper_id": "52111066"}]}}
{"idx": "230395", "paper_id": "16049704", "title": "Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models", "abstract": "This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.", "context_section_header": "", "context_paragraph": "Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "sentence": "Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "cited_ids": [{"paper_id": "5344007", "citation": "(He, 2007)"}], "y": "The authors' system [a system that used as a monolingual model improves the state of the art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, it outperforms the state-of-the-art word alignment model as measured by alignment error rate], used as a bilingual model, outperforms the current state-of-the-art WDHMM word alignment model as measured by alignment error rate (AER).", "snippet_surface": "The authors' system, used as a bilingual model, outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "questions": {"d+L547/CZx": "What is the authors' system?", "SkxaPs8bKr": "What does wdhmm stand for?"}, "answers": {"d+L547/CZx": "A system that used as a monolingual model improves the state of the art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, outperforms the state-of-the-art word alignment model as measured by alignment error rate.", "SkxaPs8bKr": "Transition probabilities except that the transition probability is computed."}, "evidence": {"d+L547/CZx": [{"section": "Introduction", "paragraph": "Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "selected": "Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER).", "paper_id": "16049704"}], "SkxaPs8bKr": [{"section": "Word alignment on the Canadian Hansards English-French corpus", "paragraph": "where S denotes the set of sure gold alignments, P denotes the set of possible gold alignments, A denotes the set of alignments generated by the word alignment method under test. We first trained the IBM model 1 and then a baseline HMM model as described in section 2 on the Hansards corpus. As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur together in a same sentence pair. HMM was initia-lized with uniform transition probabilities and model 1 translation probabilities. Both model 1 and HMM were trained with 5 iterations. For the proposed word dependent transition model based HMM (WDHMM), we used the same settings as the HMM baseline except that the transition probability is computed according to (11). We also trained IBM model 4 using GIZA++ provided by Och and Ney (2000c), where 5 iterations of model 4 training was performed after 5 iterations of model 1 plus 5 iterations of HMM.", "selected": "As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur together in a same sentence pair. HMM was initia-lized with uniform transition probabilities and model 1 translation probabilities. Both model 1 and HMM were trained with 5 iterations. For the proposed word dependent transition model based HMM (WDHMM), we used the same settings as the HMM baseline except that the transition probability is computed according to (11).", "paper_id": "5344007"}]}}
{"idx": "236031", "paper_id": "248780089", "title": "Towards Detecting Political Bias in Hindi News Articles", "abstract": "Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles.We propose a transformer-based transfer learning method to fine-tune the pre-trained network on our data for this bias detection. As the required dataset for this particular task was not available, we created our dataset comprising 1388 Hindi news articles and their headlines from various Hindi news media outlets. We marked them on whether they are biased towards, against, or neutral to BJP, a political party, and the current ruling party at the centre in India.", "context_section_header": "", "context_paragraph": "We present several baseline Machine learning and Deep Learning approaches to detecting political bias on our dataset. We observe that XLM-RoBERTa (Conneau et al. (2020)), a transformerbased model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "sentence": "We observe that XLM-RoBERTa (Conneau et al. (2020)), a transformerbased model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "cited_ids": [{"paper_id": "207880568", "citation": "(Conneau et al. (2020)"}], "y": "The authors observe that XLM-RoBERTa (Conneau et al. (2020)), a transformer-based model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC [Matthews Correlation].", "snippet_surface": "The authors observe that XLM-RoBERTa (Conneau et al. (2020)), a transformer-based model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "questions": {"+sj7uOwgaQ": "What is mcc metric?"}, "answers": {"+sj7uOwgaQ": "Matthews Correlation"}, "evidence": {"+sj7uOwgaQ": [{"section": "Introduction", "paragraph": "We present several baseline Machine learning and Deep Learning approaches to detecting political bias on our dataset. We observe that XLM-RoBERTa (Conneau et al. (2020)), a transformerbased model, outperforms other baseline models and achieves a score of 83% accuracy, 76.4% F1macro, and 72.1% MCC.", "selected": "MCC", "paper_id": "248780089"}, {"section": "MCC Matthews Correlation", "paragraph": "Coefficient (Matthews, 1975) takes all parameters of the confusion matrix into account and is less vulnerable to bias. It reports a number in the range \u22121 to 1, and a key advantage of it is its easy interpretability.", "selected": "Coefficient (Matthews, 1975) takes all parameters of the confusion matrix into account and is less vulnerable to bias. It reports a number in the range \u22121 to 1, and a key advantage of it is its easy interpretability.", "paper_id": "248780089"}]}}
{"idx": "236106", "paper_id": "86111", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "abstract": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "context_section_header": "", "context_paragraph": "Discriminative models in MT have been proposed before. Carpuat and Wu (2007) trained a maximum entropy classifier for each source phrase type which used source context information to disambiguate its translations. The models did not capture target-side information and they were independent; no parameters were shared between classifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the \"discriminative word lexicon\" and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as explicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type of features. Our implementation is freely available and can be further extended by other researchers in the future.", "sentence": "Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features.", "cited_ids": [{"paper_id": "11410088", "citation": "Jeong et al. (2010)"}], "y": "Jeong et al. (2010) proposed a discriminative lexicon with a rich [feature set split into three parts--the first set contains label-independent features which depend on the source sentence; second set contains shared features and depends on target-side context; and, the third set contains label-dependent features which describe the currently predicted phrasal translation--] tailored to translation into morphologically rich languages; unlike the authors' [new discriminative model for MT which utilises source and target context information], their model only used source-context features [either full word forms or lemmata].", "snippet_surface": "Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike the authors' work, their model only used source-context features.", "questions": {"Z55QKhq88b": "What are \"morphologically rich languages\"?", "YKw0Bh6gbk": "What is the authors' work?", "WHZLOktcBq": "What are \"source-context features\"?", "GYzWGs4AbI": "How does the feature set look like?"}, "answers": {"Z55QKhq88b": "Morphologically rich languages are languages which have complex semantics and morphology (and syntax), e.g. Czech.", "YKw0Bh6gbk": "The authors presented a new discriminative model for MT which utilises source and target context information. The authors shows that their model significantly improves the quality of English-Czech translation via improved lexical selections and morphological coherence.", "WHZLOktcBq": "The source-context features are either full word forms, or lemmata.", "GYzWGs4AbI": "The feature set is split into three parts: first set contains label-independent features which depend on the source sentence; second set contains shared features and depends on target-side context; and, the third set contains label-dependent features which describe the currently predicted phrasal translation."}, "evidence": {"Z55QKhq88b": [{"section": "Introduction", "paragraph": "Discriminative lexicons address some of the core challenges of phrase-based MT (PBMT) when translating to morphologically rich languages, such as Czech, namely sense disambiguation and morphological coherence. The first issue is semantic: given a source word or phrase, which of its possible meanings (i.e., which stem or lemma) should we choose? Previous work has shown that this can be addressed using a discriminative lexicon. The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only help with semantics but also to improve morphological and syntactic coherence.", "selected": "morphologically rich languages, such as Czech, namely sense disambiguation and morphological coherence. The first issue is semantic: given a source word or phrase, which of its possible meanings (i.e., which stem or lemma) should we choose? Previous work has shown that this can be addressed using a discriminative lexicon. The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder.", "paper_id": "86111"}], "YKw0Bh6gbk": [{"section": "Abstract", "paragraph": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "selected": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information.", "paper_id": "86111"}, {"section": "Abstract", "paragraph": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "selected": "We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence.", "paper_id": "86111"}], "WHZLOktcBq": [{"section": "Feature Set", "paragraph": "Going back to the examples from Figure 1, our model can disambiguate the translation of \"shooting\" based on the source-context features (either the full form or lemma). For the morphological disambiguation of the translation of \"yellowish cat\", the model has access to the morphological tags of the preceding target words which can disambiguate the correct morphological case.", "selected": "source-context features (either the full form or lemma)", "paper_id": "86111"}], "GYzWGs4AbI": [{"section": "Global Model", "paragraph": "Finally, we add the Cartesian product between the two namespaces to the feature set: every shared feature is combined with every translation feature.", "selected": "every shared feature is combined with every translation feature", "paper_id": "86111"}, {"section": "Feature Set", "paragraph": "Our feature set requires some linguistic processing of the data. We use the factored MT setting (Koehn and Hoang, 2007) and we represent each type of information as an individual factor. On the source side, we use the word surface form, its lemma, morphological tag, analytical function (such as Subj for subjects) and the lemma of the parent node in the dependency parse tree. On the target side, we only use word lemmas and morphological tags. Table 1 lists our feature sets for each language pair. We implemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phrase. Context features are extracted either from a window of a fixed size around the current phrase (on the source side) or from a limited left-hand side context (on the target side). Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011). Each of our feature types can be configured to look at any individual factors or their combinations.", "selected": "Table 1 lists our feature sets for each language pair", "paper_id": "86111"}, {"section": "Feature Set", "paragraph": "The features in Table 1 are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied during decoding. We use target context size two in all our experiments. 2 Finally, the third set contains label-dependent features which describe the currently predicted phrasal translation.", "selected": "The features in Table 1 are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied during decoding. We use target context size two in all our experiments. 2 Finally, the third set contains label-dependent features which describe the currently predicted phrasal translation.", "paper_id": "86111"}, {"section": "Feature Set", "paragraph": "We used slightly different subsets of the full feature set for different languages. In particular, we left out surface form features and/or bilingual features in some settings because they decreased performance, presumably due to over-fitting.", "selected": "We used slightly different subsets of the full feature set for different languages. In particular, we left out surface form features and/or bilingual features in some settings because they decreased performance, presumably due to over-fitting.", "paper_id": "86111"}]}}
{"idx": "236242", "paper_id": "8332619", "title": "Generalized Character-Level Spelling Error Correction", "abstract": "We present a generalized discriminative model for spelling error correction which targets character-level transformations. While operating at the character level, the model makes use of wordlevel and contextual information. In contrast to previous work, the proposed approach learns to correct a variety of error types without guidance of manuallyselected constraints or language-specific features. We apply the model to correct errors in Egyptian Arabic dialect text, achieving 65% reduction in word error rate over the input baseline, and improving over the earlier state-of-the-art system.", "context_section_header": "", "context_paragraph": "Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints. We present a detailed analysis of mistakes and demonstrate that the proposed model indeed learns to correct a wider variety of errors.", "sentence": "Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints.", "cited_ids": [{"paper_id": "16622975", "citation": "(Eskander et al., 2013)"}], "y": "The authors show a 65% reduction in word-error-rate (WER) over a do-nothing input benchmark, and they outperform a state-of-the-art system (Eskander et al., 2013) that depends extensively on language-specific and manually-selected constraints where selections are besed on the [classification of the character additions or removals].", "snippet_surface": "The authors demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and they improve over a state-of-the-art system (Eskander et al., 2013) which relies heavily on language-specific and manually-selected constraints.", "questions": {"HZMLj5odej": "How are the manually-selected constraints selected?"}, "answers": {"HZMLj5odej": "classification of the character additions/deletions"}, "evidence": {"HZMLj5odej": [{"section": "Transformation Statistics", "paragraph": "We observe two types of transformations when converting from spontaneous orthography to CODA: character substitutions that do not affect the word length, and character additions/deletions that change the word length. Tables 1 and 2 show the most common character substitution and addition/deletion transformations, respectively, as they appear in the training corpus, associated with their frequencies relative to the occurrence of transformations. The character substitutions are dominant, and constitute about 84% of all the transformations in the training corpus. While the classification of the character substitutions is automatically generated, the classification of the character additions/deletions is done manually using a random sample of 400 additions/deletions in the training corpus. This is because many additions/deletions are ambiguous.", "selected": "While the classification of the character substitutions is automatically generated, the classification of the character additions/deletions is done manually using a random sample of 400 additions/deletions in the training corpus.", "paper_id": "16622975"}]}}
{"idx": "239068", "paper_id": "13118529", "title": "A Probabilistic Framework for Answer Selection in Question Answering", "abstract": "This paper describes a probabilistic answer selection framework for question answering. In contrast with previous work using individual resources such as ontologies and the Web to validate answer candidates, our work focuses on developing a unified framework that not only uses multiple resources for validating answer candidates, but also considers evidence of similarity among answer candidates in order to boost the ranking of the correct answer. This framework has been used to select answers from candidates generated by four different answer extraction methods. An extensive set of empirical results based on TREC factoid questions demonstrates the effectiveness of the unified framework.", "context_section_header": "", "context_paragraph": "In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "sentence": "Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "cited_ids": [{"paper_id": "237199732", "citation": "(Voorhees, 2004)"}], "y": "Experimental results on TREC factoid questions (Voorhees, 2004) show that the authors' framework [a probabilistic answer selection framework which uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation and similarity features] significantly improved answer selection performance for four different extraction techniques [feature representation, answer validation features and answer similarity features], when compared to default selection using the individual candidate scores produced by each extractor.", "snippet_surface": "Experimental results on TREC factoid questions (Voorhees, 2004) show that the authors' framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "questions": {"TM9lYDyHKf": "What are trec factoid questions?", "EB8diJjZ9J": "What is the authors' framework?", "LW67SP6+HH": "What difference extraction techniques are referred here?"}, "answers": {"TM9lYDyHKf": "Factoid questions in the main test of the TREC 2003 question answering track", "EB8diJjZ9J": "probabilistic answer selection framework which uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features", "LW67SP6+HH": "Feature representation, answer validation features, knowledge-based features and gazetteers"}, "evidence": {"TM9lYDyHKf": [{"section": "Abstract", "paragraph": "The TREC 2003 question answering track contained two tasks, the passages task and the main task. In the passages task, systems returned a single text snippet in res ponse to factoid questions; the evaluation metric was the number of snippets that contained a correct answer. The main task contained three separate types of questions, factoid questions, list questions, and definition questions. Each o f t e questions was tagged as to its type and the different question types were evaluated separately. The final score fo r a main task run was a combination of the scores for the separate question types. This paper defines the various tasks included in the track and reports the evaluation results. Since the TREC 2003 track was the first time for significant participation in the d finition and list subtasks, the paper also examines the reliability of the evaluation for these tasks. TREC introduced the first question answering (QA) track in TR EC-8 (1999). The goal of the track is to foster research on systems that retrieve answers rather than docum ents in response to a question, with particular emphasis on systems that can function in unrestricted domains. The task s in the track have evolved over the years to focus research on particular aspects of the problem deemed important to imp roving the state-of-the-art. The task in the original QA tracks required systems to return ext snippets drawn from a large corpus of newspaper articles in response to closed-class or factoid questions s uch asWho invented the paper clip? . Each response was judged by a human assessor; a response was marked correct if a n answer to the question was contained within the snippet. Unfortunately, the relative effectiveness of dif ferent systems was masked by the fact that two different snip pets could both contain a correct answer while one was a significan tly better response then the other [4]. To force systems to demonstrate their ability to locate the actual answer, th TREC 2002 task required systems to return exact answers , text strings consisting of a complete answer and nothing els e. Strings that contained a right answer with additional text were judged to be \u201cinexact\u201d and did not contribute to a sy stem\u2019s score. Pinpointing the precise extent of an answer is a more difficul t problem than finding a text segment that contains an answer, and there are applications of QA technology that do n ot require this extra step. To provide a forum for research groups interested in these applications, the TREC 2003 trac k included a \u201cpassages\u201d task that allowed text segments containing answers to be returned. The other task in the trac k, the main task, required exact responses. While the test set of questions for the passages task contained only fa ctoid questions, the main task contained list and definition questions as well as factoid questions. Each question type w as evaluated separately, and the final score for a main task run was a combination of the scores for the three questions ty pes. This paper provides an overview of the results of the TREC 200 3 QA track. The first two sections describe the two tasks in the track and present the evaluation results for the tasks. Since the TREC 2003 track was the first time for significant participation in the definition and list subtask s, Section 4 examines the reliability of the evaluation used for these tasks. This analysis demonstrates that the evaluatio n results for the definition task must be interpreted with car e as using different assessors can cause substantial changes in th relative evaluation scores. Using more questions in the definition test set should increase the stability of the e valuation.", "selected": "The TREC 2003 question answering track contained two tasks, the passages task and the main task. In the passages task, systems returned a single text snippet in res ponse to factoid questions; the evaluation metric was the number of snippets that contained a correct answer. The main task contained three separate types of questions, factoid questions, list questions, and definition questions", "paper_id": "237199732"}], "EB8diJjZ9J": [{"section": "Introduction", "paragraph": "In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "selected": "The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features.", "paper_id": "13118529"}, {"section": "Introduction", "paragraph": "In this paper we describe a probabilistic answer selection framework to address the two issues. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer validation features and answer similarity features. Experimental results on TREC factoid questions (Voorhees, 2004) show that our framework significantly improved answer selection performance for four different extraction techniques, when compared to default selection using the individual candidate scores produced by each extractor.", "selected": "In this paper we describe a probabilistic answer selection framework", "paper_id": "13118529"}], "LW67SP6+HH": [{"section": "Gazetteers:", "paragraph": "Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc.", "selected": "Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc.", "paper_id": "13118529"}]}}
{"idx": "239373", "paper_id": "12126824", "title": "The Effect of Corpus Size in Combining Supervised and Unsupervised Training for Disambiguation", "abstract": "We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins' parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning.", "context_section_header": "", "context_paragraph": "Both data sets are available for download (Web Appendix, 2006). We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below).", "sentence": "We did not use the PP data set described by (Ratnaparkhi et al., 1994) because we are using more context than the limited context available in that set (see below).", "cited_ids": [{"paper_id": "129886", "citation": "(Ratnaparkhi et al., 1994)"}], "y": "The authors did not use the PP dataset described by (Ratnaparkhi et al., 1994) [containing prepositional phrase-attachments] because we are using more context than the limited context available in that set (see below).", "snippet_surface": "The authors did not use the PP data set described by (Ratnaparkhi et al., 1994) because they are using more context than the limited context available in that set (see below).", "questions": {"4lmZNVr3NP": "What does the pp dataset comprise of?"}, "answers": {"4lmZNVr3NP": "prepositional phrase-attachment performances of 3 tree banking experts on a set of 300 randomly selected test events from the WSJ corpus"}, "evidence": {"4lmZNVr3NP": [{"section": "Introduction", "paragraph": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence, For this example, a human annotator's attachment decision, which for our purposes is the \"correct\" attachment, is to the noun phrase. We present in this paper methods for constructing statistical models for computing the probability of attachment decisions. These models could be then integrated into scoring the probability of an overall parse. We present our methods in the context of prepositional phrase (PP) attachment.", "selected": "We present our methods in the context of prepositional phrase (PP) attachment.", "paper_id": "129886"}, {"section": "Results", "paragraph": "For comparison, we obtained the PP-attachment performances of 3 treebanking experts on a set of 300 randomly selected test events from the WSJ corpus. In the first trial, they were given only the four head words to make the attachment decision, and in the next, they were given the headwords along with the sentence in which they occurred. Figure 3 shows an example of the head words test a. The results of the treebankers and the performance of the ME model on that same set are shown in Table 5. We also identified the set of 274 events on which treebankers, given the sentence, unanimously agreed. We defined this to be the truth set. We show in Table 6 the agreement on PP-attachment of the original WSJ treebank parses with this consensus set, the average performance of the 3 human experts with head words only, and the ME model. The WSJ treebank indicates the accuracy rate of our training data, the human performance indicates how much information is in the headwords, and the ME model is still a good 12 Preposition is \"to\" (4) Bit 12 of Head Noun == 1 (9) Head Noun == \"million\", Preposition == \"in\"", "selected": "For comparison, we obtained the PP-attachment performances of 3 treebanking experts on a set of 300 randomly selected test events from the WSJ corpus.", "paper_id": "129886"}]}}
{"idx": "239597", "paper_id": "2687347", "title": "Chinese Segmentation with a Word-Based Perceptron Algorithm", "abstract": "Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.", "context_section_header": "", "context_paragraph": "One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "sentence": "Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "cited_ids": [{"paper_id": "5651543", "citation": "Sproat et al. (1996)"}], "y": "A difference from the authors' model [perceptron training algorithm with a beamsearch decoder for de-segmenting Chinese text] is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996) [starting at one end of the sentence sentence, finding the longest word and then repeating].", "snippet_surface": "Another difference from the authors' model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "questions": {"4T9JAsYA/w": "What is the authors' model?", "wj4xY8eWek": "What is a \"rule-based submodel?\"", "tG7jDCYPho": "What is a \"forward maximum match method?\""}, "answers": {"4T9JAsYA/w": "The author's model is the perceptron training algorithm used alongside a beamsearch decoder in order to de-segment Chinese text based on complete words or word sequences.\n\nIn the paper authored by Sproat et al. (1996), the model presented is a stochastic finite-state model which provides a way to incorporate a variety of Chinese text lexical information in a uniform way.", "wj4xY8eWek": "A rule-based sub-model uses a dictionary-based forward maximum match method. This involves starting at the beginning, or end, of a sentence, finding the longest word and then repeating the process based on if the beginning or the end of the sentence was used as the start.", "tG7jDCYPho": "The forward maximum match method involves starting at the beginning, or end, of a sentence, finding the longest word and then repeating the process based on if the beginning or the end of the sentence was used as the start."}, "evidence": {"4T9JAsYA/w": [{"section": "Abstract", "paragraph": "Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.", "selected": "In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.", "paper_id": "2687347"}, {"section": "Abstract", "paragraph": "Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.", "selected": "In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.", "paper_id": "2687347"}, {"section": "Conclusions and Future Work", "paragraph": "We proposed a word-based CWS model using the discriminative perceptron learning algorithm. This model is an alternative to the existing characterbased tagging models, and allows word information to be used as features. One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process. We use a beam-search decoder, which places our work in the context of recent proposals for searchbased discriminative learning algorithms. Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.", "selected": "We proposed a word-based CWS model using the discriminative perceptron learning algorithm.", "paper_id": "2687347"}, {"section": "Conclusions", "paragraph": "Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages. The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way. The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind. As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis. Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994). Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended. While size of the resulting transducers may seem daunting--the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs--recent work on minimization of weighted machines and transducers (cf. 21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by a classifier. The particular classifier used depends upon the noun.", "selected": "The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.", "paper_id": "5651543"}], "wj4xY8eWek": [{"section": "Comparison with Previous Work", "paragraph": "One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).", "selected": "the rule-based submodel, which uses a dictionary-based forward maximum match method", "paper_id": "2687347"}, {"section": "Previous Work", "paragraph": "Nonstochastic lexical-knowledge-based approaches have been much more numerous. Two issues distinguish the various proposals. The first concerns how to deal with ambiguities in segmentation. The second concerns the methods used (if any) to extend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based. The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached. Papers that use this method or minor variants thereof include Liang (1986), Li et al. (1991), Gu and Mao (1994), and Nie, Jin, and Hannan (1994). The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation. Methods that allow multiple segmentations must provide criteria for choosing the best segmentation. Some approaches depend upon some form of constraint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach). Others depend upon various lexical heuristics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word. Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth Wang, Li, and Chang 1992;Chang and Chen 1993;Nie, Jin, and Hannan 1994).", "selected": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.", "paper_id": "5651543"}], "tG7jDCYPho": [{"section": "Previous Work", "paragraph": "Nonstochastic lexical-knowledge-based approaches have been much more numerous. Two issues distinguish the various proposals. The first concerns how to deal with ambiguities in segmentation. The second concerns the methods used (if any) to extend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based. The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached. Papers that use this method or minor variants thereof include Liang (1986), Li et al. (1991), Gu and Mao (1994), and Nie, Jin, and Hannan (1994). The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation. Methods that allow multiple segmentations must provide criteria for choosing the best segmentation. Some approaches depend upon some form of constraint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach). Others depend upon various lexical heuristics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word. Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth Wang, Li, and Chang 1992;Chang and Chen 1993;Nie, Jin, and Hannan 1994).", "selected": "The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics.", "paper_id": "5651543"}, {"section": "Previous Work", "paragraph": "Nonstochastic lexical-knowledge-based approaches have been much more numerous. Two issues distinguish the various proposals. The first concerns how to deal with ambiguities in segmentation. The second concerns the methods used (if any) to extend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based. The most popular approach to dealing with segmentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the \"greedy algorithm\" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached. Papers that use this method or minor variants thereof include Liang (1986), Li et al. (1991), Gu and Mao (1994), and Nie, Jin, and Hannan (1994). The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation. Methods that allow multiple segmentations must provide criteria for choosing the best segmentation. Some approaches depend upon some form of constraint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach). Others depend upon various lexical heuristics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word. Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth Wang, Li, and Chang 1992;Chang and Chen 1993;Nie, Jin, and Hannan 1994).", "selected": "involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginning) of the sentence is reached.", "paper_id": "5651543"}]}}
{"idx": "239760", "paper_id": "235294133", "title": "Higher-order Derivatives of Weighted Finite-state Machines", "abstract": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time\u2014from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A\u02c62 N\u02c64) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.", "context_section_header": "", "context_paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "sentence": "In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "cited_ids": [{"paper_id": "5548799", "citation": "(Mohri, 1997)"}], "y": "In contrast to many presentations of WFSMs [Weighted finite-state machines - computationally efficient mathematical models used in speech processing and NLP], the authors' work provides a purely linear-algebraic take on them. This connection allows them to develop [their algorithm that is an extension to the solved case of acyclic WFSMs. It solves the problem in the general case for efficient computation of mth order derivatives over cyclic WFSM].", "snippet_surface": "In contrast to many presentations of WFSMs (Mohri, 1997), the authors' work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows them to develop their general algorithm.", "questions": {"gat8LWcO1R": "What is \"this connection\"?", "DBjnVGc3Ml": "What is their general algorithm?", "eArSQstfQp": "What are wfsms?"}, "answers": {"gat8LWcO1R": "This connection between WFSMs and the authors' purely linear-algebraic take on them.", "DBjnVGc3Ml": "Their general algorithm is an extension to the solved case of acyclic WFSMs. It solves the problem in the general case for efficient computation of m th order derivatives over cyclic WFSM.", "eArSQstfQp": "Weighted finite-state machines (WFSMs) are computationally efficient automata, mathematical models used in speech processing and more recently NLP in deep learning."}, "evidence": {"gat8LWcO1R": [{"section": "Introduction", "paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "selected": "purely linear-algebraic take", "paper_id": "235294133"}], "DBjnVGc3Ml": [{"section": "Introduction", "paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "selected": "However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM.", "paper_id": "235294133"}], "eArSQstfQp": [{"section": "Abstract", "paragraph": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time\u2014from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A\u02c62 N\u02c64) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.", "selected": "Weighted finite-state machines", "paper_id": "235294133"}, {"section": "Introduction", "paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "selected": "Weighted finite-state machines", "paper_id": "235294133"}, {"section": "Introduction", "paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "selected": "Weighted finite-state machines (WFSMs) have a storied role in NLP.", "paper_id": "235294133"}, {"section": "Introduction", "paragraph": "Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005;Lind\u00e9n et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of m th -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm.", "selected": "WFSMs have been \"neuralized\" (Rastogi et al., 2016;Hannun et al., 2020;Schwartz et al., 2018) and are still of practical use to the NLP modeler.", "paper_id": "235294133"}, {"section": "Introduction", "paragraph": "Finite-state machines have been used in many areas of computational linguistics. Their use can be justified by both linguistic and computational arguments. Linguistically, finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language. They often lead to a compact representation of lexical rules, or idioms and cliches, that appears natural to linguists (Gross 1989). Graphic tools also allow one to visualize and modify automata, which helps in correcting and completing a grammar. Other more general phenomena, such as parsing context-free grammars, can also be dealt with using finitestate machines such as RTN's (Woods 1970). Moreover, the underlying mechanisms in most of the methods used in parsing are related to automata.", "selected": "Finite-state machines have been used in many areas of computational linguistics. Their use can be justified by both linguistic and computational arguments. Linguistically, finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language. They often lead to a compact representation of lexical rules, or idioms and cliches, that appears natural to linguists", "paper_id": "5548799"}, {"section": "Introduction", "paragraph": "From the computational point of view, the use of finite-state machines is mainly motivated by considerations of time and space efficiency. Time efficiency is usually achieved using deterministic automata. The output of deterministic machines depends, in general linearly, only on the input size and can therefore be considered optimal from this point of view. Space efficiency is achieved with classical minimization algorithms (Aho, Hopcroft, and Ullman 1974) for deterministic automata. Applications such as compiler construction have shown deterministic finite automata to be very efficient in practice (Aho, Sethi, and Ullman 1986). Finite automata now also constitute a rich chapter of theoretical computer science (Perrin 1990).", "selected": "From the computational point of view, the use of finite-state machines is mainly motivated by considerations of time and space efficiency.", "paper_id": "5548799"}, {"section": "Introduction", "paragraph": "We have used most of these algorithms in speech processing. In the last section, we describe some applications of determinization and minimization of string-to-weight transducers in speech recognition, illustrating them with several results that show them to be very efficient. Our implementation of the determinization is such that it can be used on the fly: only the necessary part of the transducer needs to be expanded. This plays an important role in the space and time efficiency of speech recognition. The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.", "selected": "string-to-weight transducers in speech recognition,", "paper_id": "5548799"}, {"section": "Introduction", "paragraph": "We have used most of these algorithms in speech processing. In the last section, we describe some applications of determinization and minimization of string-to-weight transducers in speech recognition, illustrating them with several results that show them to be very efficient. Our implementation of the determinization is such that it can be used on the fly: only the necessary part of the transducer needs to be expanded. This plays an important role in the space and time efficiency of speech recognition. The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.", "selected": "This plays an important role in the space and time efficiency of speech recognition. The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.", "paper_id": "5548799"}]}}
{"idx": "242385", "paper_id": "5815524", "title": "Improving Automatic Speech Recognition for Lectures through Transformation-based Rules Learned from Minimal Data", "abstract": "We demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9%. Our method is distinguished from earlier related work by its robustness to small amounts of training data, and its resulting efficiency, in spite of its use of true word error rate computations as a rule scoring function.", "context_section_header": "", "context_paragraph": "What we show in this paper is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one-hour lecture is necessary to learn the TBL rules, and that this smaller amount of transcribed data in turn makes the true WER calculation computationally feasible. With this combination, we achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of our reimplementation of their heuristics on our lecture data (3.6%). This is on top of the average 11% RER from language model adaptation on the same data. We also achieve the RER from TBL without the obligatory round of development-set parameter tuning required by their heuristics, and in a manner that is robust to perplexity. Less is more.", "sentence": "With this combination, we achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of our reimplementation of their heuristics on our lecture data (3.6%).", "cited_ids": [{"paper_id": "16803157", "citation": "Peters and Drexel (2004)"}], "y": "[By correcting ASR output,] the authors achieve a greater average relative error reduction [RER] (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of their reimplementation of their heuristics on their lecture data (3.6%).", "snippet_surface": "The authors achieve a greater average relative error reduction (12.9%) than that reported by Peters and Drexel (2004) on their dictation corpus (9.6%), and an RER over three times greater than that of their reimplementation of their heuristics on their lecture data (3.6%).", "questions": {"eCpF3FvTk2": "How do the authors' achieve a greater rer?"}, "answers": {"eCpF3FvTk2": "The authors achieve a lower RER by correcting ASR output."}, "evidence": {"eCpF3FvTk2": [{"section": "Introduction", "paragraph": "People have also tried to correct ASR output in a second pass. Ringger and Allen (1996) treated ASR errors as noise produced by an auxiliary noisy channel, and tried to decode back to the perfect transcript. This reduced WER from 41% to 35% on a corpus of train dispatch dialogues. Others combine the transcripts or word lattices (from which transcripts are extracted) of two complementary ASR systems, a technique first proposed in the context of NIST's ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR systems.", "selected": "Others combine the transcripts or word lattices (from which transcripts are extracted) of two complementary ASR systems, a technique first proposed in the context of NIST's ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR systems.", "paper_id": "5815524"}, {"section": "Introduction", "paragraph": "This paper tries to correct ASR output using transformation-based learning (TBL). This, too, has been attempted, although on a professional dictation corpus with a 35% initial WER (Peters and Drexel, 2004). They had access to a very large amount of manually transcribed data -so large, in fact, that the computation of true WER in the TBL rule selection loop was computationally infeasible, and so they used a set of faster heuristics instead. Mangu and Padmanabhan (2001) used TBL to improve the word lattices from which the transcripts are decoded, but this method also has efficiency problems (it begins with a reduction of the lattice to a confusion network), is poorly suited to word lattices that have already been heavily domain-adapted because of the language model's low perplexity, and even with higher perplexity models (the SWITCHBOARD corpus using a lan-1 This work generally measures progress by reduction in the size of training data rather than relative WER reduction. Riccardi and Hakkani-Tur (2005) achieved a 30% WER with 68% less training data than their baseline. Huo and Li (2007) worked on a small-vocabulary name-selection task that combined active learning with acoustic model adaptation. They reduced the WER from 15% to 3% with 70 syllables of acoustic adaptation, relative to a baseline that reduced the WER to 3% with 300 syllables of acoustic adaptation. guage model trained over a diverse range of broadcast news and telephone conversation transcripts), was reported to produce only a 5% WER reduction.", "selected": "This paper tries to correct ASR output using transformation-based learning (TBL).", "paper_id": "5815524"}]}}
{"idx": "244942", "paper_id": "220446825", "title": "Human Verb Associations as the Basis for Gold Standard Verb Classes: Validation against GermaNet and FrameNet", "abstract": "We describe a gold standard for semantic verb classes which is based on human associations to verbs. The associations were collected in a web experiment and then applied as verb features in a hierarchical cluster analysis. We claim that the resulting classes represent a theory-independent gold standard classification which covers a variety of semantic verb relations, and whose features can be used to guide the feature selection in automatic processes. To evaluate our claims, the association-based classification is validated against two standard approaches to semantic verb classes, GermaNet and FrameNet.", "context_section_header": "", "context_paragraph": "This paper suggests a resource for gold standard semantic verb classes which is independent from manual definitions. We collected human associations of German verbs in a web experiment, and performed a simple hierarchical clustering on the verbs, as based on the human associations. We claim that the resulting verb classes and their underlying features (i.e. the verb associations) represent a valuable basis for a theory-independent semantic classification of the German verbs. To support this claim, we validate the associationbased classes against existing verb classes. Why are we interested in a gold standard semantic verb classification? There are a variety of manual semantic verb classifications; major frameworks are the Levin classes (Levin, 1993), WordNet (Fellbaum, 1998) with its German counterpart GermaNet (Kunze, 2000), and FrameNet (Fontenelle, 2003) with the Salsa project (Erk et al., 2003) creating its German counterpart. Different frameworks depend on different instantiations of semantic similarity, e.g. Levin relies on syntactic similarity and verb alternations, WordNet uses synonymy, and FrameNet relies on situationbased agreement as defined in Fillmore's frame semantics (Fillmore, 1982). As an alternative to the resourceintensive manual classifications, automatic methods such as classification and clustering techniques have been applied to induce verb classes from corpus data, e.g. (Schulte im Walde, 2000;Merlo and Stevenson, 2001;Joanis and Stevenson, 2003;Korhonen et al., 2003;Schulte im Walde, 2003;Ferrer, 2004). When evaluating such induced classifications, it is difficult to define a gold standard that is generally accepted and covers various aspects of semantic similarity. 1 Human associations should provide a useful source 1 Note that we refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "sentence": "1 Human associations should provide a useful source 1 Note that we refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "cited_ids": [{"paper_id": "9680240", "citation": "Merlo and Stevenson (2001)"}], "y": "The authors suggest that human associations should provide a useful source. They note that they refer to cases where it is desirable to have a generally accepted gold standard [that covers various aspects of semantic similarity]\n  (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations)", "snippet_surface": "The authors suggest that human associations should provide a useful source. They note that they refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "questions": {"UW/04x9OFI": "What does gold standard mean in this context?"}, "answers": {"UW/04x9OFI": "Gold standard means a generally accepted standard that covers various aspects of semantic similarity."}, "evidence": {"UW/04x9OFI": [{"section": "Introduction", "paragraph": "This paper suggests a resource for gold standard semantic verb classes which is independent from manual definitions. We collected human associations of German verbs in a web experiment, and performed a simple hierarchical clustering on the verbs, as based on the human associations. We claim that the resulting verb classes and their underlying features (i.e. the verb associations) represent a valuable basis for a theory-independent semantic classification of the German verbs. To support this claim, we validate the associationbased classes against existing verb classes. Why are we interested in a gold standard semantic verb classification? There are a variety of manual semantic verb classifications; major frameworks are the Levin classes (Levin, 1993), WordNet (Fellbaum, 1998) with its German counterpart GermaNet (Kunze, 2000), and FrameNet (Fontenelle, 2003) with the Salsa project (Erk et al., 2003) creating its German counterpart. Different frameworks depend on different instantiations of semantic similarity, e.g. Levin relies on syntactic similarity and verb alternations, WordNet uses synonymy, and FrameNet relies on situationbased agreement as defined in Fillmore's frame semantics (Fillmore, 1982). As an alternative to the resourceintensive manual classifications, automatic methods such as classification and clustering techniques have been applied to induce verb classes from corpus data, e.g. (Schulte im Walde, 2000;Merlo and Stevenson, 2001;Joanis and Stevenson, 2003;Korhonen et al., 2003;Schulte im Walde, 2003;Ferrer, 2004). When evaluating such induced classifications, it is difficult to define a gold standard that is generally accepted and covers various aspects of semantic similarity. 1 Human associations should provide a useful source 1 Note that we refer to cases where it is desirable to have a generally accepted gold standard (e.g. to compare clustering results independent of a specific framework), in contrast to cases where a specific type of classification is the target (e.g. Merlo and Stevenson (2001) aim for a three-class distinction of English verbs that models three types of intransitive-transitive alternations).", "selected": "When evaluating such induced classifications, it is difficult to define a gold standard that is generally accepted and covers various aspects of semantic similarity.", "paper_id": "220446825"}]}}
{"idx": "247151", "paper_id": "220047806", "title": "A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization", "abstract": "Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.", "context_section_header": "", "context_paragraph": "We propose an architecture (shown in Figure 1) that is able to consider both morphological and semantic information. We first apply a candidate generator to generate a list of candidate concepts, and then use a BERT-based list-wise classifier to rank the candidate concepts. This two-step architecture allows unlikely concept candidates to be filtered out prior to the final classification, a necessary step when dealing with ontologies with millions of concepts. In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, our BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data. We further enhance this list-wise approach with a semantic type regularizer that allows our ranker to leverage semantic type information from  the ontology during training.", "sentence": "In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, our BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data.", "cited_ids": [{"paper_id": "46896270", "citation": "(Murty et al., 2018)"}], "y": "[There are two previous list-wise classifiers for bio-medical concept classification. The first treats the selection of the best candidate concept as a flat classification problem that loses the ability to handle concepts not encountered in training. The second uses a generate-and-rank approach that does not take advantage of the synonyms and semantic type information from UMLS.] In contrast to these previous list-wise classifiers (Murty et al., 2018) which only take the [biomedical] concept mention and the candidate concept name as input, the authors' BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the [clinical notes] training data.\"", "snippet_surface": "In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, the authors' BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data.", "questions": {"ih8xvCtK/v": "What are the previous list-wise classifiers?", "gCHivGxcKd": "What does the training data look like?", "0XHKsPExpN": "What is concept mention?"}, "answers": {"ih8xvCtK/v": "There are two previous list-wise classifiers: While the first one treat the selection of the best candidate concept as a flat classification problem that loses the ability to handle concepts not encountered in training, the second one uses a generate-and-rank approach that does not take advantage of the synonyms and semantic type information from UMLS.", "gCHivGxcKd": "Trianing data includes 40 clinical notes from the released data that is constituted by 5334 mentions .", "0XHKsPExpN": "Concept mentions are refered to the explanation and usage of various terms in bio-medical science literature."}, "evidence": {"ih8xvCtK/v": [{"section": "Related work", "paragraph": "Researchers have applied point-wise learning to rank (Liu and Xu, 2017;Li et al., 2017), pairwise learning to rank (Leaman et al., 2013;Nguyen et al., 2018), and list-wise learning to rank (Murty et al., 2018;Ji et al., 2019) on concept normalization. Generally, the learning-to-rank approach has the advantage of reducing the output space by first obtaining a smaller list of possible candidate concepts via a candidate generator and then ranking them. DNorm (Leaman et al., 2013), based on a pair-wise learning-to-rank model where both mentions and concept names were represented as TF-IDF vectors, was the first to use learning-to-rank for concept normalization and achieved the best performance in the ShARe/CLEF eHealth 2013 shared task. List-wise learning-to-rank approaches are both computationally more efficient than pairwise learning-to-rank (Cao et al., 2007) and empirically outperform both point-wise and pair-wise approaches (Xia et al., 2008). There are two implementations of list-wise classifiers using neural networks for concept normalization: Murty et al. (2018) treat the selection of the best candidate concept as a flat classification problem, losing the ability to handle concepts not seen during training; Ji et al. (2019) take a generate-and-rank approach similar to ours, but they do not leverage resources such as synonyms or semantic type information from UMLS in their BERT-based ranker.", "selected": "There are two implementations of list-wise classifiers using neural networks for concept normalization: Murty et al. (2018) treat the selection of the best candidate concept as a flat classification problem, losing the ability to handle concepts not seen during training; Ji et al. (2019) take a generate-and-rank approach similar to ours, but they do not leverage resources such as synonyms or semantic type information from UMLS in their BERT-based ranker.", "paper_id": "220047806"}], "gCHivGxcKd": [{"section": "SMM4H-17", "paragraph": "MCN The MCN dataset consists of 13,609 concept mentions drawn from 100 discharge summaries from the fourth i2b2/VA shared task (Uzuner et al., 2011). The mentions are mapped to 3792 unique concepts out of 434,056 possible concepts with 125 semantic types in SNOMED-CT and RxNorm. We take 40 clinical notes from the released data as training, consisting of 5,334 mentions, and the standard evaluation data with 6,925 mentions as our test set. Around 2.7% of mentions in MCN could not be mapped to any 4 http://dx.doi.org/10.17632/rxwfb3tysd. 1 concepts in the terminology, and are assigned the CUI-less label.", "selected": "We take 40 clinical notes from the released data as training, consisting of 5,334 mentions, and the standard evaluation data with 6,925 mentions as our test set.", "paper_id": "220047806"}], "0XHKsPExpN": [{"section": "Introduction", "paragraph": "Mining and analyzing the constantly-growing unstructured text in the bio-medical domain offers great opportunities to advance scientific discovery (Gonzalez et al., 2015;Fleuren and Alkema, 2015) and improve the clinical care (Rumshisky et al., 2016;. However, lexical and grammatical variations are pervasive in such text, posing key challenges for data interoperability and the development of natural language processing (NLP) techniques. For instance, heart attack, MI, myocardial infarction, and cardiovascular stroke all refer to the same concept. It is critical to disambiguate these terms by linking them with their corresponding concepts in an ontology or knowledge base. Such linking allows downstream tasks (relation extraction, information retrieval, text classification, etc.) to access the ontology's rich knowledge about biomedical entities, their synonyms, semantic types and mutual relationships. Concept normalization is a task that maps concept mentions, the in-text natural-language mentions of ontological concepts, to concept entries in a standardized ontology or knowledge base. Techniques for concept normalization have been advancing, thanks in part to recent shared tasks including clinical disorder normalization in 2013 ShARe/CLEF (Suominen et al., 2013) and 2014 SemEval Task 7 Analysis of Clinical Text (Pradhan et al., 2014), and adverse drug event normalization in Social Media Mining for Health (SMM4H) (Sarker et al., 2018;Weissenbacher et al., 2019). Most existing systems use a string-matching or dictionary look-up approach (Leal et al., 2015;D'Souza and Ng, 2015;Lee et al., 2016), which are limited to matching morphologically similar terms, or supervised multi-class classifiers (Belousov et al., 2017;Tutubalina et al., 2018;Niu et al., 2019;Luo et al., 2019a), which may not generalize well when there are many concepts in the ontology and the concept types that must be predicted do not all appear in the training data.", "selected": "For instance, heart attack, MI, myocardial infarction, and cardiovascular stroke all refer to the same concept.", "paper_id": "220047806"}, {"section": "Introduction", "paragraph": "Mining and analyzing the constantly-growing unstructured text in the bio-medical domain offers great opportunities to advance scientific discovery (Gonzalez et al., 2015;Fleuren and Alkema, 2015) and improve the clinical care (Rumshisky et al., 2016;. However, lexical and grammatical variations are pervasive in such text, posing key challenges for data interoperability and the development of natural language processing (NLP) techniques. For instance, heart attack, MI, myocardial infarction, and cardiovascular stroke all refer to the same concept. It is critical to disambiguate these terms by linking them with their corresponding concepts in an ontology or knowledge base. Such linking allows downstream tasks (relation extraction, information retrieval, text classification, etc.) to access the ontology's rich knowledge about biomedical entities, their synonyms, semantic types and mutual relationships. Concept normalization is a task that maps concept mentions, the in-text natural-language mentions of ontological concepts, to concept entries in a standardized ontology or knowledge base. Techniques for concept normalization have been advancing, thanks in part to recent shared tasks including clinical disorder normalization in 2013 ShARe/CLEF (Suominen et al., 2013) and 2014 SemEval Task 7 Analysis of Clinical Text (Pradhan et al., 2014), and adverse drug event normalization in Social Media Mining for Health (SMM4H) (Sarker et al., 2018;Weissenbacher et al., 2019). Most existing systems use a string-matching or dictionary look-up approach (Leal et al., 2015;D'Souza and Ng, 2015;Lee et al., 2016), which are limited to matching morphologically similar terms, or supervised multi-class classifiers (Belousov et al., 2017;Tutubalina et al., 2018;Niu et al., 2019;Luo et al., 2019a), which may not generalize well when there are many concepts in the ontology and the concept types that must be predicted do not all appear in the training data.", "selected": "It is critical to disambiguate these terms by linking them with their corresponding concepts in an ontology or knowledge base.", "paper_id": "220047806"}, {"section": "Introduction", "paragraph": "Mining and analyzing the constantly-growing unstructured text in the bio-medical domain offers great opportunities to advance scientific discovery (Gonzalez et al., 2015;Fleuren and Alkema, 2015) and improve the clinical care (Rumshisky et al., 2016;. However, lexical and grammatical variations are pervasive in such text, posing key challenges for data interoperability and the development of natural language processing (NLP) techniques. For instance, heart attack, MI, myocardial infarction, and cardiovascular stroke all refer to the same concept. It is critical to disambiguate these terms by linking them with their corresponding concepts in an ontology or knowledge base. Such linking allows downstream tasks (relation extraction, information retrieval, text classification, etc.) to access the ontology's rich knowledge about biomedical entities, their synonyms, semantic types and mutual relationships. Concept normalization is a task that maps concept mentions, the in-text natural-language mentions of ontological concepts, to concept entries in a standardized ontology or knowledge base. Techniques for concept normalization have been advancing, thanks in part to recent shared tasks including clinical disorder normalization in 2013 ShARe/CLEF (Suominen et al., 2013) and 2014 SemEval Task 7 Analysis of Clinical Text (Pradhan et al., 2014), and adverse drug event normalization in Social Media Mining for Health (SMM4H) (Sarker et al., 2018;Weissenbacher et al., 2019). Most existing systems use a string-matching or dictionary look-up approach (Leal et al., 2015;D'Souza and Ng, 2015;Lee et al., 2016), which are limited to matching morphologically similar terms, or supervised multi-class classifiers (Belousov et al., 2017;Tutubalina et al., 2018;Niu et al., 2019;Luo et al., 2019a), which may not generalize well when there are many concepts in the ontology and the concept types that must be predicted do not all appear in the training data.", "selected": "Such linking allows downstream tasks (relation extraction, information retrieval, text classification, etc.) to access the ontology's rich knowledge about biomedical entities, their synonyms, semantic types and mutual relationships", "paper_id": "220047806"}]}}
{"idx": "248183", "paper_id": "218974285", "title": "Alignment Annotation for Clinic Visit Dialogue to Clinical Note Sentence Language Generation", "abstract": "For every patient\u2019s visit to a clinician, a clinical note is generated documenting their medical conversation, including complaints discussed, treatments, and medical plans. Despite advances in natural language processing, automating clinical note generation from a clinic visit conversation is a largely unexplored area of research. Due to the idiosyncrasies of the task, traditional methods of corpus creation are not effective enough approaches for this problem. In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies. Here we provide data statistics and qualitative analysis describing the unique annotation challenges. Given enough annotated data, such a resource would support multiple modeling methods including information extraction with template language generation, information retrieval type language generation, or sequence to sequence modeling.", "context_section_header": "", "context_paragraph": "In (Finley et al., 2018a), members of the EMR.AI team describe one intended approach to the problem by bridging information from clinic visit dialogue, first by classifying conversation sentences by intended note sections, then applying information extraction techniques. This data is then used to fill note templates generated by finite-state grammars. In another work (Finley et al., 2018b), they describe their method to automatically produce a parallel machine translation corpus for the special case of dictations to clinical note letter, but this focuses on just a narrow portion of the general problem. We posit that the task of clinical note generation based on dialogue is best represented as an amalgamation of different language transformations and thus the annotation efforts should not be tied to specific end to end methods. Our proposed method associates a note sentence to associated dialogue sentence sets using different tags (e.g. DICTA-TION, QA, STATEMENT, etc) and provides a higher level ordering of these sets. Compared to (Kazi and Kahanda, 2019), (Jeblee et al., 2019), and (Finley et al., 2018a), we actually annotate the final note content output. Compared to the work in (Finley et al., 2018b), we manually create our alignments and do it for the entire conversation and note. Therefore our dataset does not rely on a specific sequence of domain-dependent NLU tasks or a specific relationship between the extracted information and the final output (e.g. template-filling), nor assumes a narrow part of the problem 1 Anaphora is the phenomenon when an expression can only be understood within context of another expression (e.g. dictation), freeing users to choose their own intermediate methods. Our annotation objectives are inspired by and bear much similarity to the idea of machine translation corpus creation, the goal of which is to create sentence pairs which can be consumed by other algorithms (Koehn, 2005;Tiedemann, 2011). Several significant differences emerge from the end points being distinct mediums: one dialogue, one clinical note. For instance, dialogue data contains question and answer modes which must be mapped to prose. Additionally, the associations between the two mediums often occur out of sequence. Therefore, in contrast to machine translation corpus creation algorithms, our process cannot be easily automatized. Our annotation methodology bears most similarity to that of (Hwang et al., 2015) and (Tian et al., 2014) who create parallel corpora by manually labeling paired sentences for aligned documents as good, good partial, partial, bad, etc., between Wikipedia and Simple Wikipedia and between Chinese-English online web articles, respectively. In contrast to their work, we distinguish between different types of dialogue to clinical note transformations, e.g. dictation, question-answering etc, as well as attempt to organize related sentences on the dialogue side into groups.", "sentence": "Compared to the work in (Finley et al., 2018b), we manually create our alignments and do it for the entire conversation and note.", "cited_ids": [{"paper_id": "44178410", "citation": "(Finley et al., 2018b)"}], "y": "Compared to the work in (Finley et al., 2018b), the authors [propose a new annotation methodology that is content- and technique- agnostic while associate note sentences to sets of dialogue sentences, and] manually create their alignments [between dialogue and notes] and do so for the entire conversation and note.", "snippet_surface": "The authors compared their work to (Finley et al., 2018b), and manually created alignments for the entire conversation and note.", "questions": {"9GCq0VwE6Z": "What is the authors work?", "ednOe5Ykao": "What kind of alignments are referred here?"}, "answers": {"9GCq0VwE6Z": "The authors' work proposes a new annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences.", "ednOe5Ykao": "Alignment examples refer to matching text between dialogue and note, and matching between simple wikipedia and wikipedia pages."}, "evidence": {"9GCq0VwE6Z": [{"section": "Abstract", "paragraph": "For every patient\u2019s visit to a clinician, a clinical note is generated documenting their medical conversation, including complaints discussed, treatments, and medical plans. Despite advances in natural language processing, automating clinical note generation from a clinic visit conversation is a largely unexplored area of research. Due to the idiosyncrasies of the task, traditional methods of corpus creation are not effective enough approaches for this problem. In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies. Here we provide data statistics and qualitative analysis describing the unique annotation challenges. Given enough annotated data, such a resource would support multiple modeling methods including information extraction with template language generation, information retrieval type language generation, or sequence to sequence modeling.", "selected": "In this paper, we present an annotation methodology that is content- and technique- agnostic while associating note sentences to sets of dialogue sentences. The sets can further be grouped with higher order tags to mark sets with related information. This direct linkage from input to output decouples the annotation from specific language understanding or generation strategies.", "paper_id": "218974285"}], "ednOe5Ykao": [{"section": "Quantitative Analysis", "paragraph": "Analyzing the similarity of matched text between dialogue and note, we calculate the jaccard coefficient for unigrams 4 and UMLS concept identifiers, tagged with Metamap (Aronson and Lang, 2010), for the associated texts. The low similarity scores shown in (Table 13a) suggests that to get full matching context, simple similarity algorithms would be challenging. To quantify alignment difficulty, we calculate the percentages of sentences that cross n other sentences for tags exclusively not directed at the scribe, as well as for all sentences (Table 13b). For example, for n=3, we see that 76% of note sentences have evidence that crosses with at least three other note sentences. Since the preamble and after-visit clarifications may often provide salient information, if we did not count COMMAND, DIC-TATION, or STATEMENT2SCRIBE (the NON-SCRIBE) column, we would still find 60% of note sentences have information that crosses with at least three other note sentences' annotations. In all, the high percentages show that cross matches occurs frequently which would make automatic sentence alignment challenging.", "selected": "matched text between dialogue and note", "paper_id": "218974285"}, {"section": "Discussion", "paragraph": "Given the complexity and innate ambiguity of the task, we believe our agreements are good. Inconsistency between annotator's associations does not signify incorrectness (e.g. a sentence can have equally correct constituency parses).    (Hwang et al., 2015) marking alignments between Simple Wikipedia and Wikipedia pages, which we believe to be closest to our task, achieved an annotator agreement of 0.68 Kappa for 46 articles and 67,853 sentence pairs. Our analogous agreement metric of simple match f1 was on average 0.73, which is consistent with this baseline.", "selected": "marking alignments between Simple Wikipedia and Wikipedia pages", "paper_id": "218974285"}]}}
{"idx": "249184", "paper_id": "118589015", "title": "Pun Generation with Surprise", "abstract": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \u201cdied\u201d and \u201cdyed\u201d). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.", "context_section_header": "", "context_paragraph": "We test our approach on 150 pun-alternative word pairs. 1 First, we show a strong correlation between our surprisal metric and funniness ratings from crowdworkers. Second, human evaluation shows that our system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "sentence": "Second, human evaluation shows that our system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "cited_ids": [{"paper_id": "51874992", "citation": "(Yu et al., 2018)"}], "y": "Human evaluation shows that the [an unsupervised approach to pun generation based on raw text and a susprisal principle] generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores [mean z-scores of ratings of each worker].", "snippet_surface": "Second, human evaluation shows that the authors' system generates puns successfully 31% of the time, compared to 9% of a neural generation baseline (Yu et al., 2018), and results in higher funniness scores.", "questions": {"wzPEI+pU+e": "How is the funniness score calculated?", "d+L547/CZx": "What is the authors' system?"}, "answers": {"wzPEI+pU+e": "The funniness score is the mean of the z-scores of ratings of each worker.", "d+L547/CZx": "The authors propose an unsupervised approach to pun generation based on raw text and a susprisal principle."}, "evidence": {"wzPEI+pU+e": [{"section": "Analysis of the Surprisal Principle", "paragraph": "We collected all of our human ratings on Amazon Mechanical Turk (AMT). Workers are asked to answer the question \"How funny is this sentence?\" on a scale from 1 (not at all) to 7 (extremely). We obtained funniness ratings on 130 sentences from the development set with 33 puns, 33 swap-puns, and 64 non-puns. 48 workers each read roughly 10-20 sentences in random order, counterbalanced for sentence types of non-puns, swap-puns, and puns. Each sentence is rated by 5 workers, and we removed 10 workers whose maximum Spearman correlation with other people rating the same sentence is lower than 0.2. The average Spearman correlation among all the remaining workers (which captures inter-annotator agreement) is 0.3. We z-scored the ratings of each worker for calibration and took the average zscored ratings of a sentence as its funniness score. Table 1 shows the statistics of our annotated dataset (SEMEVAL) and Kao et al. (2015)'s dataset (KAO). Note that the two datasets have different numbers and types of sentences, and the human ratings were collected separately. As expected, puns are funnier than both swap-puns and nonpuns. Swap-puns are funnier than non-puns, possibly because they have inherit ambiguity brought by the RETRIEVE+SWAP operation.", "selected": "We collected all of our human ratings on Amazon Mechanical Turk (AMT). Workers are asked to answer the question \"How funny is this sentence?\" on a scale from 1 (not at all) to 7 (extremely). We obtained funniness ratings on 130 sentences from the development set with 33 puns, 33 swap-puns, and 64 non-puns. 48 workers each read roughly 10-20 sentences in random order, counterbalanced for sentence types of non-puns, swap-puns, and puns. Each sentence is rated by 5 workers, and we removed 10 workers whose maximum Spearman correlation with other people rating the same sentence is lower than 0.2. The average Spearman correlation among all the remaining workers (which captures inter-annotator agreement) is 0.3. We z-scored the ratings of each worker for calibration and took the average zscored ratings of a sentence as its funniness score.", "paper_id": "118589015"}], "d+L547/CZx": [{"section": "Abstract", "paragraph": "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., \u201cdied\u201d and \u201cdyed\u201d). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30% of the time, doubling the success rate of a neural generation baseline.", "selected": "n this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., \u201cdyed\u201d) and the distant context, but a strong association between the alternative word (e.g., \u201cdied\u201d) and the immediate context.", "paper_id": "118589015"}, {"section": "Introduction", "paragraph": "We instantiate our local-global surprisal principle in two ways. First, we develop a quantitative metric for surprise based on the conditional probabilities of the pun word and the alternative word given local and global contexts under a neural language model. However, we find that this metric is not sufficient for generation. We then develop an unsupervised approach to generate puns based on a retrieve-and-edit framework  given an unhumorous corpus ( Figure 2). We call our system SURGEN (SURprisal-based pun GENeration).", "selected": "We call our system SURGEN (SURprisal-based pun GENeration).", "paper_id": "118589015"}]}}
{"idx": "250701", "paper_id": "402181", "title": "Using Semantic Roles to Improve Question Answering", "abstract": "Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneficial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models.", "context_section_header": "", "context_paragraph": "In line with previous work, our method exploits syntactic information in the form of dependency relation paths together with FrameNet-like semantic roles to smooth lexical and syntactic divergences between question and answer sentences. Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database. In contrast to Kaisser (2006), we model the semantic role assignment and answer extraction tasks numerically, thereby alleviating the coverage problems encountered previously.", "sentence": "Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database.", "cited_ids": [{"paper_id": "6541034", "citation": "Narayanan and Harabagiu (2004)"}], "y": "The authors' approach [that is comprised of 3 stages, determining the expected answer type of the question, retrieving passages likely to contain answers to the question and performing a match between the question words and retrieved passaged to extract the answer], is less domain dependent and resource intensive than Narayanan and Harabagiu (2004). It solely employs a dependency parser and the FrameNet database, [which includes surface syntactic realizations of semantic roles and provides annotated example sentences from British National Corpus].", "snippet_surface": "The authors' approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "ZCV4kMh9hR": "What does the framenet database comprise of?"}, "answers": {"6gKwRw0I/Q": "The authors' approach has 3 stages: determining the expected answer type of the question, retrieving passages likely to contain answers to the question and performing a match between the question words and retrieved passaged to extract the answer.", "ZCV4kMh9hR": "The FrameNet database includes surface syntatic realizations of semantic roles and provides annotated example sentences from British National Corpus."}, "evidence": {"6gKwRw0I/Q": [{"section": "Introduction", "paragraph": "In this paper we propose an answer extraction model which effectively incorporates FrameNetstyle semantic role information. We present an automatic method for semantic role assignment which is conceptually simple and does not require extensive feature engineering. A key feature of our approach is the comparison of dependency relation paths attested in the FrameNet annotations and raw text. We formalize the search for an optimal role assignment as an optimization problem in a bipartite graph. This formalization allows us to find an exact, globally optimal solution. The graph-theoretic framework goes some way towards addressing coverage problems related with FrameNet and allows us to formulate answer extraction as a graph matching problem. As a byproduct of our main investigation we also examine the issue of FrameNet coverage and show how much it impacts performance in a TREC-style question answering setting.", "selected": "A key feature of our approach is the comparison of dependency relation paths attested in the FrameNet annotations and raw text. We formalize the search for an optimal role assignment as an optimization problem in a bipartite graph. This formalization allows us to find an exact, globally optimal solution. The graph-theoretic framework goes some way towards addressing coverage problems related with FrameNet and allows us to formulate answer extraction as a graph matching problem. As a byproduct of our main investigation we also examine the issue of FrameNet coverage and show how much it impacts performance in a TREC-style question answering setting.", "paper_id": "402181"}, {"section": "Related Work", "paragraph": "In line with previous work, our method exploits syntactic information in the form of dependency relation paths together with FrameNet-like semantic roles to smooth lexical and syntactic divergences between question and answer sentences. Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database. In contrast to Kaisser (2006), we model the semantic role assignment and answer extraction tasks numerically, thereby alleviating the coverage problems encountered previously.", "selected": "our method exploits syntactic information in the form of dependency relation paths together with FrameNet-like semantic roles to smooth lexical and syntactic divergences between question and answer sentences. Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database.", "paper_id": "402181"}, {"section": "Problem Formulation", "paragraph": "We briefly summarize the architecture of the QA system we are working with before formalizing the mechanics of our FrameNet-based answer extraction module. In common with previous work, our overall approach consists of three stages: (a) determining the expected answer type of the question, (b) retrieving passages likely to contain answers to the question, and (c) performing a match between the question words and retrieved passages in order to extract the answer. In this paper we focus on the last stage: question and answer sentences are normalized to a FrameNet-style representation and answers are retrieved by selecting the candidate whose semantic structure is most similar to the question.", "selected": "our overall approach consists of three stages: (a) determining the expected answer type of the question, (b) retrieving passages likely to contain answers to the question, and (c) performing a match between the question words and retrieved passages in order to extract the answer.", "paper_id": "402181"}], "ZCV4kMh9hR": [{"section": "Introduction", "paragraph": "More concretely, in the FrameNet paradigm, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations. Semantic roles (or frame 1 The approaches are too numerous to list; we refer the interested reader to Carreras and M\u00e0rquez (2005) for an overview. elements) are defined for each frame and correspond to salient entities present in the evoked situation. Predicates with similar semantics instantiate the same frame and are attested with the same roles. The FrameNet database lists the surface syntactic realizations of semantic roles, and provides annotated example sentences from the British National Corpus. For example, the frame Commerce Sell has three core semantic roles, namely Buyer, Goods, and Seller -each expressed by an indirect object, a direct object, and a subject (see sentences (1a)-(1c)). It can also be attested with non-core (peripheral) roles (e.g., Means, Manner, see (1d) and (1e)) that are more generic and can be instantiated in several frames, besides Commerce Sell. The verbs sell, vend, and retail can evoke this frame, but also the nouns sale and vendor.", "selected": "The FrameNet database lists the surface syntactic realizations of semantic roles, and provides annotated example sentences from the British National Corpus.", "paper_id": "402181"}]}}
{"idx": "254182", "paper_id": "5859332", "title": "Character-Level Chinese Dependency Parsing", "abstract": "Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods.", "context_section_header": "", "context_paragraph": "Zhao (2009) was the first to study character-level dependencies; they argue that since no consistent word boundaries exist over Chinese word segmentation, dependency-based representations of word structures serve as a good alternative for Chinese word segmentation. Thus their main concern is to parse intra-word dependencies. In this work, we extend their formulation, making use of largescale annotations of , so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012 and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner-and inter-word structures, studying their influences on each other.  was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing.", "sentence": "Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "cited_ids": [{"paper_id": "10011032", "citation": "Hatori et al. (2012)"}], "y": "The authors' proposed arc-standard model [a combination of character-level arceager system with the arc-standard model] is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "snippet_surface": "The authors' proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)'s work.", "questions": {"F7qyCstiH7": "What is the arc-standard model?"}, "answers": {"F7qyCstiH7": "The arc-standard model is the author's combination of character-level arceager system with the arc-standard model as used by Hatori et al."}, "evidence": {"F7qyCstiH7": [{"section": "The Arc-Standard Model", "paragraph": "The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system.", "selected": "For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system.", "paper_id": "5859332"}]}}
{"idx": "255776", "paper_id": "16956667", "title": "Joint Inference for Knowledge Base Population", "abstract": "Populating Knowledge Base (KB) with new knowledge facts from reliable text resources usually consists of linking name mentions to KB entities and identifying relationship between entity pairs. However, the task often suffers from errors propagating from upstream entity linkers to downstream relation extractors. In this paper, we propose a novel joint inference framework to allow interactions between the two subtasks and find an optimal assignment by addressing the coherence among preliminary local predictions: whether the types of entities meet the expectations of relations explicitly or implicitly, and whether the local predictions are globally compatible. We further measure the confidence of the extracted triples by looking at the details of the complete extraction process. Experiments show that the proposed framework can significantly reduce the error propagations thus obtain more reliable facts, and outperforms competitive baselines with state-of-the-art relation extraction models.", "context_section_header": "", "context_paragraph": "Joint inference over multiple local models has been applied to many NLP tasks. Our task is different from the traditional joint IE works based in the ACE framework Li and Ji, 2014;Kate and Mooney, 2010), which jointly extract and/or classify named entity mentions to several predefined types in a sentence and identify in a sentence level which relation this specific sentence describes (between a pair of entity mentions in this sentence). Li and Ji (2014) follow the ACE task definitions and present a neat incremental joint framework to simultaneously extract entity mentions and relations by structure perceptron. In contrast, we link entity mentions from a text corpus to their corresponding entities in an ex-isting KB and identify the relations between pairs of entities based on that text corpus. Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "sentence": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while we focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "cited_ids": [{"paper_id": "250761", "citation": "Choi et al. (2006)"}], "y": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while the authors focus on jointly modeling [Entity Linking (EL)] and [Relation Extraction (RE)] in open domain, which is a different and challenging task.", "snippet_surface": "Choi et al. (2006) jointly extracts the expressions and sources of opinion as well as the linking relations (i.e., a source entity expresses an opinion expression) between them, while the authors focus on jointly modeling EL and RE in open domain, which is a different and challenging task.", "questions": {"8B72nKB9ro": "What does el stand for?", "DG3MI5IuPg": "What does re stand for?"}, "answers": {"8B72nKB9ro": "EL - Entity Linking\n", "DG3MI5IuPg": "RE stands for Relation Extraction"}, "evidence": {"8B72nKB9ro": [{"section": "Introduction", "paragraph": "In order to address the above issues, we could consult populating existing KBs from reliable text resources, e.g., newswire, which usually involves enriching KBs with new entities and populating KBs with new knowledge facts, in the form of <Entity, Relation, Entity> triple. In this paper, we will focus on the latter, identifying relationship between two existing KB entities. This task can be intuitively considered in a pipeline paradigm, that is, name mentions in the texts are first linked to entities in the KB (entity linking, EL), and then the relationship between them are identified (relation extraction, RE). It is worth mentioning that the first task EL is different from the task of named entity recognition (NER) in traditional information extraction (IE) tasks, where NER recognizes and classifies the entity mentions (to several predefined types) in the texts, but EL focuses on linking the mentions to their corresponding entities in the KB. Such pipeline systems often suffer from errors propagating from upstream to downstream, since only the local best results are selected to the next step. One idea to solve the problem is to allow interactions among the local predictions of both subtasks and jointly select an optimal assignment to eliminate possible errors in the pipeline.", "selected": "This task can be intuitively considered in a pipeline paradigm, that is, name mentions in the texts are first linked to entities in the KB (entity linking, EL),", "paper_id": "16956667"}], "DG3MI5IuPg": [{"section": "Introduction", "paragraph": "In order to address the above issues, we could consult populating existing KBs from reliable text resources, e.g., newswire, which usually involves enriching KBs with new entities and populating KBs with new knowledge facts, in the form of <Entity, Relation, Entity> triple. In this paper, we will focus on the latter, identifying relationship between two existing KB entities. This task can be intuitively considered in a pipeline paradigm, that is, name mentions in the texts are first linked to entities in the KB (entity linking, EL), and then the relationship between them are identified (relation extraction, RE). It is worth mentioning that the first task EL is different from the task of named entity recognition (NER) in traditional information extraction (IE) tasks, where NER recognizes and classifies the entity mentions (to several predefined types) in the texts, but EL focuses on linking the mentions to their corresponding entities in the KB. Such pipeline systems often suffer from errors propagating from upstream to downstream, since only the local best results are selected to the next step. One idea to solve the problem is to allow interactions among the local predictions of both subtasks and jointly select an optimal assignment to eliminate possible errors in the pipeline.", "selected": ", and then the relationship between them are identified (relation extraction, RE).", "paper_id": "16956667"}]}}
{"idx": "256673", "paper_id": "8790940", "title": "Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks", "abstract": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "context_section_header": "", "context_paragraph": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "sentence": "Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "cited_ids": [{"paper_id": "11616343", "citation": "Chen and Manning (2014)"}], "y": "Compared to the parser of Chen and Manning (2014), the authors receive 0.6% (UAS [Unlabeled attachment score]) and 0.9% (LAS[ Labelled attachment score ]) improvement on PTB3 [Section 23 of the English Penn Treebank 3] test set, while they receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 [Chinese Penn Treebank dataset ]test set.", "snippet_surface": "Compared to the parser of Chen and Manning (2014), the authors receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while they receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "questions": {"XnjYPjYHnD": "What does uas stand for?", "EJBqUqFcip": "What does las stand for?", "1ikGk+Aslg": "What is ptb3 test set comprised of?", "XBZ2zxc2qj": "What is ctb5 test set comprised of?"}, "answers": {"XnjYPjYHnD": "Unlabeled attachment score", "EJBqUqFcip": "Labelled attachment score", "1ikGk+Aslg": "Section 23 of the English Penn Treebank 3", "XBZ2zxc2qj": "It is the Chinese Penn Treebank dataset, using the same split as Zhang and Clark, 2008."}, "evidence": {"XnjYPjYHnD": [{"section": "Results", "paragraph": "The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics.", "selected": "The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS)", "paper_id": "8790940"}], "EJBqUqFcip": [{"section": "Results", "paragraph": "The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics.", "selected": "The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics.", "paper_id": "8790940"}], "1ikGk+Aslg": [{"section": "Introduction", "paragraph": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "selected": "English Penn Treebank 3 (PTB3)", "paper_id": "8790940"}, {"section": "Datasets", "paragraph": "To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.", "selected": "English Penn Treebank 3 (PTB3)", "paper_id": "8790940"}, {"section": "Datasets", "paragraph": "\u2022 English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. \u2022 Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008).", "selected": "English Penn Treebank 3 (PTB3)", "paper_id": "8790940"}, {"section": "Datasets", "paragraph": "\u2022 English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. \u2022 Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008).", "selected": "section 22 and section 23 as development set and test set respectively.", "paper_id": "8790940"}], "XBZ2zxc2qj": [{"section": "Introduction", "paragraph": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set.", "selected": "To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets.", "paper_id": "8790940"}, {"section": "Datasets", "paragraph": "\u2022 English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. \u2022 Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008).", "selected": "\u2022 English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. \u2022 Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008).", "paper_id": "8790940"}]}}
{"idx": "256730", "paper_id": "14203516", "title": "A Statistical Tree Annotator and Its Applications", "abstract": "In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.", "context_section_header": "", "context_paragraph": "As for predicting projectable constituent, it is related to the work described in (Xiong et al., 2010), where they were predicting translation boundaries. A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while translation boundaries in our work are defined from source parse trees. Our work uses more resources, but the prediction accuracy is higher (modulated on a different test data): we get a F-measure 84.7%, in contrast with (Xiong et al., 2010)'s 71%.", "sentence": "A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while translation boundaries in our work are defined from source parse trees.", "cited_ids": [{"paper_id": "16085803", "citation": "(Xiong et al., 2010)"}], "y": "[The authors work was in applications to predict function tags, null elements and whether a tree constituent is projectable in machine translation.] They define projectable spans on a left-branching derivation tree solely for their phrase decoder and models. In contrast, (Xiong et al., 2010) defines translation boundaries in their work from source parse trees.", "snippet_surface": "A major difference is that (Xiong et al., 2010) defines projectable spans on a left-branching derivation tree solely for their phrase decoder and models, while the authors define translation boundaries in their work from source parse trees.", "questions": {"asfAKfrWi7": "What do you mean by \"their work\"?"}, "answers": {"asfAKfrWi7": "The authors applications to predict function tags, null elements and whether a tree constituent is projectable in machine translation."}, "evidence": {"asfAKfrWi7": [{"section": "Abstract", "paragraph": "In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.", "selected": "The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.", "paper_id": "14203516"}]}}
{"idx": "256897", "paper_id": "10409321", "title": "Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective", "abstract": "For the task of relation extraction, distant supervision is an efficient approach to generate labeled data by aligning knowledge base with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top precision improvements over the traditional state-of-the-art approaches.", "context_section_header": "", "context_paragraph": "Our work is closest to (Fan et al., 2014), since we focus on the same noisy corpus problem. Although from different perspectives, we study it along with the same line of using matrix factorization (Petroni et al., 2015) for relation extraction. In this line, (Riedel et al., 2013) initially considered the task as a matrix factorization problem. Their method consists of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, the data noise brought by the assumption of distant supervision (Mintz et al., 2009), is not considered in the work. Another line addressing the problem uses deep neural networks (Zeng et al., 2015;. The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously. In addition, (Han and Sun, 2016) explored Markov logic technique to enrich supervision knowledge, which can incorporate indirect supervision globally. Our method could be further augmented by that idea, using additional logical constraint to reduce the uncertainty for the clustered noise modeling.", "sentence": "Another line addressing the problem uses deep neural networks (Zeng et al., 2015;. The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "cited_ids": [{"paper_id": "2778800", "citation": "(Zeng et al., 2015;"}], "y": "One line of research addressing the problem [the noisy corpus problem] uses deep neural networks. The difference is that it is a supervised learning approach, while the authors' focused one [the authors approach models noisy data corpus using adaptive variance modeling approach based on Dirichlet Process instead of a fixed way of controlling complex noise weighting] is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "snippet_surface": "Another line addressing the problem uses deep neural networks (Zeng et al., 2015). The difference is that it is a supervised learning approach, while the authors' focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously.", "questions": {"8pNeOoDoS2": "What problem is being addressed?", "sN8fb5XMbP": "What is the meaning of \"the authors' focused one\"?"}, "answers": {"8pNeOoDoS2": "The noisy corpus problem that was also addressed in Fan et al. (2014)", "sN8fb5XMbP": "The authors approach which models noisy data corpus using adaptive variance modeling approach based on Dirichlet Process (Blei and Jordan, 2004) instead of a fixed way of controlling complex noise weighting."}, "evidence": {"8pNeOoDoS2": [{"section": "Related Work", "paragraph": "Our work is closest to (Fan et al., 2014), since we focus on the same noisy corpus problem. Although from different perspectives, we study it along with the same line of using matrix factorization (Petroni et al., 2015) for relation extraction. In this line, (Riedel et al., 2013) initially considered the task as a matrix factorization problem. Their method consists of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, the data noise brought by the assumption of distant supervision (Mintz et al., 2009), is not considered in the work. Another line addressing the problem uses deep neural networks (Zeng et al., 2015;. The difference is that it is a supervised learning approach, while our focused one is a joint learning approach with transductive style, in which both training and test data are exploited simultaneously. In addition, (Han and Sun, 2016) explored Markov logic technique to enrich supervision knowledge, which can incorporate indirect supervision globally. Our method could be further augmented by that idea, using additional logical constraint to reduce the uncertainty for the clustered noise modeling.", "selected": "same noisy corpus problem", "paper_id": "10409321"}], "sN8fb5XMbP": [{"section": "Introduction", "paragraph": "To tackle the problem, we develop a novel distant supervision approach from a nonparametric Bayesian perspective (Blei et al., 2016), along with the previously most effective research line (Petroni et al., 2015) of using matrix completion (Fan et al., 2014) for relation extraction. Our goal is to design a noise-tolerant relation extraction model for distantly supervised corpus with noise and sparsity problems. Different from (Fan et al., 2014) as one state-of-the-art method in this line, we model noisy data corpus using adaptive variance modeling approach , based on Dirichlet Process (Blei and Jordan, 2004) instead of a fixed way of controlling complex noise weighting. To the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision.", "selected": "model noisy data corpus using adaptive variance modeling approach , based on Dirichlet Process (Blei and Jordan, 2004) instead of a fixed way of controlling complex noise weighting. To the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision.", "paper_id": "10409321"}]}}
{"idx": "260139", "paper_id": "220045476", "title": "Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations", "abstract": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.", "context_section_header": "", "context_paragraph": "Our end-2-end transcription-free approach is similar and perhaps even motivated some of the previous works. There have been some works (Serdyuk et al., 2018;Lugosch et al., 2019) which perform prediction tasks directly from speech signals but lack in capturing the underlying linguis-tic structure of a language (sentences break into words for semantics). We believe capturing some of the important linguistic units (e.g. words) are important for spoken language understanding. (Qian et al., 2017) is most similar to our work in terms of overall architecture as they also first get word level representations and then use the encoder for utterance level prediction. However (Qian et al., 2017) uses transcribed word transcriptions but we only use word boundaries for ASR-free end-2-end spoken language understanding. As shown in Figure  1, most previous works follow the upper pipeline. They start with a transcript (manually generated or through an ASR), which is first segmented into utterances. They then use word-embeddings for each word in the transcript before feeding it into a classifier to predict target behavior codes.", "sentence": "However (Qian et al., 2017) uses transcribed word transcriptions but we only use word boundaries for ASR-free end-2-end spoken language understanding.", "cited_ids": [{"paper_id": "1451605", "citation": "(Qian et al., 2017)"}], "y": "Qian et al. (2017) uses transcribed word transcriptions but the authors only use word boundaries for ASR-free end-2-end spoken language understanding [that predicts labels from only audio features] .", "snippet_surface": "However, Qian et al. (2017) uses transcribed word transcriptions but the authors only use word boundaries for ASR-free end-2-end spoken language understanding.", "questions": {"qR/U4Z09MB": "What is \"asr-free\"?"}, "answers": {"qR/U4Z09MB": "The ASR-free framework is one that predicts labels from audio features rather than predicting labels based on a transcript obtained from performing automatic speech recognition (ASR)."}, "evidence": {"qR/U4Z09MB": [{"section": "Abstract", "paragraph": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.", "selected": "Automatic speech recognition (ASR)", "paper_id": "220045476"}, {"section": "Introduction", "paragraph": "Speech interfaces have seen a widely growing trend and this has brought about increasing interest in advancing computational approaches to spoken language understanding (SLU). (Tur and De Mori, 2011;Xu and Sarikaya, 2014;Yao et al., 2013;Ravuri and Stolcke, 2015). SLU systems often rely on Automatic speech recognition (ASR) for generating lexical features. The ASR output is then used for the target natural language understanding task. Furthermore, end-2-end SLU systems for various applications, including speech synthesis (Oord et al., 2016), ASR tasks (Amodei et al., 2016;Chan et al., 2016;Soltau et al., 2016) and speech-2-text translation (Chung et al., 2019) have shown promising results. Recently (Haque et al., 2019) propose a method for learning audio-linguistuc embedding but that too depends on using transcribed text. Lower part shows our proposed approach where we predict behavior codes without using transcripts Due to the nature of the speech processing pipeline, natural language understanding tasks suffer from two major problems, 1) error propagation through ASR leading to noisy lexical features 2) loss of rich information which supplement lexical features, such as prosodic and acoustic expressive speech patterns.", "selected": "Automatic speech recognition (ASR)", "paper_id": "220045476"}, {"section": "Abstract", "paragraph": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.", "selected": "Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding", "paper_id": "220045476"}]}}
{"idx": "261809", "paper_id": "52126537", "title": "Rule induction for global explanation of trained models", "abstract": "Understanding the behavior of a trained network and finding explanations for its outputs is important for improving the network\u2019s performance and generalization ability, and for ensuring trust in automated systems. Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network. However, the relations between different features and classes are lost in most cases. We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network. We first calculate the importance of the features in the trained network. We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and finally fit a rule induction model to explain the model predictions. We find that the output rule-sets can explain the predictions of a neural network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80. We make the code available at https://github.com/clips/interpret_with_rules.", "context_section_header": "", "context_paragraph": "In our approach, we aim to generate a set of if-then-else rules that approximate the interaction between the most important features and classes for a trained model. As opposed to Lakkaraju et al. (2017), before learning an explanation model, we modify the input data based on the importance of the features in the trained network. In doing so, we already encode some information about the network's performance within these input features.", "sentence": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, we modify the input data based on the importance of the features in the trained network.", "cited_ids": [{"paper_id": "19598815", "citation": "Lakkaraju et al. (2017)"}], "y": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, [which uses transparent approximations to explain the behaviour of black box classifiers by optimizing for fidelity to the original model and interpretability of the explanation], the authors modify the input data based on the importance of the features in the trained network.", "snippet_surface": "As opposed to Lakkaraju et al. (2017), before learning an explanation model, the authors modify the input data based on the importance of the features in the trained network.", "questions": {"VVSVJdcfC/": "What does explanation model mean in this context?"}, "answers": {"VVSVJdcfC/": "An explanation model is based on transparent approximations to explain the behaviour of any black box classifier by optimizing for fidelity to the original model and interpretability of the explanation."}, "evidence": {"VVSVJdcfC/": [{"section": "Abstract", "paragraph": "We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.", "selected": "We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation.", "paper_id": "19598815"}]}}
{"idx": "269699", "paper_id": "4902154", "title": "Towards a Probabilistic Model for Lexical Entailment", "abstract": "While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. The impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems.", "context_section_header": "", "context_paragraph": "In contrary to these systems, our model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003). As Majumdar and Bhattacharyya (2010), our model considers the impact of hypothesis length, however it does not require the tuning of a unique threshold for each length. Finally, most of the above systems do not differentiate between the various lexical resources they use, even though it is known that resources reliability vary considerably (Mirkin et al., 2009b). Our probabilistic model, on the other hand, learns a unique reliability parameter for each resource it utilizes. As mentioned above, this work extends the base model in (Shnarch et al., 2011), which is described in the next section.", "sentence": "In contrary to these systems, our model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003).", "cited_ids": [{"paper_id": "2903805", "citation": "(Habash and Dorr, 2003)"}], "y": "The authors' model showed improvement when utilizing high quality resources, such as WordNet and the CatVar (Categorial Variation) databases, [that comprise English language lexemes and categorical variants] (Habash and Dorr, 2003).", "snippet_surface": "In contrast to these systems, the authors' model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003).", "questions": {"+tQ+I1E0h3": "What is catvar database comprised of?"}, "answers": {"+tQ+I1E0h3": "CatVar is a database comprised of English language lexemes and their categorical variants."}, "evidence": {"+tQ+I1E0h3": [{"section": "EM for the Extended Model", "paragraph": "were 1 cov is the number of H terms which are covered. We can find the \u03b8 R that maximizes this equation in one of the methods described above. (Habash and Dorr, 2003). From WordNet we took as entailment rules synonyms, derivations, hyponyms and meronyms of the first senses of T and H terms. CatVar is a database of clusters of uninflected words (lexemes) and their categorial (i.e. part-of-speech) variants (e.g. announce (verb), announcer and announcement(noun) and announced (adjective)). We deduce an entailment relation between any two lexemes in the same cluster. Model's parameters were estimated from the development set, taken as training. Based on these parameters, the entailment probability was estimated for each pair (T, H) in the test set, and the classification threshold was tuned by classification over the development set.", "selected": "CatVar is a database of clusters of uninflected words (lexemes) and their categorial (i.e. part-of-speech) variants (e.g. announce (verb), announcer and announcement(noun) and announced (adjective)).", "paper_id": "4902154"}, {"section": "Abstract", "paragraph": "We describe our approach to the construction and evaluation of a large-scale database called \"CatVar\" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities.We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard. This evaluation reveals that the categorial database achieves a high degree of precision and recall. Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%.", "selected": "\"CatVar\" which contains categorial variations of English lexemes.", "paper_id": "2903805"}]}}
{"idx": "27645", "paper_id": "3781940", "title": "ICRC-HIT: A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge", "abstract": "In this paper, we present a comment labeling system based on a deep learning strategy. We treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments. In the recurrent architecture of our system, our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question-comment pair, and assigns the labels to the comment sequence with a recurrent neural network over CNN. Compared with the conditional random fields based method, our approach performs better performance on Macro-F1 (53.82%), and achieves the highest accuracy (73.18%), F1-value (79.76%) on predicting the Good class in this answer selection challenge.", "context_section_header": "", "context_paragraph": "In this work, we present a novel comment labeling system based on deep learning. We propose the recurrent convolutional neural networks (R&CNN) approach to assign the labels to comments given a question. Based on the distributed representations learned form 2-dimensional CNN (2D-CNN) matching, our approach achieves to comment sequence learning and predict the classes of comments. Using the word embedding trained by provided Qatar Living data, R&CNN not only models the semantic relevance for question and comment, but also captures the correlative context in comment sequence for predicting the class of comment. The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "sentence": "The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "cited_ids": [{"paper_id": "13243922", "citation": "(Ding et al., 2008)"}], "y": "The experiment results show that the authors' system [recurrent convolutional neural networks (RCNN) to assign the labels to comments given a question] performs better than the CRF [Conditional Random Fields] based method [which is used to detect the contexts and answers of questions from forum threads] (Ding et al., 2008) on recognizing good comments, and performs more adaptively [on the Qatar Living data].", "snippet_surface": "The experiment results show that the authors' system performs better than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptively on the development and test dataset.", "questions": {"Aso+GTea6Q": "What is the crf based system?", "d+L547/CZx": "What is the authors' system?", "cPu6gU/lNk": "What dataset is being referred to?"}, "answers": {"Aso+GTea6Q": "The CRF based system is Conditional Random Fields, which is used to detect the contexts and answers of questions from forum threads.", "d+L547/CZx": "The author's system is based on deep learning and uses recurrent convolutional neural networks (RCNN) to assigned the labels to comments given a question.", "cPu6gU/lNk": "The dataset here refers to the Qatar Living data."}, "evidence": {"Aso+GTea6Q": [{"section": "Abstract", "paragraph": "Online forum discussions often contain vast amounts of questions that are the focuses of discussions. Extracting contexts and answers together with the questions will yield not only a coherent forum summary but also a valuable QA knowledge base. In this paper, we propose a general framework based on Conditional Random Fields (CRFs) to detect the contexts and answers of questions from forum threads. We improve the basic framework by Skip-chain CRFs and 2D CRFs to better accommodate the features of forums for better performance. Experimental results show that our techniques are very promising.", "selected": "In this paper, we propose a general framework based on Conditional Random Fields (CRFs) to detect the contexts and answers of questions from forum threads.", "paper_id": "13243922"}], "d+L547/CZx": [{"section": "Introduction", "paragraph": "In this work, we present a novel comment labeling system based on deep learning. We propose the recurrent convolutional neural networks (R&CNN) approach to assign the labels to comments given a question. Based on the distributed representations learned form 2-dimensional CNN (2D-CNN) matching, our approach achieves to comment sequence learning and predict the classes of comments. Using the word embedding trained by provided Qatar Living data, R&CNN not only models the semantic relevance for question and comment, but also captures the correlative context in comment sequence for predicting the class of comment. The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "selected": "In this work, we present a novel comment labeling system based on deep learning. We propose the recurrent convolutional neural networks (R&CNN) approach to assign the labels to comments given a question.", "paper_id": "3781940"}], "cPu6gU/lNk": [{"section": "Introduction", "paragraph": "In this work, we present a novel comment labeling system based on deep learning. We propose the recurrent convolutional neural networks (R&CNN) approach to assign the labels to comments given a question. Based on the distributed representations learned form 2-dimensional CNN (2D-CNN) matching, our approach achieves to comment sequence learning and predict the classes of comments. Using the word embedding trained by provided Qatar Living data, R&CNN not only models the semantic relevance for question and comment, but also captures the correlative context in comment sequence for predicting the class of comment. The experimental results show that our system performs better performances than the CRF based method (Ding et al., 2008) on recognizing good comments, and performs more adaptive on the development and test dataset.", "selected": "Using the word embedding trained by provided Qatar Living data", "paper_id": "3781940"}, {"section": "Experimental setup", "paragraph": "We evaluate our approach (R&CNN) on both the development and test data of this answer selection challenge. The statistics of experimental dataset are summarized in Table 1 In our approach, we use 100-dimensional word embedding trained on the provided Qatar Living data with Word2vec (Mikolov et al., 2013). The maximum size of coding the sentences with word embedding is set to be 100, and we use 3-words sliding window for 1D-convolution. The learning rate is initialized to be 0.01 and adapted dynamically using ADADELTA Method (Matthew, 2012). Based on the results on development set, all the hyperparameters of our approach are optimized on train set. Table 2 lists the experimental methods and the corresponding official results. The baselines of comment sequence labeling include the method based on CRF and the approach CRF+V, which integrates distributed representation learnt from our approach (R&CNN). In addition, we illustrate the best result achieved by the supervised feature-rich approach SFR 1 .", "selected": "In our approach, we use 100-dimensional word embedding trained on the provided Qatar Living data with Word2vec", "paper_id": "3781940"}]}}
{"idx": "279713", "paper_id": "236486137", "title": "Unsupervised Paradigm Clustering Using Transformation Rules", "abstract": "This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering. Our system generates paradigms using morphological transformation rules which are discovered from raw data. We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios.", "context_section_header": "", "context_paragraph": "We experiment with two methods for discovering rules, described in Section 3.3. Our first approach is inspired by work on morphology discovery by Soricut and Och (2015), who generate prefix and suffix transformations between similar strings. This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule demonstrate that prefix and suffix rules deliver stronger performance for most languages in the shared task dataset but our more general transformations rules are beneficial for templatic languages like Maltese and languages with a high morpheme-to-word ratio like Basque.", "sentence": "This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets.", "cited_ids": [{"paper_id": "16326127", "citation": "Soricut and Och (2015)"}], "y": "This idea closely parallels the authors' approach for extracting rules. Unlike Soricut and Och (2015), the authors do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets, [and instead generate paradigms from raw data using not only prefix and suffix rules but also more general transformation rules.]", "snippet_surface": "This idea closely parallels the authors' approach for extracting rules. Unlike Soricut and Och (2015), the authors do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?"}, "answers": {"6gKwRw0I/Q": "The author's system generates paradigms from raw data using not only prefix and suffix rules but also more general transformation rules."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering. Our system generates paradigms using morphological transformation rules which are discovered from raw data. We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios.", "selected": "We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios.", "paper_id": "236486137"}]}}
{"idx": "282589", "paper_id": "13694072", "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings", "abstract": "The positive effect of adding subword information to word embeddings has been demonstrated for predictive models. In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.", "context_section_header": "", "context_paragraph": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "sentence": "These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "cited_ids": [{"paper_id": "8796808", "citation": "Cotterell et al. (2016)"}], "y": "The authors' work differs from Cotterell et al. (2016) in that [the presented model] incorporates morphological information at training time, and that only [some of the pre existing models are] able to generate embeddings for [out-of-vocabulary] (OOV) words.", "snippet_surface": "These differ from the authors' work in that they incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "questions": {"Q4HPY3aL6n": "What does \"these\" refer to?", "OG6Kf3KBkl": "Who does \"they\" refer to?", "t3W9A92LU3": "What are \"OOV words?\""}, "answers": {"Q4HPY3aL6n": "To the pre-trained models of Cotterell et al. and Vulic et al.", "OG6Kf3KBkl": "It refers to the authors of the paper", "t3W9A92LU3": "Out-of-vocabulary words."}, "evidence": {"Q4HPY3aL6n": [{"section": "Related Work", "paragraph": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "selected": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "paper_id": "13694072"}], "OG6Kf3KBkl": [{"section": "Related Work", "paragraph": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "selected": "Finally, Cotterell et al. (2016) and V\u00falic et al. (2017) retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only Cotterell et al. (2016) is able to generate embeddings for OOV words.", "paper_id": "13694072"}], "t3W9A92LU3": [{"section": "Introduction", "paragraph": "These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words.", "selected": "These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words.", "paper_id": "13694072"}]}}
{"idx": "28347", "paper_id": "14922514", "title": "A Comprehensive Gold Standard for the Enron Organizational Hierarchy", "abstract": "Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems.", "context_section_header": "", "context_paragraph": "We show the usefulness of this resource by investigating a simple predictor for hierarchy based on social network analysis (SNA), namely degree centrality of the social network induced by the email correspondence (Section 4). We call this a lower bound for SNA-based systems because we are only using a single simple metric (degree centrality) to establish dominance. Degree centrality is one of the features used by Rowe et al. (2007), but they did not perform a quantitative evaluation, and to our knowledge there are no published experiments using only degree centrality. Current systems using natural language processing (NLP) are restricted to making informed predictions on dominance pairs for which email exchange is available. We show (Section 5) that the upper bound performance of such NLP-based systems is much lower than our SNAbased system on the entire gold standard. We also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if we restrict ourselves to pairs for which email exchange is available, our simple SNA-based systems outperforms the NLP-based system.", "sentence": "We also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if we restrict ourselves to pairs for which email exchange is available, our simple SNA-based systems outperforms the NLP-based system.", "cited_ids": [{"paper_id": "16452861", "citation": "(Gilbert, 2012)"}], "y": "The authors contrast the simple SN-based [, also called SNA-based, social network analysis-based] system with a specific NLP system based on (Gilbert, 2012), and show that even if they restrict themselves to pairs for which email exchange is available, their simple SNA-based systems outperform the NLP-based system.", "snippet_surface": "The authors also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if they restrict themselves to pairs for which email exchange is available, their simple SNA-based systems outperform the NLP-based system.", "questions": {"BnNCzQH305": "What is an \"sna-based system\"?"}, "answers": {"BnNCzQH305": "The authors use \"sn-based\" and \"sna-based\" interchangeably. \"SNA-based\" stands for \"social network analyis based\" systems."}, "evidence": {"BnNCzQH305": [{"section": "Introduction", "paragraph": "We show the usefulness of this resource by investigating a simple predictor for hierarchy based on social network analysis (SNA), namely degree centrality of the social network induced by the email correspondence (Section 4). We call this a lower bound for SNA-based systems because we are only using a single simple metric (degree centrality) to establish dominance. Degree centrality is one of the features used by Rowe et al. (2007), but they did not perform a quantitative evaluation, and to our knowledge there are no published experiments using only degree centrality. Current systems using natural language processing (NLP) are restricted to making informed predictions on dominance pairs for which email exchange is available. We show (Section 5) that the upper bound performance of such NLP-based systems is much lower than our SNAbased system on the entire gold standard. We also contrast the simple SN-based system with a specific NLP system based on (Gilbert, 2012), and show that even if we restrict ourselves to pairs for which email exchange is available, our simple SNA-based systems outperforms the NLP-based system.", "selected": "based on social network analysis (SNA)", "paper_id": "14922514"}]}}
{"idx": "291353", "paper_id": "11170627", "title": "Enriching a statistical machine translation system trained on small parallel corpora with rule-based bilingual phrases", "abstract": "Work funded by the Spanish Ministry of Science and Innovation through project TIN2009-14009-C02-01 and by Generalitat Valenciana through grant ACIF/2010/174 (VALi+d programme).", "context_section_header": "", "context_paragraph": "When both parallel corpora and linguistic information exist, hybrid approaches (Thurmair, 2009) may be followed in order to make the most of such resources. We focus on alleviating the data sparseness problem suffered by phrase-based statistical machine translation (PBSMT) systems (Koehn, 2010, ch. 5) when trained on small parallel corpora. We present a new hybrid approach which enriches a PBSMT system with resources from shallow-transfer RBMT. Shallow-transfer RBMT systems, which are described in detail below, do not perform a complete syntactic analysis of the input sentences, but rather work with much simpler intermediate representations. Hybridisation between shallow-transfer RBMT and SMT has not yet been explored. Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, our approach directly uses the RBMT dictionaries and rules.", "sentence": "Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, our approach directly uses the RBMT dictionaries and rules.", "cited_ids": [{"paper_id": "6612052", "citation": "(Eisele et al., 2008)"}], "y": "Existing hybridisation strategies involve more complex RBMT [Rule-based machine translation] systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, the authors' approach directly uses the RBMT dictionaries and rules.", "snippet_surface": "Existing hybridisation strategies involve more complex RBMT systems (Eisele et al., 2008) which are usually treated as black boxes; in contrast, the authors' approach directly uses the RBMT dictionaries and rules.", "questions": {"ewhIcihwo4": "What does \"RBMT\" stand for?"}, "answers": {"ewhIcihwo4": "rule-based machine translation systems (RBMT)"}, "evidence": {"ewhIcihwo4": [{"section": "Introduction", "paragraph": "Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in machine translation research. SMT systems are very attractive because they may be built with little human effort when enough monolingual and bilingual corpora are available. However, bilingual corpora large enough to build competitive SMT systems are not always easy to harvest, and they may not even exist for some language pairs. On the contrary, rule-based machine translation systems (RBMT) may be built without any parallel corpus; however, they need an explicit representation of linguistic information whose coding by human experts requires a considerable amount of time.", "selected": "rule-based machine translation systems (RBMT)", "paper_id": "11170627"}]}}
{"idx": "29321", "paper_id": "222290903", "title": "When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models", "abstract": "We address hypernymy detection, i.e., whether an is-a relationship exists between words (x, y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x, y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases. We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework achieves competitive improvements and the case study shows its better interpretability.", "context_section_header": "", "context_paragraph": "For the second question, we present ComHyper, a complementary framework (Sec. 4.1) which takes advantage of both pattern-based models' superior performance on Type-I cases and the broad coverage of distributional models on Type-II ones. Specifically, to deal with Type-II sparsity, instead of directly using unsupervised distributional models, ComHyper uses a training stage (Sec. 4.3) to sample from output space of a pattern-based model to train another supervised distribution model implemented by different context encoders (Sec. 4.2). In the inference stage, ComHyper uses the two models to separately handle the type of sparsity they are good at, as illustrated in Figure 1. In this manner, ComHyper relies on the partial use of pattern-based models on Type-I sparsity to secure performance no lower than distributional ones, and further attempts to lift the performance by fixing the former's blind spots (Type-II sparsity) with the latter. On several benchmarks and evaluation settings, the distributional model in ComHyper proves effective on its targeted cases, making our complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018). Further analysis also suggests that ComHyper is robust when facing different mixtures of Type-I and -II sparsity.", "sentence": "On several benchmarks and evaluation settings, the distributional model in ComHyper proves effective on its targeted cases, making our complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018).", "cited_ids": [{"paper_id": "47016219", "citation": "(Roller et al., 2018)"}], "y": "On several benchmarks and evaluation settings, the authors' distributional model in ComHyper[, a complementary framework,] proves effective on its targeted cases[ (i.e. pattern-based approaches that need to be complemented with distributional ones)] making their complementary approach outperform a competitive class of pattern-based baselines [(approaches that employ pattern pairs)] (Roller et al., 2018).", "snippet_surface": "On several benchmarks and evaluation settings, the authors' distributional model in ComHyper proves effective on its targeted cases, making their complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018).", "questions": {"2TK3fQ62p9": "What are the pattern-based baselines?", "pLFB1AIyA1": "What are the targetted cases?"}, "answers": {"2TK3fQ62p9": "Pattern-based baselines are approaches that employ pattern pairs (x,y)", "pLFB1AIyA1": "The targeted cases are pattern-based approaches that need to be complemented with distributional ones"}, "evidence": {"2TK3fQ62p9": [{"section": "Introduction", "paragraph": "Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., \"y such as x\" and \"x and other y\". An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: x q and y q separately appear in some extracted pairs, but the pair (x q , y q ) is absent e.g., (dog, animal); or Type-II: either x q or y q is not involved in any extracted pair e.g., (crocodile, animal).", "selected": "Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., \"y such as x\" and \"x and other y\". An example of extracted pattern pairs from corpus are shown in Figure 1.", "paper_id": "222290903"}], "pLFB1AIyA1": [{"section": "Introduction", "paragraph": "\u2022 If so, how to complement pattern-based approaches with distributional ones where the former is invalid?", "selected": "If so, how to complement pattern-based approaches with distributional ones where the former is invalid?", "paper_id": "222290903"}, {"section": "Introduction", "paragraph": "For the second question, we present ComHyper, a complementary framework (Sec. 4.1) which takes advantage of both pattern-based models' superior performance on Type-I cases and the broad coverage of distributional models on Type-II ones. Specifically, to deal with Type-II sparsity, instead of directly using unsupervised distributional models, ComHyper uses a training stage (Sec. 4.3) to sample from output space of a pattern-based model to train another supervised distribution model implemented by different context encoders (Sec. 4.2). In the inference stage, ComHyper uses the two models to separately handle the type of sparsity they are good at, as illustrated in Figure 1. In this manner, ComHyper relies on the partial use of pattern-based models on Type-I sparsity to secure performance no lower than distributional ones, and further attempts to lift the performance by fixing the former's blind spots (Type-II sparsity) with the latter. On several benchmarks and evaluation settings, the distributional model in ComHyper proves effective on its targeted cases, making our complementary approach outperform a competitive class of pattern-based baselines (Roller et al., 2018). Further analysis also suggests that ComHyper is robust when facing different mixtures of Type-I and -II sparsity.", "selected": "For the second question, we present ComHyper, a complementary framework (Sec. 4.1) which takes advantage of both pattern-based models' superior performance on Type-I cases and the broad coverage of distributional models on Type-II ones.", "paper_id": "222290903"}]}}
{"idx": "299877", "paper_id": "102350797", "title": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training", "abstract": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.", "context_section_header": "", "context_paragraph": "We conduct a series of experiments with six different language pairs (in both directions) comprising European, non-European, and low-resource languages from two different datasets. Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures. Our method also gives better initial mapping compared to other existing methods (Artetxe et al., 2018b).", "sentence": "Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures.", "cited_ids": [{"paper_id": "3470398", "citation": "Conneau et al. (2018)"}], "y": "The authors' results show that their model [an adversarial autoencoder with a target encoder as an extra adversary and a regularization term to enforce cycle consistency] is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks [using the European language datasets] in all evaluation measures [mapping accuracy and convergence].", "snippet_surface": "The authors' results show that their model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures.", "questions": {"4T9JAsYA/w": "What is the authors' model?", "cJTcy30cvi": "What evaluation measures are used?", "XsovZg7eaK": "What are the translation tasks referred to?"}, "answers": {"4T9JAsYA/w": "An adversarial autoencoder with two novelties: the target encoder is an extra adversary, and an added regularisation term to enforce cycle consistency.", "cJTcy30cvi": "Comparing mapping accuracy and convergence with previous models.", "XsovZg7eaK": "The translation tasks are the European language datasets used in the Conneau et al. and Dinu et al. references."}, "evidence": {"4T9JAsYA/w": [{"section": "Abstract", "paragraph": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.", "selected": "In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator.", "paper_id": "102350797"}, {"section": "Introduction", "paragraph": "In our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. This forces the discriminator to improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. To guide the mapping, we include two additional constraints. Our first constraint enforces cycle consistency so that code vectors after being translated from one language to another, and then translated back to their source space remain close to the original vectors. The second constraint ensures reconstruction of the original input word embeddings from the back-translated codes. This grounding step forces the model to retain word semantics during the mapping process.", "selected": "In our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator.", "paper_id": "102350797"}, {"section": "Introduction", "paragraph": "In our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. This forces the discriminator to improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. To guide the mapping, we include two additional constraints. Our first constraint enforces cycle consistency so that code vectors after being translated from one language to another, and then translated back to their source space remain close to the original vectors. The second constraint ensures reconstruction of the original input word embeddings from the back-translated codes. This grounding step forces the model to retain word semantics during the mapping process.", "selected": "Our first constraint enforces cycle consistency so that code vectors after being translated from one language to another, and then translated back to their source space remain close to the original vectors. The second constraint ensures reconstruction of the original input word embeddings from the back-translated codes. This grounding step forces the model to retain word semantics during the mapping process.", "paper_id": "102350797"}], "cJTcy30cvi": [{"section": "Results", "paragraph": "2. How does our unsupervised mapping method compare with other unsupervised and supervised approaches (Section 5.2)? 3. Which components of our adversarial autoencoder model attribute to improvements (Section 5.3)?", "selected": "2. How does our unsupervised mapping method compare with other unsupervised and supervised approaches (Section 5.2)? 3. Which components of our adversarial autoencoder model attribute to improvements (Section 5.3)?", "paper_id": "102350797"}], "XsovZg7eaK": [{"section": "Results", "paragraph": "We present our results on European languages on the datasets of Conneau et al. (2018) and Dinu et al. (2015) in Tables 1 and 3, while the results on non-European languages are shown in Table 2. Through experiments, our goal is to assess:", "selected": "We present our results on European languages on the datasets of Conneau et al. (2018) and Dinu et al. (2015) in Tables 1 and 3, while the results on non-European languages are shown in Table 2.", "paper_id": "102350797"}]}}
{"idx": "303207", "paper_id": "6440511", "title": "Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts", "abstract": "This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co-occurrence and error-driven filtering. We present initial evaluation results and discuss future directions.", "context_section_header": "", "context_paragraph": "Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints. The fourth third difference is in the level of abstraction of transfer rules candidates; in (Meyers et al., 1998), the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminal nodes), while in our approach the nodes of transfer rules do not have to be lexicalized.", "sentence": "The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction.", "cited_ids": [{"paper_id": "5088141", "citation": "(Meyers et al., 1998)"}], "y": "Meyers et al (1998) designs the alignment of source and target nodes in a way that preserves node dominancy [nodes in the input tree being above the corresponding the output tree] in the source and target parses, while the authors\u2019 approach [using syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process] does not have such restriction.", "snippet_surface": "The second difference concerns the node alignment; (Meyers et al., 1998) designs the alignment of source and target nodes in a way that preserves node dominancy in the source and target parses, while the authors' approach does not have such restriction.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "t0sx9r6a7h": "What is \"node dominancy\"?"}, "answers": {"6gKwRw0I/Q": "Their approach is a generalization of syntactic approaches to example-based machine translation  that, as opposed to other research, instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process", "t0sx9r6a7h": "Nodes are the syntactic components in the parse of a sentence, and one node dominates another if it is above the other node in the parser's output tree."}, "evidence": {"6gKwRw0I/Q": [{"section": "Introduction", "paragraph": "Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984;Sato and Nagao, 1990;Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process.", "selected": "a generalization of syntactic approaches to example-based machine translation (", "paper_id": "6440511"}], "t0sx9r6a7h": [{"section": "Overall Approach", "paragraph": "\u2022 Nodes of the corresponding source and target parses are aligned using the baseline transfer dictionary and some heuristics based on the similarity of part-of-speech and syntactic context.", "selected": "Nodes of the corresponding source and target parses are aligned using the baseline transfer dictionary and some heuristics based on the similarity of part-of-speech and syntactic context.", "paper_id": "6440511"}, {"section": "Abstract", "paragraph": "1 Introduction Automatic acquisition of translation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Gr-ishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived bv \"cutting up\" the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a transla.-tion of the source tree. This technique resembles work on MT using synchronous Tree-Adjoining Grammars (cf. (Abeille et al., 1990)). The Proteus translation system learns transfer rules from pairs of aligned source and target reg-ularized parses, Proteus's representation of predicate argument structure(cf. Figure 1)? Then it uses these transfer rules to map source lan-L We thank Cristina Olmeda Moreno for work on parsing our Spanish text. 2Regularized parses (henceforth, \"pmse trees\") are like F-structures of Lexical }\"tmctiou Grammar (LEG), except, that a depenclency st.ruct.ure is used.\" guage regularized parses generated by ou r source language parser into target language regularized parses. Finally a generator converts target reg-ularized parses into target language sentences. All Mignment f is a I-to-1 partial mapping from source nodes to target nodes. We consider only alignments which preserve the dominance relationship: If node a dominates node b in the source tree. then f(a) dominates f(b) in the target tree. In Figure 1. source nodes ,4. B, C and D map to the corresponding target nodes, marked with a prime, e.g., f(A) = A'. The alignment)nay be represented by the setsign a score to each Mignment f, based on the (weighted) number of pairs in f; finding the best alignment translates into finding the alignment with the highest score. Our algorithms are based on (Farach et al., 199.5) and related work. We needed efficient alignment algorithms because: (1) Corpus-based training requires processing a lot of text; and (2) An exhaustive search of all \u2026", "selected": "We consider only alignments which preserve the dominance relationship: If node a dominates node b in the source tree. then f(a) dominates f(b) in the target tree.", "paper_id": "5088141"}, {"section": "Evaluation", "paragraph": "tree alignment algorithms (cf.(Meyers et al., 1996)) were designed to produce alignments which preserve the least common ancestor relationship: If nodes a and b map into nodesa' = f(a) and b' = f(b), then f(LCA(a,b)) = LCA(f(a), f(b)) = LCA(a', b'). The least common ancestor (LCA) of a and b is the lowest node in the tree dominating both a and b. The LCApreserving approach imposes limitations on the quality of the resulting alignments. [n Figure 1, the LCA-preserving algorithm will match node E with node D' and report that as the best match overall. The score S(D; D'I would take into account only the match (E, D~), which in turn includes (B, B') and (C, C'). (S(D, D') would be penalized for collapsing the arc from D to E.)We seek a better alignment scheme, in which the score S(D, D') could benefit from S(A, A').", "selected": "The least common ancestor (LCA) of a and b is the lowest node in the tree dominating both a and b.", "paper_id": "5088141"}]}}
{"idx": "303210", "paper_id": "6440511", "title": "Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts", "abstract": "This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co-occurrence and error-driven filtering. We present initial evaluation results and discuss future directions.", "context_section_header": "", "context_paragraph": "Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints. The fourth third difference is in the level of abstraction of transfer rules candidates; in (Meyers et al., 1998), the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminal nodes), while in our approach the nodes of transfer rules do not have to be lexicalized.", "sentence": "The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "cited_ids": [{"paper_id": "5088141", "citation": "(Meyers et al., 1998)"}], "y": "The third difference is in the process of identification of transfer rules candidates[, which can be compiled into a transfer dictionary for use in the translation process]; (Meyers et al., 1998) use the exact tree fragments in the source and target parse that are delimited by the alignment [which which preserved the dominant relationship], while the authors use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "snippet_surface": "The third difference is in the process of identification of transfer rules candidates; (Meyers et al., 1998) use the exact tree fragments in the source and target parse that are delimited by the alignment, while the authors use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints.", "questions": {"0qepdo9syF": "What are \"transfer rules?\"", "BOCjOsmUs9": "What alignment did Meyers et al. use to delimit the source and target parse?"}, "answers": {"0qepdo9syF": "Transfer rules can be compiled into a transfer dictionary for use in the translation process. The author identifies transfer rules candidates by using all source and target tree sub-patterns matching the subset of the parse features that satisfy a customised set of alignment and attribute constraints.", "BOCjOsmUs9": "Meyers et al., 1998 used only alignments which preserved the dominant relationship, e.g. if node a dominates node b in the source tree, then f(a) dominates f(b) in the target tree."}, "evidence": {"0qepdo9syF": [{"section": "Introduction", "paragraph": "Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984;Sato and Nagao, 1990;Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process.", "selected": "transfer rules that can be compiled into a transfer dictionary for use in the actual translation process.", "paper_id": "6440511"}, {"section": "Introduction", "paragraph": "Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints. The fourth third difference is in the level of abstraction of transfer rules candidates; in (Meyers et al., 1998), the source and target patterns of each transfer rule are fully lexicalized (except possibly the terminal nodes), while in our approach the nodes of transfer rules do not have to be lexicalized.", "selected": "we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment constraints and attribute constraints", "paper_id": "6440511"}, {"section": "Overall Approach", "paragraph": "In its most general form, our approach to transfer rules induction includes three different processes, data preparation, transfer rule induction and evaluation. An overview of each process is provided below; further details are provided in subsequent sections.", "selected": "In its most general form, our approach to transfer rules induction includes three different processes, data preparation, transfer rule induction and evaluation.", "paper_id": "6440511"}], "BOCjOsmUs9": [{"section": "Introduction", "paragraph": "An alignment f is a 1-to-1 partial mapping from source nodes to target nodes. We consider only alignments which preserve the dominance relationship: If node a dominates node b in the source tree, then f(a) dominates f(b) in the target tree. In Figure 1. source nodes .4. B, C and D map to the corresponding target nodes, marked with a prime, e.g., f(A) = A'. The alignment may be represented by the set {(d, A'), (B, B'), (C, C'), (D, D')}. We can assign a score to each alignment f, based on the (weighted) number of pairs in f; finding the best alignment translates into finding the alignment with the highest score. Our algorithms are based on (Farach et al., 1995) and related work.", "selected": "An alignment f is a 1-to-1 partial mapping from source nodes to target nodes. We consider only alignments which preserve the dominance relationship: If node a dominates node b in the source tree, then f(a) dominates f(b) in the target tree.", "paper_id": "5088141"}, {"section": "Acquiring Transfer Rules", "paragraph": "First, the best-scoring alignment is recovered from the Children-Pairing matrix, (Table t(b)). 4 Start by including the root node-pair in the alignment, (here (D, DI)). Then, for each pair (v, v ~) already in the alignment, repeat the following steps, until no more pairs can be added to the alignment: (t) look up the Children:Pairing for (v.v The transfer rules derived from Figure t may be written as follows:", "selected": "the best-scoring alignment is recovered from the Children-Pairing matrix", "paper_id": "5088141"}]}}
{"idx": "310204", "paper_id": "6354513", "title": "Learning Hidden Markov Models with Distributed State Representations for Domain Adaptation", "abstract": "Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains. In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance. We reformulate the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectationmaximization algorithm to jointly learn distributed state representations and model parameters. We empirically investigate the proposed model on cross-domain part-ofspeech tagging and noun-phrase chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation.", "context_section_header": "", "context_paragraph": "In terms of performing distributed representation learning for output variables, our proposed model shares similarity with the structured output representation learning approach developed by Srikumar and Manning (2014), which extends the structured support vector machines to simultaneously learn the prediction model and the distributed representations of the output labels. However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), and the spectral learning method (Stratos et al., 2013). But none of them performs representation learning to address cross-domain adaptation problems.", "sentence": "However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning.", "cited_ids": [{"paper_id": "716020", "citation": "(Srikumar and Manning, 2014)"}], "y": "The approach presented in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while the authors' proposed distributed [Hidden Markov Models] address cross-domain learning problems by performing unsupervised representation learning.", "snippet_surface": "However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while the authors' proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning.", "questions": {"blx1XUaZPV": "What does hmm stand for?"}, "answers": {"blx1XUaZPV": "HMM = Hidden Markov Model"}, "evidence": {"blx1XUaZPV": [{"section": "Proposed Model", "paragraph": "In this paper, we propose a novel distributed hidden Markov model (dHMM) for representation learning over sequence data. This model extends the hidden Markov models (Rabiner and Juang, 1986) to learn distributed state representations. Similar as HMMs, a dHMM (shown in Figure 1) is a two-layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using Markov", "selected": "a novel distributed hidden Markov model (dHMM)", "paper_id": "6354513"}]}}
{"idx": "316712", "paper_id": "52186890", "title": "Unsupervised Cross-lingual Transfer of Word Embedding Spaces", "abstract": "Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.", "context_section_header": "", "context_paragraph": "Unsupervised Domain Transfer: Generally speaking, learning the cross-lingual transfer of word embedding can be viewed as a domain transfer problem, where the domains are word sets in different languages. Thus various work in the field of unsupervised domain adaptation or unsupervised transfer learning can shed light on our problem. For example, He et al. (2016) proposed a semi-supervised method for machine translation to utilize large monolingual corpora. Shen et al. (2017) used unsupervised learning to transfer sentences of different sentiments. Recent work in computer vision addresses the problem of image style transfer without any annotated training data (Zhu et al., 2017;Taigman et al., 2016;Yi et al., 2017). Among those, our work is mostly inspired by the work on CycleGAN (Zhu et al., 2017), and we adopt their cycled consistent loss over images into our back-translation loss. One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, we introduce the Sinkhorn distance in our objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "sentence": "One key difference of our method from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, we introduce the Sinkhorn distance in our objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "cited_ids": [{"paper_id": "26873455", "citation": "(Zhang et al., 2017a)"}], "y": "One key difference between the authors method [an unsupervised cross-lingual transformation of monolingual embeddings] from CycleGAN is that they used the training loss of an adversarial classifier as an indicator of the distributional distance, but instead, the authors introduce the Sinkhorn distance in their objective function and demonstrate its superiority over the representative method [that showed the feasibility of connecting word embeddings of various languages without cross-lingual signal] using adversarial loss (Zhang et al., 2017a).", "snippet_surface": "One key difference between the authors' method and CycleGAN is that the latter used the training loss of an adversarial classifier as an indicator of the distributional distance, while the authors introduce the Sinkhorn distance in their objective function and demonstrate its superiority over the representative method using adversarial loss (Zhang et al., 2017a).", "questions": {"lUscp/5lYZ": "What adversarial classifier is used to find distributional distance for the CycleGAN method?", "VxCqlAwvTe": "What is the authors' method?", "ZuJ91HzPcB": "What does \"the representative method\" refer to?"}, "answers": {"lUscp/5lYZ": "The adversarial classifier used matched the distributions of the transformed source language embeddings and target ones without any cross-lingual signal.", "VxCqlAwvTe": "The author presents a new method for cross-lingual transformation of monolingual embeddings in an unsupervised manner (versus a supervised manner).", "ZuJ91HzPcB": "The representative method refers to the method used in the paper by Zhang et al., 2017a. This paper demonstrated the feasibility of connecting word embedding of various languages without the need for cross-lingual signal, this was done via adversarial training."}, "evidence": {"lUscp/5lYZ": [{"section": "Adversarial Training", "paragraph": "Generative adversarial networks are originally proposed for generating realistic images as an implicit generative model, but the adversarial training technique for matching distributions is generalizable to much more tasks, including natural language processing. For example, Ganin et al. (2016) address domain adaptation by adversarially training features to be domain invariant, and test on sentiment classification.  extend this idea to cross-lingual sentiment classification. Our research deals with unsupervised bilingual lexicon induction based on word embeddings, and therefore works with word embedding distributions, which are more interpretable than the neural feature space of classifiers in the above works.", "selected": "Our research deals with unsupervised bilingual lexicon induction based on word embeddings, and therefore works with word embedding distributions,", "paper_id": "26873455"}, {"section": "Conclusion", "paragraph": "In this work, we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training. The success of our approach signifies the existence of universal lexical semantic structure across languages. Our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely.", "selected": "we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training.", "paper_id": "26873455"}], "VxCqlAwvTe": [{"section": "Abstract", "paragraph": "Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.", "selected": "This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation.", "paper_id": "52186890"}, {"section": "Conclusion", "paragraph": "In this paper, we presented a novel method for cross-lingual transformation of monolingual embeddings in an unsupervised manner. By simultaneously optimizing the bi-directional mappings w.r.t. Sinkhorn distances and back-translation losses on both ends, our model enjoys its prediction power as well as robustness, with the impressive performance on multiple evaluation benchmarks. For future work, we would like to extend this work in the semi-supervised setting where insufficient bilingual dictionaries are available.", "selected": "we presented a novel method for cross-lingual transformation of monolingual embeddings in an unsupervised manner. By simultaneously optimizing the bi-directional mappings w.r.t. Sinkhorn distances and back-translation losses on both ends, our model enjoys its prediction power as well as robustness, with the impressive performance on multiple evaluation benchmarks.", "paper_id": "52186890"}], "ZuJ91HzPcB": [{"section": "Abstract", "paragraph": "Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.", "selected": "we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.", "paper_id": "26873455"}, {"section": "Conclusion", "paragraph": "In this work, we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training. The success of our approach signifies the existence of universal lexical semantic structure across languages. Our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely.", "selected": "we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal. This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training. The success of our approach signifies the existence of universal lexical semantic structure across languages.", "paper_id": "26873455"}]}}
{"idx": "318102", "paper_id": "249204487", "title": "Synthetic Data Generation for Multilingual Domain-Adaptable Question Answering Systems", "abstract": "Deep learning models have significantly advanced the state of the art of question answering systems. However, the majority of datasets available for training such models have been annotated by humans, are open-domain, and are composed primarily in English. To deal with these limitations, we introduce a pipeline that creates synthetic data from natural text. To illustrate the domain-adaptability of our approach, as well as its multilingual potential, we use our pipeline to obtain synthetic data in English and Dutch. We combine the synthetic data with non-synthetic data (SQuAD 2.0) and evaluate multilingual BERT models on the question answering task. Models trained with synthetically augmented data demonstrate a clear improvement in performance when evaluated on the domain-specific test set, compared to the models trained exclusively on SQuAD 2.0. We expect our work to be beneficial for training domain-specific question-answering systems when the amount of available data is limited.", "context_section_header": "", "context_paragraph": "These approaches, however, focus on potential answers that are primarily named entities or noun phrases (Tang et al., 2017;Alberti et al., 2019;Puri et al., 2020;Shakeri et al., 2020). For our use-case, we are interested in finding answers of longer spans that might contain administrative procedures in a multilingual setting (e.g. answers to such questions as \"how do I request an interna-OG Results tend to be scattered across different websites that often lack any guarantee of quality or reliability, and significant information gaps remain in many areas, leaving important questions unanswered MT Resultaten zijn meestal verspreid over verschillende websites die vaak geen enkele garantie voor kwaliteit of betrouwbaarheid hebben, en er blijven op veel gebieden aanzienlijke informatielacunes, waardoor belangrijke vragen onbeantwoord blijven OG information gaps MT informatie hiaten Table 1: Translation of Segments via Google Translate tional passport?\" or \"Waar kan ik mijn wagen registreren?\" -\"Where can I register my car?\"). Moreover, we are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "sentence": "Moreover, we are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "cited_ids": [{"paper_id": "4570782", "citation": "(Dhingra et al., 2018)"}], "y": "The authors are interested in finding right answers in a document that might contain multiple [administrative procedures in a multilingual setting], i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018) [which trains a neural network model on cloze-style questions and fine-tuning].", "snippet_surface": "Moreover, the authors are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "questions": {"cQEesWqMC9": "What are the proposed methods?", "YKlSRuRuxB": "What are \"multiple procedures\"?"}, "answers": {"cQEesWqMC9": "training a neural network model on cloze-style questions and fine-tuning", "YKlSRuRuxB": "administrative procedures in a multilingual setting"}, "evidence": {"cQEesWqMC9": [{"section": "Introduction", "paragraph": "In this work, we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question-answer pairs over a subset of these documents. Our proposed system consists of three stages. First, we construct cloze-style questions (predicting missing spans of text) from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA (Clark and Gardner, 2017;Dhingra et al., 2017); and finally, we fine-tune the model on the small set of provided QA pairs.", "selected": "Our proposed system consists of three stages. First, we construct cloze-style questions (predicting missing spans of text) from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA (Clark and Gardner, 2017;Dhingra et al., 2017); and finally, we fine-tune the model on the small set of provided QA pairs.", "paper_id": "4570782"}], "YKlSRuRuxB": [{"section": "Related Work", "paragraph": "These approaches, however, focus on potential answers that are primarily named entities or noun phrases (Tang et al., 2017;Alberti et al., 2019;Puri et al., 2020;Shakeri et al., 2020). For our use-case, we are interested in finding answers of longer spans that might contain administrative procedures in a multilingual setting (e.g. answers to such questions as \"how do I request an interna-OG Results tend to be scattered across different websites that often lack any guarantee of quality or reliability, and significant information gaps remain in many areas, leaving important questions unanswered MT Resultaten zijn meestal verspreid over verschillende websites die vaak geen enkele garantie voor kwaliteit of betrouwbaarheid hebben, en er blijven op veel gebieden aanzienlijke informatielacunes, waardoor belangrijke vragen onbeantwoord blijven OG information gaps MT informatie hiaten Table 1: Translation of Segments via Google Translate tional passport?\" or \"Waar kan ik mijn wagen registreren?\" -\"Where can I register my car?\"). Moreover, we are interested in finding right answers in a document that might contain multiple procedures, i.e. the introduction might not match the subsequent content at all, unlike the assumption of the methods proposed in (Dhingra et al., 2018).", "selected": "administrative procedures in a multilingual setting (e.g. answers to such questions as \"how do I request an interna-OG", "paper_id": "249204487"}]}}
{"idx": "320211", "paper_id": "237010913", "title": "Sentiment Preservation in Review Translation using Curriculum-based Re-inforcement Framework", "abstract": "Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment and emotion and gender traits and etc.) to the target and especially in a low-resource scenario. Such loss can affect the performance of any downstream Natural Language Processing (NLP) task and such as sentiment analysis and that heavily relies on the output of the MT systems. The susceptibility to sentiment polarity loss becomes even more severe when an MT system is employed for translating a source content that lacks a legitimate language structure (e.g. review text). Therefore and we must find ways to minimize the undesirable effects of sentiment loss in translation without compromising with the adequacy. In our current work and we present a deep re-inforcement learning (RL) framework in conjunction with the curriculum learning (as per difficulties of the reward) to fine-tune the parameters of a pre-trained neural MT system so that the generated translation successfully encodes the underlying sentiment of the source without compromising the adequacy unlike previous methods. We evaluate our proposed method on the English\u2013Hindi (product domain) and French\u2013English (restaurant domain) review datasets and and found that our method brings a significant improvement over several baselines in the machine translation and and sentiment classification tasks.", "context_section_header": "", "context_paragraph": "Recently, Tebbifakhr et al. (2019) proposed Machine-Oriented (MO) Reinforce, a policybased method to pursue a machine-oriented objective 2 in a sentiment classification task unlike the traditional human-oriented objective 3 . It gives a new perspective for a use-case of the MT system (i.e. machine translation for machine). To perform this task-specific adaption (i.e. produce output to feed a machine), Tebbifakhr et al. (2019) adapted the REINFORCE of Williams (1992) by incorporating an exploration-oriented sampling strategy. As opposed to one sampling of REINFORCE, MO Reinforce samples k times, (k = 5), and obtains a reward for each sample from the sentiment classifier. A final update of the model parameters are done w.r.t the highest rewarding sample. Although they achieved a performance boost in the sentiment classification task, they had to greatly compromise with the translation quality. In contrast to Tebbifakhr et al. (2019), we focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL. The adapted NMT system, thus obtained, is expected to produce a more accurate (high-quality) translation as well as improve the performance of a sentiment analyser. Bahdanau et al. (2017); Nguyen et al. (2017), unlike us, used the popular AC method, and focused only on preserving the semantics (translation quality) of a text. Additionally, we develop a CL based strategy to guide the training. Recently, Zhao et al. (2020) also studied AC method in the context of NMT. However, they used this method to learn the curriculum for re-selecting influential data samples from the existing training set that can further improve the performance (translation quality) of a pre-trained NMT system.", "sentence": "In contrast to Tebbifakhr et al. (2019), we focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL.", "cited_ids": [{"paper_id": "202786381", "citation": "Tebbifakhr et al. (2019)"}], "y": "In contrast to Tebbifakhr et al. (2019), the authors focus on performing a task-specific customisation of a pre-trained MT [Machine Translation] system via a harmonic reward-based deep reinforcement framework that uses an AC method [actor-critic reinforcement learning framework] in conjunction with CL [Curriculum Learning].", "snippet_surface": "In contrast to Tebbifakhr et al. (2019), the authors focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL.", "questions": {"imYGVFynS3": "What is an mt system?", "28/M2QcWiC": "What is an ac method?", "p5gNB/nbhe": "What does cl stand for?"}, "answers": {"imYGVFynS3": "An MT system is a Machine Translation system.", "28/M2QcWiC": "AC  method refers to an actor-critic reinforcement learning framework.", "p5gNB/nbhe": "CL = \"Curriculum Learning\"."}, "evidence": {"imYGVFynS3": [{"section": "Abstract", "paragraph": "Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment and emotion and gender traits and etc.) to the target and especially in a low-resource scenario. Such loss can affect the performance of any downstream Natural Language Processing (NLP) task and such as sentiment analysis and that heavily relies on the output of the MT systems. The susceptibility to sentiment polarity loss becomes even more severe when an MT system is employed for translating a source content that lacks a legitimate language structure (e.g. review text). Therefore and we must find ways to minimize the undesirable effects of sentiment loss in translation without compromising with the adequacy. In our current work and we present a deep re-inforcement learning (RL) framework in conjunction with the curriculum learning (as per difficulties of the reward) to fine-tune the parameters of a pre-trained neural MT system so that the generated translation successfully encodes the underlying sentiment of the source without compromising the adequacy unlike previous methods. We evaluate our proposed method on the English\u2013Hindi (product domain) and French\u2013English (restaurant domain) review datasets and and found that our method brings a significant improvement over several baselines in the machine translation and and sentiment classification tasks.", "selected": "Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment and emotion and gender traits and etc.) to the target and especially in a low-resource scenario.", "paper_id": "237010913"}], "28/M2QcWiC": [{"section": "Introduction", "paragraph": "More specifically, we propose a deep actor-critic (AC) reinforcement learning framework in the ambit of curriculum learning (CL) to alleviate the issue of sentiment loss while improving the quality of translation in a cross-lingual setup. The idea of actor-critic is to have two neural networks, viz. (i). an actor (i.e. a pre-trained NMT) that takes an action (policy-based), and (ii). a critic that observes how good the action taken is and provides feedback (value-based). This feedback acts as a guiding signal to train the actor. Further, to better utilize the data, we also integrate curriculum learning into our framework.", "selected": "More specifically, we propose a deep actor-critic (AC) reinforcement learning framework in the ambit of curriculum learning (CL) to alleviate the issue of sentiment loss while improving the quality of translation in a cross-lingual setup.", "paper_id": "237010913"}], "p5gNB/nbhe": [{"section": "Introduction", "paragraph": "More specifically, we propose a deep actor-critic (AC) reinforcement learning framework in the ambit of curriculum learning (CL) to alleviate the issue of sentiment loss while improving the quality of translation in a cross-lingual setup. The idea of actor-critic is to have two neural networks, viz. (i). an actor (i.e. a pre-trained NMT) that takes an action (policy-based), and (ii). a critic that observes how good the action taken is and provides feedback (value-based). This feedback acts as a guiding signal to train the actor. Further, to better utilize the data, we also integrate curriculum learning into our framework.", "selected": "More specifically, we propose a deep actor-critic (AC) reinforcement learning framework in the ambit of curriculum learning (CL)", "paper_id": "237010913"}]}}
{"idx": "324225", "paper_id": "184488238", "title": "Identifying Visible Actions in Lifestyle Vlogs", "abstract": "We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.", "context_section_header": "", "context_paragraph": "Although we use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, we search for routine videos that contain rich audio descriptions of the actions being performed, and we use this transcribed audio to extract actions. In these lifestyle vlogs, a vlogger typically performs an action while also describing it in detail. To the best of our knowledge, we are the first to build a video action recognition dataset using both transcribed audio and video information.", "sentence": "Although we use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, we search for routine videos that contain rich audio descriptions of the actions being performed, and we use this transcribed audio to extract actions.", "cited_ids": [{"paper_id": "22264672", "citation": "(Fouhey et al., 2018)"}], "y": "Although the authors use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, they search for routine videos that contain rich audio descriptions of the actions being performed, and they use this transcribed audio to extract [activities performed in the videos]. [Actions are extracted in several steps; data gathering, manual selection, transcript filtering, and then using the Stanford parser to split the transcript into sentences and verb phrases].", "snippet_surface": "The authors use implicit data gathering as proposed in the past, unlike (Fouhey et al., 2018) and other human action recognition datasets, they search for routine videos that contain rich audio descriptions of the actions being performed, and they use this transcribed audio to extract actions.", "questions": {"9LoRfmiF/S": "What are human action recognition datasets?", "kAJasZR3Iy": "How are the actions extracted?"}, "answers": {"9LoRfmiF/S": "Human recognition datasets are actions in videos that refer to human activities.", "kAJasZR3Iy": "The actions are extracted through using several steps, data gathering, manual selection, transcript filtering, and then using the Stanford parser to split the transcript into sentences and verb phrases."}, "evidence": {"9LoRfmiF/S": [{"section": "Related Work", "paragraph": "There has been substantial work on action recognition in the computer vision community, focusing on creating datasets (Soomro et al., 2012;Karpathy et al., 2014;Sigurdsson et al., 2016;Fabian Caba Heilbron and Niebles, 2015) or introducing new methods (Herath et al., 2017;Carreira and Zisserman, 2017;Donahue et al., 2015;Tran et al., 2015). Table 1 compares our dataset with previous action recognition datasets. 1 The largest datasets that have been compiled to date are based on YouTube videos (Fabian Caba Heilbron and Niebles, 2015;Real et al., 2017;Kay et al., 2017). These actions cover a broad range of classes including human-object interactions such as cooking (Rohrbach et al., 2014;Das et al., 2013;Rohrbach et al., 2012) and playing tennis (Karpathy et al., 2014), as well as human-human interactions such as shaking hands and hugging (Gu et al., 2018). Similar to our work, some of these previous datasets have considered everyday routine actions (Fabian Caba Heilbron and Niebles, 2015;Real et al., 2017;Kay et al., 2017). However, because these datasets rely on videos uploaded on YouTube, it has been observed they can be potentially biased towards unusual situations (Kay et al., 2017). For example, searching for videos with the query \"drinking tea\" results mainly in unusual videos such as dogs or birds drinking tea. This bias can be addressed by paying people to act out everyday scenarios (Sigurdsson et al., 2016), but this can end up being very expensive. In our work, we address this bias by changing the approach used to search for videos. Instead of searching for actions in an explicit way, using queries such as \"opening a fridge\" or \"making the bed,\" we search for more general videos using queries such as \"my morning routine.\" This approach has been referred to as implicit (as opposed to explicit) data gathering, and was shown to result in a greater number of videos with more realistic action depictions (Fouhey et al., 2018).", "selected": "These actions cover a broad range of classes including human-object interactions such as cooking (Rohrbach et al., 2014;Das et al., 2013;Rohrbach et al., 2012) and playing tennis (Karpathy et al., 2014), as well as human-human interactions such as shaking hands and hugging (Gu et al., 2018).", "paper_id": "184488238"}], "kAJasZR3Iy": [{"section": "Related Work", "paragraph": "Another important difference between our methodology and previously proposed methods is that we extract action labels from the transcripts. By gathering data before annotating the actions, our action labels are post-defined (as in Fouhey et al. 2018). This is unlike the majority of the existing human action datasets that use pre-defined labels (Sigurdsson et al., 2016;Fabian Caba Heilbron and Niebles, 2015;Real et al., 2017;Kay et al., 2017;Gu et al., 2018;Das et al., 2013;Rohrbach et al., 2012;Monfort et al., 2019). Postdefined labels allow us to use a larger set of labels, expanding on the simplified label set used in earlier datasets. These action labels are more inline with everyday scenarios, where people often use different names for the same action. For example, when interacting with a robot, a user could refer to an action in a variety of ways; our dataset includes the actions \"stick it into the freezer,\" \"freeze it,\" \"pop into the freezer,\" and \"put into the freezer,\" variations, which would not be included in current human action recognition datasets.", "selected": "Postdefined labels allow us to use a larger set of labels, expanding on the simplified label set used in earlier datasets.", "paper_id": "184488238"}, {"section": "Data Gathering", "paragraph": "The following data processing steps are applied: Transcript Filtering. Transcripts are automatically generated by YouTube. We filter out videos that do not contain any transcripts or that contain transcripts with an average (over the entire video) of less than 0.5 words per second. These videos do not contain detailed action descriptions so we cannot effectively leverage textual information.", "selected": "The following data processing steps are applied: Transcript Filtering. Transcripts are automatically generated by YouTube. We filter out videos that do not contain any transcripts or that contain transcripts with an average (over the entire video) of less than 0.5 words per second. These videos do not contain detailed action descriptions so we cannot effectively leverage textual information.", "paper_id": "184488238"}, {"section": "Data Gathering", "paragraph": "Extract Candidate Actions from Transcript. Starting with the transcript, we generate a noisy list of potential actions. This is done using the Stanford parser (Chen and Manning, 2014) to split the transcript into sentences and identify verb phrases, augmented by a set of hand-crafted rules to eliminate some parsing errors. The resulting actions are noisy, containing phrases such as \"found it helpful if you\" and \"created before up the top you.\" Segment Videos into Miniclips. The length of our collected videos varies from two minutes to twenty minutes. To ease the annotation process, we split each video into miniclips (short video sequences of maximum one minute). Miniclips are split to minimize the chance that the same action is shown across multiple miniclips. This is done automatically, based on the transcript timestamp of each action. Because YouTube transcripts have timing information, we are able to line up each action with its corresponding frames in the video. We sometimes notice a gap of several seconds between the time an action occurs in the transcript and the time it is shown in the video. To address this misalignment, we first map the actions to the miniclips using the time information from the transcript. We then expand the miniclip by 15 seconds before the first action and 15 seconds after the last action. This increases the chance that all actions will be captured in the miniclip. Motion Filtering. We remove miniclips that do not contain much movement. We sample one out of every one hundred frames of the miniclip, and compute the 2D correlation coefficient between these sampled frames. If the median of the obtained values is greater than a certain threshold (we choose 0.8), we filter out the miniclip. Videos with low movement tend to show people sitting in front of the camera, describing their routine, but not acting out what they are saying. There can be many actions in the transcript, but if they are not depicted in the video, we cannot leverage the video information.", "selected": "Extract Candidate Actions from Transcript. Starting with the transcript, we generate a noisy list of potential actions. This is done using the Stanford parser (Chen and Manning, 2014) to split the transcript into sentences and identify verb phrases, augmented by a set of hand-crafted rules to eliminate some parsing errors.", "paper_id": "184488238"}]}}
{"idx": "325708", "paper_id": "15307540", "title": "Automatically Creating General-Purpose Opinion Summaries from Text", "abstract": "We present and evaluate the first method known to us that can create rich nonextract-based opinion summaries from general text (e.g. newspaper articles). We first describe two possible representations for opinion summaries and then present our system OASIS, which identifies, and optionally aggregates, fine-grained opinions from the same source on the same topic. We propose new evaluation measures for both types of opinion summary and employ the metrics in an evaluation of OASIS on a standard opinion corpus. Our results are encouraging \u2014 OASIS substantially outperforms a competitive baseline when creating document-level aggregate summaries that compute the average polarity value across the multiple opinions identified for each source about each topic. We further show that as state-ofthe-art performance on fine-grained opinion extraction improves, we can expect to see opinion summaries of very high quality \u2014 with F-scores of 54-78% using our OSEM evaluation measure.", "context_section_header": "", "context_paragraph": "Our works falls in the area of fine-grained subjectivity analysis concerned with analyzing opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work builds on research on fine-grained opinion extraction by extracting additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "sentence": "In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "cited_ids": [{"paper_id": "388", "citation": "Pang and Lee (2004)"}], "y": "In contrast to the opinion extracts [movie reviews] produced by Pang and Lee (2004), the authors' summaries are not text extracts, but rather explicitly identify and characterize [by using an opinion polarity classifier from Choi and Cardie (2009)] the relations between opinions and their source [either positive, negative, or neutral].", "snippet_surface": "In contrast to the opinion extracts produced by Pang and Lee (2004), the authors' summaries are not text extracts, but rather explicitly identify and characterize the relations between opinions and their sources.", "questions": {"tgAzhaEuo7": "What opinions are extracted?", "eLSeMftJ5F": "What are the relations?", "fmxNPZQe4P": "How are the relations identified and characterized?"}, "answers": {"tgAzhaEuo7": "For Pang and Lee (2004), the opinions are movie reviews. For the authors, the opinions are from MPQA dataset of global news text.", "eLSeMftJ5F": "The relations are the polarity of the opinion toward the source: positive, negative, or neutral.", "fmxNPZQe4P": "They use an opinion polarity classifier from Choi and Cardie (2009)."}, "evidence": {"tgAzhaEuo7": [{"section": "Evaluation Framework", "paragraph": "Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons. First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com. Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002;Dave, Lawrence, and Pennock, 2003). Third, the correct label can be extracted automatically from rating information (e.g., number of stars). Our data 4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset.", "selected": "Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons.", "paper_id": "388"}], "eLSeMftJ5F": [{"section": "Fine-grained", "paragraph": "Opinion Extraction OA-SIS starts with the output of Choi et al.'s (2006) extractor, which recognizes opinion sources and triggers. These predictions can be described as a tuple [opinion trigger, source] with each component representing a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples.", "selected": "Opinion Extraction OA-SIS starts with the output of Choi et al.'s (2006) extractor, which recognizes opinion sources and triggers. These predictions can be described as a tuple [opinion trigger, source] with each component representing a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples.", "paper_id": "15307540"}], "fmxNPZQe4P": [{"section": "Fine-grained", "paragraph": "Opinion Extraction OA-SIS starts with the output of Choi et al.'s (2006) extractor, which recognizes opinion sources and triggers. These predictions can be described as a tuple [opinion trigger, source] with each component representing a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples.", "selected": "Opinion Extraction OA-SIS starts with the output of Choi et al.'s (2006) extractor, which recognizes opinion sources and triggers. These predictions can be described as a tuple [opinion trigger, source] with each component representing a span of text in the original document. We enhance these fine-grained opinion predictions by using the opinion polarity classifier from Choi and Cardie (2009), which adds polarity predictions as one of three possible values: positive, negative or neutral. This value is added to the opinion tuple to obtain [opinion trigger, source, polarity] triples.", "paper_id": "15307540"}]}}
{"idx": "326512", "paper_id": "8586038", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT\u201914 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT\u201914 English-to-German task.", "context_section_header": "", "context_paragraph": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45 (Sutskever et al., 2014). Our models are also validated on the more difficult WMT'14 English-to-German task.", "sentence": "With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points.", "cited_ids": [{"paper_id": "1245593", "citation": "(Luong et al., 2015)"}, {"paper_id": "1245593", "citation": "(Luong et al., 2015)"}], "y": "The authors' deep attention model improved the BLEU score to 37.7, outperforming the shallow model which has six layers (Luong et al., 2015) [as opposed to the authors' model which has 16] by 6.2 BLEU points.", "snippet_surface": "The authors' deep attention model improved the BLEU score to 37.7, outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points.", "questions": {"Kyz+cj6z2z": "How many layers does the authors' model have?"}, "answers": {"Kyz+cj6z2z": "The authors' model has 16 layers."}, "evidence": {"Kyz+cj6z2z": [{"section": "Introduction", "paragraph": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45 (Sutskever et al., 2014). Our models are also validated on the more difficult WMT'14 English-to-German task.", "selected": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16", "paper_id": "8586038"}, {"section": "Introduction", "paragraph": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45 (Sutskever et al., 2014). Our models are also validated on the more difficult WMT'14 English-to-German task.", "selected": "In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16", "paper_id": "8586038"}]}}
{"idx": "328432", "paper_id": "13320571", "title": "Reducing Lexical Features in Parsing by Word Embeddings", "abstract": "The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words.", "context_section_header": "", "context_paragraph": "Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003;Nivre et al., 2006;Zhang and Clark, 2008;Huang and Sagae, 2010; Templates: and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s 0 w and q 0 w, respectively) are typically used as features to calculate scores of transitions. When s 0 w is used as a feature template, the features in this template (e.g. s 0 w saw and s 0 w look ) can be viewed as one-hot vectors of a dimension of the lexicon size ( Figure 1). Corresponding to s 0 w, a weight is assigned to each word (e.g. W (s 0 w saw ) and W (s 0 w look )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s 0 w by d features, namely s 0 e 1 , . . . , s 0 e d . Given the vector representation of a word (e.g., e saw = (0.6, . . . , 0.2)), we replace the lexical feature (e.g. s 0 w saw ) by a linear combination of the d features (e.g., s 0 e saw := 0.6s 0 e 1 + . . . + 0.2s 0 e d ). Then, instead of the weights in a number of lexicon size assigned to s 0 w, now we use d weights (i.e., W (s 0 e 1 ), . . . , W (s 0 e d )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s 0 wq 0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008;Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014).", "sentence": "In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline.", "cited_ids": [{"paper_id": "1356465", "citation": "Andreas and Klein (2014)"}], "y": "The authors' framework [using a parser that has has low-dimensional features induced from word embeddings] effects that significantly improve the baseline [by connecting unseen words to known ones and encouraging common behaviors among in-vocabulary words], in contrast to the negative results reported in Andreas and Klein (2014) [which used word vectors in their parser.]", "snippet_surface": "The authors' framework has effects that significantly improve the baseline, in contrast to the negative results reported in Andreas and Klein (2014).", "questions": {"xIAPqYzjFr": "What does \"the baseline\" refer to?", "aQUfJEQxjW": "What effects does the authors' framework have that result in improvements over the baseline?", "EB8diJjZ9J": "What is the authors' framework?", "zcUnm5453F": "What is Andreas and Klein's approach?"}, "answers": {"xIAPqYzjFr": "The baseline here is parser that is being compared to in the experiment i.e.  the point of reference parser.", "aQUfJEQxjW": "The author's framework result in improvement with connecting unseen words to known ones and encouraging common behaviors among in-vocabulary words.", "EB8diJjZ9J": "The authors framework is using a parser that has low-dimensional features induced from word embeddings.", "zcUnm5453F": "Andres and Klein's approach is to utilize word vectors in various NLP tasks by performing word embedding in their parser in three ways."}, "evidence": {"xIAPqYzjFr": [{"section": "Analysis", "paragraph": "Is our modified parser really a feature reduction of the baseline system, i.e. is the parsing model trained for embedding features actually correlated to the baseline parsing model using lexical features? In Figure 3, we plot weights learned for the feature s 0 w x as X, and weights for s 0 e x as Y , where x ranges over high or middle frequency words. The weight for s 0 e x is calculated by taking inner product of the vector s 0 e x and the weight vector (W (s 0 e 1 ), . . . , W (s 0 e d )). As the direction of the regression lines show, weights learned for s 0 e x are positively correlated to weights learned for s 0 w x . It suggests that the parsing model trained for embedding features is indeed correlated to the parsing model of the baseline, which implies that the baseline parser and our modified parser would have similar behaviors. This may explain the significance results reported in Table 1: though our improvements against the baseline is fairly moderate, they are still statistically significant because our modified parser behaves similarly as the baseline parser, but would correct the mistakes made by the baseline while preserving most originally correct labels. Such improvements are easier to achieve statistical significance (Berg-Kirkpatrick et al., 2012), and are arguably indicating better generalization. So how does our modified parser improve from the baseline? In Figure 4, we plot cosine similarities between word vectors as X, and cosine similarities between weight vectors of all one-word lexical features as Y , compared to the similarities of weights of the corresponding embedding features. The plots show that, for similar words, the learned weights for the corresponding lexical features are only slightly similar; but after the lexical features are reduced to low-dimensional embedding features, the learned weights for the corresponding features are more strongly correlated. In other words, weights for embedding features encourage similar behaviors between similar words, due to a much lower di-  mensionality. This property may have two favorable effects on parsing, as hypothesized in Andreas and Klein (2014): (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words.", "selected": "It suggests that the parsing model trained for embedding features is indeed correlated to the parsing model of the baseline, which implies that the baseline parser and our modified parser would have similar behaviors.", "paper_id": "13320571"}, {"section": "Analysis", "paragraph": "Is our modified parser really a feature reduction of the baseline system, i.e. is the parsing model trained for embedding features actually correlated to the baseline parsing model using lexical features? In Figure 3, we plot weights learned for the feature s 0 w x as X, and weights for s 0 e x as Y , where x ranges over high or middle frequency words. The weight for s 0 e x is calculated by taking inner product of the vector s 0 e x and the weight vector (W (s 0 e 1 ), . . . , W (s 0 e d )). As the direction of the regression lines show, weights learned for s 0 e x are positively correlated to weights learned for s 0 w x . It suggests that the parsing model trained for embedding features is indeed correlated to the parsing model of the baseline, which implies that the baseline parser and our modified parser would have similar behaviors. This may explain the significance results reported in Table 1: though our improvements against the baseline is fairly moderate, they are still statistically significant because our modified parser behaves similarly as the baseline parser, but would correct the mistakes made by the baseline while preserving most originally correct labels. Such improvements are easier to achieve statistical significance (Berg-Kirkpatrick et al., 2012), and are arguably indicating better generalization. So how does our modified parser improve from the baseline? In Figure 4, we plot cosine similarities between word vectors as X, and cosine similarities between weight vectors of all one-word lexical features as Y , compared to the similarities of weights of the corresponding embedding features. The plots show that, for similar words, the learned weights for the corresponding lexical features are only slightly similar; but after the lexical features are reduced to low-dimensional embedding features, the learned weights for the corresponding features are more strongly correlated. In other words, weights for embedding features encourage similar behaviors between similar words, due to a much lower di-  mensionality. This property may have two favorable effects on parsing, as hypothesized in Andreas and Klein (2014): (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words.", "selected": "Is our modified parser really a feature reduction of the baseline system, i.e. is the parsing model trained for embedding features actually correlated to the baseline parsing model using lexical features?", "paper_id": "13320571"}], "aQUfJEQxjW": [{"section": "Introduction", "paragraph": "Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003;Nivre et al., 2006;Zhang and Clark, 2008;Huang and Sagae, 2010; Templates: and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s 0 w and q 0 w, respectively) are typically used as features to calculate scores of transitions. When s 0 w is used as a feature template, the features in this template (e.g. s 0 w saw and s 0 w look ) can be viewed as one-hot vectors of a dimension of the lexicon size ( Figure 1). Corresponding to s 0 w, a weight is assigned to each word (e.g. W (s 0 w saw ) and W (s 0 w look )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s 0 w by d features, namely s 0 e 1 , . . . , s 0 e d . Given the vector representation of a word (e.g., e saw = (0.6, . . . , 0.2)), we replace the lexical feature (e.g. s 0 w saw ) by a linear combination of the d features (e.g., s 0 e saw := 0.6s 0 e 1 + . . . + 0.2s 0 e d ). Then, instead of the weights in a number of lexicon size assigned to s 0 w, now we use d weights (i.e., W (s 0 e 1 ), . . . , W (s 0 e d )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s 0 wq 0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008;Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014).", "selected": "We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words.", "paper_id": "13320571"}], "EB8diJjZ9J": [{"section": "Abstract", "paragraph": "The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words.", "selected": "We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings.", "paper_id": "13320571"}, {"section": "Formalization", "paragraph": "In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features.", "selected": "In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features.", "paper_id": "13320571"}], "zcUnm5453F": [{"section": "Related Work", "paragraph": "A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009;Mikolov et al., 2013;Lebret and Collobert, 2014;Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010;Andreas and Klein, 2014;Bansal et al., 2014). The common approach (Turian et al., 2010;Koo et al., 2008;Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better.", "selected": "A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009;Mikolov et al., 2013;Lebret and Collobert, 2014;Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010;Andreas and Klein, 2014;Bansal et al., 2014).", "paper_id": "13320571"}, {"section": "Abstract", "paragraph": "Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.", "selected": "We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.", "paper_id": "1356465"}]}}
{"idx": "333254", "paper_id": "202775978", "title": "Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching", "abstract": "Sentence matching is a key issue in natural language inference and paraphrase identification. Despite the recent progress on multi-layered neural network with cross sentence attention, one sentence learns attention to the intermediate representations of another sentence, which are propagated from preceding layers and therefore are uncertain and unstable for matching, particularly at the risk of error propagation. In this paper, we present an original semantics-oriented attention and deep fusion network (OSOA-DFN) for sentence matching. Unlike existing models, each attention layer of OSOA-DFN is oriented to the original semantic representation of another sentence, which captures the relevant information from a fixed matching target. The multiple attention layers allow one sentence to repeatedly read the important information of another sentence for better matching. We then additionally design deep fusion to propagate the attention information at each matching layer. At last, we introduce a self-attention mechanism to capture global context to enhance attention-aware representation within each sentence. Experiment results on three sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN has the ability to model sentence matching more precisely.", "context_section_header": "", "context_paragraph": "Our proposed OSOA-DFN conducts original semantics-oriented cross sentence attention to model the matching. We design deep fusion to augment the propagation of attention information. At last, we introduce a self-attention mechanism to capture global context to enhance semantic representation. Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "sentence": "Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "cited_ids": [{"paper_id": "51610106", "citation": "(Duan et al., 2018)"}], "y": "Compared to AF-DMN (Duan et al., 2018), the authors [use a model model containing three mechanisms where the first two are combined to form one unit of cross-attention] and just use one self-attention layer instead of multiple layers, which reduces model complexity [by setting the number of cross attention layers to 3] but achieves outperformed accuracy.", "snippet_surface": "Compared to AF-DMN (Duan et al., 2018), the authors just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "questions": {"k4Va490PC8": "How does the model look like?", "6gKwRw0I/Q": "What is the authors' approach?", "AMxy33lqVE": "How is the complexity reduced?", "kPXiitHLQC": "How does the model achieve outperformed accuracy?"}, "answers": {"k4Va490PC8": "The model contains three mechanisms where the first two are combined to form one unit of cross-attention.", "6gKwRw0I/Q": "The author uses one self-attention layer as opposed to multiple layers.", "AMxy33lqVE": "The complexity was reduced by setting the number of cross attention layers to 3.", "kPXiitHLQC": "Outperformed accuracy is achieved using the Ensemble strategy."}, "evidence": {"k4Va490PC8": [{"section": "Figure 1 :", "paragraph": "(b). OSOA-DFN is mainly composed of: (1) original semantics-oriented cross sentence attention; (2) deep fusion; and (3) selfattention mechanism. (1) and (2) are combined to form one unit of cross attention, as shown in Figure 1(c), and there are T units in attention-based matching layer. Finally, one layer of self-attention is introduced after the T units of cross attention.", "selected": "OSOA-DFN is mainly composed of: (1) original semantics-oriented cross sentence attention; (2) deep fusion; and (3) selfattention mechanism. (1) and (2) are combined to form one unit of cross attention, as shown in Figure 1(c), and there are T units in attention-based matching layer. Finally, one layer of self-attention is introduced after the T units of cross attention.", "paper_id": "202775978"}], "6gKwRw0I/Q": [{"section": "Related Works", "paragraph": "Our proposed OSOA-DFN conducts original semantics-oriented cross sentence attention to model the matching. We design deep fusion to augment the propagation of attention information. At last, we introduce a self-attention mechanism to capture global context to enhance semantic representation. Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "selected": "Compared to AF-DMN (Duan et al., 2018), we just use one self-attention layer instead of multiple layers, which reduces model complexity but achieves outperformed accuracy.", "paper_id": "202775978"}], "AMxy33lqVE": [{"section": "Effect of Original Semantics-Oriented Cross Sentence Attention", "paragraph": "We further verify the effect of the depth of cross sentence attention on performance, as shown in Table 6. As the number of stacked attention layers increases from 1 to 5, we can see that the performance increases both on the development set and the test set of SciTail dataset. But from 3 to 5, the increased accuracy is slower. We can conclude that the multiple original semantics-oriented attentions are effective in improving matching performance. However, the parameters will grow rapidly with the increasing of the number of stacked attention layer, and a large of number of parameters will increase model complexity. Because of computational cost, we just set the number of cross attention layers to 3 in our experiment.", "selected": "We can conclude that the multiple original semantics-oriented attentions are effective in improving matching performance. However, the parameters will grow rapidly with the increasing of the number of stacked attention layer, and a large of number of parameters will increase model complexity. Because of computational cost, we just set the number of cross attention layers to 3 in our experiment.", "paper_id": "202775978"}], "kPXiitHLQC": [{"section": "Ensemble", "paragraph": "The ensemble strategy has been proved to effectively improve model accuracy. Following (Duan et al., 2018), our ensemble model averages the probability distributions from three individual single OSOA-DFNs, and each of them has the same architecture but different parameter initialization.", "selected": "The ensemble strategy has been proved to effectively improve model accuracy. Following (Duan et al., 2018), our ensemble model averages the probability distributions from three individual single OSOA-DFNs, and each of them has the same architecture but different parameter initialization.", "paper_id": "202775978"}]}}
{"idx": "33357", "paper_id": "14186198", "title": "Personalized Page Rank for Named Entity Disambiguation", "abstract": "The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.", "context_section_header": "", "context_paragraph": "Our method has the following properties: 1) as our system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, our method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) we tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "sentence": "Our method has the following properties: 1) as our system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, our method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) we tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "cited_ids": [{"paper_id": "17826594", "citation": "(Alhelbawy and Gaizauskas, 2014)"}], "y": "The authors' [collective disambiguation approach using a graph model] has the following properties: 1) as their system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike PageRank based approaches in [name entity disambiguation] (NED) (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, the authors' method is able to better utilize the local similarity between a candidate and a KB node; and 3) the authors' tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise.", "snippet_surface": "The authors' method has the following properties: 1) as their system is based on a random walk algorithm, it does not require training model parameters ; 2) unlike previous PageRank based approaches in NED (Alhelbawy and Gaizauskas, 2014) which mainly rely on global coherence, their method is able to better utilize the local similarity between a candidate and a KB node (Section 3); and 3) they tailor the Personalized PageRank algorithm to only focus on one high-confidence entity at a time to reduce noise (Section 4).", "questions": {"rppdzdmBG9": "What does ned stand for?", "Im7pRWGe/v": "What is \"their method\"?"}, "answers": {"rppdzdmBG9": "NED means name entity disambiguation", "Im7pRWGe/v": "Their method refers to a collective disambiguation approach using a graph model, where all NE candidates are represented as nodes in the graph and associations between candidates are represented by edges."}, "evidence": {"rppdzdmBG9": [{"section": "Introduction", "paragraph": "Name entity disambiguation (NED) is the task in which entity mentions in a document are mapped to real world entities. NED is both useful on its own, and serves as a valuable component in larger Knowledge Base Construction systems (Mayfield, 2014).", "selected": "Name entity disambiguation (NED) is the task in which entity mentions in a document are mapped to real world entities.", "paper_id": "14186198"}], "Im7pRWGe/v": [{"section": "Abstract", "paragraph": "Named Entity Disambiguation (NED) refers to the task of mapping different named entity mentions in running text to their correct interpretations in a specific knowledge base (KB). This paper presents a collective disambiguation approach using a graph model. All possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes. Each node has an initial confidence score, e.g. entity popularity. Page-Rank is used to rank nodes and the final rank is combined with the initial confidence for candidate selection. Experiments on 27,819 NE textual mentions show the effectiveness of using Page-Rank in conjunction with initial confidence: 87% accuracy is achieved, outperforming both baseline and state-of-the-art approaches.", "selected": "This paper presents a collective disambiguation approach using a graph model. All possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes. Each node has an initial confidence score, e.g. entity popularity. Page-Rank is used to rank nodes and the final rank is combined with the initial confidence for candidate selection.", "paper_id": "17826594"}]}}
{"idx": "333952", "paper_id": "249394694", "title": "Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation", "abstract": "We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves strong translation performance across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU scores of 31.16 for \\texttt{en}\\rightarrow\\texttt{de} and 38.37 for \\texttt{de}\\rightarrow\\texttt{en} on the IWSLT14 dataset, 30.78 for \\texttt{en}\\rightarrow\\texttt{de} and 35.15 for \\texttt{de}\\rightarrow\\texttt{en} on the WMT14 dataset, and 27.17 for \\texttt{zh}\\rightarrow\\texttt{en} on the WMT17 dataset. SimCut is not a new method, but a version of Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be considered as a perturbation-based method. Given the universality and simplicity of Bi-SimCut and SimCut, we believe they can serve as strong baselines for future NMT research.", "context_section_header": "", "context_paragraph": "\u2022 Our experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "sentence": "\u2022 Our experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "cited_ids": [{"paper_id": "237485586", "citation": "(Xu et al., 2021)"}], "y": "[The authors use a new two-stage training strategy consisting of bidirectional pretraining and unidirectional finetuning, combined with SimCut Regularisation to improve generality of NMT.] Their experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "snippet_surface": "The authors' experimental results show that NMT training with Bi-SimCut achieves significant improvements over the Transformer model on five translation benchmarks (data sizes range from 160K to 20.2M), and outperforms the current SOTA method BiBERT (Xu et al., 2021) on several benchmarks.", "questions": {"njufrk0Ds2": "How does the authors' method achieve significant improvements?"}, "answers": {"njufrk0Ds2": "The authors use a new two-stage training strategy consisting of bidirectional pretraining and unidirectional finetuning, combined with SimCut Regularisation to improve generality of NMT."}, "evidence": {"njufrk0Ds2": [{"section": "Bi-SimCut", "paragraph": "In this section, we formally propose Bidirectional Pretrain and Unidirectional Finetune with Simple Cutoff Regularization (Bi-SimCut), a simple but effective training strategy that can greatly enhance the generalization of the NMT model. Bi-SimCut consists of a simple cutoff regularization and a two-phase pretraining and finetuning strategy. We introduce the details of each part below.", "selected": "a simple but effective training strategy that can greatly enhance the generalization of the NMT model", "paper_id": "249394694"}, {"section": "SimCut: A Simple Cutoff Regularization for NMT", "paragraph": "Despite the impressive performance reported in Shen et al. (2020), finding the proper hyperparameters (p cut , \u03b1, \u03b2, N ) in Token Cutoff seems to be tedious and time-consuming if there are limited resources available, which hinders its practical value in the NMT community. To reduce the burden in hyper-parameter searching, we propose SimCut, a simple regularization method that forces the consistency between the output distributions of the original sentence pairs and the cutoff samples. Our problem formulation is motivated by Virtual Adversarial Training (VAT), where Sato et al. (2019) introduces a KL-based adversarial regularization that forces the output distribution of the samples with adversarial perturbations \u03b4 x \u2208 R d\u00d7I and \u03b4 y \u2208 R d\u00d7J to be consistent with that of the original samples:", "selected": "reduce the burden in hyper-parameter searching, we propose SimCut, a simple regularization method that forces the consistency between the output distributions of the original sentence pairs and the cutoff samples", "paper_id": "249394694"}, {"section": "Conclusion", "paragraph": "In this paper, we propose Bi-SimCut: a simple but effective two-stage training strategy to improve NMT performance. Bi-SimCut consists of bidirectional pretraining and unidirectional finetuning procedures equipped with SimCut regularization for improving the generality of the NMT model. Experiments on low (IWSLT14 en\u2194de), standard (WMT14 en\u2194de), and high (WMT17 zh\u2192en) resource translation benchmarks demonstrate Bi-SimCut and SimCut's capabilities to improve translation performance and robustness. Given the universality and simplicity of Bi-SimCut and Sim-Cut, we believe: 1) SimCut could be regarded as a perturbation-based method, and it could be used as a strong baseline for the robustness research. 2) Bi-SimCut outperforms many complicated methods which incorporate large-scaled pretrained models or sophisticated mechanisms, and it could be used as a strong baseline for future NMT research. We hope researchers of perturbations and NMT could use SimCut and Bi-SimCut as strong baselines to make the usefulness and effectiveness of their proposed methods clear. For future work, we will explore the effectiveness of SimCut and Bi-SimCut on more sequence learning tasks, such as multilingual machine translation, domain adaptation, text classification, natural language understanding, etc.", "selected": "consists of bidirectional pretraining and unidirectional finetuning procedures equipped with SimCut regularization for improving the generality of the NMT model", "paper_id": "249394694"}]}}
{"idx": "334608", "paper_id": "237332190", "title": "Stylistic approaches to predicting Reddit popularity in diglossia", "abstract": "Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit\u2019s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (\u2018Singlish\u2019) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect\u2019s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity.", "context_section_header": "", "context_paragraph": "Much research has gone into investigating what makes a social media post popular, including some specifically focused on Reddit. Lakkaraju et al. (2013) controlled for the content of the post by concentrating on image submissions, which are frequently re-or cross-posted to different communities by different authors. They found that the title of a submission played a role in determining its success, where titles specifically engineered towards the community it was posted in (for example, by using community-specific words) performed better. Tran and Ostendorf (2016) took this a step further and trained separate models for the content (using Latent Dirichlet Allocation (LDA)) and the style of the language used (by replacing topic words with their part-of-speech tags). They computed the Spearman rank correlation between scores and post representations, and found that the style model was much better at predicting of the success of a post than the content model. In other words, they found that these subreddits had their own community style, and posts which are stylisti-cally more similar to it are more likely to be wellreceived. Fang et al. (2016) is the paper which is closest to the aim of this paper. They divided posts into eight different bins which are automatically determined by the score distribution of that particular subreddit, and evaluated model performance using a modified macro F1 score (details in Section 5.1). However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "sentence": "However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "cited_ids": [{"paper_id": "5958042", "citation": "Fang et al. (2016)"}], "y": "While Fang et al. (2016) focus on modelling the conversational context of a post [text features, response structure and text of the discussion], the authors focus on modelling the community style [i.e. use of punctuation, stopwords, and part-of-speech tags] of subreddits.", "snippet_surface": "However, while Fang et al. (2016) focused on modelling the conversational context of a post, the authors instead focus on modelling the community style.", "questions": {"h7Vo5Kxbq1": "What is the community style?", "Tu4F4L66Pu": "What is the conversational context?", "WvhTyM/3qh": "What does \"modelling\" refer to?"}, "answers": {"h7Vo5Kxbq1": "The community style refers to the style a Reddit post is published and its affect on popularity, or endorsement, within the community. Subreddits have their own community style, and posts which adhered stylistically (i.e. use of punctuation, stopwords, and part-of-speech tags) where better received.", "Tu4F4L66Pu": "The author focuses on a model that learns the latent modes of discussion structure that perform similarly to deep neural network. The latent modes can used to weight text features and thus improve prediction accuracy of the level of endorsement if a comment based on the response structure of the discussion and the text of the comment.", "WvhTyM/3qh": "This author focuses on modelling the community style of Reddit subreddits, whereas Fang et al., (2016) focuses on modelling the conversational context of post. Both use stylistic approaches to predict Reddit post popularity."}, "evidence": {"h7Vo5Kxbq1": [{"section": "Abstract", "paragraph": "Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit\u2019s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (\u2018Singlish\u2019) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect\u2019s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity.", "selected": "what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit\u2019s community style are better received.", "paper_id": "237332190"}, {"section": "Introduction", "paragraph": "But what exactly makes a post popular? In this paper, I apply natural language processing (NLP) techniques to predicting the popularity of a Reddit post. As past research has found style to be a strong predictor of community response (Tran and Ostendorf, 2016), I focus on stylistic approaches using punctuation, stopwords and part-of-speech tags, as inspired by Bergsma et al. (2012).", "selected": "As past research has found style to be a strong predictor of community response (Tran and Ostendorf, 2016), I focus on stylistic approaches using punctuation, stopwords and part-of-speech tags,", "paper_id": "237332190"}, {"section": "Introduction", "paragraph": "But what exactly makes a post popular? In this paper, I apply natural language processing (NLP) techniques to predicting the popularity of a Reddit post. As past research has found style to be a strong predictor of community response (Tran and Ostendorf, 2016), I focus on stylistic approaches using punctuation, stopwords and part-of-speech tags, as inspired by Bergsma et al. (2012).", "selected": "tylistic approaches using punctuation, stopwords and part-of-speech tags", "paper_id": "237332190"}, {"section": "Related Work", "paragraph": "Much research has gone into investigating what makes a social media post popular, including some specifically focused on Reddit. Lakkaraju et al. (2013) controlled for the content of the post by concentrating on image submissions, which are frequently re-or cross-posted to different communities by different authors. They found that the title of a submission played a role in determining its success, where titles specifically engineered towards the community it was posted in (for example, by using community-specific words) performed better. Tran and Ostendorf (2016) took this a step further and trained separate models for the content (using Latent Dirichlet Allocation (LDA)) and the style of the language used (by replacing topic words with their part-of-speech tags). They computed the Spearman rank correlation between scores and post representations, and found that the style model was much better at predicting of the success of a post than the content model. In other words, they found that these subreddits had their own community style, and posts which are stylisti-cally more similar to it are more likely to be wellreceived. Fang et al. (2016) is the paper which is closest to the aim of this paper. They divided posts into eight different bins which are automatically determined by the score distribution of that particular subreddit, and evaluated model performance using a modified macro F1 score (details in Section 5.1). However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "selected": "they found that these subreddits had their own community style, and posts which are stylisti-cally more similar to it are more likely to be wellreceived.", "paper_id": "237332190"}, {"section": "Conclusion", "paragraph": "I investigate also the hypothesis that posts conforming to a group's style receive greater community endorsement (Tran and Ostendorf, 2016). I show that in a diglossic situation, although the acrolect draws greater prestige, the most successful posts draw on features from the basilect in order to connect with the audience.", "selected": "I investigate also the hypothesis that posts conforming to a group's style receive greater community endorsement (Tran and Ostendorf, 2016). I show that in a diglossic situation, although the acrolect draws greater prestige, the most successful posts draw on features from the basilect in order to connect with the audience.", "paper_id": "237332190"}], "Tu4F4L66Pu": [{"section": "Abstract", "paragraph": "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.", "selected": "This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment.", "paper_id": "5958042"}, {"section": "Abstract", "paragraph": "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.", "selected": "he different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable.", "paper_id": "5958042"}, {"section": "Introduction", "paragraph": "Online discussion forums provide a platform for people with shared interests (online communities) to discuss current events and common concerns. Many forums provide a mechanism for readers to indicate positive/negative reactions to comments in the discussion, with up/down votes, \"liking,\" or indicating whether a comment is useful. The cumulative reaction, which we will refer to as \"community endorsement,\" can be useful to readers for prioritizing what they read or in gathering information for decision making. This paper introduces the task of automatically predicting the level of endorsement of a comment based on the response structure of the discussion and the text of the comment. To address this task, we introduce a neural network architecture that learns latent discussion structure (or, conversation) modes and adjusts the relative dependence on text vs. structural cues in classification. The neural network framework is also useful for combining text with the disparate features that characterize the submission context of a comment, i.e. relative timing in the discussion, response structure (characterized by graph features), and author indexing.", "selected": "This paper introduces the task of automatically predicting the level of endorsement of a comment based on the response structure of the discussion and the text of the comment.", "paper_id": "5958042"}, {"section": "Conclusion", "paragraph": "In summary, this work has addressed the problem of predicting community endorsement of comments in a discussion forum using a new neural network architecture that integrates submission context features (including relative timing and response struc-ture) with features extracted from the text of a comment. The approach represents the submission context in terms of a linear combination of latent basis vectors that characterize the dynamic conversation mode, which gives results similar to using a deep network but is more interpretable. The model also includes a dynamic gate for the text content, and analysis shows that when response structure is available to the predictor, the content of a comment has the most utility for comments that are not in active regions of the discussion. These results are based on characterizing quantized levels of karma with a series of binary classifiers. Quantized karma prediction could also be framed as an ordinal regression task, which would involve a straightforward change to the neural network learning objective.", "selected": "this work has addressed the problem of predicting community endorsement of comments in a discussion forum using a new neural network architecture that integrates submission context features (including relative timing and response struc-ture) with features extracted from the text of a comment. The approach represents the submission context in terms of a linear combination of latent basis vectors that characterize the dynamic conversation mode, which gives results similar to using a deep network but is more interpretable.", "paper_id": "5958042"}], "WvhTyM/3qh": [{"section": "Related Work", "paragraph": "Much research has gone into investigating what makes a social media post popular, including some specifically focused on Reddit. Lakkaraju et al. (2013) controlled for the content of the post by concentrating on image submissions, which are frequently re-or cross-posted to different communities by different authors. They found that the title of a submission played a role in determining its success, where titles specifically engineered towards the community it was posted in (for example, by using community-specific words) performed better. Tran and Ostendorf (2016) took this a step further and trained separate models for the content (using Latent Dirichlet Allocation (LDA)) and the style of the language used (by replacing topic words with their part-of-speech tags). They computed the Spearman rank correlation between scores and post representations, and found that the style model was much better at predicting of the success of a post than the content model. In other words, they found that these subreddits had their own community style, and posts which are stylisti-cally more similar to it are more likely to be wellreceived. Fang et al. (2016) is the paper which is closest to the aim of this paper. They divided posts into eight different bins which are automatically determined by the score distribution of that particular subreddit, and evaluated model performance using a modified macro F1 score (details in Section 5.1). However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "selected": "However, while Fang et al. (2016) focused on modelling the conversational context of a post, I instead focus on modelling the community style.", "paper_id": "237332190"}]}}
{"idx": "340739", "paper_id": "15890923", "title": "Extracting Social Networks from Literary Text with Word Embedding Tools", "abstract": "In this paper a social network is extracted from a literary text. The social network shows, how frequent the characters interact and how similar their social behavior is. Two types of similarity measures are used: the first applies co-occurrence statistics, while the second exploits cosine similarity on different types of word embedding vectors. The results are evaluated by a paid micro-task crowdsourcing survey. The experiments suggest that specific types of word embeddings like word2vec are well-suited for the task at hand and the specific circumstances of literary fiction text.", "context_section_header": "", "context_paragraph": "An obvious next step is the actual extraction of social networks from novels. In the method by (Elson et al., 2010) the networks are derived from dialog interactions. Therefore their method includes finding instances of quoted speech, attributing each quote to a character, and identifying when certain characters are in conversation. They construct a weighted graph, where nodes correspond to actors, and the weights on the edges represents the frequency and amount of exchanges. In contrast to our work, (Elson et al., 2010) are solely focus on length and number of dialogues between persons to measure relatedness, whereas our approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings. Similarly, (Celikyilmaz et al., 2010) address a the problem related to the extraction of relations between characters. They attribute utterances in literary dialogues to actors, and apply the similarities in the language used to predict similarity and hidden relations between those actors. In contrast to our work, the approach also is restricted to dialogues between authors, and the evaluation of the method is of limited scope.", "sentence": "In contrast to our work, (Elson et al., 2010) are solely focus on length and number of dialogues between persons to measure relatedness, whereas our approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings.", "cited_ids": [{"paper_id": "1974676", "citation": "(Elson et al., 2010)"}], "y": "In contrast to the authors' work [based on co-occurence statistics and cosine similarity], (Elson et al., 2010) focus solely on length and number of dialogues between persons to measure relatedness, whereas the authors' approach looks at general co-occurrence or similarity as measured by [Language Technology] (LT) tools which use word embeddings.", "snippet_surface": "In contrast to the authors' work, (Elson et al., 2010) focus solely on length and number of dialogues between persons to measure relatedness, whereas their approach looks at general co-occurrence or similarity as measured by LT tools which use word embeddings.", "questions": {"YKw0Bh6gbk": "What is the authors' work?", "QGYoqEFWaT": "What are lt tools?"}, "answers": {"YKw0Bh6gbk": "The authors work is experimentation based on co-occurence statistics and cosine similarity.", "QGYoqEFWaT": "LT Tools are state of the art word embedding tools. \"LT\" = Language Technology"}, "evidence": {"YKw0Bh6gbk": [{"section": "Abstract", "paragraph": "In this paper a social network is extracted from a literary text. The social network shows, how frequent the characters interact and how similar their social behavior is. Two types of similarity measures are used: the first applies co-occurrence statistics, while the second exploits cosine similarity on different types of word embedding vectors. The results are evaluated by a paid micro-task crowdsourcing survey. The experiments suggest that specific types of word embeddings like word2vec are well-suited for the task at hand and the specific circumstances of literary fiction text.", "selected": "Two types of similarity measures are used: the first applies co-occurrence statistics, while the second exploits cosine similarity on different types of word embedding vectors. The results are evaluated by a paid micro-task crowdsourcing survey. The experiments suggest that specific types of word embeddings like word2vec are well-suited for the task at hand and the specific circumstances of literary fiction text.", "paper_id": "15890923"}], "QGYoqEFWaT": [{"section": "Introduction", "paragraph": "Word embeddings are language modeling techniques that transform the vocabulary of an input corpus into a continuous and low-dimensional vector representation. Word embeddings have shown state-ofthe-art performance as language technology (LT) tools esp. for word similarity estimations, but also for more sophisticated operations like word analogies and as input component to various natural language processing (NLP) tasks (Mikolov et al., 2013;Ghannay et al., 2016). Word embeddings use artificial neural networks for generating the vector representations. Neural networks have become very popular and successful tools in NLP in the last couple of years, esp. with recent improvements in the deep learning field.", "selected": "Word embeddings have shown state-ofthe-art performance as language technology (LT) tools esp. for word similarity estimations,", "paper_id": "15890923"}]}}
{"idx": "341873", "paper_id": "52118895", "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction", "abstract": "We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.", "context_section_header": "", "context_paragraph": "To explore this problem, we create a dataset SCI-ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b;. In addition, we build a scientific knowledge graph integrating terms and relations extracted from each article. Human evaluation shows that propagating coreference can significantly improve the quality of the automatic constructed knowledge graph.", "sentence": "Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b;. In addition, we build a scientific knowledge graph integrating terms and relations extracted from each article.", "cited_ids": [{"paper_id": "10822819", "citation": "(Luan et al., 2017b;"}], "y": "The authors experiments [comparing their unified model, a multi task model], show that the unified model shows that it is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b. The authors build a scientific knowledge graph integrating terms and relations extracted from each article.", "snippet_surface": "The authors' experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b. In addition, they build a scientific knowledge graph integrating terms and relations extracted from each article.", "questions": {"E9iDqkb1SF": "What is \"the unified model\"?", "qbwt9u+haK": "What is the authors' experiment?"}, "answers": {"E9iDqkb1SF": "A multi task setup to make predictions.", "qbwt9u+haK": "Comparing the model with various baselines"}, "evidence": {"E9iDqkb1SF": [{"section": "Introduction", "paragraph": "In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b;Gupta and Manning, 2011;Tsai et al., 2013;G\u00e1bor et al., 2018) which often addresses these tasks as independent components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017;. Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations.", "selected": "multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links", "paper_id": "52118895"}], "qbwt9u+haK": [{"section": "Experimental Setup", "paragraph": "We evaluate our unified framework SCIIE on SCI-ERC and SemEval 17. The knowledge graph for With Coref. Without Coref. Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. Linking entities through coreference helps disambiguate phrases when generating the knowledge graph. scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total).", "selected": "We evaluate our unified framework SCIIE on SCI-ERC and SemEval 17. The knowledge graph for With Coref. Without Coref. Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. Linking entities through coreference helps disambiguate phrases when generating the knowledge graph. scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total).", "paper_id": "52118895"}]}}
{"idx": "354670", "paper_id": "16212021", "title": "Neural Automatic Post-Editing Using Prior Alignment and Reranking", "abstract": "We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a first-stage MT system. Our APE system (APE_Sym) is an extended version of an attention based NMT model with bilingual symmetry employing bidirectional models, mt\u2013pe and pe\u2013mt. APE translations produced by our system show statistically significant improvements over the first-stage MT, phrase-based APE and the best reported score on the WMT 2016 APE dataset by a previous neural APE system. Re-ranking (APE_Rerank) of the n-best translations from the phrase-based APE and APE_Sym systems provides further substantial improvements over the symmetric neural APE model. Human evaluation confirms that the APE_Rerank generated PE translations improve on the previous best neural APE system at WMT 2016.", "context_section_header": "", "context_paragraph": "In this paper we present a neural network based APE system to improve raw first-stage MT output quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt \u2192 pe and pe \u2192 mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the other systems.", "sentence": "Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach.", "cited_ids": [{"paper_id": "1964946", "citation": "Cohn et al. (2016)"}], "y": "Unlike in previous work, the authors employed prior alignment computed by a hybrid multi-alignment approach [an approach to align a monolingual mt-pe parallel corpus based on previous alignments using three different statistical methods].", "snippet_surface": "Unlike Cohn et al. (2016), the authors employed prior alignment computed by a hybrid multi-alignment approach.", "questions": {"Ha42v211sq": "What is the \"hybrid multi-alignment approach\"?"}, "answers": {"Ha42v211sq": "An approach to align a monolingual mt-pe parallel corpus based on previous alignments using three different statistical methods."}, "evidence": {"Ha42v211sq": [{"section": "Introduction", "paragraph": "The main contributions of our research are (i) an application of bilingual symmetry of the bidirectional RNN for APE, (ii) using a hybrid multialignment based approach for the prior alignments, (iii) a smart way of embedding word alignment information in neural APE, and (iv) applying reranking for the APE task.", "selected": "using a hybrid multialignment based approach for the prior alignments", "paper_id": "16212021"}, {"section": "Hybrid Prior Alignment", "paragraph": "The monolingual mt-pe parallel corpus is first word aligned using a hybrid word alignment method based on the alignment combination of three different statistical word alignment methods: (i) GIZA++ (Och, 2003) word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Sza\u0142, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013;Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (sw id ). Each mt-pe word alignment also gets a unique identification number (a id ) and a vector representation is generated for each such a id . Given a sw id , the neural APE model is trained to generate a corresponding a id based on the context sw id appears in. The APE words are generated from a id by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by sw id and a id , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (sw id , a id ) pair.", "selected": "The monolingual mt-pe parallel corpus is first word aligned using a hybrid word alignment method based on the alignment combination of three different statistical word alignment methods:", "paper_id": "16212021"}]}}
{"idx": "356941", "paper_id": "7426857", "title": "Common Space Embedding of Primal-Dual Relation Semantic Spaces", "abstract": "Explicit continuous vector representation such as vector representation of words, phrases, etc. has been proven effective for various NLP tasks. This paper proposes a novel method of constructing such vector representation for both entity-pairs and relation expressions which link them in text. Based on the insight of the duality of relations, the representation is constructed by embedding of two separately constructed semantic spaces, one for entity-pairs and the other for relation expressions, into a common semantic space. By representing the two different types of objects (i.e. entity-pairs and relation expressions) in the same semantic space, we can treat the two tasks, relation mining and relation expression mining (a.k.a. pattern mining), systematically and in a unified manner. The approach is the first attempt to construct a continuous vector representation for expressions whose validity can be explicitly checked by their proximities to known sets of entity-pairs. We also experimentally validate the effectiveness of the common space for relation mining and relation expression mining.", "context_section_header": "", "context_paragraph": "The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given relation. We first construct two separate semantic spaces, one for pairs of named entities and another for relation expressions in text which link an entity-pair. A relation is supposed to correspond to a subset in each of these two spaces. The subset of entity-pairs is a set of pairs between which the relation holds. The subset is called the extension set of the relation. The subset of relation expressions consists a set of expressions which are used to link entity-pairs in the extension set.", "sentence": "While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given relation.", "cited_ids": [{"paper_id": "1646061", "citation": "Bollegala et al. (2010)"}], "y": "While Bollegala et al. (2010) uses duality [a binary relation where two sets define relations of interest (there is a set of conditions that the pair satisfies)] in the co-clustering algorithm, the authors construct an explicit semantic space which reflects the two aspects of a given relation [two semantic spaces; one aspect is for pairs of named entities and another for relation expressions linking entity-pair] of a given relation.", "snippet_surface": "While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, the authors construct an explicit semantic space which reflects the two aspects of a given relation.", "questions": {"1siyt+Wu2U": "What does duality refer to?", "2Xj1kd6V6h": "What are the two aspects?"}, "answers": {"1siyt+Wu2U": "Duality refers to a binary relation where two sets define relations of interest (there is a set of conditions that the pair satisfies).", "2Xj1kd6V6h": "The aspects refer to two semantic spaces; one aspect is for pairs of named entities and another for relation expressions linking entity-pair."}, "evidence": {"1siyt+Wu2U": [{"section": "Duality of Relation and a Common Space", "paragraph": "A binary relation is defined either extensionally by a set of pairs in the relation or intensionally by a set of conditions which a pair in the relation should satisfy. However, in actual applications of text mining, either of these definitions is given in a complete form. We are only given a subset of the whole set of pairs and have to complete the set (i.e. relation mining). Instead of an explicit intensional definition, we only have a set of observations in text where pairs in a relation are linked by certain linguistic expressions. Based on such observations, we have to judge whether a given pair holds the relation or not. Though some observed expressions are non-ambiguous and explicit for a relation (for example, \"the birth place of A is B\"), most of expressions are not (such as\"A comes from B\"). We call a set of pairs which define a relation as Extension set of a relation, while we call their observed expressions in text as Manifestation set. While these two sets are only partially given, they define relations which we are interested in. Such duality of a relation has been recognized by many previous work and has been exploited in relation mining and relation expression mining. (Bollegala et al., 2010), for example, used the duality in their work on co-clustering of entity-pairs and relation expressions. (Baroni and Lenci, 2010) presented a more general approach which defines a tensor associating a triplet < e 1 , l, e 2 > with a weight. e 1 and e 2 are entity pairs, while l is a linking expression in text. By projecting the tensor to matrices, they showed that diverse concepts used in distributional semantics could be captured in a unified manner. In particular, their tensors capture directly the duality of entity pairs and their linking expressions (i.e. relation expressions).", "selected": "A binary relation is defined either extensionally by a set of pairs in the relation or intensionally by a set of conditions which a pair in the relation should satisfy. However, in actual applications of text mining, either of these definitions is given in a complete form. We are only given a subset of the whole set of pairs and have to complete the set (i.e. relation mining). Instead of an explicit intensional definition, we only have a set of observations in text where pairs in a relation are linked by certain linguistic expressions. Based on such observations, we have to judge whether a given pair holds the relation or not. Though some observed expressions are non-ambiguous and explicit for a relation (for example, \"the birth place of A is B\"), most of expressions are not (such as\"A comes from B\"). We call a set of pairs which define a relation as Extension set of a relation, while we call their observed expressions in text as Manifestation set. While these two sets are only partially given, they define relations which we are interested in. Such duality of a relation has been recognized by many previous work and has been exploited in relation mining and relation expression mining. (Bollegala et al., 2010), for example, used the duality in their work on co-clustering of entity-pairs and relation expressions. (Baroni and Lenci, 2010) presented a more general approach which defines a tensor associating a triplet < e 1 , l, e 2 > with a weight. e 1 and e 2 are entity pairs, while l is a linking expression in text. By projecting the tensor to matrices, they showed that diverse concepts used in distributional semantics could be captured in a unified manner. In particular, their tensors capture directly the duality of entity pairs and their linking expressions (i.e. relation expressions).", "paper_id": "7426857"}], "2Xj1kd6V6h": [{"section": "Introduction", "paragraph": "The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given relation. We first construct two separate semantic spaces, one for pairs of named entities and another for relation expressions in text which link an entity-pair. A relation is supposed to correspond to a subset in each of these two spaces. The subset of entity-pairs is a set of pairs between which the relation holds. The subset is called the extension set of the relation. The subset of relation expressions consists a set of expressions which are used to link entity-pairs in the extension set.", "selected": "We first construct two separate semantic spaces, one for pairs of named entities and another for relation expressions in text which link an entity-pair.", "paper_id": "7426857"}]}}
{"idx": "361274", "paper_id": "218502538", "title": "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction", "abstract": "Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.", "context_section_header": "", "context_paragraph": "In this paper, we propose a hill-climbing approach to unsupervised sentence summarization, directly extracting words from the source sentence. This is motivated by the observation that humanwritten reference summaries exhibit high word overlap with the source sentence, even preserving word order to a large extent. To perform word extraction for summarization, we define a scoring function -similar to Miao et al. (2019) and Zhou and Rush (2019) -that evaluates the quality of a candidate summary by language fluency, semantic similarity to the source, and a hard constraint on output length. We search towards our scoring function by first choice hill-climbing (FCHC), shown in Figure 1. We start from a random subset of words of the required output length. For each search step, a new candidate is sampled by randomly swapping a selected word and a non-selected word. We accept the new candidate if its score is higher than the current one. In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "sentence": "In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "cited_ids": [{"paper_id": "196201849", "citation": "(Zhou and Rush, 2019)"}], "y": "In contrast to beam search (Zhou and Rush, 2019), the authors' summary is not generated sequentially from the beginning of a sentence, [with first choice hill-climbing instead] and therefore not biased towards the first few words.", "snippet_surface": "In contrast to beam search (Zhou and Rush, 2019), the authors' summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "questions": {"KfsRVJHhM4": "How is the authors' summary being generated?"}, "answers": {"KfsRVJHhM4": "with first choice hill-climbing"}, "evidence": {"KfsRVJHhM4": [{"section": "Introduction", "paragraph": "In this paper, we propose a hill-climbing approach to unsupervised sentence summarization, directly extracting words from the source sentence. This is motivated by the observation that humanwritten reference summaries exhibit high word overlap with the source sentence, even preserving word order to a large extent. To perform word extraction for summarization, we define a scoring function -similar to Miao et al. (2019) and Zhou and Rush (2019) -that evaluates the quality of a candidate summary by language fluency, semantic similarity to the source, and a hard constraint on output length. We search towards our scoring function by first choice hill-climbing (FCHC), shown in Figure 1. We start from a random subset of words of the required output length. For each search step, a new candidate is sampled by randomly swapping a selected word and a non-selected word. We accept the new candidate if its score is higher than the current one. In contrast to beam search (Zhou and Rush, 2019), our summary is not generated sequentially from the beginning of a sentence, and therefore not biased towards the first few words.", "selected": "We search towards our scoring function by first choice hill-climbing (FCHC), shown in Figure 1. We start from a random subset of words of the required output length. For each search step, a new candidate is sampled by randomly swapping a selected word and a non-selected word. We accept the new candidate if its score is higher than the current one.", "paper_id": "218502538"}]}}
{"idx": "3717", "paper_id": "141120", "title": "Sentence Simplification by Monolingual Machine Translation", "abstract": "In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems.", "context_section_header": "", "context_paragraph": "We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases. Our approach differs from Coster and Kauchak (2011) in the sense that instead of focusing on deletion in the PBMT decoding stage, we focus on dissimilarity, as simplification does not necessarily imply shortening (Woodsend and Lapata, 2011), or as the Simple Wikipedia guidelines state, \"simpler does not mean short\" 1 .  (Zhu et al., 2010). These numbers suggest that, although the selection criteria for sentences to be included in this dataset are biased (see Section 2.2), Simple Wikipedia sentences are about 17% shorter, while the average word length is virtually equal. Statistical machine translation (SMT) has already been successfully applied to the related task of paraphrasing (Quirk et al., 2004;Bannard and Callison-Burch, 2005;Madnani et al., 2007;Callison-Burch, 2008;Zhao et al., 2009;Wubben et al., 2010). SMT typically makes use of large parallel corpora to train a model on. These corpora need to be aligned at the sentence level. Large parallel corpora, such as the multilingual proceedings of the European Parliament (Europarl), are readily available for many languages. Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words (\"phrases\") in one go, solving part of the word ordering problem along the way that would be left to the target language model in a word-based SMT system. PMBT operates purely on statistics and no linguistic knowledge is involved in the process: the phrases that are aligned are motivated statistically, rather than linguistically. This makes PBMT adaptable to any language pair for which there is a parallel corpus available. The PBMT model makes use of a translation model, derived from the parallel corpus, and a language model, derived from a monolingual corpus in the target language. The language model is typically an n-gram model with smoothing. For any given input sentence, a search is carried out producing an n-best list of candidate translations, ranked by the decoder score, a complex scoring function including likelihood scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, pro-vided that a parallel corpus is available that pairs text to simplified versions of that text.", "sentence": "We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases.", "cited_ids": [{"paper_id": "15636533", "citation": "Zhu et al. (2010)"}], "y": "The authors differ from the approach of Zhu et al. (2010) in the sense that they do not take syntactic information into account; they rely on PBMT [Phrase-Based Machine Translation] to do its work and implicitly learn simplifying paraphrasings of phrases.", "snippet_surface": "The authors differ from the approach of Zhu et al. (2010) in the sense that they do not take syntactic information into account; they rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases.", "questions": {"7BXOEEN6AM": "What does pbmt stand for?"}, "answers": {"7BXOEEN6AM": "PBMT - Phrase-Based Machine Translation"}, "evidence": {"7BXOEEN6AM": [{"section": "Related work", "paragraph": "We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases. Our approach differs from Coster and Kauchak (2011) in the sense that instead of focusing on deletion in the PBMT decoding stage, we focus on dissimilarity, as simplification does not necessarily imply shortening (Woodsend and Lapata, 2011), or as the Simple Wikipedia guidelines state, \"simpler does not mean short\" 1 .  (Zhu et al., 2010). These numbers suggest that, although the selection criteria for sentences to be included in this dataset are biased (see Section 2.2), Simple Wikipedia sentences are about 17% shorter, while the average word length is virtually equal. Statistical machine translation (SMT) has already been successfully applied to the related task of paraphrasing (Quirk et al., 2004;Bannard and Callison-Burch, 2005;Madnani et al., 2007;Callison-Burch, 2008;Zhao et al., 2009;Wubben et al., 2010). SMT typically makes use of large parallel corpora to train a model on. These corpora need to be aligned at the sentence level. Large parallel corpora, such as the multilingual proceedings of the European Parliament (Europarl), are readily available for many languages. Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words (\"phrases\") in one go, solving part of the word ordering problem along the way that would be left to the target language model in a word-based SMT system. PMBT operates purely on statistics and no linguistic knowledge is involved in the process: the phrases that are aligned are motivated statistically, rather than linguistically. This makes PBMT adaptable to any language pair for which there is a parallel corpus available. The PBMT model makes use of a translation model, derived from the parallel corpus, and a language model, derived from a monolingual corpus in the target language. The language model is typically an n-gram model with smoothing. For any given input sentence, a search is carried out producing an n-best list of candidate translations, ranked by the decoder score, a complex scoring function including likelihood scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, pro-vided that a parallel corpus is available that pairs text to simplified versions of that text.", "selected": "Phrase-Based Machine Translation (PBMT)", "paper_id": "141120"}, {"section": "Related work", "paragraph": "We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntactic information into account; we rely on PBMT to do its work and implicitly learn simplifying paraphrasings of phrases. Our approach differs from Coster and Kauchak (2011) in the sense that instead of focusing on deletion in the PBMT decoding stage, we focus on dissimilarity, as simplification does not necessarily imply shortening (Woodsend and Lapata, 2011), or as the Simple Wikipedia guidelines state, \"simpler does not mean short\" 1 .  (Zhu et al., 2010). These numbers suggest that, although the selection criteria for sentences to be included in this dataset are biased (see Section 2.2), Simple Wikipedia sentences are about 17% shorter, while the average word length is virtually equal. Statistical machine translation (SMT) has already been successfully applied to the related task of paraphrasing (Quirk et al., 2004;Bannard and Callison-Burch, 2005;Madnani et al., 2007;Callison-Burch, 2008;Zhao et al., 2009;Wubben et al., 2010). SMT typically makes use of large parallel corpora to train a model on. These corpora need to be aligned at the sentence level. Large parallel corpora, such as the multilingual proceedings of the European Parliament (Europarl), are readily available for many languages. Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words (\"phrases\") in one go, solving part of the word ordering problem along the way that would be left to the target language model in a word-based SMT system. PMBT operates purely on statistics and no linguistic knowledge is involved in the process: the phrases that are aligned are motivated statistically, rather than linguistically. This makes PBMT adaptable to any language pair for which there is a parallel corpus available. The PBMT model makes use of a translation model, derived from the parallel corpus, and a language model, derived from a monolingual corpus in the target language. The language model is typically an n-gram model with smoothing. For any given input sentence, a search is carried out producing an n-best list of candidate translations, ranked by the decoder score, a complex scoring function including likelihood scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, pro-vided that a parallel corpus is available that pairs text to simplified versions of that text.", "selected": "Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words (\"phrases\") in one go, solving part of the word ordering problem along the way that would be left to the target language model in a word-based SMT system", "paper_id": "141120"}]}}
{"idx": "372943", "paper_id": "5050185", "title": "Cross Language Text Classification by Model Translation and Semi-Supervised Learning", "abstract": "In this paper, we introduce a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semi-supervised learning, and adapt the translated model to better fit the data distribution of the target language.", "context_section_header": "", "context_paragraph": "In this paper, we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm. Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features. The translated model serves as an initial classifier for a semi-supervised process, by which the model is further adjusted to fit the distribution of the target language. Our method does not require any labeled data in the target language, nor a machine translation system. Instead, the only requirement is a reasonable amount of unlabeled data in the target language, which is often easy to obtain.", "sentence": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features.", "cited_ids": [{"paper_id": "1876524", "citation": "(Fortuna and Shawe-Taylor, 2005)"}], "y": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), the authors' method [to automatically build text classifiers in a new language by training on already labeled data in another language] takes into account different possible translations for [individual words from a bag-of-words feature functions rather than just leveraging the single \"best\" translation].", "snippet_surface": "Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), the authors' method takes into account different possible translations for model features.", "questions": {"VxCqlAwvTe": "What is the authors' method?", "Jo741C5ZBf": "What are \"different possible translations\"?", "VKOykl3VB7": "What model features are being refered?"}, "answers": {"VxCqlAwvTe": "The author's method refers to the main contribution of this paper: a method that automatically builds text classifiers in a new language by training on already labeled data in another language.", "Jo741C5ZBf": "Since there could be multiple possible translations of text (and some could be erroneous), the authors propose that their model doesn't just leverage the single \"best\" translation but rather takes into account alternative possible translations.", "VKOykl3VB7": "The model features referred to here are individual words from a bag-of-words feature function."}, "evidence": {"VxCqlAwvTe": [{"section": "Abstract", "paragraph": "In this paper, we introduce a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word. We further exploit the readily available unlabeled data in the target language via semi-supervised learning, and adapt the translated model to better fit the data distribution of the target language.", "selected": "a method that automatically builds text classifiers in a new language by training on already labeled data in another language. Our method transfers the classification knowledge across languages by translating the model features and by using an Expectation Maximization (EM) algorithm that naturally takes into account the ambiguity associated with the translation of a word", "paper_id": "5050185"}, {"section": "Introduction", "paragraph": "In this paper, we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm. Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features. The translated model serves as an initial classifier for a semi-supervised process, by which the model is further adjusted to fit the distribution of the target language. Our method does not require any labeled data in the target language, nor a machine translation system. Instead, the only requirement is a reasonable amount of unlabeled data in the target language, which is often easy to obtain.", "selected": "we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm", "paper_id": "5050185"}], "Jo741C5ZBf": [{"section": "Introduction", "paragraph": "First, most off-the-shelf machine translation systems typically generate only their best translation for a given text. Since machine translation is known to be a notoriously hard problem, applying monolingual text classification algorithms directly on the erroneous translation of training or test data may severely deteriorate the classification accuracy.", "selected": "rroneous translation of training or test", "paper_id": "5050185"}, {"section": "Introduction", "paragraph": "First, most off-the-shelf machine translation systems typically generate only their best translation for a given text. Since machine translation is known to be a notoriously hard problem, applying monolingual text classification algorithms directly on the erroneous translation of training or test data may severely deteriorate the classification accuracy.", "selected": "most off-the-shelf machine translation systems typically generate only their best translation for a given text", "paper_id": "5050185"}, {"section": "Introduction", "paragraph": "In this paper, we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm. Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features. The translated model serves as an initial classifier for a semi-supervised process, by which the model is further adjusted to fit the distribution of the target language. Our method does not require any labeled data in the target language, nor a machine translation system. Instead, the only requirement is a reasonable amount of unlabeled data in the target language, which is often easy to obtain.", "selected": "our method takes into account dif-ferent possible translations for model features", "paper_id": "5050185"}, {"section": "Introduction", "paragraph": "In this paper, we propose a new approach to CLTC, which trains a classification model in the source language and ports the model to the target language, with the translation knowledge learned using the EM algorithm. Unlike previous methods based on machine translation (Fortuna and Shawe-Taylor, 2005), our method takes into account dif-ferent possible translations for model features. The translated model serves as an initial classifier for a semi-supervised process, by which the model is further adjusted to fit the distribution of the target language. Our method does not require any labeled data in the target language, nor a machine translation system. Instead, the only requirement is a reasonable amount of unlabeled data in the target language, which is often easy to obtain.", "selected": "the model is further adjusted to fit the distribution of the target language.", "paper_id": "5050185"}], "VKOykl3VB7": [{"section": "Model Translation", "paragraph": "In order to classify documents in the target language, a straightforward approach to transferring the classification model learned from the labeled source language training data is to translate each feature from the bag-of-words model according to the bilingual lexicon. However, because of the translation ambiguity of each word, a model in the source language could be potentially translated into many different models in the target language. Thus, we think of the probability of the class of a target language document as the mixture of the probabilities by each translated model from the source language model, weighed by their translation probabilities.", "selected": "In order to classify documents in the target language, a straightforward approach to transferring the classification model learned from the labeled source language training data is to translate each feature from the bag-of-words model according to the bilingual lexicon. However, because of the translation ambiguity of each word, a model in the source language could be potentially translated into many different models in the target language. Thus, we think of the probability of the class of a target language document as the mixture of the probabilities by each translated model from the source language model, weighed by their translation probabilities.", "paper_id": "5050185"}, {"section": "Model Translation", "paragraph": "To transfer a model learned in one language to another, we can translate all the bag-of-word features according to a bilingual lexicon. Due to the translation ambiguity of each feature word, we compare three different ways of model translation. One method is to equally assign probabilities to all the translations for a given source language word, and to translate a word we randomly pick a translation from all of its translation candidates. We denote this as \"EQUAL\" and it is our baseline method. Another way is to calculate the translation probability based on the frequencies of the translation words in the target language itself. For instance, the English word \"bush\" can be translated into \"\u5e03\u4ec0\" , \"\u6811\u4e1b\" or \"\u5957 \u7ba1\" . We can obtain the following unigram counts of these translation words in our Yahoo! RSS news corpus. We can estimate that P (\u5e03\u4ec0|bush) = 582/(582 + 43+2) = 92.8% and so forth. This method often allows us to estimate reasonable translation probabilities and we use \"UNIGRAM\" to denote this method.", "selected": "To transfer a model learned in one language to another, we can translate all the bag-of-word features according to a bilingual lexicon.", "paper_id": "5050185"}]}}
{"idx": "373206", "paper_id": "18253563", "title": "Large-scale Word Alignment Using Soft Dependency Cohesion Constraints", "abstract": "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.", "context_section_header": "", "context_paragraph": "There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a \"loosely\" tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So 2006a)    out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and they trained the model with only 100 hand-annotated sentence pairs. Therefore, their method is not suitable for large-scale tasks. In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "sentence": "In this paper, we also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), we integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "cited_ids": [{"paper_id": "2787289", "citation": "Cherry and Lin (2006b)"}], "y": "The authors also use dependency cohesion, [which is the assumption that disjoint dependency subtrees remain disjoint after translation,] as a soft constraint. But, unlike Cherry and Lin (2006b), they integrate the soft dependency cohesion constraint into a [HMM word alignment] generative model that is more suitable for large-scale word alignment tasks", "snippet_surface": "The authors also use dependency cohesion as a soft constraint. But, unlike Cherry and Lin (2006b), they integrate the soft dependency cohesion constraint into a generative model that is more suitable for large-scale word alignment tasks.", "questions": {"bAE/RJPE1x": "What is \"dependency cohesion?\""}, "answers": {"bAE/RJPE1x": "Dependency cohesion is the assumption that disjoint dependency subtrees remain disjoint after translation."}, "evidence": {"bAE/RJPE1x": [{"section": "Abstract", "paragraph": "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.", "selected": "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language.", "paper_id": "18253563"}]}}
{"idx": "374871", "paper_id": "14171478", "title": "A Graph-Based Approach to String Regeneration", "abstract": "The string regeneration problem is the problem of generating a fluent sentence from a bag of words. We explore the Ngram language model approach to string regeneration. The approach computes the highest probability permutation of the input bag of words under an N-gram language model. We describe a graph-based approach for finding the optimal permutation. The evaluation of the approach on a number of datasets yielded promising results, which were confirmed by conducting a manual evaluation study.", "context_section_header": "", "context_paragraph": "In contrast to the baseline N-gram approach by Wan et al. (2009), our approach finds optimal solutions. We built several models based on 2-gram, 3-gram, and 4-gram language models. We experimentally evaluated the graph-based approach on several datasets. The BLEU scores and example output indicated that our approach is successful in constructing a fairly fluent version of the original sentence. We confirmed the results of automatic evaluation by conducting a manual evaluation. The human judges were asked to compare the outputs of two systems and decide which is more fluent. The results are statistically significant and confirm the ranking of the systems obtained using the BLEU scores. Additionally, we explored computing approximate solutions with time constraints. We found that approximate solutions significantly decrease the quality of the output compared to optimal ones. This paper describes work conducted in the MPhil thesis by Horvat (2013).", "sentence": "In contrast to the baseline N-gram approach by Wan et al. (2009), our approach finds optimal solutions.", "cited_ids": [{"paper_id": "10752264", "citation": "Wan et al. (2009)"}], "y": "In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach[, which is a graph-based approach to evaluate the highest probability permutation from a bag of words,] finds optimal solutions.In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach finds optimal solutions [for N-gram language modeling].", "snippet_surface": "In contrast to the baseline N-gram approach by Wan et al. (2009), the authors' approach finds optimal solutions.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "MY84SpRi/N": "What problem does the authors' approach find solutions for?"}, "answers": {"6gKwRw0I/Q": "Authors approach to tackle the problem is called the graph-based approach to evaluate the highest probability permutation from a bag of words.", "MY84SpRi/N": "Previous research on N-gram language model shows that N-gram language can be improved with different approaches. Starting from this point, the authors argue that N-gram language model has not been explored sufficiently. It has been trained with a limited number of words."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "The string regeneration problem is the problem of generating a fluent sentence from a bag of words. We explore the Ngram language model approach to string regeneration. The approach computes the highest probability permutation of the input bag of words under an N-gram language model. We describe a graph-based approach for finding the optimal permutation. The evaluation of the approach on a number of datasets yielded promising results, which were confirmed by conducting a manual evaluation study.", "selected": "We describe a graph-based approach for finding the optimal permutation.", "paper_id": "14171478"}], "MY84SpRi/N": [{"section": "Introduction", "paragraph": "The string regeneration can also be viewed as a natural language realization problem. The basic task of all realization approaches is to take a meaning representation as input and generate humanreadable output. The approaches differ on how much information is required from the meaning representation, ranging from semantically annotated dependency graphs to shallow syntactic dependency trees. A simple bag of words can then be considered as the least constrained input provided to a natural language realization system. The bag of words can be combined with partial constraints to form a more realistic meaning representation. Wan et al. (2009) proposed an algorithm for grammaticality improvement based on dependency spanning trees and evaluated it on the string regeneration task. They compared its performance against a baseline N-gram language model generator. They found that their approach performs better with regards to BLEU score. The latter approach does well at a local level but nonetheless often produces ungrammatical sentences.", "selected": "The latter approach does well at a local level but nonetheless often produces ungrammatical sentences.", "paper_id": "14171478"}, {"section": "Introduction", "paragraph": "We argue that the authors have not fully explored the N-gram language model approach to string regeneration. They used a Viterbi-like generator with a 4-gram language model and beam pruning to find approximate solutions. Additionally, the 4-gram language model was trained on a relatively small dataset of around 20 million words.", "selected": "the 4-gram language model was trained on a relatively small dataset of around 20 million words", "paper_id": "14171478"}, {"section": "Introduction", "paragraph": "We argue that the authors have not fully explored the N-gram language model approach to string regeneration. They used a Viterbi-like generator with a 4-gram language model and beam pruning to find approximate solutions. Additionally, the 4-gram language model was trained on a relatively small dataset of around 20 million words.", "selected": "We argue that the authors have not fully explored the N-gram language model approach to string regeneration.", "paper_id": "14171478"}, {"section": "Introduction", "paragraph": "In contrast to the baseline N-gram approach by Wan et al. (2009), our approach finds optimal solutions. We built several models based on 2-gram, 3-gram, and 4-gram language models. We experimentally evaluated the graph-based approach on several datasets. The BLEU scores and example output indicated that our approach is successful in constructing a fairly fluent version of the original sentence. We confirmed the results of automatic evaluation by conducting a manual evaluation. The human judges were asked to compare the outputs of two systems and decide which is more fluent. The results are statistically significant and confirm the ranking of the systems obtained using the BLEU scores. Additionally, we explored computing approximate solutions with time constraints. We found that approximate solutions significantly decrease the quality of the output compared to optimal ones. This paper describes work conducted in the MPhil thesis by Horvat (2013).", "selected": "The BLEU scores and example output indicated that our approach is successful in constructing a fairly fluent version of the original sentence", "paper_id": "14171478"}]}}
{"idx": "375620", "paper_id": "237421077", "title": "Semi-Automated Labeling of Requirement Datasets for Relation Extraction", "abstract": "Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.", "context_section_header": "", "context_paragraph": "2 Related Work Gamallo et al. (2012) propose a simple Open Information Extraction system based on dependency parse trees. The algorithm extracts triples with two arguments and a sentence part relating those. However, the patterns are not very sophisticated and put a large part of the sentence into the relation. Hence, this approach is not suitable for our use case as we would eventually like to generate object diagrams from the relations we extracted. Erkan et al. (2007) use dependency parse trees to extract relations between proteins from sentences. They do so by classifying whether a sentence, given a dependency tree, describes a relation between any pair of proteins occurring in the sentence using semi-supervised harmonic functions and support vector machines. However, their entities (the protein names) are already annotated which is not the case if we only have the raw sentences as in our approach. Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike our approach which does not require to annotate any data manually but instead to produce patterns. While this approach might be able to extract simple triples well, one needs either a larger annotated dataset, defeating the purpose of our work, or the patterns might not generalize well, thus being unsuitable for constructing a qualitative annotated corpus. Reddy et al. (2016) propose an algorithm to automatically extract logical expressions from dependency parse trees for question answering. These were then converted into a graph indicating the relations between the named entities in the sentence by applying semantic parsing. However, this approach always converts the entire sentence into a graph and may include information that is irrelevant for a dataset that is to be generated. Inago et al. (2019) use a rule-based approach on dependency trees to process natural language car parking instructions with decision trees for automated driving systems. Unlike our data (or most datasets in general), sentences of the application domain are very short and similar in structure. While our approach could be effectively converted into a decision tree, it is easier to construct rules with our pattern engine for more complex data.", "sentence": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike our approach which does not require to annotate any data manually but instead to produce patterns.", "cited_ids": [{"paper_id": "74065", "citation": "Mausam et al. (2012)"}], "y": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset [ from the ClueWeb 3 corpus] to automatically generate patterns for information extraction, unlike the authors' approach which does not require any data to be annotated manually but instead to produce [sequences of triples].", "snippet_surface": "Mausam et al. (2012) use dependency trees and a labeled bootstrap dataset to automatically generate patterns for information extraction, unlike the authors' approach which does not require any data to be annotated manually but instead to produce patterns.", "questions": {"DzUjTjHJqL": "What patterns are being produced?", "cPu6gU/lNk": "What dataset is being referred to?"}, "answers": {"DzUjTjHJqL": "Sequences of triples.", "cPu6gU/lNk": "The dataset is a bootstrapped dataset from the ClueWeb 3 corpus. They start with a seed set of 110,000 relation tuples extracted using REVERB for relations between proper nouns and then retrieve all sentences that contain all the words in the tuples to increase the size of their dataset."}, "evidence": {"DzUjTjHJqL": [{"section": "Labeling", "paragraph": "These final 2,093 sentences (1,628 functional, 465 non-functional requirements) are parsed to extract dependencies using the Neural Adobe-UCSD Parser (Mrini et al., 2020) which achieved state-ofthe-art performance on the Penn Treebank dataset (Marcus et al., 1993). Based on these dependencies, we handcraft a total of 102 patterns to label 91.03% of the functional and 78.71% of the non-functional sentences without any further human interaction. Each pattern is a sequence of triples (l, dp, c) where l is a label, dp a sequence of dependency labels forming a path downwards a dependency tree and c a Boolean value indicating whether all children (direct and indirect) should be left out from labeling or not. Each sequence applies all or a subset of the following entity tags to the sentences:", "selected": "Based on these dependencies, we handcraft a total of 102 patterns to label 91.03% of the functional and 78.71% of the non-functional sentences without any further human interaction. Each pattern is a sequence of triples (l, dp, c) where l is a label, dp a sequence of dependency labels forming a path downwards a dependency tree and c a Boolean value indicating whether all children (direct and indirect) should be left out from labeling or not.", "paper_id": "237421077"}], "cPu6gU/lNk": [{"section": "Constructing a Bootstrapping Set", "paragraph": "Our goal is to automatically create a large training set, which encapsulates the multitudes of ways in which information is expressed in text. The key observation is that almost every relation can also be expressed via a REVERB-style verb-based expression. So, bootstrapping sentences based on REVERB's tuples will likely capture all relation expressions. We start with over 110,000 seed tuples -these are high confidence REVERB extractions from a large Web corpus (ClueWeb) 3 that are asserted at least twice and contain only proper nouns in the arguments. These restrictions reduce ambiguity while still covering a broad range of relations. For example, a seed tuple may be (Paul Annacone; is the coach of; Federer) that REVERB extracts from the sentence \"Paul Annacone is the coach of Federer.\"", "selected": "Our goal is to automatically create a large training set, which encapsulates the multitudes of ways in which information is expressed in text. The key observation is that almost every relation can also be expressed via a REVERB-style verb-based expression. So, bootstrapping sentences based on REVERB's tuples will likely capture all relation expressions. We start with over 110,000 seed tuples -these are high confidence REVERB extractions from a large Web corpus (ClueWeb) 3 that are asserted at least twice and contain only proper nouns in the arguments. These restrictions reduce ambiguity while still covering a broad range of relations. For example, a seed tuple may be (Paul Annacone; is the coach of; Federer) that REVERB extracts from the sentence \"Paul Annacone is the coach of Federer.\"", "paper_id": "74065"}, {"section": "Constructing a Bootstrapping Set", "paragraph": "For each seed tuple, we retrieve all sentences in a Web corpus that contains all content words in the tuple. We obtain a total of 18 million sentences. For our example, we will retrieve all sentences that contain 'Federer', 'Paul', 'Annacone' and some syntactic variation of 'coach'. We may find sentences like \"Now coached by Annacone, Federer is winning more titles than ever.\"", "selected": "For each seed tuple, we retrieve all sentences in a Web corpus that contains all content words in the tuple. We obtain a total of 18 million sentences. For our example, we will retrieve all sentences that contain 'Federer', 'Paul', 'Annacone' and some syntactic variation of 'coach'. We may find sentences like \"Now coached by Annacone, Federer is winning more titles than ever.\"", "paper_id": "74065"}]}}
{"idx": "376367", "paper_id": "17078659", "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "abstract": "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.", "context_section_header": "", "context_paragraph": "Research on topic adaptation most closely related to our work was performed by (Hasler et al., 2014), but the features proposed there were added to the log-linear model of a phrase-based system. Here, we use the topic information as part of the input to the NMT system. Another difference is that we primarily work with human-labeled topics, whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "sentence": "Another difference is that we primarily work with human-labeled topics, whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "cited_ids": [{"paper_id": "7161937", "citation": "(Hasler et al., 2014)"}], "y": "A difference is that the authors primarily work with human-labeled topics [product categories in an e-commerce setting], whereas in (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "snippet_surface": "Another difference is that the authors primarily work with human-labeled topics, whereas (Hasler et al., 2014) the topic distribution is inferred automatically from data.", "questions": {"VmNtd+K1uC": "What do the topics describe?"}, "answers": {"VmNtd+K1uC": "The topics represent different product categories for the machine translation task in an e-commerce setting."}, "evidence": {"VmNtd+K1uC": [{"section": "Topic-aware Machine Translation", "paragraph": "In the e-commerce domain, the information on the product category (e.g., \"men's clothing\", \"mobile phones\", \"kitchen appliances\") often accompanies the product title and description and can be used as an additional source of information both in the training of a MT system and during translation.", "selected": "In the e-commerce domain, the information on the product category (e.g., \"men's clothing\", \"mobile phones\", \"kitchen appliances\") often accompanies the product title and description and can be used as an additional source of information both in the training of a MT system and during translation.", "paper_id": "17078659"}, {"section": "Topic-aware Machine Translation", "paragraph": "Here, we propose to feed such meta-information into the recurrent neural network to help generate words which are appropriate given a particular category or topic.", "selected": "Here, we propose to feed such meta-information into the recurrent neural network to help generate words which are appropriate given a particular category or topic.", "paper_id": "17078659"}]}}
{"idx": "386656", "paper_id": "16263338", "title": "Enriching SMT Training Data via Paraphrasing", "abstract": "This paper proposes a novel method to resolve the coverage problem of SMT system. The method generates paraphrases for source-side sentences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. Within a statistical paraphrase generation framework, we employ an object function, named Sentence Novelty, to select paraphrases which having the most novel information to the bilingual training corpus of the SMT model. Meanwhile, the context is considered via a language model in the source language to ensure the fluency and accuracy of paraphrase substitution. Compared to a state-of-the-art phrase based SMT system (Moses), our method achieves an improvement of 1.66 points in terms of BLEU on a small training corpus which simulates a resource-poor environment, and 1.06 points on a training corpus of medium size.", "context_section_header": "", "context_paragraph": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen-tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "sentence": "Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "cited_ids": [{"paper_id": "791881", "citation": "Callison-Burch et al. (2006)"}], "y": "The authors' results [resolve the coverage problem of SMT system] indicate that their method [a statistical paraphrase generating (SPG) model] gains a signficant improvement [in terms of BLEU] over the method of Callison-Burch et al. (2006).", "snippet_surface": "The authors' results indicate that their method gains a significant improvement over the method of Callison-Burch et al. (2006).", "questions": {"BKAswZ+98V": "What are the authors' results?", "2X/Xc8m48l": "What is \"their model\"?", "rKIxXN+DsQ": "What type of improvement is being gained?"}, "answers": {"BKAswZ+98V": "The authors find a new method to resolve the coverage problem of SMT system which improves on previous methods.", "2X/Xc8m48l": "Their model is a statistical paraphrase generating (SPG) model.", "rKIxXN+DsQ": "The new method gained improvement in terms of BLEU."}, "evidence": {"BKAswZ+98V": [{"section": "Abstract", "paragraph": "This paper proposes a novel method to resolve the coverage problem of SMT system. The method generates paraphrases for source-side sentences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. Within a statistical paraphrase generation framework, we employ an object function, named Sentence Novelty, to select paraphrases which having the most novel information to the bilingual training corpus of the SMT model. Meanwhile, the context is considered via a language model in the source language to ensure the fluency and accuracy of paraphrase substitution. Compared to a state-of-the-art phrase based SMT system (Moses), our method achieves an improvement of 1.66 points in terms of BLEU on a small training corpus which simulates a resource-poor environment, and 1.06 points on a training corpus of medium size.", "selected": "This paper proposes a novel method to resolve the coverage problem of SMT system.", "paper_id": "16263338"}, {"section": "Introduction", "paragraph": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen-tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "selected": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model.", "paper_id": "16263338"}], "2X/Xc8m48l": [{"section": "Introduction", "paragraph": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen-tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "selected": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model.", "paper_id": "16263338"}], "rKIxXN+DsQ": [{"section": "Introduction", "paragraph": "In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen-tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006).", "selected": "Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size.", "paper_id": "16263338"}]}}
{"idx": "391591", "paper_id": "227231569", "title": "Improving Low-Resource NMT through Relevance Based Linguistic Features Incorporation", "abstract": "In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.", "context_section_header": "", "context_paragraph": "The above point motivates us to conduct the present research. We hypothesize that only including the features alongside the word by using a generalized embedding layer or forming a composite representation by taking all features together does not exploit the features completely. There should be some mechanism which can justify the relationship between a word and its supporting features as well. Driven by this idea, we come up with two simple strategies which measure the relevance of the word and the feature embeddings obtained from the output of the embedding layers. The first one is self relevance which considers the relevance of a feature with respect to itself. We apply an attention function to the feature embedding, which in turn generates a mask determining the importance of that feature. Finally, the mask is applied on the feature to effectuate the attention. Our second approach considers the feature relevance with respect to the corresponding word which is the most vital component of a source token. In this case, the attention function operates on a word-feature pair and returns the mask determining the word-based relevance of the input feature. For experimentation, we choose the Transformer network of Vaswani et al. (2017) and assess our proposed techniques on eight low-resource language pairs having diverse morphological variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of  and Vaswani et al. (2017). In the next section, the related works are briefly described.", "sentence": "We achieve up to 3.09 BLEU points gain over the standard baseline models of  and Vaswani et al. (2017).", "cited_ids": [{"paper_id": "13756489", "citation": "Vaswani et al. (2017)"}], "y": "[Using two methods for translation tasks, namely self relevance and word-based relevance,] the authors achieve up to 3.09 BLEU points gain over the [transformer models used in previous papers] of and Vaswani et al. (2017).", "snippet_surface": "The authors achieve up to 3.09 BLEU points gain over the standard baseline models of and Vaswani et al. (2017).", "questions": {"n4Sjaq8lO/": "What are the standard baseline models?"}, "answers": {"n4Sjaq8lO/": "Transformer models used in previous papers."}, "evidence": {"n4Sjaq8lO/": [{"section": "Conclusion", "paragraph": "In this article we revisit the ways to incorporate linguistic features in NMT. We argue that it is important to check the relevance of the features instead of just plugging them into the model. To establish our claim two novel methods are proposed and evaluated under extremely low-resource condition on eight language pairs. We design word-dependent as well as word-agnostic relevance checking mechanisms and show that by controlling the effects of features we can obtain substantial improvement in translation quality. Our models yield significantly higher BLEU scores compared to the baselines with a modest increase in the number of model parameters. Additionally we observe lower validation perplexity that shows applying feature relevance helps to reduce prediction uncertainty. The methods are further analyzed by visualization of the relevance weights. In case of the word-based relevance, the feature embeddings are tuned similarly based on the semantics of the corresponding word. It indicates that the proposed models actually pay attention to morphological features leading to enriched word representation. Moreover, we assess the proposed relevance methods on RNN model with attention. The results are not as satisfactory as obtained for the Transformer model, which requires further investigation. One notable issue in the present work is that the source language has been annotated by a robust and highly accurate parser which is unlikely to exist for a low-resource language. Checking the effectiveness of relevance methods is necessary when the annotation is noisy. The future extension of the present work will also focus to exploit the features under high resource scenario. In a resource rich setting usually features are redundant as the model learns from a large variety of context. Hence, using them effectively in that case will be beneficial for NMT research.", "selected": "Our models yield significantly higher BLEU scores compared to the baselines with a modest increase in the number of model parameters.", "paper_id": "227231569"}]}}
{"idx": "391894", "paper_id": "218613715", "title": "BioMRC: A Dataset for Biomedical Machine Reading Comprehension", "abstract": "We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.", "context_section_header": "", "context_paragraph": "In this paper, we introduce BIOMRC, a new dataset for biomedical MRC that can be viewed as an improved version of BIOREAD. To avoid crossing sections, extracting text from references, captions, tables etc., we use abstracts and titles of biomedical articles as passages and questions, respectively, which are clearly marked up in PUBMED data, instead of using the full text of the articles. Using titles and abstracts is a decision that favors precision over recall. Titles are likely to be related to their abstracts, which reduces the noise-tosignal ratio significantly and makes it less likely to generate irrelevant questions for a passage. We replace a biomedical entity in each title with a placeholder, and we require systems to guess the hidden entity by considering the entities of the abstract as candidate answers. Unlike BIOREAD, we use PUBTATOR (Wei et al., 2012), a repository that provides approximately 25 million abstracts and their corresponding titles from PUBMED, with multiple annotations. 2 We use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013). We also perform several checks, discussed below, to discard passage-question instances that are too easy, and we show that the accuracy of experts and nonexpert humans reaches 85% and 82%, respectively, on a sample of 30 instances for each annotator type, which is an indication that the new dataset is indeed less noisy, or at least that the task is more feasible for humans. Following Pappas et al. (2018), we release two versions of BIOMRC, LARGE and LITE, containing 812k and 100k instances respectively, for researchers with more or fewer resources, along with the 60 instances (TINY) humans answered. Random samples from BIOMRC LARGE where selected to create LITE and TINY. BIOMRC TINY is used only as a test set; it has no training and validation subsets.", "sentence": "2 We use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013).", "cited_ids": [{"paper_id": "18987749", "citation": "(Leaman et al., 2013)"}], "y": "The authors use DNORM's [Disease Name Normalization] biomedical entity annotations, which are more accurate than METAMAP's [Metathesaurus Mapping] (Leaman et al., 2013).", "snippet_surface": "The authors use DNORM's biomedical entity annotations, which are more accurate than METAMAP's (Leaman et al., 2013).", "questions": {"to+EJKIw8L": "What does dnorm stand for?"}, "answers": {"to+EJKIw8L": "DNORM stands for Disease Name Normalisation"}, "evidence": {"to+EJKIw8L": [{"section": "Table 5 :", "paragraph": "DNorm: disease name normalization with pairwise learning to rank. Robert Leaman, Zhiyong Rezarta Islamaj Dogan, Lu, Bioinformatics. 2922Robert Leaman, Rezarta Islamaj Dogan, and Zhiy- ong Lu. 2013. DNorm: disease name normaliza- tion with pairwise learning to rank. Bioinformatics, 29(22):2909-2917.", "selected": "DNorm: disease name normalization with pairwise learning to rank. Robert Leaman, Zhiyong Rezarta Islamaj Dogan, Lu, Bioinformatics. 2922Robert Leaman, Rezarta Islamaj Dogan, and Zhiy- ong Lu. 2013. DNorm: disease name normaliza- tion with pairwise learning to rank. Bioinformatics, 29(22):2909-2917.", "paper_id": "218613715"}]}}
{"idx": "400200", "paper_id": "7925642", "title": "Collaborative Partitioning for Coreference Resolution", "abstract": "This paper presents a collaborative partitioning algorithm\u2014a novel ensemble-based approach to coreference resolution. Starting from the all-singleton partition, we search for a solution close to the ensemble\u2019s outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the ensemble and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).", "context_section_header": "", "context_paragraph": "Although coreference is formally a partitioning problem, the setting is rather different from a typical clustering scenario. Thus, individual mentions and mention properties are very important for coreference and should carefully be assessed one by one. The resolution clues are very heterogeneous and different elements (mentions) of clusters (entities) can be rather dissimilar in a strict sense. This is why, for example, clustering evaluation measures are not reliable for coreferenceand, indeed, task-specific metrics have been put forward. While algorithms of (Strehl and Ghosh, 2003) constitute the state of the art in the ensemble clustering in general, we propose a coreferencespecific approach. More specifically, (i) while Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, we explicitly integrate coreference metrics, such as MUC and MELA and (ii) since our partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task. In our future work, we plan to evaluate our approach against the general-purpose algorithms proposed in (Strehl and Ghosh, 2003).", "sentence": "More specifically, (i) while Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, we explicitly integrate coreference metrics, such as MUC and MELA and (ii) since our partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task.", "cited_ids": [{"paper_id": "3068944", "citation": "Strehl and Ghosh (2003)"}], "y": "(i) While Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, the authors explicitly integrate coreference metrics, such as MUC and MELA [average of MUC and two other coreference metrics, B3 and CEAFE] and (ii) Since the authors' partitions are much smaller than typical clustering outputs, we can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task [creating an ensemble-based approach to coreference resolution].", "snippet_surface": "More specifically, (i) Strehl and Ghosh (2003) rely on task-agnostic measures of similarity between partitions (mutual information), approximating the search for its maximum with various heuristics, whereas the authors explicitly integrate coreference metrics, such as MUC and MELA and (ii) since the authors' partitions are much smaller than typical clustering outputs, they can afford a less greedy agglomerative search strategy, again, motivated by the specifics of the final task.", "questions": {"qgCE5KHOB2": "What are \"coreference metrics?\"", "rhIZjSQ4Ds": "What does \"MELA\" stand for?", "PIvG9eoPyD": "What is being partitioned?", "+IFlTXQZdd": "What is the final task?"}, "answers": {"qgCE5KHOB2": "They are task-specifc scores that are selected across a selected number of shared tasks that act as a similarity basis for classifiers", "rhIZjSQ4Ds": "MELA is the average of three commonly accepted co-reference metrics (MUC, B3 and CEAFE) that serve as similarity checkers for the CoNLL shared task.", "PIvG9eoPyD": "The results of three state-of-the-art coreference resolvers are being partitioned.", "+IFlTXQZdd": "The final task refers to creating an ensemble-based approach to coreference resolution as opposed to other individual approaches."}, "evidence": {"qgCE5KHOB2": [{"section": "Introduction", "paragraph": "The present study aims at finding a partition combining the outputs of individual coreference resolvers in a collaborative way. To this end, we search the space of possible partitions, starting from the all-singleton solution and incrementally growing coreference entities, with the objective of getting a partition similar to the individual outputs. As a measure of similarity, we rely on task-specific metrics, such as, for example, MUC or MELA scores. To our knowledge, this is the first ensemble-based approach to coreference, operating directly on the partition level. While traditional ensemble techniques, such as boosting or co-training, have been successfully used for coreference resolution before, they are applicable to classification tasks and can only be used on lower levels (e.g., for classifying mention pairs). Combining partitions directly is a non-trivial problem that requires an extra modeling effort. The rest of the paper is organized as follows. In the next section, we discuss the previous ensemble-based approaches to coreference resolution. Section 3 presents our collaborative partitioning algorithm. In Section 4, we evaluate our approach on the English portion of the OntoNotes dataset. Section 5 summarizes our contributions and highlights directions for future research.", "selected": "As a measure of similarity, we rely on task-specific metrics, such as, for example, MUC or MELA scores.", "paper_id": "7925642"}], "rhIZjSQ4Ds": [{"section": "Choosing the scoring metric", "paragraph": "In our first experiment, we evaluate different ways of defining similarity between partitions. Recall that each merge is evaluated based on whether it makes the constructed partition closer to the outputs of individual components. The similarity between two partitions is assessed with a taskspecific measure. Multiple metrics have been proposed to evaluate coreference resolvers, we refer the reader to (Luo and Pradhan, 2016) for a detailed description and to (Moosavi and Strube, 2016) for a discussion of their problematic properties. In the present experiment, we assess three commonly accepted metrics, MUC, B 3 and CEAFE as well as their average, MELA, used for the official ranking of the CoNLL shared task. Table 3 summarizes the results achieved by ensembles of the top-3 CoNLL systems. The upper half of the table presents individual components, re-evaluated with the v8 scorer. The lower part presents the performance achieved by four different ensembles, varying the underlying similarity measure used for growing up the partitions. For each performance metric, we highlight the best approach with boldface.", "selected": "In the present experiment, we assess three commonly accepted metrics, MUC, B 3 and CEAFE as well as their average, MELA, used for the official ranking of the CoNLL shared task. Table 3 summarizes the results achieved by ensembles of the top-3 CoNLL systems.", "paper_id": "7925642"}], "PIvG9eoPyD": [{"section": "Abstract", "paragraph": "This paper presents a collaborative partitioning algorithm\u2014a novel ensemble-based approach to coreference resolution. Starting from the all-singleton partition, we search for a solution close to the ensemble\u2019s outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the ensemble and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).", "selected": "Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature", "paper_id": "7925642"}], "+IFlTXQZdd": [{"section": "Conclusion", "paragraph": "This paper presents collaborative partitioninga novel ensemble-based approach to coreference resolution. Starting from the all-singleton solution, we search the space of all partitions, aiming at finding the solution close to the components' partitions according to a coreference-specific metric. Our algorithm assumes a loose coupling of individual components within the ensemble, allowing for a straightforward integration of any third party coreference resolution system.", "selected": "This paper presents collaborative partitioninga novel ensemble-based approach to coreference resolution. Starting from the all-singleton solution, we search the space of all partitions, aiming at finding the solution close to the components' partitions according to a coreference-specific metric.", "paper_id": "7925642"}]}}
{"idx": "401162", "paper_id": "9286820", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.", "context_section_header": "", "context_paragraph": "Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of Gubbins and Vlachos (2013), while achieving slightly worse than the recent state-of-the-art results by Mnih and Kavukcuoglu (2013). Finally, we make the code and preprocessed data available to facilitate comparisons with future work.", "sentence": "Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "cited_ids": [{"paper_id": "15076873", "citation": "Mikolov et al. (2011)"}], "y": "The authors' results show that the dependency RNN language model proposed[, which makes use of the syntactic dependency parse of a sentence instead of considering tokens in a sentence in the order they appear,] outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "snippet_surface": "The authors' results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy.", "questions": {"nOaiB1pt/t": "What is the dependency rnn language model?"}, "answers": {"nOaiB1pt/t": "It is an RNN model that makes use of the syntactic dependency parse of a sentence instead of considering tokens in a sentence in the order they appear as in standard RNNs."}, "evidence": {"nOaiB1pt/t": [{"section": "Introduction", "paragraph": "Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published simultaneously with another model, introduced in Tai et al. (2015), who extend the Long-Short Term Memory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model.", "selected": "In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published", "paper_id": "9286820"}]}}
{"idx": "401269", "paper_id": "5747513", "title": "Semi-Supervised Bootstrapping of Relationship Extractors with Distributional Semantics", "abstract": "Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. We research bootstrapping for relationship extraction using word embeddings to find similar relationships. Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TFIDF to find similar relationships.", "context_section_header": "", "context_paragraph": "We implemented these ideas in BREDS, a bootstrapping system for RE based on word embeddings. BREDS was evaluated with a collection of 1.2 million sentences from news articles. The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "sentence": "The experimental results show that our method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "cited_ids": [{"paper_id": "7579604", "citation": "Agichtein and Gravano (2000)"}], "y": "The experimental results show that the authors method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations [to expand the seed set with new relationship instances, while limiting the semantic drift through the bootstrapping system].", "snippet_surface": "The experimental results show that the authors' method outperforms a baseline bootstrapping system based on the ideas of Agichtein and Gravano (2000) which relies on TF-IDF representations.", "questions": {"Vh0tNalV97": "What is the bootstrapping system?", "VxCqlAwvTe": "What is the authors' method?"}, "answers": {"Vh0tNalV97": "A system to expand the seed set.", "VxCqlAwvTe": "Expanding the seed set with new relationship instances, while limiting the semantic drift through the bootstrapping system."}, "evidence": {"Vh0tNalV97": [{"section": "Introduction", "paragraph": "The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships.", "selected": "The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships.", "paper_id": "5747513"}], "VxCqlAwvTe": [{"section": "Introduction", "paragraph": "The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships.", "selected": "The objective of bootstrapping is thus to expand the seed set with new relationship instances, while limiting the semantic drift, i.e. the progressive deviation of the semantics for the extracted relationships from the semantics of the seed relationships.", "paper_id": "5747513"}]}}
{"idx": "408504", "paper_id": "52979524", "title": "SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task", "abstract": "Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5% in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at https://yale-lily.github.io/seq2sql/spider.", "context_section_header": "", "context_paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "sentence": "Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module.", "cited_ids": [{"paper_id": "13529592", "citation": "(Rabinovich et al., 2017)"}], "y": "[This work first differentiates itself from previous other work by using a SQL specific grammar instead of a AST. Additionally, different sequence-to-set modules are used to avoid the \"ordering issue\". Lastly] different from (Rabinovich et al., 2017), the authors pass a pre-order traverse of SQL decoding history to each module [representing a component of the grammar].", "snippet_surface": "Third, differently from (Rabinovich et al., 2017), the authors pass a pre-order traverse of SQL decoding history to each module.", "questions": {"RFUs0cLj5x": "What is the first difference?", "2lFRRKXRjC": "What is the second difference?", "zaxzPLHEJ5": "What are the modules?"}, "answers": {"RFUs0cLj5x": "SQL specific grammar is used instead of AST.", "2lFRRKXRjC": "Different sequence-to-set modules are used to avoid \"ordering issue\".", "zaxzPLHEJ5": "One module is used for each component of the grammar."}, "evidence": {"RFUs0cLj5x": [{"section": "Related Work", "paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "selected": "Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST.", "paper_id": "52979524"}], "2lFRRKXRjC": [{"section": "Related Work", "paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "selected": "Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks.", "paper_id": "52979524"}, {"section": "Related Work", "paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "selected": "Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST.", "paper_id": "52979524"}], "zaxzPLHEJ5": [{"section": "Related Work", "paragraph": "Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017;Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specific grammar enables direct prediction of SQL tokens. Second, our model uses different sequence-to-set modules to avoid the \"or-dering issue\" (Xu et al., 2017) in many code generation tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence information: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column have appeared in SELECT too.", "selected": "our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree.", "paper_id": "52979524"}]}}
{"idx": "413936", "paper_id": "1774259", "title": "Bidirectional Recurrent Convolutional Neural Network for Relation Classification", "abstract": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neural networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset.", "context_section_header": "", "context_paragraph": "Convolutional neural works are widely used in relation classification. Zeng et al. (2014) proposed an approach for relation classification where sentence-level features are learned through a CNN, which has word embedding and position features as its input. In parallel, lexical features were extracted according to given nouns. dos Santos et al. (2015) tackled the relation classification task using a convolutional neural network and proposed a new pairwise ranking loss function, which achieved the state-of-the-art result in SemEval-2010 Task 8. Yu et al. (2014) proposed a Factor-based Compositional Embedding Model (FCM) by deriving sentence-level and substructure embeddings from word embeddings, utilizing dependency trees and named entities. It achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "sentence": "It achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "cited_ids": [{"paper_id": "12873739", "citation": "Zeng et al. (2014)"}], "y": "The authors achieved slightly higher accuracy on the same dataset than Zeng et al. (2014) [by using a bidirectional neural network model], but only when syntactic information [chunking and parse tree features] is used.", "snippet_surface": "The authors achieved slightly higher accuracy on the same dataset than Zeng et al. (2014), but only when syntactic information is used.", "questions": {"XKsGJu/3N5": "What does the \"syntatic information\" refer to?", "u4bTaIVSH2": "How do the authors' achieve higher accuracy?"}, "answers": {"XKsGJu/3N5": "Syntactic information refers to chunking and parse tree features.", "u4bTaIVSH2": "The authors achieve higher accuracy by by using a bidirectional neural network model."}, "evidence": {"XKsGJu/3N5": [{"section": "Related Work", "paragraph": "In feature-based approaches, different types of features are extracted and fed into a classifier. Generally, three types of features are often used. Lexical features concentrate on the entities of interest, e.g., POS. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class. Kambhatla (2004) used a maximum entropy model for feature combination. Rink and Harabagiu (2010) collected various features, including lexical, syntactic as well as semantic features.", "selected": "Syntactic features include chunking, parse trees, etc.", "paper_id": "1774259"}], "u4bTaIVSH2": [{"section": "Related Work", "paragraph": "Nowadays, many works concentrate on extracting features from the SDP based on neural networks. Xu et al. (2015a) learned robust relation representations from SDP through a CNN, and proposed a straightforward negative sampling strategy to improve the assignment of subjects and objects. Liu et al. (2015) proposed a recursive neural network designed to model the subtrees, and CNN to capture the most important features on the shortest dependency path. Xu et al. (2015b) picked up heterogeneous information along the left and right sub-path of the SDP respectively, leveraging recurrent neural networks with long short term memory units. We propose BRCNN to model the SDP, which can pick up bidirectional information with a combination of LSTM and CNN.", "selected": "We propose BRCNN to model the SDP, which can pick up bidirectional information with a combination of LSTM and CNN.", "paper_id": "1774259"}, {"section": "Conclusion", "paragraph": "In this paper, we proposed a novel bidirectional neural network BRCNN, to improve the performance of relation classification. The BRCNN model, consisting of two RCNNs, learns features along SDP and inversely at the same time. Information of words and dependency relations are used utilizing a two-channel recurrent neural network with LSTM units. The features of dependency units in SDP are extracted by a convolution layer.", "selected": "In this paper, we proposed a novel bidirectional neural network BRCNN, to improve the performance of relation classification.", "paper_id": "1774259"}]}}
{"idx": "414396", "paper_id": "13875176", "title": "Relation Extraction from Community Generated Question-Answer Pairs", "abstract": "Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users\u2019 interests. Traditional methods for relation extraction from natural language text operate over individual sentences. However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question. This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences. Experiments on 2 publicly available datasets demonstrate that the model can extract from 20% to 40% additional relation triples, not extracted by existing sentence-based models.", "context_section_header": "", "context_paragraph": "Community question-answering data has been a subject of active research during the last decade. Bian et al. (2008) and Shtok et al. (2012) show how such data can be used for question answering, an area with a long history of research, and numerous different approaches proposed over the decades (Kolomiyets and Moens, 2011). One particular way to answer questions is to utilize structured KBs and perform semantic parsing of questions to transform natural language questions into KB queries. Berant et al. (2013) proposed a semantic parsing model that can be trained from QnA pairs, which are much easier to obtain than correct KB queries used previ-ously. However, unlike our approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities. Later Yao and Van Durme (2014) presented an information extraction inspired approach, that predicts which of the entities related to an entity in the question could be the answer to the question.", "sentence": "However, unlike our approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities.", "cited_ids": [{"paper_id": "6401679", "citation": "Berant et al. (2013)"}], "y": "\"However, unlike the authors' approach, which takes noisy answer text [from real users] provided by a CQA website user [to predict relations between entities mentioned in the question and answer], the work of Berant et al. (2013) uses manually created answers in a form of single or lists of [knowledge base (KB)] entities [(named entities, proper nouns or a sequence of at least two tokens)].", "snippet_surface": "However, unlike the authors' approach, which takes noisy answer text provided by a CQA website user, the work of Berant et al. (2013) uses manually created answers in a form of single or lists of KB entities.", "questions": {"cW6fWWRkNr": "What is a \"cqa website\"?", "6gKwRw0I/Q": "What is the authors' approach?", "QVvHO02Prx": "What is noisy answer text?", "mNlJXL91rb": "What are \"kb entities\"?"}, "answers": {"cW6fWWRkNr": "A website containing questions and answers from reals users.", "6gKwRw0I/Q": "A model for extraction from CQA data to predict relations between entities mentioned in the question and answer sentences.", "QVvHO02Prx": "It is something that they are not expecting to have as a valid answer.", "mNlJXL91rb": "They are named entities, proper nouns or a sequence of at least two tokens (e.g. BarackObama) taken from the Freebase knowledge base. KB stands for \"Knowledge Base\"."}, "evidence": {"cW6fWWRkNr": [{"section": "Abstract", "paragraph": "Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users\u2019 interests. Traditional methods for relation extraction from natural language text operate over individual sentences. However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question. This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences. Experiments on 2 publicly available datasets demonstrate that the model can extract from 20% to 40% additional relation triples, not extracted by existing sentence-based models.", "selected": "Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users\u2019 interests", "paper_id": "13875176"}], "6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users\u2019 interests. Traditional methods for relation extraction from natural language text operate over individual sentences. However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question. This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences. Experiments on 2 publicly available datasets demonstrate that the model can extract from 20% to 40% additional relation triples, not extracted by existing sentence-based models.", "selected": "This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences.", "paper_id": "13875176"}], "QVvHO02Prx": [{"section": "Introduction", "paragraph": "We experimented with two question answering datasets on Freebase. First, on the dataset of Cai and Yates (2013), we showed that our system outperforms their state-of-the-art system 62% to 59%, despite using no annotated logical forms. Second, we collected a new realistic dataset of questions by performing a breadth-first search using the Google Suggest API; these questions are then answered by Amazon Mechanical Turk workers. Although this dataset is much more challenging and noisy, we are still able to achieve 31.4% accuracy, a 4.5% absolute improvement over a natural baseline. Both datasets as well as the source code for SEMPRE, our semantic parser, are publicly released and can be downloaded from http://nlp.stanford.edu/ software/sempre/.", "selected": "We experimented with two question answering datasets on Freebase. First, on the dataset of Cai and Yates (2013), we showed that our system outperforms their state-of-the-art system 62% to 59%, despite using no annotated logical forms. Second, we collected a new realistic dataset of questions by performing a breadth-first search using the Google Suggest API; these questions are then answered by Amazon Mechanical Turk workers. Although this dataset is much more challenging and noisy,", "paper_id": "6401679"}], "mNlJXL91rb": [{"section": "Knowledge base", "paragraph": "We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions. 1", "selected": "We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions. 1", "paper_id": "6401679"}, {"section": "Setup", "paragraph": "We used POS tagging and named-entity recognition to restrict what phrases in the utterance could be mapped by the lexicon. Entities must be named entities, proper nouns or a sequence of at least two tokens. Unaries must be a sequence of nouns, and binaries must be either a content word, or a verb followed by either a noun phrase or a particle. In addition, we used 17 hand-written rules to map question words such as \"where\" and \"how many\" to logical forms such as Type.Location and Count.", "selected": "We used POS tagging and named-entity recognition to restrict what phrases in the utterance could be mapped by the lexicon. Entities must be named entities, proper nouns or a sequence of at least two tokens.", "paper_id": "6401679"}]}}
{"idx": "41947", "paper_id": "53093808", "title": "What can we gain from language models for morphological inflection?", "abstract": "This paper investigates the attempts to augment neural-based inflection models with characterbased language models. We found that in most cases this slightly improves performance, however, the effect is marginal. We also propose another language-model based approach that can be used as a strong baseline in low-resource setting.", "context_section_header": "", "context_paragraph": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.", "sentence": "We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "cited_ids": [{"paper_id": "28704517", "citation": "(Makarov et al., 2017)"}], "y": "The authors expected to improve performance [measured by percentage points in average accuracy in the Sigmorphon 2018 shared task,] especially in low and medium resource settings [which is a 1000 word training dataset]. Their approach does not have clear advantages: their joint system[, which combines the model from Makarov et al., 2017 with a language model component following the architecture of Gulcehre,] is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "snippet_surface": "The authors expected to improve performance especially in low and medium resource settings, however, their approach does not have clear advantages: their joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages.", "questions": {"d+L547/CZx": "What is the authors' system?", "dC/JZ3hU+q": "How is the performance improved?", "W8jqfOvcq9": "What is the \"medium resource setting\"?", "b5HHvybooL": "What is \"their joint system\"?"}, "answers": {"d+L547/CZx": "A joint system enriching the previous system described by Makarov (encoder-decoder model with a copy mechanism/neural state-transition system over a set of explicit edit actions) with a language model component.", "dC/JZ3hU+q": "percentage points in average accuracy in the Sigmorphon 2018 shared task", "W8jqfOvcq9": "1000 word training dataset", "b5HHvybooL": "Combining the model from Makarov et al., 2017 with a language model component following the architecture of Gulcehre"}, "evidence": {"d+L547/CZx": [{"section": "Introduction", "paragraph": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.", "selected": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer.", "paper_id": "53093808"}, {"section": "Abstract", "paragraph": "This paper presents the submissions by the University of Zurich to the SIGMORPHON 2017 shared task on morphological reinflection. The task is to predict the inflected form given a lemma and a set of morpho-syntactic features. We focus on neural network approaches that can tackle the task in a limited-resource setting. As the transduction of the lemma into the inflected form is dominated by copying over lemma characters, we propose two recurrent neural network architectures with hard monotonic attention that are strong at copying and, yet, substantially different in how they achieve this. The first approach is an encoder-decoder model with a copy mechanism. The second approach is a neural state-transition system over a set of explicit edit actions, including a designated COPY action. We experiment with character alignment and find that naive, greedy alignment consistently produces strong results for some languages. Our best system combination is the overall winner of the SIGMORPHON 2017 Shared Task 1 without external resources. At a setting with 100 training samples, both our approaches, as ensembles of models, outperform the next best competitor.", "selected": "The first approach is an encoder-decoder model with a copy mechanism. The second approach is a neural state-transition system over a set of explicit edit actions, including a designated COPY action. We experiment with character alignment and find that naive, greedy alignment consistently produces strong results for some languages. Our best system combination is the overall winner of the SIGMORPHON 2017 Shared Task 1 without external resources.", "paper_id": "28704517"}], "dC/JZ3hU+q": [{"section": "Dataset", "paragraph": "We tested our model in Sigmorphon 2018 Shared Task (Cotterell et al., 2018). For an extended description we refer the reader to this papers. The dataset contained three subsets: high, medium and low. The size of the training dataset was 10000 words in the high subset 4 , 1000 in medium and 100 in low. The dataset also contained a development set containing 1000 instances most of the time, for all languages we used this subset as validation data. Overall, there were 86 languages in the high setting, 102 in medium and 103 in low.", "selected": "We tested our model in Sigmorphon 2018 Shared Task", "paper_id": "53093808"}, {"section": "Results and Discussion", "paragraph": "At the low setting, both the HACM and HAEM ensembles (Run 2 and Run 4) outperform the next best competitor (LMU-02-0 with 46.59%) by 0.23 and 1.94 percentage points in average accuracy. The margin between Run 7 and the next best system is an impressive 4.02 percentage points.", "selected": "At the low setting, both the HACM and HAEM ensembles (Run 2 and Run 4) outperform the next best competitor (LMU-02-0 with 46.59%) by 0.23 and 1.94 percentage points in average accuracy. The margin between Run 7 and the next best system is an impressive 4.02 percentage points.", "paper_id": "28704517"}], "W8jqfOvcq9": [{"section": "Dataset", "paragraph": "We tested our model in Sigmorphon 2018 Shared Task (Cotterell et al., 2018). For an extended description we refer the reader to this papers. The dataset contained three subsets: high, medium and low. The size of the training dataset was 10000 words in the high subset 4 , 1000 in medium and 100 in low. The dataset also contained a development set containing 1000 instances most of the time, for all languages we used this subset as validation data. Overall, there were 86 languages in the high setting, 102 in medium and 103 in low.", "selected": "The dataset contained three subsets: high, medium and low. The size of the training dataset was 10000 words in the high subset 4 , 1000 in medium and 100 in low.", "paper_id": "53093808"}], "b5HHvybooL": [{"section": "Introduction", "paragraph": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. We expected to improve performance especially in low and medium resource setting, however, our approach does not have clear advantages: our joint system is only slightly ahead the baseline system of (Makarov et al., 2017) for most of the languages. We conclude that the language model job is already executed by the decoder. However, given the vitality of language model approach in other areas of modern NLP (Peters et al., 2018), we describe our attempts in detail to give other researchers the ideas for future work in this direction.", "selected": "Summarizing, our approach was to enrich the model of (Makarov et al., 2017) with the language model component. We followed the architecture of (Gulcehre et al., 2017), whose approach is simply to concatenate the state of the neural decoder with the state of the neural language model before passing it to the output projection layer.", "paper_id": "53093808"}]}}
{"idx": "425823", "paper_id": "29306617", "title": "Coarse \u201csplit and lump\u201d bilingual language models for richer source information in SMT", "abstract": "Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these \u201ccoarse models\u201d. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing \u201cgeneric\u201d coarse configuration chosen on English > French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English > French, +0.35 for French > English, +1.0 for Arabic > English, and +0.6 for Chinese > English.", "context_section_header": "", "context_paragraph": "This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM -a coarse reordering model -none of the Eng<>Fre experiments do. In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data. Many experiments in this paper involve coarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013;Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one.", "sentence": "In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data.", "cited_ids": [{"paper_id": "18721941", "citation": "Wuebker et al (2013)"}], "y": "The authors also studied coarse phrase translation models in prior experiments [that used discriminative heirarchical RMs], but found that, unlike Wuebker et al (2013), they did not yield significant improvements to the [machine translation] system, except when there is little training data.", "snippet_surface": "The authors also studied coarse phrase translation models in prior experiments, but found that, unlike Wuebker et al (2013), they did not yield significant improvements to the system, except when there is little training data.", "questions": {"I2ml0EFqpE": "What were the previous experiments?", "8uE7lRJmPT": "What does \"the system\" refer to?"}, "answers": {"I2ml0EFqpE": "Prior experiment here refers to work done with discriminative heirarchical RMs", "8uE7lRJmPT": "The system refers to a tool used in statistical natural language processing for improving machine translations."}, "evidence": {"I2ml0EFqpE": [{"section": "Related work", "paragraph": "This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM -a coarse reordering model -none of the Eng<>Fre experiments do. In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data. Many experiments in this paper involve coarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013;Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one.", "selected": "Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls", "paper_id": "29306617"}], "8uE7lRJmPT": [{"section": "Abstract", "paragraph": "Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these \u201ccoarse models\u201d. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing \u201cgeneric\u201d coarse configuration chosen on English > French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English > French, +0.35 for French > English, +1.0 for Arabic > English, and +0.6 for Chinese > English.", "selected": "there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality", "paper_id": "29306617"}, {"section": "Related work", "paragraph": "This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM -a coarse reordering model -none of the Eng<>Fre experiments do. In prior experiments, we also studied coarse phrase translation models, but unlike Wuebker et al (2013), we found they did not yield significant improvements to our system, except when there is little training data. Many experiments in this paper involve coarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013;Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one.", "selected": "szkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models.", "paper_id": "29306617"}, {"section": "Abstract", "paragraph": "Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German!English and a larger French!German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French!German task and 0.3% BLEU and 1.1% TER on the German!English task.", "selected": "We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models.", "paper_id": "18721941"}]}}
{"idx": "434565", "paper_id": "11553116", "title": "A Seed-driven Bottom-up Machine Learning Framework for Extracting Relations of Various Complexity", "abstract": "A minimally supervised machine learning framework is described for extracting relations of various complexity. Bootstrapping starts from a small set of n-ary relation instances as \u201cseeds\u201d, in order to automatically learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Performance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition approaches.", "context_section_header": "", "context_paragraph": "We propose a rule representation that supports this strategy. Therefore, our learning approach is seed-driven and bottom-up. Here we use dependency trees as input for pattern extraction. We consider only trees or their subtrees containing seed arguments. Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account. Our pattern rule ranking and filtering method considers two aspects of a pattern: its domain relevance and the trustworthiness of its origin. We tested our framework in two domains: Nobel Prize awards and management succession. Evaluations have been conducted to investigate the performance with respect to the seed parameters: the number of seeds and the influence of data size and its redundancy property. The whole system has been evaluated for the two domains considering precision and recall. We utilize the evaluation strategy \"Ideal Matrix\" of Agichtein and Gravano (2000) to deal with unannotated test data.", "sentence": "Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "cited_ids": [{"paper_id": "16749388", "citation": "Sudo et al., (2003)"}], "y": "Therefore, the authors' method, [a seed-driven machine learning approach that uses dependency trees as input for relation extraction], is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "snippet_surface": "Therefore, the authors' method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account.", "questions": {"Hcg1qdRdeI": "How is the authors' method more efficient?", "VxCqlAwvTe": "What is the authors' method?"}, "answers": {"Hcg1qdRdeI": "Their method is more efficient because in the other methods relation-specific argument role information is missing.", "VxCqlAwvTe": "Author's method is a seed-driven machine learning approach with bottom-up strategy using dependency trees as input for pattern extraction that is used for extracting relations of various complexity"}, "evidence": {"Hcg1qdRdeI": [{"section": "Introduction", "paragraph": "(1) describes a ternary relation referring to three properties of a prize: year, area and prize name. We also observe that the automatically acquired patterns in Riloff (1996), Yangarber (2003), Sudo et al. (2003),  cannot be directly used as relation extraction rules because the relation-specific argument role information is missing. E.g., in the management succession domain that concerns the identification of job changing events, a person can either move into a job (called Person_In) or leave a job (called Per-son_Out).", "selected": "E.g., in the management succession domain that concerns the identification of job changing events, a person can either move into a job (called Person_In) or leave a job (called Per-son_Out).", "paper_id": "11553116"}], "VxCqlAwvTe": [{"section": "Abstract", "paragraph": "A minimally supervised machine learning framework is described for extracting relations of various complexity. Bootstrapping starts from a small set of n-ary relation instances as \u201cseeds\u201d, in order to automatically learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Performance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition approaches.", "selected": "We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule construction is supported by a bottom-up pattern extraction method. In comparison to other automatic approaches, our rules cannot only localize relation arguments but also assign their exact target argument roles.", "paper_id": "11553116"}, {"section": "Introduction", "paragraph": "We propose a rule representation that supports this strategy. Therefore, our learning approach is seed-driven and bottom-up. Here we use dependency trees as input for pattern extraction. We consider only trees or their subtrees containing seed arguments. Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account. Our pattern rule ranking and filtering method considers two aspects of a pattern: its domain relevance and the trustworthiness of its origin. We tested our framework in two domains: Nobel Prize awards and management succession. Evaluations have been conducted to investigate the performance with respect to the seed parameters: the number of seeds and the influence of data size and its redundancy property. The whole system has been evaluated for the two domains considering precision and recall. We utilize the evaluation strategy \"Ideal Matrix\" of Agichtein and Gravano (2000) to deal with unannotated test data.", "selected": "We propose a rule representation that supports this strategy. Therefore, our learning approach is seed-driven and bottom-up. Here we use dependency trees as input for pattern extraction. We consider only trees or their subtrees containing seed arguments.", "paper_id": "11553116"}]}}
{"idx": "43855", "paper_id": "243865635", "title": "A Language Model-based Generative Classifier for Sentence-level Discourse Parsing", "abstract": "Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing.", "context_section_header": "", "context_paragraph": "As for the base parser, we retrained two models, 2-stage Parser 5 (Wang et al., 2017) and span-based Parser 6 (Kobayashi et al., 2020). Different from the setting of Lin et al. (2019), we retrained 2stage Parser in the sentence-level rather than in the document-level. Since the experimental re-4 https://github.com/PKU-TANGENT/ NeuralEDUSeg 5 https://github.com/yizhongw/StageDP 6 https://github.com/nttcslab-nlp/ Top-Down-RST-Parser sults show our retrained 2-stage Parser achieved the highest F 1 scores among several parsers (See Appendix C), we selected it as our base parser in the following experiments.", "sentence": "Different from the setting of Lin et al. (2019), we retrained 2stage Parser in the sentence-level rather than in the document-level.", "cited_ids": [{"paper_id": "153312413", "citation": "Lin et al. (2019)"}], "y": "Different from the setting of Lin et al. (2019) the authors retrained 2stage Parser in the sentence-level rather than in the document-level. [This parser being a model for sentence-level discourse parsing, which extracts several features and does classification with SVMs in two stages.]", "snippet_surface": "Different from the setting of Lin et al. (2019), the authors retrained 2stage Parser in the sentence-level rather than in the document-level.", "questions": {"vzjxvR4EB5": "What is \"2stage Parser?\""}, "answers": {"vzjxvR4EB5": "2-stage Parser is a model for sentence-level discourse parsing, which extracts several features and does classification with SVMs in two stages."}, "evidence": {"vzjxvR4EB5": [{"section": "Discourse Parser", "paragraph": "In discourse parsing, given an input text x and its EDUs e, we can build a binary tree p = {p 1 , \u00b7 \u00b7 \u00b7 , p 2n\u22121 }, where each node p i \u2208 p has three kinds of labels: span s i , nuclearity u i , and relation r i . The sequences of span s and nuclearity u can be predicted simultaneously, as in 2-stage Parser (Wang et al., 2017), or span s can be predicted in advance for labeling nuclearity u and relation r, as in pointer-networks (Lin et al., 2019) and span-based Parser (Kobayashi et al., 2020). Because of its better performance, we choose 2stage Parser as our base model for sentence-level discourse parsing. 2-stage Parser extracts several features and does classification with SVMs in two stages. In the first stage, it identifies the span and nuclearity simultaneously to construct a tree based on the transition-based system with four types of actions: Shift, Reduce-NN, Reduce-NS, and Reduce-SN. In the second stage, for a given node p i , r i is predicted as the relation between the left and right children nodes of p i by using features extracted from p i and its children nodes. In spite of its limited features, it achieves the best results compared with pointer-networks and span-based Parser. Since 2-stage Parser utilizes SVMs, we normalize the action scores and inherit top-k beam search results of 2-stage Parser for LMGC to perform discourse parsing.", "selected": "The sequences of span s and nuclearity u can be predicted simultaneously, as in 2-stage Parser", "paper_id": "243865635"}, {"section": "Discourse Parser", "paragraph": "In discourse parsing, given an input text x and its EDUs e, we can build a binary tree p = {p 1 , \u00b7 \u00b7 \u00b7 , p 2n\u22121 }, where each node p i \u2208 p has three kinds of labels: span s i , nuclearity u i , and relation r i . The sequences of span s and nuclearity u can be predicted simultaneously, as in 2-stage Parser (Wang et al., 2017), or span s can be predicted in advance for labeling nuclearity u and relation r, as in pointer-networks (Lin et al., 2019) and span-based Parser (Kobayashi et al., 2020). Because of its better performance, we choose 2stage Parser as our base model for sentence-level discourse parsing. 2-stage Parser extracts several features and does classification with SVMs in two stages. In the first stage, it identifies the span and nuclearity simultaneously to construct a tree based on the transition-based system with four types of actions: Shift, Reduce-NN, Reduce-NS, and Reduce-SN. In the second stage, for a given node p i , r i is predicted as the relation between the left and right children nodes of p i by using features extracted from p i and its children nodes. In spite of its limited features, it achieves the best results compared with pointer-networks and span-based Parser. Since 2-stage Parser utilizes SVMs, we normalize the action scores and inherit top-k beam search results of 2-stage Parser for LMGC to perform discourse parsing.", "selected": "2-stage Parser extracts several features and does classification with SVMs in two stages.", "paper_id": "243865635"}, {"section": "Discourse Parser", "paragraph": "In discourse parsing, given an input text x and its EDUs e, we can build a binary tree p = {p 1 , \u00b7 \u00b7 \u00b7 , p 2n\u22121 }, where each node p i \u2208 p has three kinds of labels: span s i , nuclearity u i , and relation r i . The sequences of span s and nuclearity u can be predicted simultaneously, as in 2-stage Parser (Wang et al., 2017), or span s can be predicted in advance for labeling nuclearity u and relation r, as in pointer-networks (Lin et al., 2019) and span-based Parser (Kobayashi et al., 2020). Because of its better performance, we choose 2stage Parser as our base model for sentence-level discourse parsing. 2-stage Parser extracts several features and does classification with SVMs in two stages. In the first stage, it identifies the span and nuclearity simultaneously to construct a tree based on the transition-based system with four types of actions: Shift, Reduce-NN, Reduce-NS, and Reduce-SN. In the second stage, for a given node p i , r i is predicted as the relation between the left and right children nodes of p i by using features extracted from p i and its children nodes. In spite of its limited features, it achieves the best results compared with pointer-networks and span-based Parser. Since 2-stage Parser utilizes SVMs, we normalize the action scores and inherit top-k beam search results of 2-stage Parser for LMGC to perform discourse parsing.", "selected": "Since 2-stage Parser utilizes SVMs, we normalize the action scores and inherit top-k beam search results of 2-stage Parser for LMGC to perform discourse parsing", "paper_id": "243865635"}]}}
{"idx": "44656", "paper_id": "6397366", "title": "Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars", "abstract": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "context_section_header": "", "context_paragraph": "It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "sentence": "When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "cited_ids": [{"paper_id": "14829769", "citation": "(Smith and Eisner, 2004)"}], "y": "When applied to unsupervised grammar learning, [deterministic annealing] has been shown to lead to worse parsing accuracy than [the standard expectation-maximization algorithm] (Smith and Eisner, 2004); in contrast, the authors show that their approach, [unambiguity regularization for unsupervised learning of probabilistic natural language grammars, which introduces an inductive bias into grammar learning based on the observation that natural language is remarkably unambiguous], leads to significantly higher parsing accuracy than standard [expectation-maximization] in unsupervised dependency grammar learning.", "snippet_surface": "When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, the authors show that their approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "questions": {"Imjldu29pR": "What does da stand for?", "TfYUiQM8X/": "What does em stand for?", "JVehNQtHtV": "What is the author's approach?"}, "answers": {"Imjldu29pR": "DA means deterministic annealing", "TfYUiQM8X/": "EM stands for expectation-maximization algorithm", "JVehNQtHtV": "The approach is named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. It is based on the observation that natural language is remarkably unambiguous and an inductive bias into grammar learning is introduced."}, "evidence": {"Imjldu29pR": [{"section": "Introduction", "paragraph": "It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998). However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5. When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM (Smith and Eisner, 2004); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "selected": "It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature (Rose, 1998).", "paper_id": "6397366"}], "TfYUiQM8X/": [{"section": "Abstract", "paragraph": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "selected": "expectation-maximization algorithm (EM)", "paper_id": "6397366"}], "JVehNQtHtV": [{"section": "Abstract", "paragraph": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "selected": "We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences.", "paper_id": "6397366"}, {"section": "Introduction", "paragraph": "Against this background, we propose the use of a novel type of prior information for unsupervised learning of probabilistic natural language grammars, namely the syntactic unambiguity of natural language. Although it is often possible to correctly parse a natural language sentence in more than one way, natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is rather small in comparison with the total number of possible parses. Thus, we incorporate into learning an inductive bias in favor of grammars that lead to unambiguous parses on natural language sentences, by using the posterior regularization framework . We name this approach unambiguity regularization. The resulting family of algorithms includes standard EM and Viterbi EM, as well as an algorithm that falls between standard EM and Viterbi EM which we call softmax-EM. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. The fact that Viterbi EM is a special case of our approach also gives an explanation of the advantage of Viterbi EM observed in previous work: it is because Viterbi EM implicitly utilizes unambiguity regularization. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "selected": "we propose the use of a novel type of prior information for unsupervised learning of probabilistic natural language grammars, namely the syntactic unambiguity of natural language.", "paper_id": "6397366"}, {"section": "Introduction", "paragraph": "Against this background, we propose the use of a novel type of prior information for unsupervised learning of probabilistic natural language grammars, namely the syntactic unambiguity of natural language. Although it is often possible to correctly parse a natural language sentence in more than one way, natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is rather small in comparison with the total number of possible parses. Thus, we incorporate into learning an inductive bias in favor of grammars that lead to unambiguous parses on natural language sentences, by using the posterior regularization framework . We name this approach unambiguity regularization. The resulting family of algorithms includes standard EM and Viterbi EM, as well as an algorithm that falls between standard EM and Viterbi EM which we call softmax-EM. The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM. The fact that Viterbi EM is a special case of our approach also gives an explanation of the advantage of Viterbi EM observed in previous work: it is because Viterbi EM implicitly utilizes unambiguity regularization. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "selected": "We name this approach unambiguity regularization.", "paper_id": "6397366"}]}}
{"idx": "447642", "paper_id": "381368", "title": "Generating and Validating Abstracts of Meeting Conversations: a User Study", "abstract": "In this paper we present a complete system for automatically generating natural language abstracts of meeting conversations. This system is comprised of components relating to interpretation of the meeting documents according to a meeting ontology, transformation or content selection from that source representation to a summary representation, and generation of new summary text. In a formative user study, we compare this approach to gold-standard human abstracts and extracts to gauge the usefulness of the different summary types for browsing meeting conversations. We find that our automatically generated summaries are ranked significantly higher than human-selected extracts on coherence and usability criteria. More generally, users demonstrate a strong preference for abstract-style summaries over extracts.", "context_section_header": "", "context_paragraph": "While extraction remains the most common ap-proach to text summarization, one application in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain (Portet et al., 2009) and data from gas turbine sensors (Yu et al., 2007). Our approach is similar except that our input is text data in the form of conversations. We otherwise utilize a very similar architecture of pattern recognition, pattern abstraction, pattern selection and summary generation. Kleinbauer et al. (2007) carry out topic-based meeting abstraction. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "sentence": "Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "cited_ids": [{"paper_id": "6118869", "citation": "(Carletta et al., 2005)"}], "y": "The authors' system differs [from Kleinbaur et al.'s] in two major respects: the summarization process [Kleinbaur et al.] use utilizes human gold-standard annotations of topic segments, topic labels and content items from the ontology, while the authors' summarizer is fully automatic; secondly, the ontology used by them is specific not just to meetings but to the AMI scenario meetings [from the AMI corpus where participants play roles in a fictitious company] (Carletta et al., 2005), while the authors' ontology applies to conversations in general, allowing their approach to be extended to emails, blogs, etc.", "snippet_surface": "The authors' systems differ in two major respects: the summarization process used by them utilizes human gold-standard annotations of topic segments, topic labels and content items from the ontology, while the authors' summarizer is fully automatic; secondly, the ontology used by them is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while the authors' ontology applies to conversations in general, allowing their approach to be extended to emails, blogs, etc.", "questions": {"0JAM7qAcA5": "Who does \"them\" refer to?", "3YGOD0xZWf": "What does \"the ontology\" refer to?", "Hsun41weR2": "What are \"AMI scenario meetings?\""}, "answers": {"0JAM7qAcA5": "To Kleinbauer et al. who used a similar system", "3YGOD0xZWf": "Ontology refers to some specific information describing classes and their properties", "Hsun41weR2": "It refers to a scenario in the AMI corpus where participants play roles within a fictitious company"}, "evidence": {"0JAM7qAcA5": [{"section": "Related Research", "paragraph": "While extraction remains the most common ap-proach to text summarization, one application in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain (Portet et al., 2009) and data from gas turbine sensors (Yu et al., 2007). Our approach is similar except that our input is text data in the form of conversations. We otherwise utilize a very similar architecture of pattern recognition, pattern abstraction, pattern selection and summary generation. Kleinbauer et al. (2007) carry out topic-based meeting abstraction. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "selected": "Our approach is similar except that our input is text data in the form of conversations. We otherwise utilize a very similar architecture of pattern recognition, pattern abstraction, pattern selection and summary generation. Kleinbauer et al. (2007) carry out topic-based meeting abstraction. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc.", "paper_id": "381368"}], "3YGOD0xZWf": [{"section": "Interpretation -Ontology Mapping", "paragraph": "Source document interpretation in our system relies on a general conversation ontology. The ontology is written in OWL/RDF and contains upperlevel classes such as Participant, Entity, Utterance, and DialogueAct. When additional information is available about participant roles in a given domain, Participant subclasses such as ProjectManager can be utilized. Object properties connect instances of ontology classes; for example, the following entry in the ontology states that the object property has-Speaker has an instance of Utterance as its domain and an instance of Participant as its range.", "selected": "Source document interpretation in our system relies on a general conversation ontology. The ontology is written in OWL/RDF and contains upperlevel classes such as Participant, Entity, Utterance, and DialogueAct. When additional information is available about participant roles in a given domain, Participant subclasses such as ProjectManager can be utilized. Object properties connect instances of ontology classes; for example, the following entry in the ontology states that the object property has-Speaker has an instance of Utterance as its domain and an instance of Participant as its range.", "paper_id": "381368"}], "Hsun41weR2": [{"section": "AMI Meeting Corpus", "paragraph": "For our meeting summarization experiments, we use the scenario portion of the AMI corpus (Carletta et al., 2005), where groups of four participants take part in a series of four meetings and play roles within a fictitious company. There are 140 of these meetings in total. For the summary annotation, annotators wrote abstract summaries of each meeting and extracted sentences that best conveyed or supported the information in the abstracts. The human-authored abstracts each contain a general abstract summary and three subsections for \"decisions,\" \"actions\" and \"problems\" from the meeting. A many-to-many mapping between transcript sentences and sentences from the human abstract was obtained for each annotator. Approximately 13% of the total transcript sentences are ultimately labeled as extracted sentences. A sentence is considered a decision item if it is linked to the decision portion of the abstract, and action and problem sentences are derived similarly. We additionally use subjectivity and polarity annotations for the AMI corpus (Wilson, 2008).", "selected": "For our meeting summarization experiments, we use the scenario portion of the AMI corpus (Carletta et al., 2005), where groups of four participants take part in a series of four meetings and play roles within a fictitious company. There are 140 of these meetings in total. For the summary annotation, annotators wrote abstract summaries of each meeting and extracted sentences that best conveyed or supported the information in the abstracts. The human-authored abstracts each contain a general abstract summary and three subsections for \"decisions,\" \"actions\" and \"problems\" from the meeting. A many-to-many mapping between transcript sentences and sentences from the human abstract was obtained for each annotator. Approximately 13% of the total transcript sentences are ultimately labeled as extracted sentences. A sentence is considered a decision item if it is linked to the decision portion of the abstract, and action and problem sentences are derived similarly. We additionally use subjectivity and polarity annotations for the AMI corpus (Wilson, 2008).", "paper_id": "381368"}]}}
{"idx": "448821", "paper_id": "226262372", "title": "Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots", "abstract": "Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.", "context_section_header": "", "context_paragraph": "In the third decoupling forward step, we feed e to the same BiLSTM model and calculate a new adversarial loss L . The final loss is a weighted sum of L and L controlled by a hyperparameter \u03b1 3 :   Baselines For a fair comparison, we use the same slot filling architecture BiLSTM (Liu and Lane, 2016) as (Kim et al., 2019;Ray et al., 2019). Kim et al. (2019) proposes two model variants, where random noise means adding random noise in the embeddings of all slot words and cw represents concatenating the context word window as input. Note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from our method. Our adversarial semantic decoupling method can take into account the impact of different contexts (global semantics) on local semantics, thereby enabling more accurate decoupling. Ray et al. (2019) proposes greedy delex and iterative delex methods for open-vocabulary slots. We also validate our method in the BERT-based models (Devlin et al., 2019) for comprehensive analysis.", "sentence": "Note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from our method.", "cited_ids": [{"paper_id": "186206148", "citation": "(Kim et al., 2019)"}], "y": "The authors note that the random noise [(added in the embeddings of all slot words)] in Kim et al., 2019 is independently sampled regardless of the global context, which is significantly different from the authors' [adversarial semantic decoupling method].", "snippet_surface": "The authors note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from their method.", "questions": {"Im7pRWGe/v": "What is \"their method\"?"}, "answers": {"Im7pRWGe/v": "Their method refers to an adversarial semantic decoupling method."}, "evidence": {"Im7pRWGe/v": [{"section": "Abstract", "paragraph": "Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.", "selected": "In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context.", "paper_id": "226262372"}, {"section": "Introduction", "paragraph": "In the third decoupling forward step, we feed e to the same BiLSTM model and calculate a new adversarial loss L . The final loss is a weighted sum of L and L controlled by a hyperparameter \u03b1 3 :   Baselines For a fair comparison, we use the same slot filling architecture BiLSTM (Liu and Lane, 2016) as (Kim et al., 2019;Ray et al., 2019). Kim et al. (2019) proposes two model variants, where random noise means adding random noise in the embeddings of all slot words and cw represents concatenating the context word window as input. Note that the random noise in (Kim et al., 2019) is independently sampled regardless of the global context, which is significantly different from our method. Our adversarial semantic decoupling method can take into account the impact of different contexts (global semantics) on local semantics, thereby enabling more accurate decoupling. Ray et al. (2019) proposes greedy delex and iterative delex methods for open-vocabulary slots. We also validate our method in the BERT-based models (Devlin et al., 2019) for comprehensive analysis.", "selected": "Our adversarial semantic decoupling method can take into account the impact of different contexts (global semantics) on local semantics, thereby enabling more accurate decoupling.", "paper_id": "226262372"}]}}
{"idx": "450638", "paper_id": "7017683", "title": "Function Assistant: A Tool for NL Querying of APIs", "abstract": "In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.", "context_section_header": "", "context_paragraph": "In this paper, we focus on the first usage of our tool. To explore building models for new API collections, we run our pipeline on 27 open source Python projects from the well-known Awesome Python project list. 1 As in previous work, we perform synthetic experiments on these datasets, which measure how well our models can generate function representations for unseen API descriptions, which mimic user queries. 1 github.com/vinta/awesome-python 2 Related Work Natural language querying of APIs has long been a goal in software engineering, related to the general problem of software reuse (Krueger, 1992). To date, a number of industrial scale products are available in this area. 2 To our knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from our methods that use more conventional NLP components and techniques. As we show in this paper and in RK, term matching and related techniques can sometimes serve as a competitive baseline, but are almost always outperformed by our translation approach.", "sentence": "2 To our knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from our methods that use more conventional NLP components and techniques.", "cited_ids": [{"paper_id": "15813825", "citation": "(Lv et al., 2015)"}], "y": "To the authors' knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from the authors' methods [for querying and exploring source code repositories using natural language, stochastic gradient ascent algorithm under a maximum conditional log-likelihood objective] that use more conventional NLP components and techniques.", "snippet_surface": "To the authors' knowledge, most implementations use shallow term matching and/or information-extraction techniques (Lv et al., 2015), differing from the authors' methods that use more conventional NLP components and techniques.", "questions": {"LBqWYeFPjn": "What algorithm or model is being implemented?"}, "answers": {"LBqWYeFPjn": "a lightweight Python-based toolkit for querying and exploring source code repositories using natural language, also referred to as an online, stochastic gradient ascent algorithm under a maximum conditional log-likelihood objective"}, "evidence": {"LBqWYeFPjn": [{"section": "Abstract", "paragraph": "In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.", "selected": "a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions throug", "paper_id": "7017683"}, {"section": "Discriminative Reranking", "paragraph": "Our tool implements several different training and optimization methods. For the purpose of this paper, we train our models using an online, stochastic gradient ascent algorithm under a maximum conditional log-likelihood objective.", "selected": "an online, stochastic gradient ascent algorithm under a maximum conditional log-likelihood objective.", "paper_id": "7017683"}]}}
{"idx": "454256", "paper_id": "202777021", "title": "Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders", "abstract": "Understanding text often requires identifying meaningful constituent spans such as noun phrases and verb phrases. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the model\u2019s labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19% relative error reduction).", "context_section_header": "", "context_paragraph": "Our code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing (19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people).", "sentence": "Our code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing (19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags).", "cited_ids": [{"paper_id": "5930290", "citation": "Haghighi and Klein 2006"}], "y": "The authors' code-enhanced DIORA architecture, which outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10, [a corpus comprising sentences with 10 or fewer words from the WSJ section of the English Penn treebank] when labelling a gold bracketing (19% relative error reduction over the previous best model Haghighi and Klein, 2006, which unlike the authors' approach uses gold part-of-speech tags.", "snippet_surface": "The authors' code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing, resulting in a 19% relative error reduction over the previous best model (Haghighi and Klein 2006), which unlike the authors' approach uses gold part-of-speech tags.", "questions": {"hHP4Oik+0W": "What was the previous best model?", "El2fbnJWIn": "What is \"WSJ-10\"?"}, "answers": {"hHP4Oik+0W": "The one by Haghighi and Klein", "El2fbnJWIn": "The WSJ-10 corpus contains sentences with 10 or fewer words from the WSJ section of the English Penn treebank."}, "evidence": {"hHP4Oik+0W": [{"section": "Introduction", "paragraph": "Our code-enhanced DIORA architecture outperforms DIORA and achieves a new state of the art of 76.7 F1 on WSJ-10 when labeling a gold bracketing (19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags). Furthermore, we show DIORA is competitive when a ground truth bracketing is not provided, and instead must be induced. On the full WSJ test set, DIORA outperforms two strong baselines, ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019). We analyze the clustered constituents and observe they are separated syntactically (i.e. past tense vs. present participle verbs) and semantically (i.e. time-related phrases vs. references to people).", "selected": "19% relative error reduction over the previous best model, Haghighi and Klein 2006, which unlike our approach uses gold partof-speech tags", "paper_id": "202777021"}, {"section": "Conclusions", "paragraph": "In this paper, we show that DIORA can be used for unsupervised labeled constituency parsing. We also introduce a new codebook-enhanced variant of DIORA that improves labeling performance. Our model outperforms the previous state of the art in unsupervised labeled constituency parsing for the WSJ-10 dataset, even though the previous best uses ground truth part-of-speech tags and ours does not, and introduces the first results on the full WSJ test set. The results indicate that grammar induction with types is viable using recent neuralnetwork-based models, and our analysis warrants further exploration in this area.", "selected": "the previous best uses ground truth part-of-speech tags and ours does not, and introduces the first results on the full WSJ test set", "paper_id": "202777021"}], "El2fbnJWIn": [{"section": "Experimental Setup", "paragraph": "The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank (Marcus et al., 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005). We learned models on all or part of this data and compared their predictions to the manually annotated treebank trees for the sentences on which the model was trained. As in previous work, we begin with the part-of-speech (POS) tag sequences for each sentence rather than lexical sequences (Carroll and Charniak, 1992;Klein and Manning, 2002).", "selected": "The majority of our experiments induced tree structures from the WSJ section of the English Penn treebank (Marcus et al., 1994), though see section 7.4 for an experiment on Chinese. To facilitate comparison with previous work, we extracted WSJ-10, the 7,422 sentences which contain 10 or fewer words after the removal of punctuation and null elements according to the scheme detailed in Klein (2005).", "paper_id": "5930290"}]}}
{"idx": "455133", "paper_id": "15876808", "title": "Guided Learning for Bidirectional Sequence Classification", "abstract": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.", "context_section_header": "", "context_paragraph": "Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004;Daum\u00e9 III and Marcu, 2005) for guided learning. We apply this algorithm to POS tagging, a classic sequence learning problem. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features. By using deterministic search, it obtains an error rate of 2.73%, a 5.9% relative error reduction 760 over the previous best deterministic algorithm (Tsuruoka and Tsujii, 2005).", "sentence": "Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features.", "cited_ids": [{"paper_id": "14835360", "citation": "(Toutanova et al., 2003)"}], "y": "The authors' system [a novel learning algorithm for bidirectional sequence classification] reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system [using feature-rich part-of-speech tagging with a cyclic dependency network](Toutanova et al., 2003) by using fewer features.", "snippet_surface": "The authors' system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al., 2003) by using fewer features.", "questions": {"d+L547/CZx": "What is the authors' system?"}, "answers": {"d+L547/CZx": "The author's system is 'guided learning', a novel learning algorithm for bidirectional sequence classification."}, "evidence": {"d+L547/CZx": [{"section": "Abstract", "paragraph": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.", "selected": "We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set,", "paper_id": "15876808"}, {"section": "Abstract", "paragraph": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features.", "selected": "In this paper, we propose guided learning", "paper_id": "15876808"}, {"section": "Conclusions", "paragraph": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like algorithm.", "selected": "In this paper, we propose guided learning", "paper_id": "15876808"}]}}
{"idx": "45874", "paper_id": "8692445", "title": "Learning Field Compatibilities to Extract Database Records from Unstructured Text", "abstract": "Named-entity recognition systems extract entities such as people, organizations, and locations from unstructured text. Rather than extract these mentions in isolation, this paper presents a record extraction system that assembles mentions into records (i.e. database tuples). We construct a probabilistic model of the compatibility between field values, then employ graph partitioning algorithms to cluster fields into cohesive records. We also investigate compatibility functions over sets of fields, rather than simply pairs of fields, to examine how higher representational power can impact performance. We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches.", "context_section_header": "", "context_paragraph": "McCallum and Wellner (2005) discriminatively train a model to learn binary coreference decisions, then perform joint inference using graph partitioning. This is analogous to our work, with two distinctions. First, instead of binary coreference decisions, our model makes binary compatibility decisions, reflecting whether a set of fields belong together in the same record. Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices. As we show in our experiments, factoring decisions into sets of vertices enables more powerful features that can improve performance. These higher-order features have also recently been investigated in other models of coreference, both discriminative (Culotta and McCallum, 2006) and generative (Milch et al., 2005). Viola and Narasimhan (2005) present a probabilistic grammar to parse contact information blocks. While this model is capable of learning long-distance compatibilities (such as City and State relations), features to enable this are not explored. Additionally, their work focuses on labeling fields in documents that have been presegmented into records. This record segmentation is precisely what we address in this paper. Borkar et al. (2001) and Kristjannson et al. (2004) also label contact address blocks, but ignore the problem of clustering fields into records. Also,  automatically extract contact records from web pages, but use heuristics to cluster fields into records. Embley et al. (1999) provide heuristics to detect record boundaries in highly structured web documents, such as classified ads, and Embley and Xu (2000) improve upon these heuristics for slightly more ambiguous domains using a vector space model. Both of these techniques apply to data for which the records are highly contiguous and have a distinctive separator between records. These heuristic approaches are unlikely to be successful in the unstructured text domain we address in this paper.", "sentence": "Second, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, our compatibility decisions are made between sets of vertices.", "cited_ids": [{"paper_id": "11747348", "citation": "McCallum and Wellner (2005)"}], "y": "[The authors aim to extract contact records from homepages]. McCallum and Wellner (2005) factor [what their models learn before producing results] into pairs of vertices, but their compatibility decisions [used to determine the compatibility functions over sets of fields] are made between sets of vertices.", "snippet_surface": "The authors note that, whereas McCallum and Wellner (2005) factor the coreference decisions into pairs of vertices, their compatibility decisions are made between sets of vertices.", "questions": {"Ub6P8qLHTe": "What is the authors' task?"}, "answers": {"Ub6P8qLHTe": "The authors task is extraction of contact records from homepages."}, "evidence": {"Ub6P8qLHTe": [{"section": "Abstract", "paragraph": "Named-entity recognition systems extract entities such as people, organizations, and locations from unstructured text. Rather than extract these mentions in isolation, this paper presents a record extraction system that assembles mentions into records (i.e. database tuples). We construct a probabilistic model of the compatibility between field values, then employ graph partitioning algorithms to cluster fields into cohesive records. We also investigate compatibility functions over sets of fields, rather than simply pairs of fields, to examine how higher representational power can impact performance. We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches.", "selected": "We apply our techniques to the task of extracting contact records from faculty and student homepages, demonstrating a 53% error reduction over baseline approaches.", "paper_id": "8692445"}]}}
{"idx": "470003", "paper_id": "227230509", "title": "Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction", "abstract": "Bilingual dictionary induction (BDI) is the task of accurately translating words to the target language. It is of great importance in many low-resource scenarios where cross-lingual training data is not available. To perform BDI, bilingual word embeddings (BWEs) are often used due to their low bilingual training signal requirements. They achieve high performance, but problematic cases still remain, such as the translation of rare words or named entities, which often need to be transliterated. In this paper, we enrich BWE-based BDI with transliteration information by using Bilingual Orthography Embeddings (BOEs). BOEs represent source and target language transliteration word pairs with similar vectors. A key problem in our BDI setup is to decide which information source \u2013 BWEs (or semantics) vs. BOEs (or orthography) \u2013 is more reliable for a particular word pair. We propose a novel classification-based BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on English-Russian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity.", "context_section_header": "", "context_paragraph": "In this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities. Our aim is to improve BDI systems in two respects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation. We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information. For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors. We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr. seq2seqTr can also be used to extract candidate transliterations for a source word. It is applicable to any language pair (as opposed to Levenshtein distance). To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be semantically translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017), exploiting our pretrained encoder from seq2seqTr. In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision. We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks.", "sentence": "Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set.", "cited_ids": [{"paper_id": "218594574", "citation": "Severini et al. (2020)"}], "y": "The authors show that their classification system [(seq2seqTr)] is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set [(test dictionaries released in three frequency categories: high, middle and low)].", "snippet_surface": "The authors show that their classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set.", "questions": {"oRHWI7eJKJ": "What is the authors' classification system?", "rce31Y99Ue": "What freqency set is being referred to?", "FrQvstMTk7": "What specialized tuning is referred to?"}, "answers": {"oRHWI7eJKJ": "The authors classification system is called \"seq2seqTr\".", "rce31Y99Ue": "The frequency sets are test dictionaries released in three frequency categories: high, middle and low.", "FrQvstMTk7": "The specialized tuning is from Severini et al., and done with special development sets"}, "evidence": {"oRHWI7eJKJ": [{"section": "Introduction", "paragraph": "In this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities. Our aim is to improve BDI systems in two respects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation. We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information. For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors. We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr. seq2seqTr can also be used to extract candidate transliterations for a source word. It is applicable to any language pair (as opposed to Levenshtein distance). To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be semantically translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017), exploiting our pretrained encoder from seq2seqTr. In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision. We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks.", "selected": "We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr", "paper_id": "227230509"}], "rce31Y99Ue": [{"section": "Introduction", "paragraph": "In this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities. Our aim is to improve BDI systems in two respects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation. We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information. For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors. We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr. seq2seqTr can also be used to extract candidate transliterations for a source word. It is applicable to any language pair (as opposed to Levenshtein distance). To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be semantically translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017), exploiting our pretrained encoder from seq2seqTr. In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision. We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of Severini et al. (2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks.", "selected": "Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches.", "paper_id": "227230509"}], "FrQvstMTk7": [{"section": "Experimental Setup", "paragraph": "We submitted BDI outputs for both the closed and open tracks which differ only in the used BWEs. For the closed track we only relied on the released monolingual corpora and training dictionaries. For the MWEs we used the WaCKy corpora (Baroni et al., 2009) and built word2vec cbow and skipgram models (Mikolov et al., 2013a), and fasttext cbow models (Bojanowski et al., 2017), while we used the released fasttext skipgram models from the shared task website. We used the same parameters used by the organizers for both methods: minimum word count 30; vector dimension 300; context window size 7; number of negatives sampled 10 and in addition, number of epochs 10 for fasttext. To align MWEs of the same type, we used VecMap (Artetxe et al., 2018) in a supervised setup. As the training signal we used the official shared task dictionaries which are a subset of the MUSE dictionaries released in . We split them into train, development and test sets (70%/15%/15%) 2 which we used for training BWEs and the transliteration model, tuning parameters and reporting final results respectively. Since we tuned various parameters, such as ensembling weights or threshold values for margin based translation, for each language pair and frequency category, we do not report each value here but discuss them in the following section. For the generation of BWE based similarity dictionaries we only considered the most frequent 200K words when calculating CSLS similarities as in . We experimented with larger vocabulary sizes but achieved lower scores. In contrast, for the orthography and transliteration based dictionaries we considered all words in the monolingual corpora which have at least frequency 5 3 .", "selected": "We split them into train, development and test sets (70%/15%/15%) 2 which we used for training BWEs and the transliteration model, tuning parameters and reporting final results respectively.", "paper_id": "218594574"}]}}
{"idx": "470296", "paper_id": "864605", "title": "SPred: Large-scale Harvesting of Semantic Predicates", "abstract": "We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ) and learn the semantic classes that best fit the argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner\u2019s Dictionary with high precision and recall, and perform well against the most similar approach.", "context_section_header": "", "context_paragraph": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "sentence": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "cited_ids": [{"paper_id": "743925", "citation": "Kozareva and Hovy (2010)"}], "y": "The closest technical approach to that of the authors is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns [lexical predicates like \"work for\" or \"fly to\"]. Whereas Kozareva and Hovy's approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, the authors' proposed method [using only recursive relation patterns to create large repositories of semantic predicates] requires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "snippet_surface": "The closest technical approach to that of the authors is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, the authors' proposed method requires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "pbBOOLVmCY": "What is the unbounded number?", "/WKLVNIykZ": "What is a relation pattern?"}, "answers": {"6gKwRw0I/Q": "The authors' approach use recursive relation patterns, only, to create large repositories of semantic predicates.", "pbBOOLVmCY": "The number of semantic classes that the model can learn/create.", "/WKLVNIykZ": "Relation patterns are lexical predicates (like \"work for\" or \"fly to\")."}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ) and learn the semantic classes that best fit the argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner\u2019s Dictionary with high precision and recall, and perform well against the most similar approach.", "selected": "We present SPred, a novel method for the creation of large repositories of semantic predicates.", "paper_id": "864605"}, {"section": "Related work", "paragraph": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "selected": "our proposed method re-quires only the pattern itself,", "paper_id": "864605"}, {"section": "Related work", "paragraph": "The closest technical approach to ours is that of Kozareva and Hovy (2010), who use recursive patterns to induce semantic classes for the arguments of relational patterns. Whereas their approach requires both a relation pattern and one or more seeds, which bias the types of semantic classes that are learned, our proposed method re-quires only the pattern itself, and as a result is capable of learning an unbounded number of different semantic classes.", "selected": "recursive patterns", "paper_id": "864605"}], "pbBOOLVmCY": [{"section": "Disambiguation heuristics impact", "paragraph": "Methodology. We carried out the same evaluation as in Section 4.3. We calculated precision@k of the semantic classes obtained for each relation in the dataset of K&H. Because the set of applicable classes was potentially unbounded, we were not able to report recall directly.", "selected": "Because the set of applicable classes was potentially unbounded, we were not able to report recall directly.", "paper_id": "864605"}], "/WKLVNIykZ": [{"section": "Disambiguation heuristics impact", "paragraph": "As a follow-up analysis, for each dataset we considered the impact of each disambiguation heuristic described in Section 3.2 according to how many times it was triggered. Starting from the entire set of 1,446 lexical predicates from the Oxford dictionary (see Section 4.3), we counted the number of argument triples (a, s, l) already disambiguated in Wikipedia (i.e., l = ) and those disambiguated thanks to our disambiguation strategies. Table  5 shows the statistics. We note that, while the amount of originally linked arguments is very low (about 2.5% of total), our strategies are able to considerably increase the size of the initial set of linked instances. The most effective strategies appear to be the One sense per page and the Trust the inventory, which contribute 26.16% and 31.33% of the total links, respectively. Even though most of the triples (i.e., 68 out of almost 74 million) remain unlinked, the ratio of distinct arguments which we linked to WordNet is considerably higher, calculated as 3,723,979 linked arguments over 12,431,564 distinct arguments, i.e., about 30%. 5 Experiment 2: Comparison with Kozareva & Hovy (2010) Due to the novelty of the task carried out by SPred, the resulting output may be compared with only a limited number of existing approaches. The most similar approach is that of Kozareva and Hovy (2010, K&H) who assign supertypes to the arguments of arbitrary relations, a task which resembles our semantic predicate ranking. We therefore performed a comparison on the quality of the most highly-ranked supertypes (i.e., semantic classes) using their dataset of 24 relation patterns (i.e., lexical predicates).", "selected": "Starting from the entire set of 1,446 lexical predicates from the Oxford dictionary", "paper_id": "864605"}, {"section": "Disambiguation heuristics impact", "paragraph": "As a follow-up analysis, for each dataset we considered the impact of each disambiguation heuristic described in Section 3.2 according to how many times it was triggered. Starting from the entire set of 1,446 lexical predicates from the Oxford dictionary (see Section 4.3), we counted the number of argument triples (a, s, l) already disambiguated in Wikipedia (i.e., l = ) and those disambiguated thanks to our disambiguation strategies. Table  5 shows the statistics. We note that, while the amount of originally linked arguments is very low (about 2.5% of total), our strategies are able to considerably increase the size of the initial set of linked instances. The most effective strategies appear to be the One sense per page and the Trust the inventory, which contribute 26.16% and 31.33% of the total links, respectively. Even though most of the triples (i.e., 68 out of almost 74 million) remain unlinked, the ratio of distinct arguments which we linked to WordNet is considerably higher, calculated as 3,723,979 linked arguments over 12,431,564 distinct arguments, i.e., about 30%. 5 Experiment 2: Comparison with Kozareva & Hovy (2010) Due to the novelty of the task carried out by SPred, the resulting output may be compared with only a limited number of existing approaches. The most similar approach is that of Kozareva and Hovy (2010, K&H) who assign supertypes to the arguments of arbitrary relations, a task which resembles our semantic predicate ranking. We therefore performed a comparison on the quality of the most highly-ranked supertypes (i.e., semantic classes) using their dataset of 24 relation patterns (i.e., lexical predicates).", "selected": "dataset of 24 relation patterns (i.e., lexical predicates).", "paper_id": "864605"}, {"section": "Disambiguation heuristics impact", "paragraph": "Dataset. The dataset contained 14 lexical predicates (e.g., work for * or * fly to), 10 of which were expanded in order to semantify their left-and right-side arguments (e.g., * work for and work for *); for the remaining 4 predicates just a single    Kozareva and Hovy (2010).", "selected": "lexical predicates (e.g., work for * or * fly to)", "paper_id": "864605"}]}}
{"idx": "470558", "paper_id": "1160159", "title": "Joint Inference for Knowledge Extraction from Biomedical Literature", "abstract": "Knowledge extraction from online repositories such as PubMed holds the promise of dramatically speeding up biomedical research and drug design. After initially focusing on recognizing proteins and binary interactions, the community has recently shifted their attention to the more ambitious task of recognizing complex, nested event structures. State-of-the-art systems use a pipeline architecture in which the candidate events are identified first, and subsequently the arguments. This fails to leverage joint inference among events and arguments for mutual disambiguation. Some joint approaches have been proposed, but they still lag much behind in accuracy. In this paper, we present the first joint approach for bio-event extraction that obtains state-of-the-art results. Our system is based on Markov logic and adopts a novel formulation by jointly predicting events and arguments, as well as individual dependency edges that compose the argument paths. On the BioNLP'09 Shared Task dataset, it reduced F1 errors by more than 10% compared to the previous best joint approach.", "context_section_header": "", "context_paragraph": "We also presented a heuristic method to fix errors in syntactic parsing by leveraging available semantic information from task input, and showed that this in turn led to substantial performance gain in the task. Overall, our final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "sentence": "Overall, our final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "cited_ids": [{"paper_id": "475213", "citation": "Riedel et al. (2009)"}], "y": "The authors' final system [a bio-events extraction classification system based on Markov logic] reduced F1 error by more than 10% compared to Riedel et al. (2009).", "snippet_surface": "The authors' final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "questions": {"5Qc+0PUeYh": "What is the authors' final system?"}, "answers": {"5Qc+0PUeYh": "A bio-events extraction model classification system for language that is based on Markov logic but with a new formulation that models dependency edges in argument paths and jointly predicts them along with events."}, "evidence": {"5Qc+0PUeYh": [{"section": "Introduction", "paragraph": "In this paper, we present the first joint approach that achieves state-of-the-art results for bio-event extraction. Like Riedel et al. (2009), our system is based on Markov logic, but we adopted a novel formulation that models dependency edges in argument paths and jointly predicts them along with events and arguments. By expanding the scope of joint inference to include individual argument edges, our system can leverage fine-grained correlations to make learning more effective. On the development set, by merely adding a few joint inference formulas to a simple logistic regression model, our system raised F1 from 28% to 54%, already tying UTurku.", "selected": "ike Riedel et al. (2009), our system is based on Markov logic, but we adopted a novel formulation that models dependency edges in argument paths and jointly predicts them along with events and arguments.", "paper_id": "1160159"}, {"section": "Introduction", "paragraph": "In this paper, we present the first joint approach that achieves state-of-the-art results for bio-event extraction. Like Riedel et al. (2009), our system is based on Markov logic, but we adopted a novel formulation that models dependency edges in argument paths and jointly predicts them along with events and arguments. By expanding the scope of joint inference to include individual argument edges, our system can leverage fine-grained correlations to make learning more effective. On the development set, by merely adding a few joint inference formulas to a simple logistic regression model, our system raised F1 from 28% to 54%, already tying UTurku.", "selected": "we present the first joint approach that achieves state-of-the-art results for bio-event extraction.", "paper_id": "1160159"}, {"section": "Introduction", "paragraph": "We also presented a heuristic method to fix errors in syntactic parsing by leveraging available semantic information from task input, and showed that this in turn led to substantial performance gain in the task. Overall, our final system reduced F1 error by more than 10% compared to Riedel et al. (2009).", "selected": "We also presented a heuristic method to fix errors in syntactic parsing by leveraging available semantic information from task input, and showed that this in turn led to substantial performance gain in the task", "paper_id": "1160159"}, {"section": "An MLN for Joint Bio-Event Extraction", "paragraph": "In this section, we present our MLN for joint bioevent extraction. As standard for this task, we assume that Stanford dependency parses are available in the input. Our MLN jointly makes the following predictions: for each token, whether it is a trigger word (and if so, what is the event type), and for each dependency edge, whether it is in an argument path leading to a theme or cause.", "selected": "n this section, we present our MLN for joint bioevent extraction. As standard for this task, we assume that Stanford dependency parses are available in the input.", "paper_id": "1160159"}, {"section": "An MLN for Joint Bio-Event Extraction", "paragraph": "Joint Inference: Like any classification system, the formulas in the base MLN make independent predictions at inference time. This is suboptimal, because query atoms are interdependent due to either hard constraints (e.g., an event must have a type) or soft correlation (e.g., \"increase\" signifies an event and the dobj edge from it leads to a theme). We thus augment the base MLN with two groups of jointinference formulas. First we incorporate the following hard constraints.", "selected": "Like any classification system, the for", "paper_id": "1160159"}]}}
{"idx": "471099", "paper_id": "15146734", "title": "Sentiment Propagation via Implicature Constraints", "abstract": "Opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities (goodFor/badFor events). We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities. The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.", "context_section_header": "", "context_paragraph": "Most work in NLP addresses explicit sentiment, but some address implicit sentiment. For example,  identify noun product features that imply opinions, and (Feng et al., 2013) identify objective words that have positive or negative connotations. However, identifying terms that imply opinions is a different task than sentiment propagation between entities. (Dasigi et al., 2012) search for implicit attitudes shared between authors, while we address inferences within a single text.", "sentence": "However, identifying terms that imply opinions is a different task than sentiment propagation between entities. (Dasigi et al., 2012) search for implicit attitudes shared between authors, while we address inferences within a single text.", "cited_ids": [{"paper_id": "5283021", "citation": "(Dasigi et al., 2012)"}], "y": "Identifying terms that imply opinions is a different task than sentiment propagation between entities. Dasigi et al. (2012) search for implicit attitudes shared between authors, while the authors address inferences within a single text [using Loopy Belief Propagation].", "snippet_surface": "However, identifying terms that imply opinions is a different task than sentiment propagation between entities. Dasigi et al. (2012) search for implicit attitudes shared between authors, while the authors address inferences within a single text.", "questions": {"r4VWioIf+u": "How do the authors addess inferences?"}, "answers": {"r4VWioIf+u": "The authors address inferences in their work using Loopy Belief Propagation"}, "evidence": {"r4VWioIf+u": [{"section": "Abstract", "paragraph": "Opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities (goodFor/badFor events). We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities. The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.", "selected": "We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities.", "paper_id": "15146734"}, {"section": "Related Work", "paragraph": "Most work in NLP addresses explicit sentiment, but some address implicit sentiment. For example,  identify noun product features that imply opinions, and (Feng et al., 2013) identify objective words that have positive or negative connotations. However, identifying terms that imply opinions is a different task than sentiment propagation between entities. (Dasigi et al., 2012) search for implicit attitudes shared between authors, while we address inferences within a single text.", "selected": "while we address inferences within a single text.", "paper_id": "15146734"}]}}
{"idx": "481201", "paper_id": "10016808", "title": "Annotating Abstract Pronominal Anaphora in the DAD Project", "abstract": "In this paper we present an extension of the MATE/GNOME annotation scheme for anaphora (Poesio, 2004) which accounts for abstract anaphora in Danish and Italian. By abstract anaphora it is here meant pronouns whose linguistic antecedents are verbal phrases, clauses and discourse segments. The extended scheme, which we call the DAD annotation scheme, allows to annotate information about abstract anaphora which is important to investigate their use, see i.a. (Webber, 1988; Gundel et al., 2003; Navarretta, 2004; Navarretta, 2007) and which can influence their automatic treatment. Intercoder agreement scores obtained by applying the DAD annotation scheme on texts and dialogues in the two languages are given and show that the information proposed in the scheme can be recognised in a reliable way.", "context_section_header": "", "context_paragraph": "There are only few studies describing the annotation of abstract anaphora in English. These studies have been done with different purposes, thus focussing on different types of information. Gundel et al. (2003), Gundel et al. (2004) and Hedberg et al. (2007) in their work focus on the investigation of the salience state of abstract entities in discourse and on their relation to the types of referring pronoun as proposed by Gundel et al. (1993) in their Givenness Hierar-chy. Hedberg et al. (2007) use XML as annotation format. Eckert and Strube (2001) and M\u00fcller (2007) annotate third person singular anaphors and their antecedents as a basis for their automatic recognition and \"resolution\". They do not focus on the format of their annotation. M\u00fcller (2007) lets non experts identify the antecedents of abstract anaphora according to very general instructions and reports low inter-coder agreement. Fraurud (1992) and Navarretta (2002) annotate occurrences of abstract anaphora in Swedish and Danish respectively to investigate the characteristics of abstract pronominal reference in these languages. Their results indicate that there are differences in the way abstract anaphora are used in these two languages respect to English. Navarretta (2007) investigates the occurrences of abstract anaphors in a parallel corpus of fairy tales (Danish, English and Italian) and also her data indicate that there are language specific characteristics in the way abstract anaphora are used in these three languages, both for what regards their frequency, the contexts in which they are used and the types of pronoun used in similar contexts. In Fraurud's and Navarretta's studies only one person (the respective authors) made the annotation following different annotation conventions and focussing on different phenomena. In the DAD project we include much of the information which has been used by i.a. Hedberg et al. (2007) for English and by Navarretta (2007) for Danish and Italian and integrate it in the existing MATE/GNOME scheme for anaphora. Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and our work is that we do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead we provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance. Furthermore we work with abstract anaphora in Danish and Italian which involve more types of pronoun than they do in English.", "sentence": "Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and our work is that we do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead we provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance.", "cited_ids": [{"paper_id": "14697968", "citation": "Hedberg et al. (2007)"}], "y": "Apart from the fact that the authors use the MATE/GNOME scheme [a form of anaphoric annotation of information for conference resolution], the main difference between the work by Hedberg et al. (2007) and the authors' work is that [Hedberg et al., 2007] do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead the authors provide more detailed information about the syntactic type of non nominal antecedents [elements in the MATE/GNOME scheme that are distinguished into several types] and mark information such as [the distance between the anaphor and antecedent].", "snippet_surface": "Apart from the fact that the authors use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and the authors' work is that they do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead they provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance.", "questions": {"tZzqnrx+SK": "What is the \"MATE/GNOME scheme?\"", "OG6Kf3KBkl": "Who does \"they\" refer to?", "Ca1CFYZSBD": "What are \"non nominal antecedents?\"", "xzlQwCZN77": "What is \"anaphoric distance?\""}, "answers": {"tZzqnrx+SK": "The MATE/GNOME scheme is a form of anaphoric annotation of information for coreference resolution.", "OG6Kf3KBkl": "They refers to Hedberg et al. (the group of researchers led by Hedberg)", "Ca1CFYZSBD": "Non-nominal antecedents are elements in the MATE/GNOME scheme and are distinguished into several types.", "xzlQwCZN77": "The anaphoric distance is the distance between the anaphor and antecedent."}, "evidence": {"tZzqnrx+SK": [{"section": "Introduction", "paragraph": "In this paper we describe an extension of the MATE/GNOME annotation scheme for (co)reference (Poesio, 2004) which allows the annotation of information which is relevant to the study and the automatic treatment of abstract anaphora. Abstract anaphora in the paper indicate third-person singular pronouns whose interpretation depends on linguistic expressions such as verbal phrases, clauses and discourse segments (henceforth, the antecedents). An example of abstract anaphor in English is in 1 where the demonstrative pronoun that has as antecedent the infinitive clause \"to book a seat close to the aisle wings, where the exit windows are located\".", "selected": "In this paper we describe an extension of the MATE/GNOME annotation scheme for (co)reference (Poesio, 2004) which allows the annotation of information which is relevant to the study and the automatic treatment of abstract anaphora.", "paper_id": "10016808"}], "OG6Kf3KBkl": [{"section": "Background and related work", "paragraph": "There are only few studies describing the annotation of abstract anaphora in English. These studies have been done with different purposes, thus focussing on different types of information. Gundel et al. (2003), Gundel et al. (2004) and Hedberg et al. (2007) in their work focus on the investigation of the salience state of abstract entities in discourse and on their relation to the types of referring pronoun as proposed by Gundel et al. (1993) in their Givenness Hierar-chy. Hedberg et al. (2007) use XML as annotation format. Eckert and Strube (2001) and M\u00fcller (2007) annotate third person singular anaphors and their antecedents as a basis for their automatic recognition and \"resolution\". They do not focus on the format of their annotation. M\u00fcller (2007) lets non experts identify the antecedents of abstract anaphora according to very general instructions and reports low inter-coder agreement. Fraurud (1992) and Navarretta (2002) annotate occurrences of abstract anaphora in Swedish and Danish respectively to investigate the characteristics of abstract pronominal reference in these languages. Their results indicate that there are differences in the way abstract anaphora are used in these two languages respect to English. Navarretta (2007) investigates the occurrences of abstract anaphors in a parallel corpus of fairy tales (Danish, English and Italian) and also her data indicate that there are language specific characteristics in the way abstract anaphora are used in these three languages, both for what regards their frequency, the contexts in which they are used and the types of pronoun used in similar contexts. In Fraurud's and Navarretta's studies only one person (the respective authors) made the annotation following different annotation conventions and focussing on different phenomena. In the DAD project we include much of the information which has been used by i.a. Hedberg et al. (2007) for English and by Navarretta (2007) for Danish and Italian and integrate it in the existing MATE/GNOME scheme for anaphora. Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007) and our work is that we do not include the annotation of the cognitive status of the antecedents (\"in focus\" and \"activated\"), but instead we provide more detailed information about the syntactic type of non nominal antecedents and mark information such as the anaphoric distance. Furthermore we work with abstract anaphora in Danish and Italian which involve more types of pronoun than they do in English.", "selected": "Apart from the fact that we use the MATE/GNOME scheme, the main difference between the work by Hedberg et al. (2007)", "paper_id": "10016808"}], "Ca1CFYZSBD": [{"section": "The type of information and the data annotated", "paragraph": "When a pronoun is an anaphor (individual or abstract) and when it is a textual deictic it is relevant to mark its antecedent and the relation between the anaphor and its antecedent. We only distinguish between two types of relation between anaphora and antecedents: \"identity\" (the case of coreference) and \"non identity\" (the remaining cases). Webber (1988) and Gundel et al. (2004) notice that in English normally only demonstrative pronouns can be used when the antecedents are clauses. Navarretta (2004) notices that this is not always the case in written Danish where the most frequent abstract pronoun det is ambiguous regarding its pronominal type. Navarretta (2007) reports that both personal and zero anaphora can have clausal antecedents in Italian in contexts where a demonstrative pronoun is used in English. Navarretta notices also that the demonstrative pronoun dette is often used in Danish texts to mark the antecedent is the latter subclause and not the whole preceding clause. To study further whether there is a relation between the syntactic type of antecedent and the type of pronoun used we have decided to annotate the syntactic type of the antecedent including different types of clause. We distinguish the following types of non nominal antecedent, see i.a. (Navarretta, 2004;Navarretta, 2007): predicates in copula constructions 4 , verbal phrases, simple main clauses, matrix clauses, subordinate clauses, complex clauses, discourse segments. Complex clauses are main clauses with all their subordinate clauses and/or coordinated clauses. The clausal types we use are inspired by the classification of clauses proposed by Kameyama (1998) in a completely different context. Because the anaphoric distance (the distance between anaphor and antecedent) is one of the factors influencing saliency of entities, see i.a. (Ariel, 1988;Giv\u00f3n, 1976), we also decided to annotate it for abstract anaphors. In this case we measure anaphoric distance in terms of clauses inbetween the abstract anaphor and its antecedent. We have also decided to annotate the semantic type of the referent. The individuation of the semantic type of the referent is necessary for the identification of the referent. However most work on the automatic resolution of abstract anaphora only focusses on the individuation of the antecedent, following the approach used for coreference resolution, see i.a. (Grosz et al., 1995). Most of the semantic types we have used are taken from the middle layer of the hierarchy of abstract objects proposed by Asher (1993). These types include eventualities comprising states and events, fact-like entities which comprise facts, situations and possibilities and propositions. To these types we have added questions, speech-acts and other entities. The latter type is i.a. used for predicates in copula constructions. Many of these types have also been used by i.a. Hedberg et al. (2007) in their annotation of English abstract anaphora.", "selected": "We distinguish the following types of non nominal antecedent, see i.a. (Navarretta, 2004;Navarretta, 2007): predicates in copula constructions 4 , verbal phrases, simple main clauses, matrix clauses, subordinate clauses, complex clauses, discourse segments.", "paper_id": "10016808"}, {"section": "The annotation scheme", "paragraph": "We have extended the MATE/GNOME annotation scheme for anaphora (Poesio, 2004) with the information presented in section 3. because this scheme, as previously described, also accounts for the annotation of anaphora in general. The central elements in the MATE/GNOME scheme are the following: de (discourse entities) used to mark nominal expressions, link used to mark the relation between an anaphor and its antecedent and seg used to mark non nominal antecedents. All these elements contain an ID attribute with a unique value. In our extension of the coding scheme we have added attributes to the de and seg elements. We have also introduced an element pleonastic to mark pleonastic pronouns which do not introduce a discourse entity in the discourse model. In cases where the attributes we have defined are not relevant for the marked words, their value is set to nonappliable. In cases where the attributes are relevant for the marked words, but they are not annotated in the current project (we only annotate some pronouns) the values of the attributes are set to none. The attribute ATYPE, in which the pronominal function is annotated, is for example defined for the deelements. If the nominal phrase marked by the de element is not a pronoun, the value of ATYPE is automatically set to \"non appliable\", if the element marked is a pronoun, but not a relevant third person singular pronoun, the value of the attribute is set to \"none\". The attribute PTYPE for the element de is used to indicate the type of pronoun if the marked words are pronouns. In Danish the attribute may have one of the following values det, dette, stressed det, unstressed det, det her, det der. In Italian the values of the PTYPE attributes are the following: lo, clitic lo, ne, clitic ne, questo, quello, codesto 5 . For de elements marking abstract anaphors the following attributes are marked:", "selected": "The central elements in the MATE/GNOME scheme are the following: de (discourse entities) used to mark nominal expressions, link used to mark the relation between an anaphor and its antecedent and seg used to mark non nominal antecedents.", "paper_id": "10016808"}], "xzlQwCZN77": [{"section": "The type of information and the data annotated", "paragraph": "When a pronoun is an anaphor (individual or abstract) and when it is a textual deictic it is relevant to mark its antecedent and the relation between the anaphor and its antecedent. We only distinguish between two types of relation between anaphora and antecedents: \"identity\" (the case of coreference) and \"non identity\" (the remaining cases). Webber (1988) and Gundel et al. (2004) notice that in English normally only demonstrative pronouns can be used when the antecedents are clauses. Navarretta (2004) notices that this is not always the case in written Danish where the most frequent abstract pronoun det is ambiguous regarding its pronominal type. Navarretta (2007) reports that both personal and zero anaphora can have clausal antecedents in Italian in contexts where a demonstrative pronoun is used in English. Navarretta notices also that the demonstrative pronoun dette is often used in Danish texts to mark the antecedent is the latter subclause and not the whole preceding clause. To study further whether there is a relation between the syntactic type of antecedent and the type of pronoun used we have decided to annotate the syntactic type of the antecedent including different types of clause. We distinguish the following types of non nominal antecedent, see i.a. (Navarretta, 2004;Navarretta, 2007): predicates in copula constructions 4 , verbal phrases, simple main clauses, matrix clauses, subordinate clauses, complex clauses, discourse segments. Complex clauses are main clauses with all their subordinate clauses and/or coordinated clauses. The clausal types we use are inspired by the classification of clauses proposed by Kameyama (1998) in a completely different context. Because the anaphoric distance (the distance between anaphor and antecedent) is one of the factors influencing saliency of entities, see i.a. (Ariel, 1988;Giv\u00f3n, 1976), we also decided to annotate it for abstract anaphors. In this case we measure anaphoric distance in terms of clauses inbetween the abstract anaphor and its antecedent. We have also decided to annotate the semantic type of the referent. The individuation of the semantic type of the referent is necessary for the identification of the referent. However most work on the automatic resolution of abstract anaphora only focusses on the individuation of the antecedent, following the approach used for coreference resolution, see i.a. (Grosz et al., 1995). Most of the semantic types we have used are taken from the middle layer of the hierarchy of abstract objects proposed by Asher (1993). These types include eventualities comprising states and events, fact-like entities which comprise facts, situations and possibilities and propositions. To these types we have added questions, speech-acts and other entities. The latter type is i.a. used for predicates in copula constructions. Many of these types have also been used by i.a. Hedberg et al. (2007) in their annotation of English abstract anaphora.", "selected": "Because the anaphoric distance (the distance between anaphor and antecedent) is one of the factors influencing saliency of entities", "paper_id": "10016808"}]}}
{"idx": "481291", "paper_id": "235097439", "title": "Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction", "abstract": "Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information.", "context_section_header": "", "context_paragraph": "A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011;Zeng et al., 2015;Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3).", "sentence": "Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches.", "cited_ids": [{"paper_id": "13742825", "citation": "Han et al. (2018)"}], "y": "Han et al. (2018) propose a neural attention mechanism between a knowledge graph and supporting texts [for automatic knowledge base construction], outperforming previous approaches [which are based on distant supervision using entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base].", "snippet_surface": "Han et al. (2018) propose a neural attention mechanism between a knowledge graph and supporting texts, outperforming previous approaches.", "questions": {"DqM5C5plWg": "What are the previous approaches?"}, "answers": {"DqM5C5plWg": "The previous approaches were based on distant supervision, that utilises entity pairs in texts as a signal for the presence of propositions that  may be incorporated in a knowledge base"}, "evidence": {"DqM5C5plWg": [{"section": "Related Work", "paragraph": "A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011;Zeng et al., 2015;Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3).", "selected": "A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011;Zeng et al., 2015;Lin et al., 2016))", "paper_id": "235097439"}]}}
{"idx": "491012", "paper_id": "51869205", "title": "Learning to Control the Specificity in Neural Response Generation", "abstract": "In conversation, a general response (e.g., \u201cI don\u2019t know\u201d) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.", "context_section_header": "", "context_paragraph": "We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. Meanwhile, we assume that each word, beyond the semantic representation which relates to its meaning, also has another representation which relates to the usage preference under different response purpose. We name this representation as the usage representation of words. The specificity control variable then interacts with the usage representation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels. We refer to our model as Specificity Controlled Seq2Seq model . Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model. All the model parameters are learned on the same conversation corpus in an end-to-end way.", "sentence": "Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model.", "cited_ids": [{"paper_id": "9514751", "citation": "(Xing et al., 2017)"}], "y": "The authors note that, unlike the work by (Xing et al., 2017), they do not rely on any external corpus to learn their [Seq2Seq] model.", "snippet_surface": "The authors note that, unlike the work by (Xing et al., 2017), they do not rely on any external corpus to learn their model.", "questions": {"YKw0Bh6gbk": "What is the authors' work?", "X+SNGDh2wq": "What is the external corpus?", "tOGn/fpZGy": "What is their model?"}, "answers": {"YKw0Bh6gbk": "A novel controlled response generation mechanism.", "X+SNGDh2wq": "A person who gives information to let the mechanism to learn.", "tOGn/fpZGy": "They use a Seq2Seq model"}, "evidence": {"YKw0Bh6gbk": [{"section": "Abstract", "paragraph": "In conversation, a general response (e.g., \u201cI don\u2019t know\u201d) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.", "selected": ", we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity.", "paper_id": "51869205"}, {"section": "Introduction", "paragraph": "In our work, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. The key idea is inspired by our observation on everyday conversation between humans. In human-human conversation, people often actively control the specificity of responses depending on their own response purpose (which might be affected by a variety of underlying factors like their current mood, knowledge state and so on). For example, they may provide some interesting and specific responses if they like the conversation, or some general responses if they want to end it. They may provide very detailed responses if they are familiar with the topic, or just \"I don't know\" otherwise. Therefore, we propose to simulate the way people actively control the specificity of the response.", "selected": ", we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity.", "paper_id": "51869205"}], "X+SNGDh2wq": [{"section": "Introduction", "paragraph": "There have been a few efforts attempting to address this issue in literature. Li et al. (2016a) proposed to use the Maximum Mutual Information (MMI) as the objective to penalize general responses. It could be viewed as a post-processing approach which did not solve the generation of trivial responses fundamentally. Xing et al. (2017) pre-defined a set of topics from an external corpus to guide the generation of the Seq2Seq model. However, it is difficult to ensure that the topics learned from the external corpus are consistent with that in the conversation corpus, leading to the introduction of additional noises.  introduced latent responding factors to model multiple responding mechanisms. However, these latent factors are usually difficult in interpretation and it is hard to decide the number of the latent factors.", "selected": ") pre-defined a set of topics from an external corpus to guide the generation of the Seq2Seq model. However, it is difficult to ensure that the topics learned from the external corpus are consistent with that in the conversation corpus", "paper_id": "51869205"}], "tOGn/fpZGy": [{"section": "Introduction", "paragraph": "We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. Meanwhile, we assume that each word, beyond the semantic representation which relates to its meaning, also has another representation which relates to the usage preference under different response purpose. We name this representation as the usage representation of words. The specificity control variable then interacts with the usage representation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels. We refer to our model as Specificity Controlled Seq2Seq model . Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model. All the model parameters are learned on the same conversation corpus in an end-to-end way.", "selected": "Seq2Seq model to generate responses at different specificity levels. We refer to our model as Specificity Controlled Seq2Seq model .", "paper_id": "51869205"}]}}
{"idx": "51608", "paper_id": "235097578", "title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs", "abstract": "We present Graformer, a novel Transformerbased encoder-decoder architecture for graphto-text generation. With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph \u2013 not only direct neighbors \u2013 facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these nodenode relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.1", "context_section_header": "", "context_paragraph": "Deficiencies in modeling long-range dependencies in GNNs have been considered a serious limitation before. Various solutions orthogonal to our approach have been proposed in recent work: By 2 abstract meaning representation incorporating a connectivity score into their graph attention network, Zhang et al. (2020) manage to increase the attention span to k-hop neighborhoods but, finally, only experiment with k = 2. Our graph encoder efficiently handles dependencies between much more distant nodes. Pei et al. (2020) define an additional neighborhood based on Euclidean distance in a continuous node embedding space. Similar to our work, a node can thus receive information from distant nodes, given their embeddings are close enough. However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer. This allows Graformer to dynamically change node interaction patterns during training.", "sentence": "However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer.", "cited_ids": [{"paper_id": "210843644", "citation": "Pei et al. (2020)"}], "y": "The authors' approach is to [build a transformer-based graph-to-text generator which facilitates the detection of global patterns by making sure that the embedding of a node relies on all nodes in the input graph. This is tested on popular benchmarks, AGENDA and WebNLG]. Pei et al. (2020) compute these embeddings only once before training whereas in the authors' approach, node similarity is based on the learned representation in each encoder layer.", "snippet_surface": "However, Pei et al. (2020) compute these embeddings only once before training whereas in the authors' approach, node similarity is based on the learned representation in each encoder layer.", "questions": {"6gKwRw0I/Q": "What is the authors' approach?", "/YrBFBmLVb": "How does the encoded layer look like?"}, "answers": {"6gKwRw0I/Q": "The authors build a transformer-based gaph-to-text generator which facilitate the detection of global patterns by making sure that the encoding of a nodes relies on all nodes in the input graph. They test it on popular benchmarks, AGENDA and WebNLG", "/YrBFBmLVb": "I (L) = Dr (SelfAtt g (LN (H (L\u22121) ))) + H (L\u22121) (1) H (L) = Dr (FF (LN (I (L) ))) + I (L)(2)"}, "evidence": {"6gKwRw0I/Q": [{"section": "Abstract", "paragraph": "We present Graformer, a novel Transformerbased encoder-decoder architecture for graphto-text generation. With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph \u2013 not only direct neighbors \u2013 facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these nodenode relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.1", "selected": "With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph \u2013 not only direct neighbors \u2013 facilitating the detection of global patterns", "paper_id": "235097578"}, {"section": "Abstract", "paragraph": "We present Graformer, a novel Transformerbased encoder-decoder architecture for graphto-text generation. With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph \u2013 not only direct neighbors \u2013 facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these nodenode relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.1", "selected": "We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG,", "paper_id": "235097578"}, {"section": "Introduction", "paragraph": "In summary, our contributions are as follows: (1) We develop Graformer, a novel graph-to-text architecture that interprets shortest path lengths as relative position information in a graph self-attention network. (2) Graformer achieves competitive performance on two popular KG-to-text generation benchmarks, showing that our architecture can learn about graph structure without any guidance other than its text generation objective. (3) To further investigate what Graformer learns about graph structure, we visualize the differently connected graph views it has learned and indeed find different attention heads for more local and more global graph information. Interestingly, direct neighbors are considered particularly important even without any structural bias, such as introduced by a graph neural network. (4) Analyzing the performance w.r.t. different input graph properties, we find evidence that Graformer's more elaborate global view on the graph is an advantage when it is important to distinguish between distant but connected nodes and truly unreachable ones.", "selected": "In summary, our contributions are as follows: (1) We develop Graformer, a novel graph-to-text architecture that interprets shortest path lengths as relative position information in a graph self-attention network. (2) Graformer achieves competitive performance on two popular KG-to-text generation benchmarks, showing that our architecture can learn about graph structure without any guidance other than its text generation objective. (3) To further investigate what Graformer learns about graph structure, we visualize the differently connected graph views it has learned and indeed find different attention heads for more local and more global graph information. Interestingly, direct neighbors are considered particularly important even without any structural bias, such as introduced by a graph neural network. (4) Analyzing the performance w.r.t. different input graph properties, we find evidence that Graformer's more elaborate global view on the graph is an advantage when it is important to distinguish between distant but connected nodes and truly unreachable ones.", "paper_id": "235097578"}], "/YrBFBmLVb": [{"section": "Graformer encoder", "paragraph": "To compute the node representation H (L) in the Lth layer, we follow Vaswani et al. (2017), i.e., we first normalize the input from the previous layer H (L\u22121) via layer normalization LN , followed by multi-head graph self-attention SelfAtt g (see \u00a7 3.3 for details), which -after dropout regularization Dr and a residual connection -yields the intermediate representation I (cf. Eq. (1)). A feedforward layer FF with one hidden layer and GeLU (Hendrycks and Gimpel, 2016) activation computes the final layer output (cf. Eq. (2)). As recommended by Chen et al. (2018), we apply an additional layer normalization step to the output H (L E ) of the last encoder layer L E .", "selected": "To compute the node representation H (L) in the Lth layer, we follow Vaswani et al. (2017), i.e., we first normalize the input from the previous layer H (L\u22121) via layer normalization LN , followed by multi-head graph self-attention SelfAtt g (see \u00a7 3.3 for details), which -after dropout regularization Dr and a residual connection -yields the intermediate representation I (cf. Eq. (1)). A feedforward layer FF with one hidden layer and GeLU (Hendrycks and Gimpel, 2016) activation computes the final layer output (cf. Eq. (2)). As recommended by Chen et al. (2018), we apply an additional layer normalization step to the output H (L E ) of the last encoder layer L E .", "paper_id": "235097578"}, {"section": "Graformer encoder", "paragraph": "I (L) = Dr (SelfAtt g (LN (H (L\u22121) ))) + H (L\u22121) (1) H (L) = Dr (FF (LN (I (L) ))) + I (L)(2)", "selected": "I (L) = Dr (SelfAtt g (LN (H (L\u22121) ))) + H (L\u22121) (1) H (L) = Dr (FF (LN (I (L) ))) + I (L)(2)", "paper_id": "235097578"}]}}
{"idx": "52500", "paper_id": "5776384", "title": "On the Automatic Generation of Medical Imaging Reports", "abstract": "Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.", "context_section_header": "", "context_paragraph": "for images, Krause et al. (2017) and Liang et al. (2017) generate paragraph captions using a hierarchical LSTM. Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "sentence": "Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "cited_ids": [{"paper_id": "14464447", "citation": "Krause et al. (2017)"}], "y": "The authors' method adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), they use a co-attention network [which uses visual word recognition (letter shape recognition) along with semantic relevance of the word for tasks like transcribing captions], to generate topics.", "snippet_surface": "The authors' method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), they use a co-attention network to generate topics.", "questions": {"VxCqlAwvTe": "What is the authors' method?", "MRlHAVa55T": "What is a co-attention network?"}, "answers": {"VxCqlAwvTe": "They use hierarchical LSTM with co-attention network", "MRlHAVa55T": "A co-attention network uses visual word recognition (letter shape recognition) along with semantic relevance of the word for tasks like transcribing captions."}, "evidence": {"VxCqlAwvTe": [{"section": "Related Works", "paragraph": "for images, Krause et al. (2017) and Liang et al. (2017) generate paragraph captions using a hierarchical LSTM. Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "selected": "Our method also adopts a hierarchical LSTM for paragraph generation, but unlike Krause et al. (2017), we use a co-attention network to generate topics.", "paper_id": "5776384"}], "MRlHAVa55T": []}}
{"idx": "52710", "paper_id": "32879266", "title": "Verb Replacer: An English Verb Error Correction System", "abstract": "According to the analysis of Cambridge Learner Corpus, using a wrong verb is the most common type of grammatical errors. This paper describes Verb Replacer, a system for detecting and correcting potential verb errors in a given sentence. In our approach, alternative verbs are considered to replace the verb based on an error-annotated corpus and verb-object collocations. The method involves applying regression on channel models, parsing the sentence, identifying the verbs, retrieving a small set of alternative verbs, and evaluating each alternative. Our method combines and improves channel and language models, resulting in high recall of detecting and correcting verb misuse.", "context_section_header": "", "context_paragraph": "According to the analysis of a sample of the Cambridge Learner Corpus (CLC) with 1,244 exam scripts for First Certificate English (FCE), verb selection errors (Replace-Verb errors, RV) is the most common error type, not counting spelling errors. In content word (e.g., verb and noun) errors correction, previous systems relied on mostly manually constructed resources (e,g., (Shei and Pain, 2000;Lee and Seneff, 2008;Liu et al., 2009)). It is not clear whether these manual resources can be easily scaled up and extended to other types of writing error and domains. Classifiers have been used for correcting verb errors. (Wu et al., 2010) describe an approach based on a classifier to predict the verb in the context of a given sentence. The main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used. Similarly, (Rozovskaya et al., 2014) use classifiers with the notion of verb finiteness to identify certain types of verb errors. (Rozovskaya et al., 2014) only address the agreement, tense, and form verb errors related to a small candidate set, while we deal with the verb selection problem with an open candidate set. In a noisy-channel approach closer to our work, (Sawai et al., 2013) use large learner corpus to construct candidate sets. They show that an GEC system that uses learner corpus outperforms systems that use WordNet and roundtrip translations, improving the performance of verb error detection and suggestion.", "sentence": "The main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used.", "cited_ids": [{"paper_id": "17049904", "citation": "(Wu et al., 2010)"}], "y": "The authors are presenting a similar work to the one made by Wu (2010) albeit the context alone isnt determining the outcome while they use the channel model information related to the potentially wrong verb.", "snippet_surface": "The main difference from the authors' current work is that in (Wu et al., 2010), the context alone determines the outcome, the channel model information related to the potentially wrong verb is not used.", "questions": {"J/UcbE7ELx": "What is the authors' current work?"}, "answers": {"J/UcbE7ELx": "It is similar to the one by Wu, but they do not determine the outcome with the context alone."}, "evidence": {"J/UcbE7ELx": [{"section": "Introduction", "paragraph": "According to the analysis of a sample of the Cambridge Learner Corpus (CLC) with 1,244 exam scripts for First Certificate English (FCE), verb selection errors (Replace-Verb errors, RV) is the most common error type, not counting spelling errors. In content word (e.g., verb and noun) errors correction, previous systems relied on mostly manually constructed resources (e,g., (Shei and Pain, 2000;Lee and Seneff, 2008;Liu et al., 2009)). It is not clear whether these manual resources can be easily scaled up and extended to other types of writing error and domains. Classifiers have been used for correcting verb errors. (Wu et al., 2010) describe an approach based on a classifier to predict the verb in the context of a given sentence. The main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used. Similarly, (Rozovskaya et al., 2014) use classifiers with the notion of verb finiteness to identify certain types of verb errors. (Rozovskaya et al., 2014) only address the agreement, tense, and form verb errors related to a small candidate set, while we deal with the verb selection problem with an open candidate set. In a noisy-channel approach closer to our work, (Sawai et al., 2013) use large learner corpus to construct candidate sets. They show that an GEC system that uses learner corpus outperforms systems that use WordNet and roundtrip translations, improving the performance of verb error detection and suggestion.", "selected": "he main difference from our current work is that in (Wu et al., 2010), the context alone determine the outcome, the channel model information related to the potentially wrong verb is not used.", "paper_id": "32879266"}]}}
{"idx": "5882", "paper_id": "6383080", "title": "A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction", "abstract": "In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an infinite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction.", "context_section_header": "", "context_paragraph": "One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "sentence": "Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "cited_ids": [{"paper_id": "7923429", "citation": "(Goldwater et al., 2009)"}], "y": "The authors' segmentation model [unigram word segmentation model using a blocked sampler] is in principle the same as the unigram word segmentation model and the main difference is that the authors are using a pointwise Gibbs sampler [where the variables of interest are potential word boundaries, and each of them can take on two values] while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "snippet_surface": "The authors' segmentation model is in principle the same as the unigram word segmentation model and the main difference is that the authors are using a blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "questions": {"ko6sPpPjz5": "What is a blocked sampler?", "QuuKl2/bHq": "What is the authors' segmentation model?"}, "answers": {"ko6sPpPjz5": "A Gibbs sampler where the variables of interest are potential word boundaries, and each of them can take on two values.", "QuuKl2/bHq": "It is a unigram word segmentation model but uses a blocked sampler."}, "evidence": {"ko6sPpPjz5": [{"section": "Inference", "paragraph": "We have now defined a generative model that allows us to compute the probability of any segmentation of the input corpus. We are left with the problem of inference, or actually identifying the highest probability segmentation from among all possibilities. We used a method known as Gibbs sampling (Geman and Geman, 1984), a type of Markov chain Monte Carlo algorithm (Gilks et al., 1996) in which variables are repeatedly sampled from their conditional posterior distribution given the current values of all other variables in the model. Gibbs sampling is an iterative procedure in which (after a number of iterations used as a \"burn-in\" period to allow the sampler to converge) each successive iteration produces a sample from the full posterior distribution P (h|d). In our sampler, the variables of interest are potential word boundaries, each of which can take on two possible values, corresponding to a word boundary or no word boundary.", "selected": ". Gibbs sampling is an iterative procedure in which (after a number of iterations used as a \"burn-in\" period to allow the sampler to converge) each successive iteration produces a sample from the full posterior distribution P (h|d). In our sampler, the variables of interest are potential word boundaries, each of which can take on two possible values, corresponding to a word boundary or no word boundary.", "paper_id": "7923429"}], "QuuKl2/bHq": [{"section": "Related Work", "paragraph": "One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters.", "selected": "Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler", "paper_id": "6383080"}]}}
{"idx": "59705", "paper_id": "216914626", "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "abstract": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.", "context_section_header": "", "context_paragraph": "Two recent works also perturb the input sequence for model interpretability Li et al., 2019b). However, these works only perturb the sequence once.  utilize the original MLM objective to estimate each word's \"reducibility\" and import simple heuristics into a right-chain baseline to construct dependency trees. Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike our two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "sentence": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike our two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "cited_ids": [{"paper_id": "196176486", "citation": "Li et al. (2019b)"}], "y": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike the authors' two-step masking strategy [which consists of subsequently masking out two elements x_i, x_j to get two corrupted sequences], they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "snippet_surface": "Li et al. (2019b) focus on evaluating word alignment in NMT, but unlike the authors' two-step masking strategy, they only replace the token of interest with a zero embedding or a randomly sampled word in the vocabulary.", "questions": {"Db3ElLpqZO": "What does \"the authors' two-step masking strategy\" consist of?"}, "answers": {"Db3ElLpqZO": "It consists of subsequently masking out two elements x_i, x_j to get two corrupted sequences which are then used in the computations"}, "evidence": {"Db3ElLpqZO": [{"section": "Token Perturbation", "paragraph": "We propose a two-stage approach to achieve our goal. First, we replace x i with the [MASK] token and feed the new sequence x\\{x i } into BERT. We use H \u03b8 (x\\{x i }) i to denote the representation of x i . To calculate the impact x j \u2208 x\\{x i } has on H \u03b8 (x\\{x i }) i , we further mask out x j to obtain the second corrupted sequence x\\{x i , x j }. Similarly, H \u03b8 (x\\{x i , x j }) i denotes the new representation of token x i .", "selected": "We propose a two-stage approach to achieve our goal. First, we replace x i with the [MASK] token and feed the new sequence x\\{x i } into BERT. We use H \u03b8 (x\\{x i }) i to denote the representation of x i . To calculate the impact x j \u2208 x\\{x i } has on H \u03b8 (x\\{x i }) i , we further mask out x j to obtain the second corrupted sequence x\\{x i , x j }. Similarly, H \u03b8 (x\\{x i , x j }) i denotes the new representation of token x i .", "paper_id": "216914626"}]}}
{"idx": "609", "paper_id": "53109320", "title": "GraphIE: A Graph-Based Framework for Information Extraction", "abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.", "context_section_header": "", "context_paragraph": "In this paper, we propose GraphIE, a framework that improves predictions by automatically learning the interactions between local and non-local dependencies in the input space. Our approach integrates a graph module with the encoder-decoder architecture for sequence tagging. The algorithm operates over a graph, where nodes correspond to textual units (i.e. words or sentences) and edges describe their relations. At the core of our model, a recurrent neural network sequentially encodes local contextual representations and then the graph module iteratively propagates information between neighboring nodes using graph convolutions (Kipf and Welling, 2016). The learned representations are finally projected back to a recurrent decoder to support tagging at the word level. We evaluate GraphIE on three IE tasks, namely textual, social media, and visual (Aumann et al., 2006) information extraction. For each task, we provide in input a simple task-specific graph, which defines the data structure without access to any major processing or external resources. Our model is expected to learn from the relevant dependencies to identify and extract the appropriate information. Experimental results on multiple benchmark datasets show that GraphIE consistently outperforms a strong and commonly adopted sequential model (SeqIE, i.e. a bidirectional long-short term memory (BiLSTM) followed by a conditional random fields (CRF) module). Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015). In the social media IE task, GraphIE improves over SeqIE by 3.7% in extracting the EDU-CATION attribute from twitter users. In visual IE, finally, we outperform the baseline by 1.2%.", "sentence": "Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "cited_ids": [{"paper_id": "5973003", "citation": "(Krallinger et al., 2015)"}], "y": "In the textual IE task, the authors obtain an improvement of 0.5% over SeqIE [Sequential Information Extraction] on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "snippet_surface": "Specifically, in the textual IE task, the authors obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015).", "questions": {"t6oHd3Z0rY": "What does SeqIE stand for?"}, "answers": {"t6oHd3Z0rY": "SeqIE stands for Sequential Information Extraction"}, "evidence": {"t6oHd3Z0rY": [{"section": "Abstract", "paragraph": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.", "selected": "Information Extraction (IE)", "paper_id": "53109320"}, {"section": "Introduction", "paragraph": "Most modern Information Extraction (IE) systems are implemented as sequential taggers. While such models effectively capture relations in the local context, they have limited capability of exploiting non-local and non-sequential dependencies. In many applications, however, such dependencies can greatly reduce tagging ambiguity, thereby improving overall extraction performance. For instance, when extracting entities from a document, various types of non-local contextual information such as co-references and identical mentions may provide valuable cues. See for example Figure 1, in which the non-local relations are crucial to discriminate the entity type of the second mention of Washington (i.e. PERSON, ORGANIZATION or LOCATION).", "selected": "Information Extraction (IE)", "paper_id": "53109320"}, {"section": "Introduction", "paragraph": "In this paper, we propose GraphIE, a framework that improves predictions by automatically learning the interactions between local and non-local dependencies in the input space. Our approach integrates a graph module with the encoder-decoder architecture for sequence tagging. The algorithm operates over a graph, where nodes correspond to textual units (i.e. words or sentences) and edges describe their relations. At the core of our model, a recurrent neural network sequentially encodes local contextual representations and then the graph module iteratively propagates information between neighboring nodes using graph convolutions (Kipf and Welling, 2016). The learned representations are finally projected back to a recurrent decoder to support tagging at the word level. We evaluate GraphIE on three IE tasks, namely textual, social media, and visual (Aumann et al., 2006) information extraction. For each task, we provide in input a simple task-specific graph, which defines the data structure without access to any major processing or external resources. Our model is expected to learn from the relevant dependencies to identify and extract the appropriate information. Experimental results on multiple benchmark datasets show that GraphIE consistently outperforms a strong and commonly adopted sequential model (SeqIE, i.e. a bidirectional long-short term memory (BiLSTM) followed by a conditional random fields (CRF) module). Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015). In the social media IE task, GraphIE improves over SeqIE by 3.7% in extracting the EDU-CATION attribute from twitter users. In visual IE, finally, we outperform the baseline by 1.2%.", "selected": "sequential model (SeqIE,", "paper_id": "53109320"}, {"section": "Introduction", "paragraph": "In this paper, we propose GraphIE, a framework that improves predictions by automatically learning the interactions between local and non-local dependencies in the input space. Our approach integrates a graph module with the encoder-decoder architecture for sequence tagging. The algorithm operates over a graph, where nodes correspond to textual units (i.e. words or sentences) and edges describe their relations. At the core of our model, a recurrent neural network sequentially encodes local contextual representations and then the graph module iteratively propagates information between neighboring nodes using graph convolutions (Kipf and Welling, 2016). The learned representations are finally projected back to a recurrent decoder to support tagging at the word level. We evaluate GraphIE on three IE tasks, namely textual, social media, and visual (Aumann et al., 2006) information extraction. For each task, we provide in input a simple task-specific graph, which defines the data structure without access to any major processing or external resources. Our model is expected to learn from the relevant dependencies to identify and extract the appropriate information. Experimental results on multiple benchmark datasets show that GraphIE consistently outperforms a strong and commonly adopted sequential model (SeqIE, i.e. a bidirectional long-short term memory (BiLSTM) followed by a conditional random fields (CRF) module). Specifically, in the textual IE task, we obtain an improvement of 0.5% over SeqIE on the CONLL03 dataset, and an improvement of 1.4% on the chemical entity extraction (Krallinger et al., 2015). In the social media IE task, GraphIE improves over SeqIE by 3.7% in extracting the EDU-CATION attribute from twitter users. In visual IE, finally, we outperform the baseline by 1.2%.", "selected": "GraphIE on three IE tasks,", "paper_id": "53109320"}]}}
{"idx": "64788", "paper_id": "231709298", "title": "Representations for Question Answering from Documents with Tables and Text", "abstract": "Tables in web documents are pervasive and can be directly used to answer many of the queries searched on the web, motivating their integration in question answering. Very often information presented in tables is succinct and hard to interpret with standard language representations. On the other hand, tables often appear within textual context, such as an article describing the table. Using the information from an article as additional context can potentially enrich table representations. In this work we aim to improve question answering from tables by refining table representations based on information from surrounding text. We also present an effective method to combine text and table-based predictions for question answering from full documents, obtaining significant improvements on the Natural Questions dataset (Kwiatkowski et al., 2019).", "context_section_header": "", "context_paragraph": "Most work on QA with tables prior to BERT involves first converting the table to a Knowledge Graph (KG) where cell entries are entities with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of M\u00fceller et al. (2019), where the authors generalized the BERT architecture similarly to Shaw et al. (2018) with new types of relations to encode table-specific relationships. The main differences between our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1), while in their work the authors use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values. Finally, the two most recent works on table representation learning, TaPaS (Herzig et al., 2020) and GraPPa (Yu et al., 2020), also use pretraining on the Wikitables dataset (Bhagavatula et al., 2013) that we use in our work. Therefore, our table representations based on transformers and our pretraining method are comparable to those in recent and concurrent work.", "sentence": "The main differences between our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1), while in their work the authors use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "cited_ids": [{"paper_id": "201698399", "citation": "M\u00fceller et al. (2019)"}], "y": "The main differences between the authors' table representation and the one from M\u00fceller et al. (2019) is that the authors use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row, while the latter use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "snippet_surface": "The main differences between the authors' table representation and the one from M\u00fceller et al. (2019) is that the authors use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1) while the latter use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values.", "questions": {"wnVGHXu1hf": "What is the authors' table representation?"}, "answers": {"wnVGHXu1hf": "A table the authors made using 5 types of relations."}, "evidence": {"wnVGHXu1hf": [{"section": "Related Work", "paragraph": "Most work on QA with tables prior to BERT involves first converting the table to a Knowledge Graph (KG) where cell entries are entities with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of M\u00fceller et al. (2019), where the authors generalized the BERT architecture similarly to Shaw et al. (2018) with new types of relations to encode table-specific relationships. The main differences between our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-cell, cross-column and cross-row (more details in Section 3.1), while in their work the authors use cell-column and cell-row relations only for the table representation, but in addition use question-cell relations for marking matches between tokens in the question and corresponding cell values. Finally, the two most recent works on table representation learning, TaPaS (Herzig et al., 2020) and GraPPa (Yu et al., 2020), also use pretraining on the Wikitables dataset (Bhagavatula et al., 2013) that we use in our work. Therefore, our table representations based on transformers and our pretraining method are comparable to those in recent and concurrent work.", "selected": "our table representation and M\u00fceller et al. (2019) is that in our representation we use 5 types of relations", "paper_id": "231709298"}]}}
{"idx": "69385", "paper_id": "1085023", "title": "Constituency to Dependency Translation with Forests", "abstract": "Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts.", "context_section_header": "", "context_paragraph": "The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models.  and  use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models  respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002).", "sentence": "Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that our model runs very faster.", "cited_ids": [{"paper_id": "832217", "citation": "Shen et al. (2008)"}], "y": "Compared with another work [a string-to-dependency algorithm for statistical machine translation], the authors put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that they can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, their rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that the authors' model runs much faster.", "snippet_surface": "The authors put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules compared with this work; the other difference is that they can also extract more expressive constituency to dependency rules, since the source side of their rule can encode multi-level reordering and contain more variables being larger than two; furthermore, their rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that their model runs very faster.", "questions": {"/P19vTsYiY": "What is \"this work\" referring to?"}, "answers": {"/P19vTsYiY": "a string-to-dependency algorithm for statistical machine translation"}, "evidence": {"/P19vTsYiY": [{"section": "Related Work", "paragraph": "The concept of packed forest has been used in machine translation for several years. For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models.  and  use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. Following this direction, Liu et al. (2009) and apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models  respectively. Different from Liu et al. (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. Shen et al. (2008) present a string-todependency model. They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005). Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multi-level reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al. (2008)'s scenario; finally, the most important one is that our model runs very faster. Liu et al. (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002).", "selected": "Shen et al. (2008) present a string-todependency model.", "paper_id": "1085023"}, {"section": "Abstract", "paragraph": "In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.", "selected": "In this paper, we propose a novel string-todependency algorithm for statistical machine translation.", "paper_id": "832217"}, {"section": "Conclusions and Future Work", "paragraph": "In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005). Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set.", "selected": "In this paper, we propose a novel string-todependency algorithm for statistical machine translation.", "paper_id": "832217"}]}}
{"idx": "71087", "paper_id": "2982769", "title": "Building RDF Content for Data-to-Text Generation", "abstract": "In Natural Language Generation (NLG), one important limitation is the lack of common benchmarks on which to train, evaluate and compare data-to-text generators. In this paper, we make one step in that direction and introduce a method for automatically creating an arbitrary large repertoire of data units that could serve as input for generation. Using both automated metrics and a human evaluation, we show that the data units produced by our method are both diverse and coherent.", "context_section_header": "", "context_paragraph": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure. In particular, one important difference is that we produce trees of varying shapes and depths while the graphs produced by (Cheng et al., 2015) are restricted to trees of depth one i.e., set of DBpedia triples whose subject is the entity to be described. As discussed in Section 5.1, this allows us to produce knowledge trees which, because they vary in shape, will give rise to different linguistic structures and will therefore better support the creation of a linguistically varied benchmark for Natural Language Generation.", "sentence": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure.", "cited_ids": [{"paper_id": "2699851", "citation": "(Cheng et al., 2015)"}], "y": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, the authors' goal is to produce a large set of content units [which are snippets of texts generated by a Language model starting from a DBPedia entity,] that are varied both in terms of content and in terms of structure.", "snippet_surface": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, the authors' goal is to produce a large set of content units that are varied both in terms of content and in terms of structure.", "questions": {"pPjiob4c4e": "What are the content units?", "ynoA3SUR8/": "How do the content units vary?"}, "answers": {"pPjiob4c4e": "Snippet of text generated by a Language model starting from DBPedia entity.", "ynoA3SUR8/": "Entities and properties, such as topic and discourse coherence."}, "evidence": {"pPjiob4c4e": [{"section": "Introduction", "paragraph": "RDF data consists of (subject property object) triples (e.g., (Alan Bean occupation Test pilot)) -as illustrated in Figure 1, RDF data can be represented by a graph in which edges are labelled with properties and vertices with subject and object resources. To construct a corpus of RDF data units which could serve as input for NLG, we introduce a content selection method which, given some DBPedia entity, retrieves DBPedia subgraphs that encode relevant and coherent knowledge about that entity.", "selected": "To construct a corpus of RDF data units which could serve as input for NLG, we introduce a content selection method which, given some DBPedia entity, retrieves DBPedia subgraphs that encode relevant and coherent knowledge about that entity.", "paper_id": "2982769"}, {"section": "Related Work", "paragraph": "In terms of goals, while (Cheng et al., 2015) aim to produce entity summaries, our goal is to produce a large set of content units that are varied both in terms of content and in terms of structure. In particular, one important difference is that we produce trees of varying shapes and depths while the graphs produced by (Cheng et al., 2015) are restricted to trees of depth one i.e., set of DBpedia triples whose subject is the entity to be described. As discussed in Section 5.1, this allows us to produce knowledge trees which, because they vary in shape, will give rise to different linguistic structures and will therefore better support the creation of a linguistically varied benchmark for Natural Language Generation.", "selected": "we produce trees of varying shapes and depths", "paper_id": "2982769"}, {"section": "Task and Method", "paragraph": "Given an entity e of category C and its associated DBpedia entity graph G e , our task is to select a (target) subgraph T e of G e such that:", "selected": "DBpedia entity graph G e , our task is to select a (target) subgraph T e of G", "paper_id": "2982769"}, {"section": "Coherence", "paragraph": "Because they are retrieved from DBpedia, the data units selected by our approach are semantically coherent overall. In particular, the triples that are directly connected to the entity being described are all relevant. However when selecting a subtree of the input entity graph, the coherence between siblings and between chained triples may decrease. For instance, given the entity graph shown in Figure 1, subgraph (2a) is more topically-coherent that subgraph (2b). Similarly, subgraph (2c) is more discourse-coherent that subgraph (2d).", "selected": "Because they are retrieved from DBpedia, the data units selected by our approach are semantically coherent overall. I", "paper_id": "2982769"}, {"section": "Conclusion", "paragraph": "We presented a method for selecting content from DBpedia data which leverages the n-gram information provided by large scale knowledge bases about entities of distinct ontological type. Based on the DBpedia graphs associated with entities of a given ontological type, we learn domain-specific n-gram models of DBpedia properties. To capture both discourse and topic-based coherence, we derive these n-grams either from chain or from sequence configurations of triples. As a result, we can extract content units based either on topic similarity, on elaboration-based discourse transition or on both. Using various metrics, we showed that our method supports the selection of content units that are both coherent and diverse.", "selected": "As a result, we can extract content units based either on topic similarity, on elaboration-based discourse transition or on both. Using various metrics, we showed that our method supports the selection of content units that are both coherent and diverse.", "paper_id": "2982769"}], "ynoA3SUR8/": [{"section": "Experimental Setup", "paragraph": "\u2200i, j s.t. i = t or j = t, xt <= yi,j   Selecting Data Units. To ensure that our content selection procedure produces varied data with respect to both form and content, we run the ILP program on entities belonging to three DBPedia categories (Astronaut, University, Monument) and using each of the bigram models (S-Model and C-Model) and their combination (M-Model). Using different DBPedia categories ensures that the selected data units vary in terms of RDF resources (entities and properties). Using the different bigram models permit producing data units exhibiting different levels of topic-and discourse-coherence. The intuition is that the S-Model will yield data units where topic-based coherence dominates, the C-Model discourse data units emphasizing transition-based, discourse coherence and M-Model data units which display a balance between topic-based and discourse coherence. We set \u03b3 to 0.4 (eq.3), after running several experiments we observed that this weight balanced the solutions favouring C-bigrams which in general have smaller probability values than S-bigrams. We run the ILP with \u03b1 (the number of triples occurring in the solution) ranging from 3 to 10 and input entity graphs with depth 1 and 2.", "selected": "Using different DBPedia categories ensures that the selected data units vary in terms of RDF resources (entities and properties).", "paper_id": "2982769"}, {"section": "Experimental Setup", "paragraph": "\u2200i, j s.t. i = t or j = t, xt <= yi,j   Selecting Data Units. To ensure that our content selection procedure produces varied data with respect to both form and content, we run the ILP program on entities belonging to three DBPedia categories (Astronaut, University, Monument) and using each of the bigram models (S-Model and C-Model) and their combination (M-Model). Using different DBPedia categories ensures that the selected data units vary in terms of RDF resources (entities and properties). Using the different bigram models permit producing data units exhibiting different levels of topic-and discourse-coherence. The intuition is that the S-Model will yield data units where topic-based coherence dominates, the C-Model discourse data units emphasizing transition-based, discourse coherence and M-Model data units which display a balance between topic-based and discourse coherence. We set \u03b3 to 0.4 (eq.3), after running several experiments we observed that this weight balanced the solutions favouring C-bigrams which in general have smaller probability values than S-bigrams. We run the ILP with \u03b1 (the number of triples occurring in the solution) ranging from 3 to 10 and input entity graphs with depth 1 and 2.", "selected": "sing the different bigram models permit producing data units exhibiting different levels of topic-and discourse-coherence", "paper_id": "2982769"}]}}
{"idx": "72224", "paper_id": "201666897", "title": "Adversarial Domain Adaptation for Machine Reading Comprehension", "abstract": "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.", "context_section_header": "", "context_paragraph": "Specifically, our proposed method first generates synthetic question-answer pairs given pas-sages in the target domain. Different from Golub et al. (2017), which only used pseudo questionanswer pairs to fine-tune pre-trained MRC models, our AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passagequestion pairs in the source domain, to train an additional domain classifier as a discriminator. The passage-question encoder and the domain classifier are jointly trained via adversarial learning. In this way, the encoder is enforced to learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span.", "sentence": "Different from Golub et al. (2017), which only used pseudo questionanswer pairs to fine-tune pre-trained MRC models, our AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passagequestion pairs in the source domain, to train an additional domain classifier as a discriminator.", "cited_ids": [{"paper_id": "20298653", "citation": "Golub et al. (2017)"}], "y": "Different from Golub et al. (2017), which only used pseudo question-answer pairs to fine-tune pre-trained MRC [Machine Reading Comprehension] models, the authors' AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passage-question pairs in the source domain, to train [via adversarial training] an additional [Question Answer Module] domain classifier as a discriminator.", "snippet_surface": "Different from Golub et al. (2017), which only used pseudo question-answer pairs to fine-tune pre-trained MRC models, the authors' AdaMRC model uses the passage and the generated pseudo-questions in the target domain, in addition to the human-annotated passage-question pairs in the source domain, to train an additional domain classifier as a discriminator.", "questions": {"/RkjYTogs+": "What are mrc models?"}, "answers": {"/RkjYTogs+": "MRC stands for Machine Reading Comprehension"}, "evidence": {"/RkjYTogs+": [{"section": "Abstract", "paragraph": "In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passage-question encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semi-supervised learning.", "selected": "we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain.", "paper_id": "201666897"}]}}
{"idx": "72451", "paper_id": "52938038", "title": "Improving the Transformer Translation Model with Document-Level Context", "abstract": "Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.", "context_section_header": "", "context_paragraph": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01). The gain on the concatenated test set (i.e., \"All\") is 1.96 BLEU points. It also outperforms the cache-based method  adapted for Transformer significantly (p < 0.01), which also uses the two-step training strategy. Table 4 shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task.  Table 3: Comparison with previous works on Chinese-English translation task. The evaluation metric is caseinsensitive BLEU score.  use a hierarchical RNN to incorporate document-level context into RNNsearch.  use a cache to exploit document-level context for RNNsearch. * is an adapted version of the cache-based method for Transformer. Note that \"MT06\" is not included in \"All\".   Table 5: Subjective evaluation of the comparison between the original Transformer model and our model. \">\" means that Transformer is better than our model, \"=\" means equal, and \"<\" means worse.", "sentence": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01).", "cited_ids": [{"paper_id": "13756489", "citation": "(Vaswani et al., 2017)"}], "y": "The authors' results show that their approach [which extends the Transformer model to use the document-level context (which the authors compute and incorporate into the encoder/decoder using multihead self-attention)] achieves significant improvements over the original Transformer model [the state-of-the-art NMT model that does not exploit document-level context] (p < 0.01).", "snippet_surface": "The authors' results show that their approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01), as seen in Table 3.", "questions": {"+khQ7bcV8j": "What is \"their approach\"?", "hBHkUEHZrb": "What is the original transformer model?"}, "answers": {"+khQ7bcV8j": "The author's approach is the main contribution of this paper (which they compare against several baseline alternatives). Their approach extends the Transformer model to use the document-level context (which they compute and incorporate into the encoder/decoder using multihead self-attention).", "hBHkUEHZrb": "The original Transformer model is the work described in Vaswani et al 2017.  It is the state-of-the-art NMT model that does not exploit document-level context."}, "evidence": {"+khQ7bcV8j": [{"section": "Introduction", "paragraph": "In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context, which is then incorporated into the encoder and decoder using multi-head attention. Since largescale document-level parallel corpora are usually hard to acquire, we propose to train sentencelevel model parameters on sentence-level paral- lel corpora first and then estimate document-level model parameters on document-level parallel corpora while keeping the learned original sentencelevel Transformer model parameters fixed. Our approach has the following advantages:", "selected": "we propose to extend the Transformer model to take advantage of documentlevel context", "paper_id": "52938038"}, {"section": "Introduction", "paragraph": "In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context, which is then incorporated into the encoder and decoder using multi-head attention. Since largescale document-level parallel corpora are usually hard to acquire, we propose to train sentencelevel model parameters on sentence-level paral- lel corpora first and then estimate document-level model parameters on document-level parallel corpora while keeping the learned original sentencelevel Transformer model parameters fixed. Our approach has the following advantages:", "selected": "The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context, which is then incorporated into the encoder and decoder using multi-head attention.", "paper_id": "52938038"}], "hBHkUEHZrb": [{"section": "Comparison with Previous Work", "paragraph": "3. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit documentlevel context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. 4. *: adapting the cachebased method to the Transformer model. We implement it on top of the open-source toolkit THUMT. We also use the same training data (i.e., 2M sentence pairs) and the same twostep training strategy to estimate sentenceand document-level parameters separately.", "selected": "3. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit documentlevel context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. 4. *: adapting the cachebased method to the Transformer model. We implement it on top of the open-source toolkit THUMT. We also use the same training data (i.e., 2M sentence pairs) and the same twostep training strategy to estimate sentenceand document-level parameters separately.", "paper_id": "52938038"}, {"section": "Comparison with Previous Work", "paragraph": "As shown in Table 3, using the same data, our approach achieves significant improvements over the original Transformer model (Vaswani et al., 2017) (p < 0.01). The gain on the concatenated test set (i.e., \"All\") is 1.96 BLEU points. It also outperforms the cache-based method  adapted for Transformer significantly (p < 0.01), which also uses the two-step training strategy. Table 4 shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task.  Table 3: Comparison with previous works on Chinese-English translation task. The evaluation metric is caseinsensitive BLEU score.  use a hierarchical RNN to incorporate document-level context into RNNsearch.  use a cache to exploit document-level context for RNNsearch. * is an adapted version of the cache-based method for Transformer. Note that \"MT06\" is not included in \"All\".   Table 5: Subjective evaluation of the comparison between the original Transformer model and our model. \">\" means that Transformer is better than our model, \"=\" means equal, and \"<\" means worse.", "selected": "original Transformer model (Vaswani et al., 2017)", "paper_id": "52938038"}]}}
{"idx": "75108", "paper_id": "12847491", "title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis", "abstract": "We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.", "context_section_header": "", "context_paragraph": "Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions on tree structures, which has been used for syntactic analysis (Socher et al., 2010) and sentence sentiment analysis (Socher et al., 2013). (Dong et al., 2014;Nguyen and Shirai, 2015) adopted Rec-NN for aspect sentiment classification, by converting the opinion target as the tree root and propagating the sentiment of targets depending on the context and syntactic relationships between them. However, Rec-NN needs dependency parsing which is likely ineffective on nonstandard texts such as news comments and tweets. (Chen et al., 2016) employed Convolution NNs to identify the senti-  ment of a clause which is then used to infer the sentiment of the target. The method has an assumption that an opinion word and its target lie in the same clause. TD-LSTM (Tang et al., 2015) utilizes LSTM to model the context information of a target by placing the target in the middle and propagating the state word by word from the beginning and the tail to the target respectively to capture the information before and after it. Nevertheless, TD-LSTM might not work well when the opinion word is far from the target, because the captured feature is likely to be lost (  reported similar problems of LSTM-based models in machine translation). (Graves et al., 2014) introduced the concept of memory for NNs and proposed a differentiable process to read and write memory, which is called Neural Turing Machine (NTM). Attention mechanism, which has been used successfully in many areas Rush et al., 2015), can be treated as a simplified version of NTM because the size of memory is unlimited and we only need to read from it. Single attention or multiple attentions were applied in aspect sentiment classification in some previous works (Wang et al., 2016;Tang et al., 2016). One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\"). More importantly, we combine the results of attentions in a nonlinear way. (Wang et al., 2016) only uses one attention, while our model uses multiple attentions. The effectiveness of multiple attentions was also investigated in QA task (Kumar et al., 2015), which shows that multiple attentions allow a model to attend different parts of the input during each pass. (Kumar et al., 2015) assigns attention scores to memory slices independently and their attention process is more complex, while we produce a normalized attention distribution to attend information from the memory.", "sentence": "One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "cited_ids": [{"paper_id": "359042", "citation": "(Tang et al., 2016)"}], "y": "One difference between the authors' method [a sentiment analysis model similar to the target dependent LSTM] and (Tang et al., 2016) is that the authors introduce a memory module between the attention module and the input module, thus their method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "snippet_surface": "One difference between the authors' method and (Tang et al., 2016) is that the authors introduce a memory module between the attention module and the input module, thus allowing the method to synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "questions": {"VxCqlAwvTe": "What is the authors' method?"}, "answers": {"VxCqlAwvTe": "A model to analyze the sentiment of a phraselike word sequence very similar to target dependent LSTM (TD-LSTM)  but including some differences."}, "evidence": {"VxCqlAwvTe": [{"section": "Introduction", "paragraph": "To model the sentiment of the above phraselike word sequence (i.e. \"not wonderful enough\"), LSTM-based methods are proposed, such as target dependent LSTM (TD-LSTM) (Tang et al., 2015). TD-LSTM might suffer from the problem that after it captures a sentiment feature far from the target, it needs to propagate the feature word by word to the target, in which case it's likely to lose this feature, such as the feature \"cost-effective\" for \"the phone\" in \"My overall feeling is that the phone, after using it for three months and considering its price, is really cost-effective\". 1 Attention mechanism, which has been successfully used in machine translation , can enforce a model to pay more attention to the important part of a sentence. There are already some works using attention in sentiment analysis to exploit this advantage (Wang et al., 2016;Tang et al., 2016). Another observation is that some types of sentence structures are particularly challenging for target sentiment analysis. For example, in \"Except Patrick, all other actors don't play well\", the word \"except\" and the phrase \"don't play well\" produce a positive sentiment on \"Patrick\". It's hard to synthesize these features just by LSTM, since their positions are dispersed. Single attention based methods (e.g. (Wang et al., 2016)) are also not capable to overcome such difficulty, because attending multiple words with one attention may hide the characteristic of each attended word.", "selected": "To model the sentiment of the above phraselike word sequence (i.e. \"not wonderful enough\"), LSTM-based methods are proposed, such as target dependent LSTM (TD-LSTM) (Tang et al., 2015).", "paper_id": "12847491"}, {"section": "Related Work", "paragraph": "Neural Networks (NNs) have the capability of fusing original features to generate new representations through multiple hidden layers. Recursive NN (Rec-NN) can conduct semantic compositions on tree structures, which has been used for syntactic analysis (Socher et al., 2010) and sentence sentiment analysis (Socher et al., 2013). (Dong et al., 2014;Nguyen and Shirai, 2015) adopted Rec-NN for aspect sentiment classification, by converting the opinion target as the tree root and propagating the sentiment of targets depending on the context and syntactic relationships between them. However, Rec-NN needs dependency parsing which is likely ineffective on nonstandard texts such as news comments and tweets. (Chen et al., 2016) employed Convolution NNs to identify the senti-  ment of a clause which is then used to infer the sentiment of the target. The method has an assumption that an opinion word and its target lie in the same clause. TD-LSTM (Tang et al., 2015) utilizes LSTM to model the context information of a target by placing the target in the middle and propagating the state word by word from the beginning and the tail to the target respectively to capture the information before and after it. Nevertheless, TD-LSTM might not work well when the opinion word is far from the target, because the captured feature is likely to be lost (  reported similar problems of LSTM-based models in machine translation). (Graves et al., 2014) introduced the concept of memory for NNs and proposed a differentiable process to read and write memory, which is called Neural Turing Machine (NTM). Attention mechanism, which has been used successfully in many areas Rush et al., 2015), can be treated as a simplified version of NTM because the size of memory is unlimited and we only need to read from it. Single attention or multiple attentions were applied in aspect sentiment classification in some previous works (Wang et al., 2016;Tang et al., 2016). One difference between our method and (Tang et al., 2016) is that we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\"). More importantly, we combine the results of attentions in a nonlinear way. (Wang et al., 2016) only uses one attention, while our model uses multiple attentions. The effectiveness of multiple attentions was also investigated in QA task (Kumar et al., 2015), which shows that multiple attentions allow a model to attend different parts of the input during each pass. (Kumar et al., 2015) assigns attention scores to memory slices independently and their attention process is more complex, while we produce a normalized attention distribution to attend information from the memory.", "selected": "we introduce a memory module between the attention module and the input module, thus our method can synthesize features of word sequences such as sentiment phrases (e.g. \"not wonderful enough\").", "paper_id": "12847491"}]}}
{"idx": "8580", "paper_id": "218613822", "title": "Smart To-Do: Automatic Generation of To-Do Items from Emails", "abstract": "Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.", "context_section_header": "", "context_paragraph": "Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004;Carenini et al., 2007;Dredze et al., 2008). There has also been considerable research on identifying speech acts or tasks in emails (Carvalho and Cohen, 2005;Lampert et al., 2010;Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019). Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo. Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset. (Lin et al., 2018) and identifying intents in email conversations . However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004). The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.", "sentence": "The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.", "cited_ids": [{"paper_id": "182953152", "citation": "(Zhang and Tetreault, 2019)"}], "y": "The closest work to the authors' is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas their method [generating to-do items from emails through a two step process; content selection and subsequent text generation] relies on identifying the task-related context.", "snippet_surface": "The closest work to the authors' is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas their method relies on identifying the task-related context.", "questions": {"gKc0JxKMWY": "What supervised approach is referred to?"}, "answers": {"gKc0JxKMWY": "Sentence selection"}, "evidence": {"gKc0JxKMWY": [{"section": "Related Works", "paragraph": "Summarization of email threads has been the focus of multiple research works in the past (Rambow et al., 2004;Carenini et al., 2007;Dredze et al., 2008). There has also been considerable research on identifying speech acts or tasks in emails (Carvalho and Cohen, 2005;Lampert et al., 2010;Scerri et al., 2010) and how it can be robustly adapted across diverse email corpora (Azarbonyad et al., 2019). Recently, novel neural architectures have been explored for modeling action items in emails 3 We will release the code and data (in accordance with LDC and Avocado policy) at https://aka.ms/SmartToDo. Email examples in this paper are similar to those in our dataset but are not reproducing text from the Avocado dataset. (Lin et al., 2018) and identifying intents in email conversations . However, there has been less focus on task-specific email summarization (Corston-Oliver et al., 2004). The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection, whereas our method relies on identifying the task-related context.", "selected": "The closest to our work is that of email subject line generation (Zhang and Tetreault, 2019). But it focuses on a common email theme and uses a supervised approach for sentence selection", "paper_id": "218613822"}, {"section": "Identifying Helpful Sentences for Commitment Task", "paragraph": "In the absence of reliable labels to extract helpful sentences in a supervised fashion, we resort to an unsupervised matching-based approach. Let the commitment sentence in the email be denoted as H, and the rest of the sentences from the current email e c and previous email e p be denoted as", "selected": "In the absence of reliable labels to extract helpful sentences in a supervised fashion, we resort to an unsupervised matching-based approach.", "paper_id": "218613822"}]}}
{"idx": "90656", "paper_id": "9300324", "title": "A hybrid text classification approach for analysis of student essays", "abstract": "We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Rose et al., 2002a). CarmelTC learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a Naive Bayes classification of that text. We explore the tradeoffs between symbolic and \"bag of words\" approaches. Our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two \"bag of words\" approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach.", "context_section_header": "", "context_paragraph": "In contrast to many previous approaches to automated essay grading (Burstein et al., 1998;Larkey, 1998), our goal is not to assign a letter grade to student essays. Instead, our purpose is to tally which set of \"correct answer aspects\" are present in student essays. For example, we expect satisfactory answers to the example question above to include a detailed explanation of how Newton's first law applies to this scenario. From Newton's first law, the student should infer that the pumpkin and the man will continue at the same constant horizontal velocity that they both had before the release. Thus, they will always have the same displacement from the point of release. Therefore, after the pumpkin rises and falls, it will land back in the man's hands. Our goal is to coach students through the process of constructing good physics explanations. Thus, our focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "sentence": "Thus, our focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "cited_ids": [{"paper_id": "2242666", "citation": "(Burstein et al., 2001)"}], "y": "The authors [are trying to build a hybrid text classification approach for analysing essays, called CarmelTC, and their] focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001)", "snippet_surface": "Thus, the authors' focus is on the physics content and not the quality of the student's writing, in contrast to (Burstein et al., 2001).", "questions": {"gdsOsOJ2Jo": "What are the authors trying to do?"}, "answers": {"gdsOsOJ2Jo": "They are trying to build a hybrid text classification approach for analysing essays, called CarmelTC"}, "evidence": {"gdsOsOJ2Jo": [{"section": "Abstract", "paragraph": "We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Rose et al., 2002a). CarmelTC learns to classify units of text based on features extracted from a syntactic analysis of that text as well as on a Naive Bayes classification of that text. We explore the tradeoffs between symbolic and \"bag of words\" approaches. Our goal has been to combine the strengths of both of these approaches while avoiding some of the weaknesses. Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two \"bag of words\" approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach.", "selected": "e present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Rose et al., 2002a).", "paper_id": "9300324"}]}}
{"idx": "91100", "paper_id": "17942720", "title": "Urdu and Hindi: Translation and sharing of linguistic resources", "abstract": "Hindi and Urdu share a common phonology, morphology and grammar but are written in different scripts. In addition, the vocabularies have also diverged significantly especially in the written form. In this paper we show that we can get reasonable quality translations (we estimated the Translation Error rate at 18%) between the two languages even in absence of a parallel corpus. Linguistic resources such as treebanks, part of speech tagged data and parallel corpora with English are limited for both these languages. We use the translation system to share linguistic resources between the two languages. We demonstrate improvements on three tasks and show: statistical machine translation from Urdu to English is improved (0.8 in BLEU score) by using a Hindi-English parallel corpus, Hindi part of speech tagging is improved (upto 6% absolute) by using an Urdu part of speech corpus and a Hindi-English word aligner is improved by using a manually word aligned Urdu-English corpus (upto 9% absolute in F-Measure).", "context_section_header": "", "context_paragraph": "Converting between the scripts of Hindi and Urdu is non-trivial and has been a recent focus (Malik et al., 2008;Malik et al., 2009). (Malik et al., 2008) uses hand designed rules encoded using finite state transducers to transliterate between Hindi and Urdu. As reported in (Malik et al., 2009) these hand designed rules achieve accuracies of only about 50% in the absence of diacritical marks. (Malik et al., 2009) improves Urdu\u2192Urdu transliteration performance to 79% by post processing the output of the transducer with a statistical language model. In contrast to (Malik et al., 2009) we use a statistical model for character transliteration. As discussed in Section 1.1, due to the divergence of vocabularies in written Hindi and Urdu, transliteration is not sufficient to convert from written Urdu to written Hindi. We also use a more flexible model that allows for more natural translations by allowing Urdu words to translate into Hindi words that do not sound the same. (Sinha, 2009) builds an English-Urdu machine translation system using an English-Hindi machine translation system and a Hindi-Urdu word mapping table, suitably adjusted for part of speech and gender. Their system is not statistical, and is largely based on manual creation of a large database of Hindi-Urdu correspondences. Additionally, as mentioned in the conclusion, their system cannot be used for direct translation from Hindi to Urdu, since a grammatical analysis of the English provides information necessary for the Hindi to Urdu mapping. In contrast to this work, our techniques are largely statistical, require minimal manual effort and can directly translate between Hindi and Urdu without the associated English.", "sentence": "In contrast to (Malik et al., 2009) we use a statistical model for character transliteration.", "cited_ids": [{"paper_id": "7957798", "citation": "(Malik et al., 2009)"}], "y": "In contrast to (Malik et al., 2009), [who uses a a finite-state model,] the authors use a [model that can be used to solve some problems mentioned in Malik's work] for character transliteration.", "snippet_surface": "In contrast to (Malik et al., 2009), the authors use a statistical model for character transliteration.", "questions": {"NIZnj7u7HH": "What kind of model does Malik et al. (2009) use?"}, "answers": {"NIZnj7u7HH": "A finite-state model"}, "evidence": {"NIZnj7u7HH": [{"section": "Native", "paragraph": "A finite-state transliteration model for Hindi and Urdu transliteration using the Universal Intermediate Transcription (UIT -a pivot between the two scripts) was proposed by Malik et al. (2008). The non-probabilistic finite-state model is not powerful enough to solve all problems of Hindi \u2194 Urdu transliteration. We visit and analyze Hindi \u2194 Urdu transliteration problems in the next section and show that the solution of these problems is beyond the scope of a nonprobabilistic finite-state transliteration model. Following this, we show how a statistical model can be used to solve some of these problems, thereby enhancing the capabilities of the finitestate model.", "selected": "Following this, we show how a statistical model can be used to solve some of these problems, thereby enhancing the capabilities of the finitestate model.", "paper_id": "7957798"}]}}
{"idx": "91184", "paper_id": "1832412", "title": "Automatic prediction of aspectual class of verbs in context", "abstract": "This paper describes a new approach to predicting the aspectual class of verbs in context, i.e., whether a verb is used in a stative or dynamic sense. We identify two challenging cases of this problem: when the verb is unseen in training data, and when the verb is ambiguous for aspectual class. A semi-supervised approach using linguistically-motivated features and a novel set of distributional features based on representative verb types allows us to predict classes accurately, even for unseen verbs. Many frequent verbs can be either stative or dynamic in different contexts, which has not been modeled by previous work; we use contextual features to resolve this ambiguity. In addition, we introduce two new datasets of clauses marked for aspectual class.", "context_section_header": "", "context_paragraph": "In contrast to Siegel and McKeown (2000), we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs. Instead we predict the aspectual class of verbs in the context of their arguments and modifiers. We show that this method works better than using only type-based features, especially for verbs with ambiguous aspectual class. In addition, we show that type-based features, including novel distributional features based on representative verbs, accurately predict predominant aspectual class for unseen verb types. Our work differs from prior work in that we treat the problem as a three-way classification task, predicting DYNAMIC, STATIVE or BOTH as the aspectual class of a verb in context.", "sentence": "In contrast to Siegel and McKeown (2000), we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs.", "cited_ids": [{"paper_id": "6433813", "citation": "Siegel and McKeown (2000)"}], "y": "In contrast to Siegel and McKeown (2000), the authors do not conduct the task of predicting aspectual class, [a property of verbs related to how they are used in context] solely at the type level. This approach ignores the minority class of ambiguous verbs.", "snippet_surface": "In contrast to Siegel and McKeown (2000), the authors do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs.", "questions": {"jyo7AzVqqL": "What is aspectual class?"}, "answers": {"jyo7AzVqqL": "It is a property of verbs relative to how they are used in context."}, "evidence": {"jyo7AzVqqL": [{"section": "Abstract", "paragraph": "This paper describes a new approach to predicting the aspectual class of verbs in context, i.e., whether a verb is used in a stative or dynamic sense. We identify two challenging cases of this problem: when the verb is unseen in training data, and when the verb is ambiguous for aspectual class. A semi-supervised approach using linguistically-motivated features and a novel set of distributional features based on representative verb types allows us to predict classes accurately, even for unseen verbs. Many frequent verbs can be either stative or dynamic in different contexts, which has not been modeled by previous work; we use contextual features to resolve this ambiguity. In addition, we introduce two new datasets of clauses marked for aspectual class.", "selected": "the aspectual class of verbs in context, i.e., whether a verb is used in a stative or dynamic sense.", "paper_id": "1832412"}, {"section": "Introduction", "paragraph": "(3) Your soul was made to be filled with God", "selected": "(3) Your soul was made to be filled with God", "paper_id": "1832412"}, {"section": "Abstract", "paragraph": "Aspectual classification maps verbs to a small set of primitive categories in order to reason about time. This classification is necessary for interpreting temporal modifiers and assessing temporal relationships, and is therefore a required component for many natural language applications. A verb's aspectual category can be predicted by co-occurrence frequencies between the verb and certain linguistic modifiers. These frequency measures, called linguistic indicators, are chosen by linguistic insights. However, linguistic indicators used in isolation are predictively incomplete, and are therefore insufficient when used individually. In this article, we compare three supervised machine learning methods for combining multiple linguistic indicators for aspectual classification: decision trees, genetic programming, and logistic regression. A set of 14 indicators are combined for classification according to two aspectual distinctions. This approach improves the classification performance for both distinctions, as evaluated over unrestricted sets of verbs occurring across two corpora. This demonstrates the effectiveness of the linguistic indicators and provides a much-needed full-scale method for automatic aspectual classification. Moreover, the models resulting from learning reveal several linguistic insights that are relevant to aspectual classification. We also compare supervised learning methods with an unsupervised method for this task.", "selected": "Aspectual classification maps verbs to a small set of primitive categories in order to reason about time.", "paper_id": "6433813"}, {"section": "Aspect in Natural Language", "paragraph": "the second sentence describes a state, which begins before the event described by the first sentence. Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988;Dorr 1992;Klavans 1994). In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988;Klavans and Chodorow 1992;Klavans 1994;Dorr 1992). Table 1 sun~narizes the three aspectual distinctions, which compose five aspectual categories. In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from extended events, which have a time duration (e.g., She ran to the store). Therefore, four classes of events are derived: culmination, culminated process, process, and point.", "selected": "the second sentence describes a state, which begins before the event described by the first sentence.", "paper_id": "6433813"}]}}
{"idx": "91729", "paper_id": "235683036", "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction", "abstract": "Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.", "context_section_header": "", "context_paragraph": "NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014;Gupta et al., 2016;Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and conse-quently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE.  and , on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on , Lin et al. (2020) introduced ONEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from ONEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. While several Seq2Seq-based models Zeng et al., 2018Wei et al., 2019;Zhang et al., 2019) have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in that: (1) it is generating a BFS/DFS traversal of the target graph, which captures dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information.", "sentence": "Note that our model differs from ONEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities.", "cited_ids": [{"paper_id": "220048375", "citation": "(Lin et al., 2020)"}], "y": "The authors' model [Hybrid Span Generator (HySPA)] differs from ONEIE (Lin et al., 2020) in that their model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while the author\u2019s method efficiently generates existing relations [like PHYS, and ORG-AFF] and entities.\"", "snippet_surface": "The authors' model differs from ONEIE (Lin et al., 2020) in that their model captures global relationships automatically through autoregressive generation while ONEIE uses feature engineered templates; Moreover, ONEIE needs to do pairwise classification for relation extraction, while their method efficiently generates existing relations and entities.", "questions": {"TTxDA77GDb": "What is pairwise classfication?", "xaefmJOqQ0": "What is the author's method?", "iM0r4gwho2": "What relations are extracted?"}, "answers": {"TTxDA77GDb": "Pairwise classification is a process of identifying relation types between entities.", "xaefmJOqQ0": "The authors method is a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities.", "iM0r4gwho2": "There are 6 types of relations but the authors only specify two: PHYS, ORG-AFF"}, "evidence": {"TTxDA77GDb": [{"section": "Introduction", "paragraph": "Recent joint IE models Wang and Lu, 2020;Lin et al., 2020) have shown impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of the cases due to the sparsity of relations between entities. Also, pairwise scoring techniques evaluate each relation type independently and thus fail to capture interrelations between relation types for different pairs of mentions.", "selected": "Previous work often uses pairwise scoring techniques to identify relation types between entities.", "paper_id": "235683036"}], "xaefmJOqQ0": [{"section": "Abstract", "paragraph": "Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.", "selected": "a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities.", "paper_id": "235683036"}], "iM0r4gwho2": [{"section": "Modeling Information Graphs as Alternating Sequences", "paragraph": "An information graph can be viewed as a heterogeneous multigraph Shi et al., 2017) G = (V, E), where V is a set of nodes (typically representing spans (t s , t e ) in the input document) and E is a multiset of edges with a node type mapping function \u03c6 : V \u2192 Q and an edge type mapping function \u03c8 : E \u2192 R. Node and edge types are assumed to be drawn from a finite vocabulary. Node types can be used e.g. to represent entity types (PER, ORG, etc.), while edge types may represent relations (PHYS, ORG-AFF, etc.) between the nodes. In this work, we represent node types as separate nodes that are connected to their node v by a special edge type, [TYPE]. 2", "selected": "PHYS, ORG-AFF", "paper_id": "235683036"}, {"section": "Experimental Setting", "paragraph": "We test our model on the ACE 2005 dataset distributed by LDC 4 , which includes 14.5k sentences, 38.3k entities (with 7 types), and 7.1k relations (with 6 types), derived from the general news domain. More details can be found in Appendix C.", "selected": "7.1k relations (with 6 types)", "paper_id": "235683036"}]}}
{"idx": "92677", "paper_id": "18127129", "title": "Automatically Building a Corpus for Sentiment Analysis on Indonesian Tweets", "abstract": "The popularity of the user generated content, such as Twitter, has made it a rich source for the sentiment analysis and opinion mining tasks. This paper presents our study in automatically building a training corpus for the sentiment analysis on Indonesian tweets. We start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the Expectation and Maximization (EM) framework. We apply our automatically built corpus to perform two tasks, namely opinion tweet extraction and tweet polarity classification using various machine learning approaches. Experiment result shows that a classifier model trained on our data, which is automatically constructed using our proposed method, outperforms the baseline system in terms of opinion tweet extraction and tweet polarity classification.", "context_section_header": "", "context_paragraph": "First, we use different techniques to automatically collect training data. Second, we perform two-level sentiment analysis, namely, opinion tweet extraction and tweet polarity classification. Moreover, in the experiment section, we show that our method to collect training data is better than the one proposed by Pak and Paroubek (2010). Our method also produces much larger data since we do not rely on sheer emoticon-containing tweets to collect training data.", "sentence": "Moreover, in the experiment section, we show that our method to collect training data is better than the one proposed by Pak and Paroubek (2010).", "cited_ids": [{"paper_id": "550498", "citation": "Pak and Paroubek (2010)"}], "y": "The authors show in the experiment section that [starting with a small set of labeled seed corpus and expanding it using a classifier model] to collect training data [outperforms the baseline] proposed by Pak and Paroubek (2010).", "snippet_surface": "Moreover, in the experiment section, the authors show that their method to collect training data is better than the one proposed by Pak and Paroubek (2010).", "questions": {"VxCqlAwvTe": "What is the authors' method?"}, "answers": {"VxCqlAwvTe": "The authors' method involves starting with a small set of labeled seed corpus and expanding it using a classifier model whose parameters are estimated using the EM algorithm."}, "evidence": {"VxCqlAwvTe": [{"section": "Abstract", "paragraph": "The popularity of the user generated content, such as Twitter, has made it a rich source for the sentiment analysis and opinion mining tasks. This paper presents our study in automatically building a training corpus for the sentiment analysis on Indonesian tweets. We start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the Expectation and Maximization (EM) framework. We apply our automatically built corpus to perform two tasks, namely opinion tweet extraction and tweet polarity classification using various machine learning approaches. Experiment result shows that a classifier model trained on our data, which is automatically constructed using our proposed method, outperforms the baseline system in terms of opinion tweet extraction and tweet polarity classification.", "selected": "We start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the Expectation and Maximization (EM) framework. We apply our automatically built corpus to perform two tasks, namely opinion tweet extraction and tweet polarity classification using various machine learning approaches.", "paper_id": "18127129"}, {"section": "Introduction", "paragraph": "To overcome the aforementioned problem, we propose a method that can automatically develop 3 http://semiocast.com/en/publications/ 2012_07_30_Twitter_reaches_half_a_billion_ accounts_140m_in_the_US training data from a pool of millions of tweets. First, we automatically construct a small set of labeled seed corpus (i.e. small collection of positive and negative tweets) that will be used for expanding the training data in the next step. Next, we expand the training data using the previously constructed seed corpus. To do that, we use the rationale that sentiment can be propagated from the labeled seed tweets to the other unlabeled tweets when they share similar word features, which means that the sentiment type of an unlabeled tweet can be revealed based on its closeness to the labeled tweets. Based on that idea, we employ a classifier model whose parameters are estimated using labeled and unlabeled tweets via Expectation and Maximization (EM) framework. In this method, we incorporate two types of dataset: the first dataset is a small set of labeled seed tweets and the second dataset is a huge set of unlabeled tweets that serve as a source for expanding the training data. Intuitively, this method allows us to propagate sentiment from labeled tweets to unlabeled tweets. Later, we show that the training data automatically constructed by our method can be used by the classifiers to effectively tackle the problem of opinion tweet extraction and tweet polarity classification.", "selected": "First, we automatically construct a small set of labeled seed corpus (i.e. small collection of positive and negative tweets) that will be used for expanding the training data in the next step. Next, we expand the training data using the previously constructed seed corpus. To do that, we use the rationale that sentiment can be propagated from the labeled seed tweets to the other unlabeled tweets when they share similar word features, which means that the sentiment type of an unlabeled tweet can be revealed based on its closeness to the labeled tweets. Based on that idea, we employ a classifier model whose parameters are estimated using labeled and unlabeled tweets via Expectation and Maximization (EM) framework. In this method, we incorporate two types of dataset: the first dataset is a small set of labeled seed tweets and the second dataset is a huge set of unlabeled tweets that serve as a source for expanding the training data. Intuitively, this method allows us to propagate sentiment from labeled tweets to unlabeled tweets. Later, we show that the training data automatically constructed by our method can be used by the classifiers to effectively tackle the problem of opinion tweet extraction and tweet polarity classification.", "paper_id": "18127129"}, {"section": "Data Collection", "paragraph": "Our corpus consists of 5.3 million tweets which were collected using Twitter Streaming API between May 16th, 2013 and June 26th, 2013. As we wanted to build Indonesian sentiment corpus, we used tweet's geo-location to filter tweets posted in the area of Indonesia. We applied language filtering because based on our observation, Indonesian Twitter users also like to use English or local language in their tweets. We then divided our corpus into four disjoint datasets.  To collect DATASET3 (i.e. neutral or nonopinion tweets), we used the same approach as in (Pak and Paroubek, 2010). First, we selected some popular Indonesian news portal accounts from the overall corpus and then labeled them as objective.", "selected": "Our corpus consists of 5.3 million tweets which were collected using Twitter Streaming API between May 16th, 2013 and June 26th, 2013.", "paper_id": "18127129"}]}}
{"idx": "93538", "paper_id": "34272708", "title": "Bootstrapping Arabic-Italian SMT through Comparable Texts and Pivot Translation", "abstract": "This paper describes efforts towards the development of an Arabic to Italian SMT system for the news domain. Since only very little parallel data are available for this language pair, we investigated both the exploitation of comparable corpora and pivot translation. Experimental evaluation was conducted on a new benchmark developed by extending two Arabic-to-English NIST evaluation sets. Preliminary results show potentials of both approaches with respect to performance achieved by a popular state-of-the-art Web-based translation service.", "context_section_header": "", "context_paragraph": "In Section 4.1 we propose three different document alignment methods and experimentally evaluate them on the basis of precision and recall. All methods share the initial translation of the document title, which is then used to detect comparable documents. In such respect, also our methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "sentence": "In such respect, also our methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "cited_ids": [{"paper_id": "541460", "citation": "(Uszkoreit et al., 2010)"}], "y": "The authors' methods [three different document alignment methods] only rely on the textual content of the documents [news report texts expressing the same content in different languages] and do not require any meta-data, much alike but more affordable than Uszkoreit et al., (2010).", "snippet_surface": "The authors' methods only rely on the textual content of the documents and do not require any meta-data, much alike but more affordable than (Uszkoreit et al., 2010).", "questions": {"N2VySnmkw7": "What do the documents look like?"}, "answers": {"N2VySnmkw7": "They are news report texts expressing the same content in different languages."}, "evidence": {"N2VySnmkw7": [{"section": "Related Work", "paragraph": "Nowadays, several international news agencies deliver content through the Web in many languages. This represents a formidable opportunity to collect comparable documents, that is texts expressing the same content in different languages. Although these documents are not parallel, it often happens that some portions of them are mutual translations to some extent. In the last years, much effort has been devoted by the research community to the effective exploitation of such data for SMT.", "selected": "Nowadays, several international news agencies deliver content through the Web in many languages. This represents a formidable opportunity to collect comparable documents, that is texts expressing the same content in different languages.", "paper_id": "34272708"}, {"section": "Related Work", "paragraph": "The first problem to face is the alignment of multilingual documents reporting the same news. This problem has been rarely investigated systematically, as the usual pairing strategies aim to keep low the missing rate, that is to reward the recall. One of the most valuable methods is that presented in (Uszkoreit et al., 2010). There, all non-English documents are translated into English through a initial, even low-quality, translation system. Documents are then paired in two steps: the first generates a set of candidate pairs of documents sharing at least a certain number of rare features. This step is made linear in the number of input documents by setting a threshold defining such rare features. In the second step, a computationally more expensive and fine grained comparison is performed for deciding whether such document pairs are comparable or not.", "selected": "The first problem to face is the alignment of multilingual documents reporting the same news.", "paper_id": "34272708"}]}}
{"idx": "93720", "paper_id": "10702193", "title": "Composing Simple Image Descriptions using Web-scale N-grams", "abstract": "Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches.", "context_section_header": "", "context_paragraph": "Our work contrasts to most previous approaches in four key aspects: first, we compose fresh sentences from scratch, instead of retrieving (Farhadi et al. (2010)), or summarizing existing text fragments associated with an image (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010a)). Second, we aim to generate textual descriptions that are truthful to the specific content of the image, whereas related (but subtly different) work in automatic caption generation creates news-worthy text (Feng and Lapata (2010a)) or encyclopedic text (Aker and Gaizauskas (2010)) that is contextually relevant to the image, but not closely pertinent to the specific content of the image. Third, we aim to build a general image description method as compared to work that requires domain specific hand-written grammar rules (Yao et al. (2010)). Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "sentence": "Last, we allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "cited_ids": [{"paper_id": "53307035", "citation": "(Kulkarni et al., 2011)"}], "y": "The authors allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach [as proposed in (Kulkarni et al., 2011)] that drives annotation more directly from computer vision inputs [e.g. images and videos.]", "snippet_surface": "Lastly, the authors allow for some creativity in the generation process which produces more humanlike descriptions than a closely related, very recent approach that drives annotation more directly from computer vision inputs (Kulkarni et al., 2011).", "questions": {"N42KB7KXmp": "What does \"computer vision inputs\" refer to?"}, "answers": {"N42KB7KXmp": "It is referring to input data types that are typically processed and analysed in the field of computer vision such as images and videos."}, "evidence": {"N42KB7KXmp": []}}
{"idx": "9500", "paper_id": "20667722", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "abstract": "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.", "context_section_header": "", "context_paragraph": "In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL). In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph. The agent takes incremental steps by sampling a relation to extend its path. To better guide the RL agent for learning relational paths, we use policy gradient training (Mnih et al., 2015) with a novel reward function that jointly encourages accuracy, diversity, and efficiency. Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset.", "sentence": "In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph.", "cited_ids": [{"paper_id": "14941970", "citation": "(Bordes et al., 2013)"}], "y": "In contrast to PRA, the authors use a translation-based knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of their RL agent [which is used in a path learning process], which reasons in the vector space environment of the knowledge graph [which contains information on abstract or concrete entities].", "snippet_surface": "In contrast to PRA, the authors use a translation-based knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of their RL agent, which reasons in the vector space environment of the knowledge graph.", "questions": {"3MfvP3ejzD": "What rl agent is being referred to?", "RsliDKoSib": "What type of the knowledge does the knowledge graph contain?"}, "answers": {"3MfvP3ejzD": "The RL agent is the reinforcement learning agent used in the path learning process.", "RsliDKoSib": "Knowledge graphs contain information on entities that represent an abstract concept or concrete entity of the world."}, "evidence": {"3MfvP3ejzD": [{"section": "Introduction", "paragraph": "In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL). In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph. The agent takes incremental steps by sampling a relation to extend its path. To better guide the RL agent for learning relational paths, we use policy gradient training (Mnih et al., 2015) with a novel reward function that jointly encourages accuracy, diversity, and efficiency. Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset.", "selected": "In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL). In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph.", "paper_id": "20667722"}], "RsliDKoSib": [{"section": "Introduction", "paragraph": "Multi-relational data refers to directed graphs whose nodes correspond to entities and edges of the form (head, label, tail) (denoted (h, , t)), each of which indicates that there exists a relationship of name label between the entities head and tail. Models of multi-relational data play a pivotal role in many areas. Examples are social network analysis, where entities are members and edges (relationships) are friendship/social relationship links, recommender systems where entities are users and products and relationships are buying, rating, reviewing or searching for a product, or knowledge bases (KBs) such as Freebase 1 , Google Knowledge Graph 2 or GeneOntology 3 , where each entity of the KB represents an abstract concept or concrete entity of the world and relationships are predicates that represent facts involving two of them. Our work focuses on modeling multi-relational data from KBs (Wordnet [9] and Freebase [1] in this paper), with the goal of providing an efficient tool to complete them by automatically adding new facts, without requiring extra knowledge.", "selected": "Examples are social network analysis, where entities are members and edges (relationships) are friendship/social relationship links, recommender systems where entities are users and products and relationships are buying, rating, reviewing or searching for a product, or knowledge bases (KBs) such as Freebase 1 , Google Knowledge Graph 2 or GeneOntology 3 , where each entity of the KB represents an abstract concept or concrete entity of the world and relationships are predicates that represent facts involving two of them.", "paper_id": "14941970"}, {"section": "Introduction", "paragraph": "Multi-relational data refers to directed graphs whose nodes correspond to entities and edges of the form (head, label, tail) (denoted (h, , t)), each of which indicates that there exists a relationship of name label between the entities head and tail. Models of multi-relational data play a pivotal role in many areas. Examples are social network analysis, where entities are members and edges (relationships) are friendship/social relationship links, recommender systems where entities are users and products and relationships are buying, rating, reviewing or searching for a product, or knowledge bases (KBs) such as Freebase 1 , Google Knowledge Graph 2 or GeneOntology 3 , where each entity of the KB represents an abstract concept or concrete entity of the world and relationships are predicates that represent facts involving two of them. Our work focuses on modeling multi-relational data from KBs (Wordnet [9] and Freebase [1] in this paper), with the goal of providing an efficient tool to complete them by automatically adding new facts, without requiring extra knowledge.", "selected": "Examples are social network analysis, where entities are members and edges (relationships) are friendship/social relationship links, recommender systems where entities are users and products and relationships are buying, rating, reviewing or searching for a product, or knowledge bases (KBs) such as Freebase 1 , Google Knowledge Graph 2 or GeneOntology 3 , where each entity of the KB represents an abstract concept or concrete entity of the world and relationships are predicates that represent facts involving two of them.", "paper_id": "14941970"}]}}
