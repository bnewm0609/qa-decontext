{"idx": "1604.00400.2.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Summarization Evaluation by Relevance Analysis (Sera)", "context_paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "sentence": "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "cited_ids": [], "y": "The authors indirectly evaluate the content relevance of terms [that are semantically related] between a system generated summary and the human summary using information retrieval. To accomplish this, they use the [generated] summaries as search queries and compare the overlaps of the retrieved results [to the corresponding human written summaries].", "snippet_surface": "On a high level, the authors indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, they use the summaries as search queries and compare the overlaps of the retrieved results.", "questions": {"oov6c32bb8": "How do the authors perform \"information retrieval\"?", "Vw0oNetOfN": "What does \"on a high level\" mean?"}, "answers": {"oov6c32bb8": "The system generated scientific summaries are used as search queries where the overlap of the retrieved results is then compared (to the corresponding human written summaries).", "Vw0oNetOfN": "Compared to related work, this paper uses information retrieval to evaluate the content relevance between a system generated summary and a human written summary. Additionally, this method allows the authors to compare the relevance of terms that are semantically related (not just lexically equivalent)."}, "evidence": {"oov6c32bb8": [{"section": "Abstract", "paragraph": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "selected": "we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries.", "paper_id": "1604.00400"}, {"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "paper_id": "1604.00400"}], "Vw0oNetOfN": [{"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", "paper_id": "1604.00400"}, {"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.", "paper_id": "1604.00400"}]}}
{"idx": "1802.07862.1.1.1", "paper_id": "1802.07862", "title": "Multimodal Named Entity Recognition for Short Social Media Posts", "abstract": "We introduce a new task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images. These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.", "context_section_header": "Results: SnapCaptions Dataset", "context_paragraph": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token. In the example of \u201cdisney word essential = coffee\" with visual tags selfie, phone, person, the modality attention successfully attenuates distracting visual signals and focuses on textual modalities, consequently making correct predictions. The named entities in the examples of \u201cBeautiful night atop The Space Needle\" and \u201cSplash Mountain\" are challenging to predict because they are composed of common nouns (space, needle, splash, mountain), and thus they often need additional contexts to correctly predict. In the training data, visual contexts make stronger indicators for these named entities (space needle, splash mountain), and the modality attention module successfully attends more to stronger signals.", "sentence": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token.", "cited_ids": [], "y": "The authors confirm that the modality attention successfully attenuates [reduces] irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token for the image-aided model (W+C+V).", "snippet_surface": "The authors confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token for the image-aided model (W+C+V; upper row in Figure FIGREF19).", "questions": {"vJrz+pNHzx": "What does \"attenuates\" mean?"}, "answers": {"vJrz+pNHzx": "To diminish or reduce."}, "evidence": {"vJrz+pNHzx": []}}
{"idx": "271822", "paper_id": "235624320", "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types", "abstract": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task.", "context_section_header": "", "context_paragraph": "OpenKG Embeddings: Learning embeddings for OpenKGs has been a relatively under-explored area of research. Previous work using OpenKG embeddings has primarily focused on canonicalization. CESI (Vashishth et al., 2018) uses KG embedding models for the canonicalization of noun phrases in OpenKGs. The problem of incorporating canonicalization information into OpenKG embeddings was addressed by Gupta et al. (2019). Their method for OpenKG embeddings (i.e., CaRE) performs better than Ontological KG embedding baselines in terms of link prediction performance. The challenges in the link prediction for OpenKGs were discussed in Broscheit et al. (2020), and methods similar to CaRE were proposed. In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task.", "sentence": "In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task.", "cited_ids": [{"paper_id": "202763196", "citation": "(Gupta et al., 2019)"}], "y": "In spirit, CaRE ([an OpenKG embedding method consisting of three components; ]Gupta et al., 2019) comes closest to the authors' model, but does not address the problem of type compatibility [predicting a head (noun phrases) pair and tail noun phrase using a compatibility score] in the link prediction task.", "snippet_surface": "In spirit, CaRE (Gupta et al., 2019) comes closest to the authors' model; however, they do not address the problem of type compatibility in the link prediction task.", "questions": {"B/fqVQzKlR": "What is \"care\"?", "4T9JAsYA/w": "What is the authors' model?", "uQiTjUgIcX": "What is type compatibility?"}, "answers": {"B/fqVQzKlR": "CaRE is an OpenKG embedding method and consists of three components.", "4T9JAsYA/w": "The author's model is a form of OpenKG link prediction and improves this using novel type compatibility score and type regularization.", "uQiTjUgIcX": "Type compatibility is predicting a head (noun phrases) pair and tail noun phrase using a compatibility score."}, "evidence": {"B/fqVQzKlR": [{"section": "Background", "paragraph": "Here, h and t are called the head and tail NPs, and r is the RP between them. Each of them contains tokens from a vocabulary V, specifically, h = (w h 1 , w h 2 , . . . , w h k h ), t = (w t 1 , w t 2 , . . . , w t kt ) and r = (w r 1 , w r 2 , . . . , w r kr ). Here, k h , k r , and k t are the numbers of tokens in the head NP, the relation, and the tail NP. OpenKG embedding methods learn vector representations for NPs and RPs. Specifically, vectors for an NP e \u2208 N and an RP r \u2208 R are represented by boldface letters e \u2208 R de and r \u2208 R dr . Here, d e and d r are dimensions of NP and RP vectors. Usually, d e = d r . A score function \u03c8(h, r, t) represents the plausibility of a triple. Similarly, BERT represents tokens by d Bdimensional vectors. A type projection matrix P takes the vectors to a common d \u03c4 -dimensional type space R d\u03c4 . The vectors in the type space are denoted by \u03c4 . BERT (Devlin et al., 2019): BERT is a bidirectional language representation model based on the transformer architecture (Vaswani et al., 2017), which has shown performance improvements across multiple NLP tasks. It is pre-trained on two tasks, (1) Masked Language Modeling (MLM), where the model is trained to predict randomly masked tokens from the input sentences, and (2) Next Sentence Prediction (NSP), where the model is trained to predict whether an input pair of sentences occurs in a sequence or not. In our case, we use a pre-trained BERT model (without fine-tuning) for predicting a masked tail NP in a triple. CaRE (Gupta et al., 2019): CaRE is an OpenKG embedding method that can incorporate NP canonicalization information while learning the embeddings. NP canonicalization is the problem of grouping all surface forms of a given entity in one cluster, e.g., inferring that Barack Obama, Barack H. Obama, and President Obama all refer to the same underlying entity. CaRE consists of three components: (1) a canonicalization cluster encoder (CN), which generates NP embeddings by aggregating embeddings of canonical NPs from the corresponding cluster, (2) a bi-directional GRU based phrase encoder (PN), which encodes the tokens in RPs to generate RP embeddings, and (3) a base model, which is an Ontological KG embedding method like ConvE (Dettmers et al., 2018). It uses NP and RP embeddings for scoring triples. These triple scores are then fed to a loss function (e.g., pairwise ranking loss with negative sampling (Bordes et al., 2013) or binary cross-entropy loss (BCE) (Dettmers et al., 2018)). In this paper, we use CaRE with ConvE as the base model. This model generates a candidate tail NP vector for a given NP h and RP r, denoted by CaRE(h, r).", "selected": "CaRE is an OpenKG embedding method that can incorporate NP canonicalization information while learning the embeddings.", "paper_id": "235624320"}], "4T9JAsYA/w": [{"section": "Abstract", "paragraph": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task.", "selected": "We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization.", "paper_id": "235624320"}, {"section": "Introduction", "paragraph": "Motivated by this, we employ BERT for improving OpenKG link prediction, using novel type compatibility score (Section 4.2) and type regularizer term (Section 4.4). We propose OKGIT, a method for OpenKG link prediction with improved type compatibility. We test our model on multiple datasets and show that it achieves state-of-the-art performance on all of these datasets.", "selected": "Motivated by this, we employ BERT for improving OpenKG link prediction, using novel type compatibility score (Section 4.2) and type regularizer term (Section 4.4). We propose OKGIT, a method for OpenKG link prediction with improved type compatibility.", "paper_id": "235624320"}], "uQiTjUgIcX": [{"section": "Abstract", "paragraph": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task.", "selected": "owever, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases.", "paper_id": "235624320"}, {"section": "\u03c8 TYPE : Tail Type Compatibility Score", "paragraph": "The type compatibility between a given (head NP, RP) pair and a tail NP is measured by the type compatibility score function \u03c8 TYPE . It assigns a high score when an NP t has suitable types as candidate tail NP for given head NP h and RP r. We employ a Masked Language Model (MLM) for measuring type compatibility, specifically BERT (Devlin et al., 2019). Following (Petroni et al., 2019), we can generate a candidate tail NP vector using BERT. Specifically, given a triple (h, r, t), we replace the head NP h and RP r with their tokens and tail NP t with a special MASK token. The resulting sentence (w h 1 , . . . , w h k h , w r 1 , . . . , w r kr , MASK) is sent as input to the BERT model. We denote the output vector from BERT corresponding to the MASK tail token as t B .", "selected": "The type compatibility between a given (head NP, RP) pair and a tail NP is measured by the type compatibility score function \u03c8 TYPE .", "paper_id": "235624320"}]}}
{"idx": "392826", "paper_id": "207901266", "title": "What does the language of foods say about us?", "abstract": "In this work we investigate the signal contained in the language of food on social media. We experiment with a dataset of 24 million food-related tweets, and make several observations. First,thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), income, poverty, and education \u2013 outperforming previous work by 4\u201318%. Second, we investigate the effect of socioeconomic factors (income, poverty, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but,importantly,thelanguage of food adds distinct information that is not captured by socioeconomics. Third, we analyze how the language of food has changed over a five-year period (2013 \u2013 2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last,weprovideanonlinevisualization tool for real-time queries and semantic analysis.", "context_section_header": "", "context_paragraph": "With an average of 6,000 new tweets posted every second, Twitter 1 has become a digital footprint of everyday life for a representative sample of the United States (US) population (Mislove et al., 2011). Previously, Fried et al. (2014) demonstrated that the language of food on Twitter can be used to predict health risks, political orientation, and geographic location. Here, we use predictive models to extend this analysis -exploring the ways in which the language of food can shed insight on health and the changing trends in * Equal contribution. 1 https://twitter.com/ both food culture and language use in different communities over time. We apply this methodology to the particular use case of predicting communities which are risk for type 2 diabetes mellitus (T2DM), a serious medical condition which affects over 30 million Americans and whose diagnosis alone costs $327 billion each year 2 . We refer to T2DM as diabetes in the rest of the paper. We show that by combining knowledge from tweets with other social characteristics (e.g., average income, level of education) we can better predict risk of T2DM. The contributions of this work are four-fold: 1. We use the same methods proposed by Fried et al. (2014) with a much larger (7 times) tweet corpus gathered from 2013 -2017 to predict the risk of T2DM. We collected over 24 million tweets with meal-related hashtags (e.g., #breakfast, #lunch) and localized 5 million of them to states within the US. We show that more data helps, and that by training on this larger dataset the state-level T2DM risk prediction accuracy is improved by 4-18%, compared to the results in Fried et al. (2014). We also apply the same models to predict additional state-level indicators: income, poverty, and education levels in order to further investigate the predictive power of the language of food. On these prediction tasks, our model outperforms the majority baseline by 12-34%. We believe that this work may drive immediate policy decisions for the communities deemed at risk without awaiting for similar results from major health organizations, which take months or years to be generated and disseminated. 3 Equally as important, we believe that this state-level T2DM risk prediction task may improve predicting risks for individuals from their social media activity, a task which often suffers from sparsity (Bell et al., 2018). 2. Unlike (Fried et al., 2014), we also investigate the effect of socioeconomic factors on the diabetes prediction task itself. We observe that aggregated US social demographic information from average income 4 , poverty 5 , and education 6 is complementary to the information gained from tweet language used for predicting diabetes risk. We add the correlation between each of these socioeconomic factors and the diabetes 7 rate in US states as additional features in the models in (1). We demonstrate that the T2DM prediction model strongly benefits from the additional information, as prediction accuracy further increases by 2-6%. However, importantly, the model that relies solely on these indicators performs considerably worse than the model that includes features from the language of food, which demonstrates that the language of food provides distinct signal from these indicators. 3. Furthermore, with a dataset that spans nearly five years, we also analyze language trends over time. Specifically, using pointwise mutual information (PMI) and a custom-built collection of healthy/unhealthy food words, we investigate the strength of healthy/unhealthy food references on Twitter, and observe a downward trend for healthy food references and an upward trend for unhealthy food words in the US. 4. Lastly, we provide a visualization tool to help understand and visualize semantic relations between words and various categories such as how different genders refer to vegetarian vs. lowcarb diets. 8 Our tool is based on semantic axes plots (Heimerl and Gleicher, 2018).", "sentence": "2. Unlike (Fried et al., 2014), we also investigate the effect of socioeconomic factors on the diabetes prediction task itself.", "cited_ids": [{"paper_id": "14599127", "citation": "(Fried et al., 2014)"}], "y": "Unlike (Fried et al., 2014), the authors also investigate the effect of socioeconomic factors [income, poverty, and education] on the diabetes prediction task itself.", "snippet_surface": "Unlike (Fried et al., 2014), the authors also investigate the effect of socioeconomic factors on the diabetes prediction task itself.", "questions": {"lEUzLiMPlq": "What socioeconomic factors are bring referred to?"}, "answers": {"lEUzLiMPlq": "Income, poverty and education."}, "evidence": {"lEUzLiMPlq": [{"section": "Abstract", "paragraph": "In this work we investigate the signal contained in the language of food on social media. We experiment with a dataset of 24 million food-related tweets, and make several observations. First,thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), income, poverty, and education \u2013 outperforming previous work by 4\u201318%. Second, we investigate the effect of socioeconomic factors (income, poverty, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but,importantly,thelanguage of food adds distinct information that is not captured by socioeconomics. Third, we analyze how the language of food has changed over a five-year period (2013 \u2013 2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last,weprovideanonlinevisualization tool for real-time queries and semantic analysis.", "selected": "we investigate the effect of socioeconomic factors (income, poverty, and education) on predicting state-level T2DM rates.", "paper_id": "207901266"}]}}
{"idx": "486090", "paper_id": "227230410", "title": "A High Precision Pipeline for Financial Knowledge Graph Construction", "abstract": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.", "context_section_header": "", "context_paragraph": "Compared with (Benetka et al., 2017), the most closely related work, our pipeline extracts over 342,000 n-ary facts and covers more types of financial predicates -a total of 87 as opposed to 50 in (Benetka et al., 2017). Furthermore, our pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "sentence": "Furthermore, our pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "cited_ids": [{"paper_id": "26077690", "citation": "(Benetka et al., 2017)"}], "y": "The authors' pipeline [that combines Semantic Role Labeling (SRL) information extraction for verb predicates with typed patterns for noun-mediated relations] produces high precision extractions [of the relations between between financial entities from financial text], specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "snippet_surface": "Furthermore, the authors' pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "questions": {"A1oAR0vcX5": "What is the \"authors' pipeline\"?", "1ELxDE24dq": "What extractions are being refered to?", "lhmGxrLDhv": "How is the high precision achieved?", "/CUFrwB9+/": "What pipeline is being refered to?"}, "answers": {"A1oAR0vcX5": "A high precision knowledge extraction pipeline developed by authors that is tailored to the financial news domain by combining Semantic Role Labeling (SRL) information extraction for verb predicates with typed patterns for noun-mediated relations.", "1ELxDE24dq": "The recovered financial entities and their relationships are high precision structured representations taken from financial news stories. These extractions have the form of n-ary facts, where n is the number of entities and relations in the fact.", "lhmGxrLDhv": "A variety of strategies is used to attain high precision in the information extraction pipeline. To extract domain-targeted noun/verb-mediated relations, the pipeline integrates Semantic Role Labelling (SRL) and pattern-based information extraction. A dictionary of semantically and structurally limited sense-disambiguated financial predicates is also employed to filter out noisy SRL extractions.\n", "/CUFrwB9+/": "It refers to a process that involves a series of steps to extract high-quality structured representations of financial and business entities from thousands of financial news articles."}, "evidence": {"A1oAR0vcX5": [{"section": "Introduction", "paragraph": "In this work, we develop a high precision knowledge extraction pipeline tailored to the financial news domain by combining SRL information extraction for verb predicates with typed patterns for noun mediated relations. This pipeline filters noisy predicate-argument structures via a dictionary of semantically and structurally constrained sense-disambiguated financial predicates. In order to maximize the utility of the extractions for downstream tasks, our pipeline produces compact extractions via dictionary-guided minimization of overly-specific arguments. These extractions are scored using a binary classifier, with the score reflecting our confidence in the extracted fact. We perform a lossless decomposition of the nary relations extracted, to construct the KG. While some components of the the pipeline are customized to the financial domain, we believe with small tweaks it can be easily adapted to other domains.", "selected": "In this work, we develop a high precision knowledge extraction pipeline tailored to the financial news domain by combining SRL information extraction for verb predicates with typed patterns for noun mediated relations.", "paper_id": "227230410"}], "1ELxDE24dq": [{"section": "Introduction", "paragraph": "Our goal is to automatically extract high precision structured representations from thousands of financial news articles covering a broader range of financial entities such as markets, stocks, persons (e.g., CEOs, presidents, etc), currencies, and governments. Further, storing them in a KG can facilitate answering interesting and complex queries such as (1) company acquisitions by German drugmakers, (2) US-China trade in terms of exports, (3) companies suing each other on patent grounds, etc.", "selected": "Our goal is to automatically extract high precision structured representations from thousands of financial news articles covering a broader range of financial entities such as markets, stocks, persons (e.g., CEOs, presidents, etc), currencies, and governments.", "paper_id": "227230410"}], "lhmGxrLDhv": [], "/CUFrwB9+/": [{"section": "Abstract", "paragraph": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.", "selected": "For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.", "paper_id": "227230410"}, {"section": "Introduction", "paragraph": "In this work, we develop a high precision knowledge extraction pipeline tailored to the financial news domain by combining SRL information extraction for verb predicates with typed patterns for noun mediated relations. This pipeline filters noisy predicate-argument structures via a dictionary of semantically and structurally constrained sense-disambiguated financial predicates. In order to maximize the utility of the extractions for downstream tasks, our pipeline produces compact extractions via dictionary-guided minimization of overly-specific arguments. These extractions are scored using a binary classifier, with the score reflecting our confidence in the extracted fact. We perform a lossless decomposition of the nary relations extracted, to construct the KG. While some components of the the pipeline are customized to the financial domain, we believe with small tweaks it can be easily adapted to other domains.", "selected": "In this work, we develop a high precision knowledge extraction pipeline tailored to the financial news domain by combining SRL information extraction for verb predicates with typed patterns for noun mediated relations.", "paper_id": "227230410"}, {"section": "Introduction", "paragraph": "In this work, we develop a high precision knowledge extraction pipeline tailored to the financial news domain by combining SRL information extraction for verb predicates with typed patterns for noun mediated relations. This pipeline filters noisy predicate-argument structures via a dictionary of semantically and structurally constrained sense-disambiguated financial predicates. In order to maximize the utility of the extractions for downstream tasks, our pipeline produces compact extractions via dictionary-guided minimization of overly-specific arguments. These extractions are scored using a binary classifier, with the score reflecting our confidence in the extracted fact. We perform a lossless decomposition of the nary relations extracted, to construct the KG. While some components of the the pipeline are customized to the financial domain, we believe with small tweaks it can be easily adapted to other domains.", "selected": "This pipeline filters noisy predicate-argument structures via a dictionary of semantically and structurally constrained sense-disambiguated financial predicates.", "paper_id": "227230410"}]}}
