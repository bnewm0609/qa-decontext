{"idx": "104377", "paper_id": "15193840", "title": "Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers", "abstract": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "context_section_header": "", "context_paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "sentence": "The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "snippet_rewrite": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, whereas the authors determined the method for each instance.", "cited_ids": [{"paper_id": "2462277", "citation": "(Komiya and Okumura, 2011)"}], "questions": [{"question": "What is a \"triple\" of a target word type?", "question_id": "xKLVA/UqO6", "question_type": 1, "answer_text": "It is a group of data containing exactly 3 piece of information: target type of WSD, source data, target data.", "evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "for each triple of the target word type of WSD, source data, and target data,", "paper_id": "15193840"}, {"section": "Conclusion", "paragraph": "We described how the optimal method of DA could be determined depending on the properties of the source and target data using decision tree learning and found what properties affected the determination of the best method when Japanese WSD was performed. We defined a case as a triple of the target word type of WSD, the source data, and the target data, all of which were classified into two labels (TO and RS) or three labels (TO, RS, and SA). Here, the case with TO should only be trained with a small amount of target data, the case with RS should be trained with source data and a small amount of target data, and SA represents a case with no difference between the accuracies for the two methods. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively. We automatically generated a decision tree in eight ways, the most accurate of which was with SA label when the WSD accuracies of the two methods were totally equal, performed binary classification without SA, and classified cases without weighted word tokens. The top node in the tree that was generated indicated that simulation using ten manually labeled word tokens of the target data was an important clue enabling the optimal DA method to be predicted.", "selected": "We defined a case as a triple of the target word type of WSD, the source data, and the target data,", "paper_id": "2462277"}]}, {"question": "What does \"WSD\" stand for?", "question_id": "5WWPAq8lni", "question_type": 1, "answer_text": "WSD: Word sense disambiguation", "evidence": [{"section": "Abstract", "paragraph": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}, {"section": "Introduction", "paragraph": "However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}]}, {"question": "What does \"each instance\" refer to?", "question_id": "e0KsKMSGtK", "question_type": 2, "answer_text": "\"Each instance\" refers to the individual data contained in the triple. Instead of considering them altogether, they are investigated individually.", "evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "target word type of WSD, source data, and target data", "paper_id": "15193840"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "for each triple of the target word type of WSD, source data, and target data,", "paper_id": "15193840"}, {"section": "Conclusion", "paragraph": "We described how the optimal method of DA could be determined depending on the properties of the source and target data using decision tree learning and found what properties affected the determination of the best method when Japanese WSD was performed. We defined a case as a triple of the target word type of WSD, the source data, and the target data, all of which were classified into two labels (TO and RS) or three labels (TO, RS, and SA). Here, the case with TO should only be trained with a small amount of target data, the case with RS should be trained with source data and a small amount of target data, and SA represents a case with no difference between the accuracies for the two methods. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively. We automatically generated a decision tree in eight ways, the most accurate of which was with SA label when the WSD accuracies of the two methods were totally equal, performed binary classification without SA, and classified cases without weighted word tokens. The top node in the tree that was generated indicated that simulation using ten manually labeled word tokens of the target data was an important clue enabling the optimal DA method to be predicted.", "selected": "We defined a case as a triple of the target word type of WSD, the source data, and the target data,", "paper_id": "2462277"}, {"section": "Abstract", "paragraph": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}, {"section": "Introduction", "paragraph": "However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}], "y": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD [(Word sense disambiguation)], source data, and target data, whereas the authors determined the method for each instance [the individual data contained in the triple]."}
{"idx": "113060", "paper_id": "202538019", "title": "Mixture Content Selection for Diverse Sequence Generation", "abstract": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "context_section_header": "", "context_paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "sentence": "While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation.", "snippet_rewrite": "Shen et al. (2019) makes a RNN decoder as a MoE, whereas the authors make a SELECTOR as a MoE to diversify content selection and let the encoder-decoder models one-to-one generation.", "cited_ids": [{"paper_id": "67787922", "citation": "Shen et al. (2019)"}], "questions": [{"question": "What does \"MoE\" stand for?", "question_id": "qcZ7zMU/Ij", "question_type": 1, "answer_text": "Mixture of Experts", "evidence": [{"section": "Related Work", "paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "selected": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps.", "paper_id": "202538019"}]}, {"question": "What is \"a SELECTOR?\"", "question_id": "0nuJozFz9/", "question_type": 1, "answer_text": "SELECTOR is a general plug-and-play module that wraps around and guides an existing encoder-decoder model. This is a generic module that is specialized for increasing generation diversity.", "evidence": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": "We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model", "paper_id": "202538019"}]}, {"question": "What is \"one-to-one generation?\"", "question_id": "uW85CSDZaZ", "question_type": 1, "answer_text": "This is the generation method that employes a standard encoder-decoder model to generate a target sequence given each selected content from the source.", "evidence": [{"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}]}, {"question": "What is the content that is being selected?", "question_id": "KA0t7+Mu76", "question_type": 3, "answer_text": "Individual tokens are selected from the source to be fed into the encoder-decoder model.", "evidence": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": ". The generation stage uses a standard encoder-decoder model given each selected content from the source sequence.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "selected": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "selected": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps.", "paper_id": "202538019"}, {"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": "We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "selected": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "paper_id": "202538019"}], "y": "Shen et al. (2019) makes a RNN decoder as a MOE [deep Mixture of Experts], whereas the authors make a SELECTOR [a general plug-and-play module specialized for diversification that wraps around and guides an existing encoder-decoder model] as a MoE to diversify content selection [selection of individual tokens from the source to be fed into the encoder-decoder model] and enable the encoder-decoder model's one-to-one generation."}
{"idx": "114795", "paper_id": "196471395", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning", "abstract": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "context_section_header": "", "context_paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "sentence": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "snippet_rewrite": "The authors' system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, their multi-agent system supports any slotfilling / information-seeking domain.", "cited_ids": [{"paper_id": "1294169", "citation": "(Henderson et al., 2014)"}], "questions": [{"question": "What is the authors' system?", "question_id": "d+L547/CZx", "question_type": 2, "answer_text": "The author's system consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty.", "evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language.", "paper_id": "196471395"}]}, {"question": "What is \"dstc2\" domain?", "question_id": "N0apey0EKr", "question_type": 1, "answer_text": "DSTC2 is not a domain, but the seed data which contains the dialogues between humans asking for information and the machine providing it.", "evidence": [{"section": "Introduction", "paragraph": "Employing a user simulator is an established method for dialogue policy learning (Schatzmann et al., 2007, among others) and end-to-end dialogue training (Asri et al., 2016;Liu and Lane, 2018b). Training two conversational agents concurrently has been proposed by Georgila et al. (2014); training them via natural language communication was partially realized by Liu and Lane (2017), as they train agents that receive text input but generate dialogue acts. However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator. Inspired by Hakkani-T\u00fcr (2018), each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal. This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents. The architecture of each agent is mirrored as shown in Figure 1, so the effort of adding agents with new roles is minimal. As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information. Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "selected": "As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "paper_id": "196471395"}]}, {"question": "What is their multi-agent system?", "question_id": "GU316DPPTi", "question_type": 1, "answer_text": "The multi-agent system is a type of system that has each agent playing a different role and has their own objective. These agents create languages not present in the original data.", "evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time).", "paper_id": "196471395"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Employing a user simulator is an established method for dialogue policy learning (Schatzmann et al., 2007, among others) and end-to-end dialogue training (Asri et al., 2016;Liu and Lane, 2018b). Training two conversational agents concurrently has been proposed by Georgila et al. (2014); training them via natural language communication was partially realized by Liu and Lane (2017), as they train agents that receive text input but generate dialogue acts. However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator. Inspired by Hakkani-T\u00fcr (2018), each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal. This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents. The architecture of each agent is mirrored as shown in Figure 1, so the effort of adding agents with new roles is minimal. As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information. Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "selected": "As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "paper_id": "196471395"}], "y": "The authors' system [consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty]. The system operates in the well-known DSTC2 domain [seed data that contains the dialogues between human asking for information and the machine providing it] which concerns information about restaurants in Cambridge. However, the authors' multi-agent system [a system with multiple agent create novel languages by each playing a different role and having their own objective]. Supports any slot filling / information-seeking domain."}
{"idx": "119638", "paper_id": "17786494", "title": "Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University Joint Learning for Coreference Resolution with Markov Logic Joint Learning for Coreference Resolution with Markov Logic", "abstract": "Pairwise coreference resolution models must merge pairwise coreference decisions to generate \ufb01nal outputs. Traditional merging methods adopt different strategies such as the best-\ufb01rst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classi\ufb01cation and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance.", "context_section_header": "", "context_paragraph": "In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009;Yoshikawa et al., 2009;Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominatives) from their judging guidelines. Compared with it, our methods are more applicable for real dataset. Huang et al. (2009) used Markov logic to predict coreference probabilities for mention pairs followed by correlation clustering to generate the final results. Although they also perform joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework.", "sentence": "Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches.", "snippet_rewrite": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches.", "cited_ids": [{"paper_id": "7124715", "citation": "Poon and Domingos (2008)"}], "questions": [{"question": "How are the both approaches different?", "question_id": "fgUyACF4RQ", "question_type": 3, "answer_text": "The entity-mention model is an unsupervised machine learning method which performs joint inference across mentions and uses Markov logic as a representation language. The mention-pair model is a supervised method which splits the task into mention detection, pairwise classification and mention clustering.", "evidence": [{"section": "Introduction", "paragraph": "The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "selected": "Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "paper_id": "17786494"}, {"section": "Introduction", "paragraph": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "selected": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "paper_id": "17786494"}, {"section": "Abstract", "paragraph": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "selected": "In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "paper_id": "7124715"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "selected": "Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun phrase candidates and determines whether they refer to the same entity. Because these classification decisions are local, they do not guarantee that candidate mentions are partitioned into clusters. Therefore a mention clustering step is needed to resolve conflicts and generate the final mention clusters.", "paper_id": "17786494"}, {"section": "Introduction", "paragraph": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "selected": "Much work has been done following the mentionpair model (Soon et al., 2001;Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method.", "paper_id": "17786494"}, {"section": "Abstract", "paragraph": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "selected": "In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.", "paper_id": "7124715"}], "y": "Poon and Domingos (2008) followed the entity-mention model while the authors follow the mention-pair model, which are quite different approaches. [To be more specific, the entity-mention model is an unsupervised machine learning method which performs joint inference across mentions and uses Markov logic as a representation language. The mention-pair model is a supervised method which splits the task into mention detection, pairwise classification and mention clustering.]"}
{"idx": "135732", "paper_id": "206467", "title": "Modeling Syntactic Context Improves Morphological Segmentation", "abstract": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "context_section_header": "", "context_paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "sentence": "Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "snippet_rewrite": "The authors' full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.", "cited_ids": [{"paper_id": "9519654", "citation": "(Poon et al., 2009"}], "questions": [{"question": "What is the authors' full model?", "question_id": "6umauXyFX1", "question_type": 2, "answer_text": "The author's full model is to learn words with common affixes and use learned syntactic categories to refine morphological segmentation.", "evidence": [{"section": "Abstract", "paragraph": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "selected": "Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "selected": "We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.", "paper_id": "206467"}]}, {"question": "How does the authors' model outperforms the best published results?", "question_id": "R9xqYKYHzl", "question_type": 3, "answer_text": "The author's model outperforms Poon et al., by integrating POS categorization and realization of grammatical agreement which demonstrate that incorporating syntactic information improve morphological analysis.", "evidence": [{"section": "Introduction", "paragraph": "\u2022 Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "selected": "Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological realization of grammatical agreement. In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers. For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes. Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "selected": "Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological realization of grammatical agreement. In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers. For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes. Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "selected": "Morphological realization of grammatical agreement.", "paper_id": "206467"}, {"section": "Linguistic Intuition", "paragraph": "Morphological realization of grammatical agreement. In morphologically rich languages, agreement is commonly realized using matching suffices. In many cases, members of a dependent pair such as adjective and noun have the exact same suffix. A common example in the Arabic Treebank is the bigram \"Al-Df-p Al-grby-p\" (which is translated word-for-word as \"the-bank the-west\") where the last morpheme \"p\" is a feminine singular noun suffix.", "selected": "Morphological realization of grammatical agreement.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "selected": "Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "paper_id": "206467"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. This paper proposes a novel model for morphological segmentation that is driven by this connection. Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.", "selected": "Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "We evaluate our model on the standard Arabic treebank. Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%. We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.", "selected": "We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "selected": "Morphological consistency within POS categories. Words within the same syntactic category tend to select similar affixes. This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.", "paper_id": "206467"}, {"section": "Introduction", "paragraph": "\u2022 Morphological realization of grammatical agreement. In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers. For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes. Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "selected": "Enforcing mutually consistent segmentations can greatly reduce ambiguity of wordlevel analysis.", "paper_id": "206467"}, {"section": "Linguistic Intuition", "paragraph": "Morphological realization of grammatical agreement. In morphologically rich languages, agreement is commonly realized using matching suffices. In many cases, members of a dependent pair such as adjective and noun have the exact same suffix. A common example in the Arabic Treebank is the bigram \"Al-Df-p Al-grby-p\" (which is translated word-for-word as \"the-bank the-west\") where the last morpheme \"p\" is a feminine singular noun suffix.", "selected": "Morphological realization of grammatical agreement.", "paper_id": "206467"}], "y": "The author's full model [which is to learn words with common affixes and use learned syntactic categories to refine morphological segmentation] yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5% [by integrating POS categorization and grammatical agreement realization]."}
{"idx": "137696", "paper_id": "12186762", "title": "Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis", "abstract": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "context_section_header": "", "context_paragraph": "Several differences between this work and previous approaches make direct comparisons challenging and possibly not very informative. (Socher et al., 2013) use syntactic trees, as opposed to discourse trees, as recursive structures for training. Thus we cannot compare with his \"All\"-level results. For \"Root\"-level, (Socher et al., 2013) reports 45.7% fine-grained sentiment accuracy compared to 44.82% of our Multi-tasking. This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "sentence": "This difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from our approach.", "snippet_rewrite": "The authors suggest that this difference is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs; and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach.", "cited_ids": [{"paper_id": "12252194", "citation": "(Bhatia et al., 2015)"}], "questions": [{"question": "What is the authors' approach?", "question_id": "6gKwRw0I/Q", "question_type": 3, "answer_text": "they create a vector and apply three different recursive neural net models. They then combine these models in two joint models", "evidence": [{"section": "Abstract", "paragraph": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "selected": "In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training.", "paper_id": "12186762"}]}, {"question": "What is \"EDU\" level?", "question_id": "OIgzN1GRK4", "question_type": 1, "answer_text": "elementary discourse units level", "evidence": [{"section": "Introduction", "paragraph": "This paper focuses on studying two fundamental NLP tasks, Discourse Parsing and Sentiment Analysis. The importance of these tasks and their wide applications (e.g., (Gerani et al., 2014), (Rosenthal et al., 2014)) has initiated much interest in studying both, but no method yet exists that can come close to human performance in solving them. Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1: The Discourse Tree of a sentence from Sentiment Treebank dataset trees' text spans 1 . Nodes also have labels identifying discourse relationships (\"contrast\", \"evidence\", etc.) between their corresponding subtrees. The relation also specifies nucliearity of the children. Nuclei are the core parts of the relation and Satellites are the supportive ones.", "selected": "Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1", "paper_id": "12186762"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both tasks. In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but Multi-tasking has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.", "selected": "In the proposed approach, we first create a vector representation for all the text segments in the input sentence. Next, we apply three different Recursive Neural Net models: one for discourse structure prediction, one for discourse relation prediction and one for sentiment analysis. Finally, we combine these Neural Nets in two different joint models: Multi-tasking and Pre-training.", "paper_id": "12186762"}, {"section": "Introduction", "paragraph": "This paper focuses on studying two fundamental NLP tasks, Discourse Parsing and Sentiment Analysis. The importance of these tasks and their wide applications (e.g., (Gerani et al., 2014), (Rosenthal et al., 2014)) has initiated much interest in studying both, but no method yet exists that can come close to human performance in solving them. Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1: The Discourse Tree of a sentence from Sentiment Treebank dataset trees' text spans 1 . Nodes also have labels identifying discourse relationships (\"contrast\", \"evidence\", etc.) between their corresponding subtrees. The relation also specifies nucliearity of the children. Nuclei are the core parts of the relation and Satellites are the supportive ones.", "selected": "Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub- Figure 1", "paper_id": "12186762"}], "y": "The authors suggest that [root-level report difference between their work and other approaches] is unlikely to be significant and the sentiment annotation of syntactic structure is definitely more costly than one at the EDU [elementary discourse unit] level. (Bhatia et al., 2015) focuses on document level sentiment analysis, using bag-of-word features for EDUs and only training a binary model while assuming the discourse tree as given, which is very different from the authors' approach [of creating a vector, applying three different recursive neural net models and then combining these models in two joint models]."}
{"idx": "138193", "paper_id": "661892", "title": "Regularizing Mono- and Bi-Word Models for Word Alignment", "abstract": "Conditional probabilistic models for word alignment are popular due to the elegant way of handling them in the training stage. However, they have weaknesses such as garbage collection and scale poorly beyond single word based models (DeNero et al., 2006): not all parameters should actually be used. To alleviate the problem, in this paper we explore regularity terms that penalize the used parameters. They share the advantages of the standard training in that iterative schemes decompose over the sentence pairs. We explore the models IBM-1 and HMM, then generalize to models we term Bi-word models, where each target word can be aligned to up to two source words. We give two optimization strategies for the arising tasks, using EM and projected gradient descent. While both are well-known, to our knowledge they have never been compared experimentally for the task of word alignment. As a side-effect, we show that, against common belief, for parametric HMMs the M-step is not solved by renormalizing expectations. We demonstrate that the regularity terms improve on the f-measures of the standard HMMs and that they improve translation quality.", "context_section_header": "", "context_paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "sentence": "In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "snippet_rewrite": "In contrast to the authors' recent work (Schoenemann, 2011) (where they used an L 0 -norm) they do not use the maximum approximation and also address Bi-word models.", "cited_ids": [{"paper_id": "8518269", "citation": "(Schoenemann, 2011)"}], "questions": [{"question": "What bi-word models are being referred ?", "question_id": "d2gzArachg", "question_type": 2, "answer_text": "In Bi-word models, each target word is allowed to align to up to two source words.", "evidence": [{"section": "Bi-Word Models", "paragraph": "In this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words. The alignment of target word j is expressed as the tuple (a j,1 , a j,2 ), where the allowed set of values is a subset of {0, . . . , I} \u00d7 {0, . . . , I}. The value (0, 0) will denote unaligned words. In any other case we require that a j,2 > a j,1 . If a j,1 is 0 the word is aligned only once. If a j,1 > 0 it is aligned twice. We further forbid the case where a j,1 > 0 and a j,2 = I since at the sentence end the considered data usually contain a punctuation mark which aligns only once. Note that otherwise there are no restrictions, in particular we do not require that the two aligned words are at consecutive positions (although such knowledge could be enforced in our framework).", "selected": "n this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words.", "paper_id": "661892"}]}, {"question": "What is the authors' recent work?", "question_id": "dH+2p4ZAjS", "question_type": 2, "answer_text": "The authors' recent work on machine translation contributes to single-word based alignment of bilingual sentence pairs by combining probabilistic and non-probabilistic approaches (using the L0-norm). Unlike the current work, they do not address alignments of more than a single word.", "evidence": [{"section": "Introduction", "paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "selected": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "paper_id": "661892"}, {"section": "Introduction", "paragraph": "Traditionally (Brown et al., 1993b;Al-Onaizan et al., 1999) single word based models are trained by the EM-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences. Refinements that also allow symmetrized models are based on bipartite graph matching (Matusov et al., 2004;Taskar et al., 2005) or quadratic assignment problems (Lacoste-Julien et al., 2006). Recently, Bodrumlu et al. (2009) proposed the first method that treats a nondecomposable problem by handling all sentence pairs at once and via integer linear programming. Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "selected": "Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "paper_id": "8518269"}, {"section": "Introduction", "paragraph": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "selected": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "paper_id": "8518269"}]}], "unique_evidence": [{"section": "Bi-Word Models", "paragraph": "In this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words. The alignment of target word j is expressed as the tuple (a j,1 , a j,2 ), where the allowed set of values is a subset of {0, . . . , I} \u00d7 {0, . . . , I}. The value (0, 0) will denote unaligned words. In any other case we require that a j,2 > a j,1 . If a j,1 is 0 the word is aligned only once. If a j,1 > 0 it is aligned twice. We further forbid the case where a j,1 > 0 and a j,2 = I since at the sentence end the considered data usually contain a punctuation mark which aligns only once. Note that otherwise there are no restrictions, in particular we do not require that the two aligned words are at consecutive positions (although such knowledge could be enforced in our framework).", "selected": "n this paper we consider a more general class of conditional models, which we call Bi-word models. Here we are much generalizing on the work of (Mauser et al., 2009). Now each target word is allowed to align to up to two source words.", "paper_id": "661892"}, {"section": "Introduction", "paragraph": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "selected": "We cast all this as compact objective functions subject to simplex constraints, and show two ways to optimize these: via EM and via projected gradient descent (Bertsekas, 1999, chap. 2.1). Since each iteration decomposes over the sentence pairs, the approach is efficient and scalable. In contrast to our recent work (Schoenemann, 2011) (where we used an L 0 -norm) we do not use the maximum approximation and also address Bi-word models.", "paper_id": "661892"}, {"section": "Introduction", "paragraph": "Traditionally (Brown et al., 1993b;Al-Onaizan et al., 1999) single word based models are trained by the EM-algorithm, which has the advantageous property that the collection of counts can be decomposed over the sentences. Refinements that also allow symmetrized models are based on bipartite graph matching (Matusov et al., 2004;Taskar et al., 2005) or quadratic assignment problems (Lacoste-Julien et al., 2006). Recently, Bodrumlu et al. (2009) proposed the first method that treats a nondecomposable problem by handling all sentence pairs at once and via integer linear programming. Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "selected": "Their (non-probabilistic) approach finds dictionaries with a minimal number of entries. However, the approach does not include a position model.", "paper_id": "8518269"}, {"section": "Introduction", "paragraph": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "selected": "In this work we combine the two strategies into a single framework. That is, the dictionary sparsity objective of Bodrumlu et al. will become a regularity term in our framework. It is combined with the maximal alignment probability of every sentence pair, where we consider the models IBM-1, IBM-2 and HMM. This allows us to write dictionary sparsity as the (non-convex) L 0 norm of the dictionary parameters of the respective models.", "paper_id": "8518269"}], "y": "In contrast to the authors' recent work [on machine translation] (Schoenemann, 2011) (where they used an L 0 -norm [for single-word based alignment of bilingual sentence pairs]) they do not use the maximum approximation and also address Bi-word models [, where each target word is allowed to align to up to two source words]."}
{"idx": "154425", "paper_id": "248780060", "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression", "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "context_section_header": "", "context_paragraph": "Knowledge Distillation. Knowledge distillation (Hinton et al., 2015) is another major line of research to do model compression, which is the main concentration in this paper. Hinton et al. (2015) first proposed to minimize the KL-divergence between the predicted distributions of the teacher and the student. Sanh et al. (2019); Sun et al. (2019);Liang et al. (2020) adopted this objective to teach the student on masked language modeling or text classification tasks. Romero et al. (2014) proposed to directly match the feature activations of the teacher and the student. Jiao et al. (2020) followed the idea and took the intermediate representations in each transformer layer of the teacher as one of the knowledge to be transferred. Tian et al. (2019) proposed a contrastive distillation framework where the teacher's representations were treated as positives to the corresponding student's representations. Sun et al. (2020); Fu et al. (2021) customized this idea to language model compression and proved its effectiveness. Researchers also attempted to use the mutual relations of representations as the knowledge to transfer. In the literature of image classification, Peng et al. (2019); Tung and Mori (2019); Park et al. (2019) pointed out that the relations of the image representations of the teacher should be preserved in the student's feature space, and adopted a series of geometric measurements to model the sample relations. For distilling transformer models, Park et al. (2021) enforced the relations across tokens and layers between the teacher and the student to be consistent. Jiao et al. (2020); Wang et al. ( , 2021 used the attention dependencies between tokens to teach the student. In this paper, we propose to transfer the multi-granularity knowledge to the student. Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "sentence": "Different from previous works that only considered a single granularity of representations, we jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, our method works in a different modality, presents a different definition of granularity, ", "snippet_rewrite": "Different from previous works that only considered a single granularity of representations, the authors jointly transfer the token-level, span-level and sample-level structural knowledge. And compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality, presents a different definition of granularity.", "cited_ids": [{"paper_id": "237091399", "citation": "Shao and Chen (2021)"}], "questions": [{"question": "How do the authors jointly transfer the structural knowledge?", "question_id": "rEPElQHLo6", "question_type": 3, "answer_text": "Hierarchically across layers.", "evidence": [{"section": "Abstract", "paragraph": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "selected": "Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers.", "paper_id": "248780060"}]}, {"question": "What is the granularity of representation?", "question_id": "nGd4JG9F8T", "question_type": 2, "answer_text": "The granularities they refer to are at the semantic level: token-level, span-level, and sample-level.", "evidence": [{"section": "Method", "paragraph": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1.", "selected": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level", "paper_id": "248780060"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.", "selected": "Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers.", "paper_id": "248780060"}, {"section": "Method", "paragraph": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level. Given some granularity of representations of the teacher model, we form the knowledge as the structural relations, i.e., the pair-wise interactions and the triplet-wise geometric angles, between the representations. We then distill the well-organized structural knowledge to the student hierarchically across layers, where the token-level and the span-level knowledge are transferred to the bottom layers to provide more syntactic guidance while the sample-level knowledge is transferred to the upper layers to offer more help of semantic understanding. The framework of MGSKD is illustrated in Figure 1.", "selected": "We propose Multi-Granularity Structural Knowledge Distillation, a novel framework to distill the knowledge from a large transformer language model to a small one. Different from previous works that transferred the knowledge derived from either token-level or sample-level outputs, we prepare the knowledge in three semantic granularities: token-level, span-level and sample-level", "paper_id": "248780060"}], "y": "The authors jointly transfer the token-level, span-level and sample-level structural knowledge [hierarchically across layers], which is different from previous works that only considered a single [level or granularity] of representations. Compared with Shao and Chen (2021) which considered the multi-granularity visual features in an image as the knowledge, their method works in a different modality and presents a different definition of granularity [as a distillation mechanism].\""}
{"idx": "1604.00400.1.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Data", "context_paragraph": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "sentence": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "snippet_rewrite": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera), they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries.", "cited_ids": [], "questions": [{"question": "What does \"rouge\" mean?", "question_id": "0hh9M7Z32H", "question_type": 1, "answer_text": "Rouge is a publicly available evaluation metric widely used in summarisation evaluation, i.e. the content relevance between system generated summaries and human written summaries.", "evidence": [{"section": "Introduction", "paragraph": "Automatic text summarization has been an active research area in natural language processing for several decades. To compare and evaluate the performance of different summarization systems, the most intuitive approach is assessing the quality of the summaries by human evaluators. However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce BIBREF0 . To address these problems, automatic evaluation measures for summarization have been proposed. Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries. It is inspired by the success of a similar metric Bleu BIBREF2 which is being used in Machine Translation (MT) evaluation. The main success of Rouge is due to its high correlation with human assessment scores on standard benchmarks BIBREF1 . Rouge has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC[1] BIBREF3 .", "selected": "Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries.", "paper_id": "1604.00400"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "Automatic text summarization has been an active research area in natural language processing for several decades. To compare and evaluate the performance of different summarization systems, the most intuitive approach is assessing the quality of the summaries by human evaluators. However, manual evaluation is expensive and the obtained results are subjective and difficult to reproduce BIBREF0 . To address these problems, automatic evaluation measures for summarization have been proposed. Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries. It is inspired by the success of a similar metric Bleu BIBREF2 which is being used in Machine Translation (MT) evaluation. The main success of Rouge is due to its high correlation with human assessment scores on standard benchmarks BIBREF1 . Rouge has been used as one of the main evaluation metrics in later summarization benchmarks such as TAC[1] BIBREF3 .", "selected": "Rouge BIBREF1 is one of the first and most widely used metrics in summarization evaluation. It facilitates evaluation of system generated summaries by comparing them to a set of human written gold-standard summaries.", "paper_id": "1604.00400"}], "y": "To the best of the authors' knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and their metric (Sera) [which assess the content relevance between system generated summaries and human written summaries], they use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries."}
{"idx": "1604.00400.5.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Introduction", "context_paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "sentence": "Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "snippet_rewrite": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, they show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear.", "cited_ids": [], "questions": [{"question": "What does \"the authors' results\" refer to?", "question_id": "ICq7mtjXWU", "question_type": 2, "answer_text": "The \"results\" refer to the authors comparing Rouge scores with semi-manual evaluation score on the TAC dataset (i.e., the main contribution of the rest of the paper).", "evidence": [{"section": "Introduction", "paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "selected": "comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset", "paper_id": "1604.00400"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "Scientific summarization has attracted more attention recently (examples include works by abu2011coherent, qazvinian2013generating, and cohan2015scientific). Thus, it is important to study the validity of existing methodologies applied to the evaluation of news article summarization for this task. In particular, we raise the important question of how effective is Rouge, as an evaluation metric for scientific summarization? We answer this question by comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset[1]. Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. We then propose an evaluation metric based on relevance analysis of summaries which aims to overcome the limitation of high lexical dependence in Rouge. We call our metric Sera (Summarization Evaluation by Relevance Analysis). Results show that the proposed metric achieves higher and more consistent correlations with semi-manual assessment scores.", "selected": "comparing Rouge scores with semi-manual evaluation score (Pyramid) in TAC 2014 scientific summarization dataset", "paper_id": "1604.00400"}], "y": "The authors' results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. The authors also show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear."}
{"idx": "1604.00727.1.1.1", "paper_id": "1604.00727", "title": "Character-Level Question Answering with Attention", "abstract": "We show that a character-level encoder-decoder framework can be successfully applied to question answering with a structured knowledge base. We use our model for single-relation question answering and demonstrate the effectiveness of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensembles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with significantly less data compared to previous work, which relies on data augmentation, and is robust to new entities in testing.", "context_section_header": "Dataset and Experimental Settings", "context_paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "sentence": "In contrast, our models are trained only on the 76K questions in the training set.", "snippet_rewrite": "In contrast, the authors' models are trained only on the 76K questions in the training set.", "cited_ids": [], "questions": [{"question": "What dataset does \"the training set\" refer to?", "question_id": "KlXeu/+jcE", "question_type": 2, "answer_text": "The \"training dataset\" refer to the random extract of 76k ssingle-relation questions and their corresponding triples from the SimpleQuestions dataset. This is opposed to the additional data used for training the MemNNs model.", "evidence": [{"section": "Dataset and Experimental Settings", "paragraph": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "selected": "The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train,", "paper_id": "1604.00727"}, {"section": "Dataset and Experimental Settings", "paragraph": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "selected": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0", "paper_id": "1604.00727"}, {"section": "Dataset and Experimental Settings", "paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "selected": "76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively.", "paper_id": "1604.00727"}]}], "unique_evidence": [{"section": "Dataset and Experimental Settings", "paragraph": "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.", "selected": "The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train,", "paper_id": "1604.00727"}, {"section": "Dataset and Experimental Settings", "paragraph": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.", "selected": "76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively.", "paper_id": "1604.00727"}], "y": "The authors' models are trained only on the 76K questions in the training set [which comprises the random extract of 76k single-relation questions and their corresponding triples from the SimpleQuestions dataset]."}
{"idx": "1606.03676.1.1.1", "paper_id": "1606.03676", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "abstract": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "context_section_header": "Corpora", "context_paragraph": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43 .", "sentence": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "snippet_rewrite": "The authors carried out their experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.", "cited_ids": [], "questions": [{"question": "What does \"their experiments\" refer to?", "question_id": "5hENcascs9", "question_type": 2, "answer_text": "Their experiments refers to the comparison between performance of our systems on datasets covering 16 languages.", "evidence": [{"section": "Abstract", "paragraph": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "selected": "Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs).", "paper_id": "1606.03676"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, feature-based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "selected": "Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs).", "paper_id": "1606.03676"}], "y": "The authors carried out their experiments [comparing the performance of systems on datasets covering 16 languages] on the Universal Dependencies v1.2 treebanks BIBREF21, (UD1.2), from which morphosyntactically annotated corpora can be trivially extracted."}
{"idx": "1609.00425.1.1.1", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism data", "context_paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "sentence": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "snippet_rewrite": "To collect a diverse training dataset, the authors have randomly sampled 1000 posts from each of the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage.", "cited_ids": [], "questions": [{"question": "What task is the training dataset for?", "question_id": "l33yiX02oV", "question_type": 3, "answer_text": "The training dataset is for predicting dogmatism and to test domain-independence of the classification strategies using curated datasets from different subreddits.", "evidence": [{"section": "Abstract", "paragraph": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "selected": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "We now show how we can use the linguistic feature sets we have described to build a classifier that predicts dogmatism in comments. A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "selected": "A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "Prediction task. Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies. First, we test the performance of our model under cross-validation within the Reddit comment dataset. We then evaluate the Reddit-based model on a held out corpus of New York Times comments annotated using the technique in Section 2. We did not refer to this second dataset during feature construction.", "selected": "Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies.", "paper_id": "1609.00425"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "selected": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "We now show how we can use the linguistic feature sets we have described to build a classifier that predicts dogmatism in comments. A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "selected": "A predictive model further validates our feature sets, and also allows us to analyze dogmatism in millions of other Reddit comments in a scalable way, with multiple uses in ongoing, downstream analyses.", "paper_id": "1609.00425"}, {"section": "Predicting dogmatism", "paragraph": "Prediction task. Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies. First, we test the performance of our model under cross-validation within the Reddit comment dataset. We then evaluate the Reddit-based model on a held out corpus of New York Times comments annotated using the technique in Section 2. We did not refer to this second dataset during feature construction.", "selected": "Our goal is (1) to understand how well we can use the strategies in Section 3 to predict dogmatism, and (2) to test the domain-independence of these strategies.", "paper_id": "1609.00425"}], "y": "To collect a diverse training dataset [for training their classifier to predict dogmatism across different subreddits], the authors randomly sampled 1000 posts from each of the subreddits: \"politics\", \"business\", \"science\", \"AskReddit\", and 1000 additional posts from the Reddit front page."}
{"idx": "1609.00425.1.2.2", "paper_id": "1609.00425", "title": "Identifying Dogmatism in Social Media: Signals and Models", "abstract": "We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.", "context_section_header": "Dogmatism in the Reddit Community ", "context_paragraph": "We now apply our dogmatism classifier to a larger dataset of posts, examining how dogmatic language shapes the Reddit community. Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic). We then use these dogmatism annotations to address four research questions.", "sentence": "Concretely, we apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "snippet_rewrite": "The authors apply the BOW+LING model trained on the full Reddit dataset to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic).", "cited_ids": [], "questions": [{"question": "What does \"BOW+LING\" stand for?", "question_id": "wufYdpWojb", "question_type": 1, "answer_text": "BOW stand for bag-of-words, whereas LING stands for linguistic features from the authors' earlier analyses", "evidence": [{"section": "Predicting dogmatism", "paragraph": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).", "selected": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.", "paper_id": "1609.00425"}]}, {"question": "What does \"the full Reddit dataset\" consist of?", "question_id": "YUmLGBJKGO", "question_type": 3, "answer_text": "The full Reddit dataset consists of different subreddits representing different topics, such as politics, business, science and other posts in the Reddit home page.", "evidence": [{"section": "Dogmatism data", "paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "selected": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "paper_id": "1609.00425"}]}], "unique_evidence": [{"section": "Predicting dogmatism", "paragraph": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. BOW and SENT provide baselines for the task. We compute BOW features using term frequency-inverse document frequency (TF-IDF) and category-based features by normalizing counts for each category by the number of words in each document. The BOW classifiers are trained with regularization (L2 penalties of 1.5).", "selected": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.", "paper_id": "1609.00425"}, {"section": "Dogmatism data", "paragraph": "Data collection. Subreddits are sub-communities on Reddit oriented around specific interests or topics, such as technology or politics. Sampling from Reddit as a whole would bias the model towards the most commonly discussed content. But by sampling posts from individual subreddits, we can control the kinds of posts we use to train our model. To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "selected": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. All posts in our sample appeared between January 2007 and March 2015, and to control for length effects, contain between 300 and 400 characters. This results in a total training dataset of 5000 posts.", "paper_id": "1609.00425"}], "y": "The authors apply the [bag-of-words and linguistic features] model trained on [different subreddit representing different topics, such as politics, business, science and other other posts in the Reddit home page] to millions of new unannotated posts, labeling these posts with a probability of dogmatism according to the classifier (0=non-dogmatic, 1=dogmatic)."}
{"idx": "1611.03599.4.1.1", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "To test whether the assumption of this paper \u2013 posts attract users who hold the same stance to like them \u2013 is reliable, we examine the likes from authors of different stances. Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. As the numbers of authors in the Sup, Neu and Uns stances are largely imbalanced, these numbers are normalized by the number of users of each stance. Table TABREF13 shows the results. Posts with stances (i.e., not neutral) attract users of the same stance. Neutral posts also attract both supportive and neutral users, like what we observe in supportive posts, but just the neutral posts can attract even more neutral likers. These results do suggest that users prefer posts of the same stance, or at least posts of no obvious stance which might cause annoyance when reading, and hence support the user modeling in our approach.", "sentence": "Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts.", "snippet_rewrite": "The authors use posts in FBFans dataset for this analysis. They calculate the like statistics of each distinct author from these 32,595 posts.", "cited_ids": [], "questions": [{"question": "What does \"fbfans\" consist of?", "question_id": "Zbh8C31sut", "question_type": 3, "answer_text": "FBFans consists of a single-topic Chinese unbalanced social media dataset obtained from Facebook.", "evidence": [{"section": "Dataset", "paragraph": "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.", "selected": "FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset,", "paper_id": "1611.03599"}, {"section": "Dataset", "paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "selected": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups", "paper_id": "1611.03599"}]}], "unique_evidence": [{"section": "Dataset", "paragraph": "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show the applicability and superiority for different topics, languages, data distributions, and platforms.", "selected": "FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset,", "paper_id": "1611.03599"}, {"section": "Dataset", "paragraph": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users. Two annotators were asked to take into account only the post content to label the stance of the posts in the whole dataset as supportive, neutral, or unsupportive (hereafter denoted as Sup, Neu, and Uns). Sup/Uns posts were those in support of or against anti-reconstruction; Neu posts were those evincing a neutral standpoint on the topic, or were irrelevant. Raw agreement between annotators is 0.91, indicating high agreement. Specifically, Cohen\u2019s Kappa for Neu and not Neu labeling is 0.58 (moderate), and for Sup or Uns labeling is 0.84 (almost perfect). Posts with inconsistent labels were filtered out, and the development and testing sets were randomly selected from what was left. Posts in the development and testing sets involved at least one user who appeared in the training set. The number of posts for each stance is shown on the left-hand side of Table TABREF12 . About twenty percent of the posts were labeled with a stance, and the number of supportive (Sup) posts was much larger than that of the unsupportive (Uns) ones: this is thus highly skewed data, which complicates stance classification. On average, 161.1 users were involved in one post. The maximum was 23,297 and the minimum was one (the author). For comments, on average there were 3 comments per post. The maximum was 1,092 and the minimum was zero.", "selected": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups", "paper_id": "1611.03599"}], "y": "For this analysis, the authors use posts [from FBFans, a single-topic Chinese unbalanced social media dataset obtained from Facebook]. They calculate the like statistics of each distinct author from these 32,595 posts."}
{"idx": "1611.03599.5.2.2", "paper_id": "1611.03599", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "context_section_header": "Dataset", "context_paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "sentence": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "snippet_rewrite": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of", "cited_ids": [], "questions": [{"question": "What does \"labeling results\" refer to?", "question_id": "wQbDAHMW9c", "question_type": 2, "answer_text": "The stance labelling results, either for (F) or against (A), of the proposed method on the CreateDebate dataset.", "evidence": [{"section": "Dataset", "paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "selected": "The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12", "paper_id": "1611.03599"}]}], "unique_evidence": [{"section": "Dataset", "paragraph": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12 . We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9 , BIBREF5 .", "selected": "The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table TABREF12", "paper_id": "1611.03599"}], "y": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The authors observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng's work, the authors conducted five-fold cross-validation and present the annotation results as the average number of all folds BIBREF9, BIBREF5."}
{"idx": "1612.03226.1.1.1", "paper_id": "1612.03226", "title": "Active Learning for Speech Recognition: the Power of Gradients", "abstract": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "context_section_header": "Expected Gradient Length", "context_paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "sentence": "Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "snippet_rewrite": "Since labels are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. The following section formalizes the intuition for EGL and shows that it follows naturally from reducing the variance of an estimator.", "cited_ids": [], "questions": [{"question": "What does EGL mean?", "question_id": "nI47Fs85Is", "question_type": 1, "answer_text": "Expected Gradient Length is an approach in active learning for end-to-end speech recognition.", "evidence": [{"section": "Abstract", "paragraph": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "selected": "This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition.", "paper_id": "1612.03226"}, {"section": "Expected Gradient Length", "paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "selected": "Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length", "paper_id": "1612.03226"}]}, {"question": "What do \"labels\" refer to?", "question_id": "MvnmzllGHo", "question_type": 2, "answer_text": "Annotations of the training data with correct transcriptions for use in training automatic speech recognition systems", "evidence": [{"section": "Introduction", "paragraph": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize. Labeling thousands of hours of audio, however, is expensive and time-consuming. A natural question to ask is how to achieve better generalization with fewer training examples. Active learning studies this problem by identifying and labeling only the most informative data, potentially reducing sample complexity. How much active learning can help in large-scale, end-to-end ASR systems, however, is still an open question.", "selected": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize.", "paper_id": "1612.03226"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "In training speech recognition systems, labeling audio clips can be expensive, and not all data is equally valuable. Active learning aims to label only the most informative samples to reduce cost. For speech recognition, confidence scores and other likelihood-based active learning methods have been shown to be effective. Gradient-based active learning methods, however, are still not well-understood. This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition. We justify EGL from a variance reduction perspective, and observe that EGL's measure of informativeness picks novel samples uncorrelated with confidence scores. Experimentally, we show that EGL can reduce word errors by 11\\%, or alternatively, reduce the number of samples to label by 50\\%, when compared to random sampling.", "selected": "This work investigates the Expected Gradient Length (EGL) approach in active learning for end-to-end speech recognition.", "paper_id": "1612.03226"}, {"section": "Expected Gradient Length", "paragraph": "Intuitively, an instance can be considered informative if it results in large changes in model parameters. A natural measure of the change is gradient length, INLINEFORM0 . Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length. Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as \u201cexpected model change\u201d. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator.", "selected": "Motivated by this intuition, Expected Gradient Length (EGL) BIBREF3 picks the instances expected to have the largest gradient length", "paper_id": "1612.03226"}, {"section": "Introduction", "paragraph": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize. Labeling thousands of hours of audio, however, is expensive and time-consuming. A natural question to ask is how to achieve better generalization with fewer training examples. Active learning studies this problem by identifying and labeling only the most informative data, potentially reducing sample complexity. How much active learning can help in large-scale, end-to-end ASR systems, however, is still an open question.", "selected": "State-of-the-art automatic speech recognition (ASR) systems BIBREF0 have large model capacities and require significant quantities of training data to generalize.", "paper_id": "1612.03226"}], "y": "Since labels [annotations of the training data with correct transcriptions for use in training automatic speech recognition systems] are unknown on INLINEFORM1, the authors compute the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL [expected gradient length, which is an approach in active learning for end-to-end speech recognition] as \u201cexpected model change\u201d. The authors then formalize the intuition for for EGL and show that it follows naturally from reducing the variance of an estimator."}
{"idx": "164331", "paper_id": "14151217", "title": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text", "abstract": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "context_section_header": "", "context_paragraph": "LSTMs have proven to be very effective language models (Sundermeyer et al., 2010). Gulcehre et al. (2015) developed an LSTM model for machine translation that incorporates a monolingual language model for the target language showing improved results. We utilize similar approaches (late fusion, deep fusion) to train an LSTM for translating video to text that exploits large monolingual-English corpora (Wikipedia, BNC, UkWac) to improve RNN based video description networks. However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "sentence": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of our approach is that the output of the monolingual language model is used (as an input) when training the full underlying video description network.", "snippet_rewrite": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach is that the output of the monolingual language model is used as an input when training the full underlying video description network.", "cited_ids": [{"paper_id": "5923323", "citation": "Gulcehre et al. (2015)"}], "questions": [{"question": "What is the authors' approach?", "question_id": "6gKwRw0I/Q", "question_type": 2, "answer_text": "In their approach (Deep Fusion) unlike the other researchers on the same topic, they avoid tunning the LSTM LM parameters to prevent overwriting the already learned weights of a strong language model. However, they train the full video caption model to incorporate the LM outputs while training the caption domain.", "evidence": [{"section": "Abstract", "paragraph": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "selected": "we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Early Fusion. Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus. An LSTM model learns to estimate the probability of an output sequence given an input sequence. To learn a language model, we train the LSTM layer to predict the next word given the previous words. Following the S2VT architecture, we embed one-hot encoded words in lower dimensional vectors. The network is trained on web-scale text corpora and the parameters are learned through backpropagation using stochastic gradient descent. 1 The weights from this network are then used to initialize the embedding and weights of the LSTM layers of S2VT, which is then trained on video-text data. This trained LM is also used as the LSTM LM in the late and deep fusion models.", "selected": "Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus.", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Late Fusion. Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM). More specifically, if y t denotes the output at time step t, and if p V M and p LM denote the proposal distributions of the video captioning model, and the language models respectively, then for all words y \u2208 V in the vocabulary we can recompute the score of each new word, p(y t = y ) as:", "selected": "Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM).", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Deep Fusion. In the deep fusion approach (Fig. 2), we integrate the LM a step deeper in the generation process by concatenating the hidden state of the language model LSTM (h LM t ) with the hidden state of the S2VT video description model (h V M t ) and use the combined latent vector to predict the output word. This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation. However, our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:", "selected": "our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "where x is the visual feature input, W is the weight matrix, and b the biases. We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "selected": "We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "paper_id": "14151217"}]}, {"question": "What is a \"video description network?\"", "question_id": "pCwojJ4Fa1", "question_type": 1, "answer_text": "Video desription network is a machine learning tool that uses recurrent neural networks method which helps automatically describe videos in natural language.", "evidence": [{"section": "Introduction", "paragraph": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNs need large training corpora; however, there is a lack of highquality paired video-sentence data. In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description. Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual target language data as well as a translation model trained on more limited parallel bilingual data. This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description. This paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNNs which have shown state-of-the-art performance on the task. Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010). Our first approach (early fusion) is to pre-train the network on plain text before training on parallel video-text corpora. Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model. Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.", "selected": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language.", "paper_id": "14151217"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "selected": "we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Early Fusion. Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus. An LSTM model learns to estimate the probability of an output sequence given an input sequence. To learn a language model, we train the LSTM layer to predict the next word given the previous words. Following the S2VT architecture, we embed one-hot encoded words in lower dimensional vectors. The network is trained on web-scale text corpora and the parameters are learned through backpropagation using stochastic gradient descent. 1 The weights from this network are then used to initialize the embedding and weights of the LSTM layers of S2VT, which is then trained on video-text data. This trained LM is also used as the LSTM LM in the late and deep fusion models.", "selected": "Our first approach (early fusion), is to pre-train portions of the network modeling language on large corpora of raw NL text and then continue \"fine-tuning\" the parameters on the paired video-text corpus.", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Late Fusion. Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM). More specifically, if y t denotes the output at time step t, and if p V M and p LM denote the proposal distributions of the video captioning model, and the language models respectively, then for all words y \u2208 V in the vocabulary we can recompute the score of each new word, p(y t = y ) as:", "selected": "Our late fusion approach is similar to how neural machine translation models incorporate a trained language model during decoding. At each step of sentence generation, the video caption model proposes a distribution over the vocabulary. We then use the language model to re-score the final output by considering the weighted average of the sum of scores proposed by the LM as well as the S2VT video-description model (VM).", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "Deep Fusion. In the deep fusion approach (Fig. 2), we integrate the LM a step deeper in the generation process by concatenating the hidden state of the language model LSTM (h LM t ) with the hidden state of the S2VT video description model (h V M t ) and use the combined latent vector to predict the output word. This is similar to the technique proposed by Gulcehre et al. (2015) for incorporating language models trained on monolingual corpora for machine translation. However, our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is:", "selected": "our approach differs in two 1 The LM was trained to achieve a perplexity of 120 (1) we only concatenate the hidden states of the S2VT LSTM and language LSTM and do not use any additional context information, (2) we fix the weights of the LSTM language model but train the full video captioning network. In this case, the probability of the predicted word at time step t is", "paper_id": "14151217"}, {"section": "Approach", "paragraph": "where x is the visual feature input, W is the weight matrix, and b the biases. We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "selected": "We avoid tuning the LSTM LM to prevent overwriting already learned weights of a strong language model. But we train the full video caption model to incorporate the LM outputs while training on the caption domain.", "paper_id": "14151217"}, {"section": "Introduction", "paragraph": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language. Deep learning methods such as RNNs need large training corpora; however, there is a lack of highquality paired video-sentence data. In contrast, raw text corpora are widely available and exhibit rich linguistic structure that can aid video description. Most work in statistical MT utilizes both a language model trained on a large corpus of monolingual target language data as well as a translation model trained on more limited parallel bilingual data. This paper explores methods to incorporate knowledge from language corpora to capture general linguistic regularities to aid video description. This paper integrates linguistic information into a video-captioning model based on Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNNs which have shown state-of-the-art performance on the task. Further, LSTMs are also effective as language models (LMs) (Sundermeyer et al., 2010). Our first approach (early fusion) is to pre-train the network on plain text before training on parallel video-text corpora. Our next two approaches, inspired by recent MT work (Gulcehre et al., 2015), integrate an LSTM LM with the existing video-to-text model. Furthermore, we also explore replacing the standard one-hot word encoding with distributional vectors trained on external corpora.", "selected": "The ability to automatically describe videos in natural language (NL) enables many important applications including content-based video retrieval and video description for the visually impaired. The most effective recent methods (Venugopalan et al., 2015a;Yao et al., 2015) use recurrent neural networks (RNN) and treat the problem as machine translation (MT) from video to natural language.", "paper_id": "14151217"}], "y": "However, unlike Gulcehre et al. (2015) where the monolingual LM is used only to tune specific parameters of the translation network, the key advantage of the authors' approach [for video description generation, Deep Fusion, which combines a frozen high quality LSTM LM with a trained captioning model] is that the output of the monolingual language model is used as an input when training the full underlying video description network [an RNN that automatically describes videos in natural language]."}
{"idx": "168296", "paper_id": "5673257", "title": "Using word alignments to assist computer-aided translation users by marking which target-side words to change or keep unedited", "abstract": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "context_section_header": "", "context_paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "sentence": "In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "snippet_rewrite": "In addition, the authors' system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "cited_ids": [{"paper_id": "1079699", "citation": "Kranias and Samiotou (2004)"}], "questions": [{"question": "What is the authors' system?", "question_id": "d+L547/CZx", "question_type": 2, "answer_text": "The author's system uses various sets of pre-computed word alignments in a translation memory (TM)-based computer-aided translation (CAT) system. Word alignments are used only to recommend the words to be kept unedited or altered, without suggesting a translation, in order for the user to focus on words that require editing.", "evidence": [{"section": "Abstract", "paragraph": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "selected": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM.", "paper_id": "5673257"}, {"section": "Abstract", "paragraph": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "selected": "In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%.", "paper_id": "5673257"}, {"section": "Introduction", "paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "selected": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed.", "paper_id": "5673257"}]}, {"question": "How is the authors' system independant?", "question_id": "J26LlQoCuy", "question_type": 3, "answer_text": "Compared to other authored works, this particular author's system is independent because it does not rely on external resources, e.g. MT systems, dictionaries.", "evidence": [{"section": "Introduction", "paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "selected": "our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "paper_id": "5673257"}]}, {"question": "What are \"mt systems\"?", "question_id": "5ZLWN/eISk", "question_type": 1, "answer_text": "Machine Translation (MT) systems.", "evidence": [{"section": "Abstract", "paragraph": "An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional \"cascade\" integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.", "selected": "Machine Translation (MT)", "paper_id": "1079699"}, {"section": "Abstract", "paragraph": "An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional \"cascade\" integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.", "selected": "The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches.", "paper_id": "1079699"}, {"section": "Automatic Translation Memory Fuzzy Match Post-Editing", "paragraph": "According to the standard TM paradigm (Nagao, 1984), an input text unit (usually a sentence) to be translated is matched against the source language part of translation pairs stored in the TM. If an identical (full) or similar (fuzzy) match is located, then the system suggests its target language equivalent as the translation of the original text unit and lets the user accept/edit this suggestion in order to correspond accurately to the translation of the input text unit. When no full/fuzzy match can be located, the option is usually offered to invoke MT processing to translate the input text unit. The method proposed in this paper, can be classified as an Example-Based Machine Translation application (Somers, 1999), taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "selected": "taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "paper_id": "1079699"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA++. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used.", "selected": "This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the user\u2019s TM.", "paper_id": "5673257"}, {"section": "Introduction", "paragraph": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed. It is worth noting that as we do not change the translation proposals in any way, our approach does not affect the predictability of TM proposals and the way in which fuzzy-match scores (Sikes, 2007) are interpreted by the CAT user. In addition, our system is independent of any external resources, such as MT systems or dictionaries, as opposed to the work by Kranias and Samiotou (2004).", "selected": "In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words have to be changed.", "paper_id": "5673257"}, {"section": "Abstract", "paragraph": "An innovative way of integrating Translation Memory (TM) and Machine Translation (MT) processing is presented which goes beyond the traditional \"cascade\" integration of Translation Memory and Machine Translation. The new method aims to automatically post-edit TM similar matches by the use of an MT module thus enhancing the TM fuzzy (similar) scores as well as enabling the utilisation of low-score TM fuzzy matches. This leads to substantial translation cost reduction. The suggested method, which can be classified as an Example-Based Machine Translation application, is analysed and examples are provided for clarification. It is evaluated through test results that involve human interaction. The method has been implemented within the ESTeam Translator (ET) Language Toolbox and is already in use in the various commercial installations of ET.", "selected": "Machine Translation (MT)", "paper_id": "1079699"}, {"section": "Automatic Translation Memory Fuzzy Match Post-Editing", "paragraph": "According to the standard TM paradigm (Nagao, 1984), an input text unit (usually a sentence) to be translated is matched against the source language part of translation pairs stored in the TM. If an identical (full) or similar (fuzzy) match is located, then the system suggests its target language equivalent as the translation of the original text unit and lets the user accept/edit this suggestion in order to correspond accurately to the translation of the input text unit. When no full/fuzzy match can be located, the option is usually offered to invoke MT processing to translate the input text unit. The method proposed in this paper, can be classified as an Example-Based Machine Translation application (Somers, 1999), taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "selected": "taking the TM-MT integration one step further manipulating the fuzzy match result by invoking MT (in context) in order to automatically correct the TM-based translation suggestion.", "paper_id": "1079699"}], "y": "In addition, the authors' system is independent of any extermanl resources such as MT systems [Machine Translation] or dictionaries as opposed to the work by Kranias and Samiotou (2004)."}
{"idx": "168810", "paper_id": "17519578", "title": "Question classification using head words and their hypernyms", "abstract": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "context_section_header": "", "context_paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "sentence": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set.", "snippet_rewrite": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, the authors propose to use a compact yet effective feature set.", "cited_ids": [{"paper_id": "16137770", "citation": "Li and Roth (2006)"}], "questions": [{"question": "What is the authors' approach?", "question_id": "6gKwRw0I/Q", "question_type": 2, "answer_text": "The authors propose a head word feature and two approaches to augment semantic features of head words using WordNet.", "evidence": [{"section": "Abstract", "paragraph": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "selected": "we propose head word feature and present two approaches to augment semantic features of such head words using WordNet.", "paper_id": "17519578"}, {"section": "Introduction", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "paper_id": "17519578"}]}, {"question": "What compact feature set is the author referring to?", "question_id": "cEA5A1V1j/", "question_type": 2, "answer_text": "Five binary feature sets (question whword, head word, WordNet semantic feature for head word, word grams and word shape feature).", "evidence": [{"section": "Features", "paragraph": "Each question is represented as a bag of features and is feeded into classifiers in training stage. We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "selected": "We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "paper_id": "17519578"}, {"section": "Conclusion", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of very rich feature space, we proposed a compact yet effective feature set. In particular, we proposed head word feature and presented two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation algorithm was adapted and the depth of hypernym feature was optimized through cross validation, which was to introduce useful information while not bringing too much noise. With further augment of wh-word, unigram feature, and word shape feature, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "we proposed a compact yet effective feature set", "paper_id": "17519578"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.", "selected": "we propose head word feature and present two approaches to augment semantic features of such head words using WordNet.", "paper_id": "17519578"}, {"section": "Introduction", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. In particular, we propose head word feature and 1 available at http://12r.cs.uiuc.edu/\u223ccogcomp/Data/QA/QC present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "paper_id": "17519578"}, {"section": "Features", "paragraph": "Each question is represented as a bag of features and is feeded into classifiers in training stage. We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "selected": "We present five binary feature sets, namely question whword, head word, WordNet semantic features for head word, word grams, and word shape feature. The five feature sets will be separately used by the classifiers to determine their individual contribution. In addition, these features are used in an incremental fashion in our experiments.", "paper_id": "17519578"}, {"section": "Conclusion", "paragraph": "In contrast to Li and Roth (2006)'s approach which makes use of very rich feature space, we proposed a compact yet effective feature set. In particular, we proposed head word feature and presented two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation algorithm was adapted and the depth of hypernym feature was optimized through cross validation, which was to introduce useful information while not bringing too much noise. With further augment of wh-word, unigram feature, and word shape feature, we can obtain accuracy of 89.2% using linear SVMs, or 89.0% using ME for 50 fine classes.", "selected": "we proposed a compact yet effective feature set", "paper_id": "17519578"}], "y": "In contrast to Li and Roth (2006)'s approach which makes use of a very rich feature set [a head word feature and two approaches to augment semantic features of head words using WordNet], the authors propose to use a compact yet effective feature set [which includes five binary feature sets: question whword, head word, WordNet semantic feature for head word, word grams and word shape feature]."}
{"idx": "1702.03856.1.2.1", "paper_id": "1702.03856", "title": "Towards speech-to-text translation without speech recognition", "abstract": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.", "context_section_header": "Introduction", "context_paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "sentence": "We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ).", "snippet_rewrite": "The authors test their system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects.", "cited_ids": [], "questions": [{"question": "What does \"their system\" refer to?", "question_id": "SQXMwav7Rp", "question_type": 2, "answer_text": "Their system builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9", "evidence": [{"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 .", "paper_id": "1702.03856"}, {"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9", "paper_id": "1702.03856"}, {"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ).", "paper_id": "1702.03856"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 . The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10 , a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (\u00a7 SECREF3 ). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (\u00a7 SECREF4 ). Despite these difficulties, we demonstrate that the system learns to translate some content words (\u00a7 SECREF5 ).", "selected": "Our simple system (\u00a7 SECREF2 ) builds on unsupervised speech processing BIBREF5 , BIBREF6 , BIBREF7 , and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8 , BIBREF9 .", "paper_id": "1702.03856"}], "y": "The authors [created a system which builds on unsupervised speech processing, using unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech. They ] test this system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects."}
{"idx": "1703.10344.1.1.2", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Introduction", "context_paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "sentence": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha.", "snippet_rewrite": "While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties is not mentioned in the page for Odisha.", "cited_ids": [], "questions": [{"question": "What is the \"page for odisha\"?", "question_id": "hhMQSTf4zI", "question_type": 1, "answer_text": "The page for Odisha is the Wikipedia entity page for that Cyclone.", "evidence": [{"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive", "paper_id": "1703.10344"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive", "paper_id": "1703.10344"}], "y": "While [cyclone] Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone, which has 5 times more human casualties, is not mentioned in the [wikipedia] page for Odisha."}
{"idx": "1703.10344.4.1.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Section Placement", "context_paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "sentence": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "snippet_rewrite": "The authors model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc.", "cited_ids": [], "questions": [{"question": "What does asp stand for?", "question_id": "7Vgm31ZuAG", "question_type": 1, "answer_text": "It is one of two decompositions of news. ASP = \"Article-Section placement\".", "evidence": [{"section": "Approach Overview", "paragraph": "We approach the news suggestion problem by decomposing it into two tasks:", "selected": "We approach the news suggestion problem by decomposing it into two tasks:", "paper_id": "1703.10344"}, {"section": "Approach Overview", "paragraph": "ASP: Article\u2013Section placement", "selected": "ASP: Article\u2013Section placement", "paper_id": "1703.10344"}]}, {"question": "What does \"the task\" refer to?", "question_id": "0skFKrMyRx", "question_type": 2, "answer_text": "It refers to the matching of news sections to their correct entity sections.", "evidence": [{"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay.", "paper_id": "1703.10344"}, {"section": "Article\u2013Section Placement", "paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "selected": "or all the `relevant' news entity pairs, the task is to determine the correct entity section.", "paper_id": "1703.10344"}]}], "unique_evidence": [{"section": "Approach Overview", "paragraph": "We approach the news suggestion problem by decomposing it into two tasks:", "selected": "We approach the news suggestion problem by decomposing it into two tasks:", "paper_id": "1703.10344"}, {"section": "Approach Overview", "paragraph": "ASP: Article\u2013Section placement", "selected": "ASP: Article\u2013Section placement", "paper_id": "1703.10344"}, {"section": "Introduction", "paragraph": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay. Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hurricane Katrina and Odisha Cyclone, respectively. While Katrina finds extensive mention in the entity page for New Orleans, Odisha Cyclone which has 5 times more human casualties (cf. Figure FIGREF2 ) is not mentioned in the page for Odisha. Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also reported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previous studies have shown that there is an inherent delay or lag when facts are added to entity pages BIBREF7 .", "selected": "However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be missing or added with a delay.", "paper_id": "1703.10344"}, {"section": "Article\u2013Section Placement", "paragraph": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", "selected": "or all the `relevant' news entity pairs, the task is to determine the correct entity section.", "paper_id": "1703.10344"}], "y": "The authors model the ASP [Article-Sectionp placement] task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to [match news sections to] the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc."}
{"idx": "1703.10344.6.2.1", "paper_id": "1703.10344", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "abstract": "Wikipedia entity pages are a valuable source of information for direct consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20\\% of the references are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article suggestion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we suggest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the \\emph{salience} and \\emph{relative authority} of entities, and the \\emph{novelty} of news articles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evaluation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high precision value of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\% for the \\emph{article-section placement}. Finally, we compare our approach against competitive baselines and show significant improvements.", "context_section_header": "Article\u2013Entity Placement", "context_paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "sentence": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "snippet_rewrite": "The authors reimplemented baseline features discussed in Section SECREF2. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "cited_ids": [], "questions": [{"question": "What does \"baseline features\" refer to?", "question_id": "BVqHJB4480", "question_type": 2, "answer_text": "The baseline features refer to measure salience of an entity in text.", "evidence": [{"section": "Article\u2013Entity Placement", "paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "selected": "a variety of features that measure salience of an entity in text are available from the NLP community.", "paper_id": "1703.10344"}]}, {"question": "How many features is a \"variety of features\"?", "question_id": "fFVa3w+Wmx", "question_type": 3, "answer_text": "There are three features based on the text.", "evidence": [{"section": "Article\u2013Entity Placement", "paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "selected": "This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "paper_id": "1703.10344"}]}], "unique_evidence": [{"section": "Article\u2013Entity Placement", "paragraph": "Baseline Features. As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", "selected": "a variety of features that measure salience of an entity in text are available from the NLP community.", "paper_id": "1703.10344"}], "y": "The authors reimplemented baseline features. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in."}
{"idx": "1704.06194.1.1.1", "paper_id": "1704.06194", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "abstract": "Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.", "context_section_header": "KBQA End-Task Results", "context_paragraph": "Finally, like STAGG, which uses multiple relation detectors (see yih2015semantic for the three models used), we also try to use the top-3 relation detectors from Section \"Relation Detection Results\" . As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP.", "sentence": "As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP", "snippet_rewrite": "This gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP.", "cited_ids": [], "questions": [{"question": "What gives a significant performance boost?", "question_id": "7K+Ttn8kaU", "question_type": 2, "answer_text": "Using the top 3 relation detectors", "evidence": []}], "unique_evidence": [], "y": "[Using the top 3 relation detectors] gives a significant performance boost, resulting in the then current state-of-the-art result on SimpleQuestions and a result comparable to the then state-of-the-art best result on WebQSP."}
{"idx": "171837", "paper_id": "5679499", "title": "Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues", "abstract": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "context_section_header": "", "context_paragraph": "The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "sentence": "Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "snippet_rewrite": "Key differences between the previous approach and the authors' include their use of syntactic information as opposed to surface-level patterns, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013).", "cited_ids": [{"paper_id": "2687019", "citation": "(Riedel et al., 2013)"}], "questions": [{"question": "What is the authors' approach?", "question_id": "6gKwRw0I/Q", "question_type": 2, "answer_text": "The author's approach is to use latent edge labels in addition to surface-level labels.", "evidence": [{"section": "Abstract", "paragraph": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "selected": "For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task.", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen-cies). These additional edges, e.g., (Alex Rodriguez, \"plays for\", NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012). In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach.", "selected": "we derive edge labels by learning latent embeddings of the lexicalized edges.", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "paper_id": "5679499"}]}, {"question": "What type of syntatic information is being refered to?", "question_id": "zpDVW0r7TY", "question_type": 2, "answer_text": "The type of syntactic information refers to expressive lexicalized labels.", "evidence": [{"section": "Introduction", "paragraph": "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen-cies). These additional edges, e.g., (Alex Rodriguez, \"plays for\", NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012). In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach.", "selected": "lexicalized syntactic labels (where the labels are words instead of dependen-cies)", "paper_id": "5679499"}]}, {"question": "What is a \"pra-based method\"?", "question_id": "Wu82qV7lCn", "question_type": 1, "answer_text": "PRA is a Path Ranking Algorithm, that performs inference over a knowledge base using syntactic information from parsed text.", "evidence": [{"section": "Introduction", "paragraph": "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "selected": "recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011)", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "selected": "PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011)", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "PRA is extended to perform inference over a KB using syntactic information from parsed text.", "paper_id": "5679499"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.", "selected": "For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task.", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen-cies). These additional edges, e.g., (Alex Rodriguez, \"plays for\", NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012). In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges. Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach.", "selected": "we derive edge labels by learning latent embeddings of the lexicalized edges.", "paper_id": "5679499"}, {"section": "Related Work", "paragraph": "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "selected": "in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.", "paper_id": "5679499"}, {"section": "Introduction", "paragraph": "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., playsSport, sportOfTournament , to predict missing facts in the KB.", "selected": "recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011)", "paper_id": "5679499"}], "y": "Key differences between the previous approach and the authors' [approach which is to use latent edge labels in addition to surface-level labels] include their use of syntactic information [expressive lexicalized labels] as opposed to surface-level patterns, and also the ability of the proposed PRA-based method [Path Ranking Algorithm that performs inference over a knowledge base using syntactic information from parsed text] to generate useful inference rules [used for increasing coverage of facts in Knowledge Bases] which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013)."}
{"idx": "173414", "paper_id": "229363636", "title": "Learning Dense Representations of Phrases at Scale", "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "context_section_header": "", "context_paragraph": "tically improves phrase retrieval performance and allows for transfer learning to new domains, without re-building billions of phrase representations. As a result, all these improvements lead to a much stronger phrase retrieval model, without the use of any sparse representations (Table 1). We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets. Our model also matches the performance of state-ofthe-art retriever-reader models (Guu et al., 2020;. Due to the removal of sparse representations and careful design choices, we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB, as well as drastically improve the throughput.", "sentence": "We evaluate our model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "snippet_rewrite": "The authors evaluate their model, DensePhrases, on five standard open-domain QA datasets and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019;, with 15%-25% absolute improvement on most datasets.", "cited_ids": [{"paper_id": "189762341", "citation": "(Seo et al., 2019;"}, {"paper_id": "189762341", "citation": "(Seo et al., 2019;"}], "questions": [{"question": "What is the authors' model?", "question_id": "4T9JAsYA/w", "question_type": 2, "answer_text": "Their model is called DensePhrases and is designed to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods, and aims to provide a neural interface for retrieving phrase-level knowledge from a large text corpus", "evidence": [{"section": "Abstract", "paragraph": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "selected": "learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods.", "paper_id": "229363636"}, {"section": "Introduction", "paragraph": "Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.", "selected": "a neural interface for retrieving phrase-level knowledge from a large text corpus.", "paper_id": "229363636"}, {"section": "Introduction", "paragraph": "Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.", "selected": "a neural interface for retrieving phrase-level knowledge from a large text corpus.", "paper_id": "229363636"}]}, {"question": "What do \"most datasets\" refer to?", "question_id": "RQuZVRnVKw", "question_type": 2, "answer_text": "all datasets except SQuAD", "evidence": [{"section": "Open-domain QA.", "paragraph": "Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%-25% absolute improvement on all datasets except SQuAD. Training the model of  on C phrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models , while running much faster during inference (Table 1).  (Guu et al., 2020) {Wiki., CC-News} \u2020 40.4 40.7 42.9 --DPR-multi    Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ). * : no supervision using target training data (zero-shot). \u2020 : unlabeled data used for extra pre-training.   Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples.", "selected": "all datasets except SQuAD", "paper_id": "229363636"}, {"section": "Open-domain QA.", "paragraph": "Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%-25% absolute improvement on all datasets except SQuAD. Training the model of  on C phrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models , while running much faster during inference (Table 1).  (Guu et al., 2020) {Wiki., CC-News} \u2020 40.4 40.7 42.9 --DPR-multi    Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ). * : no supervision using target training data (zero-shot). \u2020 : unlabeled data used for extra pre-training.   Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples.", "selected": "all datasets except SQuAD", "paper_id": "229363636"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "selected": "learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods.", "paper_id": "229363636"}, {"section": "Introduction", "paragraph": "Finally, we envision that DensePhrases acts as a neural interface for retrieving phrase-level knowledge from a large text corpus. To showcase this possibility, we demonstrate that we can directly use DensePhrases for fact extraction, without rebuilding the phrase storage. With only fine-tuning the question encoder on a small number of subjectrelation-object triples, we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.", "selected": "a neural interface for retrieving phrase-level knowledge from a large text corpus.", "paper_id": "229363636"}, {"section": "Open-domain QA.", "paragraph": "Experimental results on open-domain QA are summarized in Table 3. Without any sparse representations, DensePhrases outperforms previous phrase retrieval models by a large margin and achieves a 15%-25% absolute improvement on all datasets except SQuAD. Training the model of  on C phrase = {NQ, SQuAD} only increases the result from 14.5% to 16.5% on NQ, demonstrating that it does not suffice to simply add more datasets for training phrase representations. Our performance is also competitive with recent retriever-reader models , while running much faster during inference (Table 1).  (Guu et al., 2020) {Wiki., CC-News} \u2020 40.4 40.7 42.9 --DPR-multi    Table 3: Open-domain QA results. We report exact match (EM) on the test sets. We also show the additional training or pre-training datasets for learning the retriever models (C retr ) and creating the phrase dump (C phrase ). * : no supervision using target training data (zero-shot). \u2020 : unlabeled data used for extra pre-training.   Table 4 summarizes the results on the two slot filling datasets, along with the baseline scores provided by Petroni et al. (2021). The only extractive baseline is DPR + BERT, which performs poorly in zero-shot relation extraction. On the other hand, our model achieves competitive performance on all datasets and achieves state-of-the-art performance on two datasets using only 5K training examples.", "selected": "all datasets except SQuAD", "paper_id": "229363636"}], "y": "The authors evaluate their model, DensePhrases [which is designed to learn phrase representations for retrieving phrase-level knowledge from a large text corpus], on five standard open-domain QA datasets [Natural Questions, SQuAD, TQA, Web Questions, CuratedTREC] and achieve much better accuracies than previous phrase retrieval models (Seo et al., 2019), with 15%-25% absolute improvement on most datasets."}
{"idx": "176206", "paper_id": "216642069", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube", "abstract": "Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos, a domain that, a priori, we expect to be relatively\"easy:\"speakers in instructional videos will often reference the literal objects/actions being depicted. Because instructional videos make up only a fraction of the web's diverse video content, we ask: can similar models be trained on broader corpora? And, if so, what types of videos are\"grounded\"and what types are not? We examine the diverse YouTube8M corpus, first verifying that it contains many non-instructional videos via crowd labeling. We pretrain a representative model on YouTube8M and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set still results in representations that generalize to both non-instructional and instructional domains.", "context_section_header": "", "context_paragraph": "porally corresponding (clip, ASR caption) pairs are sampled (\"Positive\" cases). For each positive case, a set of mismatched \"N egative\" cases is also sampled both from other videos and from the same video in equal proportion. In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change. 3 The following hinge loss is minimized for margin \u03b4:", "sentence": "In contrast to Miech et al. (2019), we control for clip length, and sample temporally fixed-length segments: this simplifying choice makes our error analysis significantly more straightforward, and results in minimal performance change.", "snippet_rewrite": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this simplifying choice makes their error analysis significantly more straightforward, and results in minimal performance change.", "cited_ids": [{"paper_id": "182952863", "citation": "Miech et al. (2019)"}], "questions": [{"question": "Is there a positive or a negative performance change?", "question_id": "VYfhk0y6eP", "question_type": 3, "answer_text": "A positive performance change is noted", "evidence": [{"section": "Domain", "paragraph": "We next ask: are instructional videos indeed easier to ground? While human judgements of instructional-ness and intra-video AUC are positively correlated \u03c1 = .20 (p 0), the low magnitude of this correlation provides additional empirical confirmation that other types of videos are 6 To make sure that the model is not succeeding simply because a category happened to be frequent in the dataset, we note the correlation between category AUC and category frequency is essentially zero (\u03c1 = .02, p > .58).", "selected": "positively correlated \u03c1 = .20 (p 0),", "paper_id": "216642069"}, {"section": "Domain", "paragraph": "Additionally, we train OLS regression models to predict segment AUC from lexical unigram features, while controlling for timing/length features. Lexical features add predictive capacity (p .01, F-test). While we find some patterns, e.g., intro/outro-language (e.g., \"hey\", \"welcome\", \"peace\") predictive of segment AUC for both categories, we also observe topical patterns, e.g., several unigrams associated with specific action figure body parts (\"knee\", \"shoulder\", \"joint\", etc.) are positively associated with segment AUC.", "selected": "positively associated with segment AUC.", "paper_id": "216642069"}]}], "unique_evidence": [{"section": "Domain", "paragraph": "We next ask: are instructional videos indeed easier to ground? While human judgements of instructional-ness and intra-video AUC are positively correlated \u03c1 = .20 (p 0), the low magnitude of this correlation provides additional empirical confirmation that other types of videos are 6 To make sure that the model is not succeeding simply because a category happened to be frequent in the dataset, we note the correlation between category AUC and category frequency is essentially zero (\u03c1 = .02, p > .58).", "selected": "positively correlated \u03c1 = .20 (p 0),", "paper_id": "216642069"}, {"section": "Domain", "paragraph": "Additionally, we train OLS regression models to predict segment AUC from lexical unigram features, while controlling for timing/length features. Lexical features add predictive capacity (p .01, F-test). While we find some patterns, e.g., intro/outro-language (e.g., \"hey\", \"welcome\", \"peace\") predictive of segment AUC for both categories, we also observe topical patterns, e.g., several unigrams associated with specific action figure body parts (\"knee\", \"shoulder\", \"joint\", etc.) are positively associated with segment AUC.", "selected": "positively associated with segment AUC.", "paper_id": "216642069"}], "y": "In contrast to Miech et al. (2019), the authors control for clip length, and sample temporally fixed-length segments: this makes their [simple] error analysis significantly more straightforward, and results in [a minimal positive] performance change."}
{"idx": "176260", "paper_id": "42957292", "title": "Effect Functors for Opinion Inference", "abstract": "Sentiment analysis has so far focused on the detection of explicit opinions. However, of late implicit opinions have received broader attention, the key idea being that the evaluation of an event type by a speaker depends on how the participants in the event are valued and how the event itself affects the participants. We present an annotation scheme for adding relevant information, couched in terms of so-called effect functors, to German lexical items. Our scheme synthesizes and extends previous proposals. We report on an inter-annotator agreement study. We also present results of a crowdsourcing experiment to test the utility of some known and some new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation.", "context_section_header": "", "context_paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "sentence": "Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations.", "snippet_rewrite": "The authors add explicit annotations of functor types to the annotations, unlike Ruppenhofer and Brandes (2015).", "cited_ids": [{"paper_id": "193721", "citation": "Ruppenhofer and Brandes (2015)"}], "questions": [{"question": "What \"functor types\" are considered?", "question_id": "CY2yAlX3ch", "question_type": 1.0, "answer_text": "Possession, Possibility, Sentiment, Scalarity", "evidence": [{"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5.", "paper_id": "42957292"}, {"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "embedding states of possibility, predicates expressing location, and predicates expressing similarity.", "paper_id": "42957292"}]}, {"question": "What do the explicit annotations refer to?", "question_id": "CfJJriGHG8", "question_type": 2, "answer_text": "The explicit annotations refer to the additional functors proposed for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity.", "evidence": [{"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity.", "paper_id": "42957292"}]}], "unique_evidence": [{"section": "Introduction", "paragraph": "In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke's functors (Anand and Reschke, 2010;Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Section 7.). Unlike Ruppenhofer and Brandes (2015), we add explicit annotations of functor types to the annotations. In doing so, we also allow for multiple functors applying to the same effect predicate. Moreover, we do not only index the arguments relevant to the end state (=effect), but also those encoding the cause or agent. The remainder of this paper is structured as follows. Section 2 discusses related work. In Section 3, we present the data that we annotated following the annotation scheme outlined in Section 4. We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8.", "selected": "We develop a systematic account of the inventory of possible functor types and their interrelationships in Section 5.", "paper_id": "42957292"}], "y": "[Additional functors proposed for verbs embedding states of possibility] are added by the authors to functor types [such as possession, possibility, sentiment, and scalarity], unlike alternative approaches."}
{"idx": "1604.00400.2.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Summarization Evaluation by Relevance Analysis (Sera)", "context_paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "sentence": "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "snippet_rewrite": "On a high level, the authors indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, they use the summaries as search queries and compare the overlaps of the retrieved results.", "cited_ids": [], "questions": [{"question": "How do the authors perform \"information retrieval\"?", "question_id": "oov6c32bb8", "question_type": 3, "answer_text": "The system generated scientific summaries are used as search queries where the overlap of the retrieved results is then compared (to the corresponding human written summaries).", "evidence": [{"section": "Abstract", "paragraph": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "selected": "we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries.", "paper_id": "1604.00400"}, {"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "paper_id": "1604.00400"}]}, {"question": "What does \"on a high level\" mean?", "question_id": "Vw0oNetOfN", "question_type": 1, "answer_text": "Compared to related work, this paper uses information retrieval to evaluate the content relevance between a system generated summary and a human written summary. Additionally, this method allows the authors to compare the relevance of terms that are semantically related (not just lexically equivalent).", "evidence": [{"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", "paper_id": "1604.00400"}, {"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.", "paper_id": "1604.00400"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "selected": "we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries.", "paper_id": "1604.00400"}, {"section": "Summarization Evaluation by Relevance Analysis (Sera)", "paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "selected": "To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "paper_id": "1604.00400"}], "y": "The authors indirectly evaluate the content relevance of terms [that are semantically related] between a system generated summary and the human summary using information retrieval. To accomplish this, they use the [generated] summaries as search queries and compare the overlaps of the retrieved results [to the corresponding human written summaries]."}
