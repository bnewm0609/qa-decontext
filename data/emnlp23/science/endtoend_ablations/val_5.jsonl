{"idx": "103595", "paper_id": "4957206", "title": "Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource", "abstract": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "context_section_header": "", "context_paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "sentence": "Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "snippet_rewrite": "Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text).", "cited_ids": [{"paper_id": "13475624", "citation": "Jiang et al. (2016)"}], "questions": [{"question": "What is the first difference?", "question_id": "RFUs0cLj5x", "question_type": 2, "answer_text": "The scale difference.", "evidence": [{"section": "Related Work", "paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "selected": "First, scale difference:", "paper_id": "4957206"}]}, {"question": "What is the second difference?", "question_id": "2lFRRKXRjC", "question_type": 2, "answer_text": "The granularity difference.", "evidence": [{"section": "Related Work", "paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "selected": "Second, granularity difference:", "paper_id": "4957206"}]}, {"question": "What does the event in the knowledge base look like?", "question_id": "QyGJfTBT7D", "question_type": 3, "answer_text": "Like semantic frames.", "evidence": [{"section": "Learning", "paragraph": "Due to the slight event annotation difference in TBDense, we collect our training data as follows. We first extract all the verb semantic frames from the raw text of TBDense. Then we only keep those semantic frames that are matched to an event in TBDense (about 85% semantic frames are kept in this stage). By doing so, we can simply use the TempRel annotations provided in TBDense. Hereafter the TBDense dataset used in this paper refers to this version unless otherwise specified.", "selected": "Then we only keep those semantic frames that are matched to an event in TBDense", "paper_id": "4957206"}]}, {"question": "What type of relations are extracted?", "question_id": "ewn9IQmUgT", "question_type": 3, "answer_text": "Temporal relations are extracted.", "evidence": [{"section": "Abstract", "paragraph": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "selected": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language.", "paper_id": "4957206"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "Our work significantly differs from the timesensitive relations in Jiang et al. (2016) in the following aspects. First, scale difference: Jiang et al. (2016) can only extract a small number of relations (<100), but we work on general semantic frames (tens of thousands) and the relations between any two of them, which we think has broader applications. Second, granularity difference: the smallest granularity in Jiang et al. (2016) is one year 2 , i.e., only when two events happened in different years can they know the temporal order of them, but we can handle implicit temporal orders without having to refer to the physical time points of events (i.e., the granularity can be arbitrarily small). Third, domain difference: while Jiang et al. (2016) extracts time-sensitive relations from structured knowledge bases (where events are explicitly anchored to a time point), we extract relations from unstructured natural language text (where the physical time points may not even exist in text). Our task is more general and it allows us to extract much more relations, as reflected by the 1st difference above.", "selected": "First, scale difference:", "paper_id": "4957206"}, {"section": "Learning", "paragraph": "Due to the slight event annotation difference in TBDense, we collect our training data as follows. We first extract all the verb semantic frames from the raw text of TBDense. Then we only keep those semantic frames that are matched to an event in TBDense (about 85% semantic frames are kept in this stage). By doing so, we can simply use the TempRel annotations provided in TBDense. Hereafter the TBDense dataset used in this paper refers to this version unless otherwise specified.", "selected": "Then we only keep those semantic frames that are matched to an event in TBDense", "paper_id": "4957206"}, {"section": "Abstract", "paragraph": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language. We argue that this task would gain from the availability of a resource that provides prior knowledge in the form of the temporal order that events usually follow. This paper develops such a resource \u2013 a probabilistic knowledge base acquired in the news domain \u2013 by extracting temporal relations between events from the New York Times (NYT) articles over a 20-year span (1987\u20132007). We show that existing temporal extraction systems can be improved via this resource. As a byproduct, we also show that interesting statistics can be retrieved from this resource, which can potentially benefit other time-aware tasks. The proposed system and resource are both publicly available.", "selected": "Extracting temporal relations (before, after, overlapping, etc.) is a key aspect of understanding events described in natural language.", "paper_id": "4957206"}], "y": "[There are three differences between Jiang et al's and the authors work on relation extraction. First, a difference in scale. Second, a difference in granularity.] Third, there is a difference in domain: Jiang et al. (2016) extracts time-sensitive relations [which look like semantic frames] from structured knowledge bases (where events are explicitly anchored to a time point), whereas the authors extract relations from unstructured natural language text (where the physical time points may not even exist in text)."}
{"idx": "104377", "paper_id": "15193840", "title": "Automatic Domain Adaptation for Word Sense Disambiguation Based on Comparison of Multiple Classifiers", "abstract": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "context_section_header": "", "context_paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "sentence": "The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "snippet_rewrite": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, whereas the authors determined the method for each instance.", "cited_ids": [{"paper_id": "2462277", "citation": "(Komiya and Okumura, 2011)"}], "questions": [{"question": "What is a \"triple\" of a target word type?", "question_id": "xKLVA/UqO6", "question_type": 1, "answer_text": "It is a group of data containing exactly 3 piece of information: target type of WSD, source data, target data.", "evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "for each triple of the target word type of WSD, source data, and target data,", "paper_id": "15193840"}, {"section": "Conclusion", "paragraph": "We described how the optimal method of DA could be determined depending on the properties of the source and target data using decision tree learning and found what properties affected the determination of the best method when Japanese WSD was performed. We defined a case as a triple of the target word type of WSD, the source data, and the target data, all of which were classified into two labels (TO and RS) or three labels (TO, RS, and SA). Here, the case with TO should only be trained with a small amount of target data, the case with RS should be trained with source data and a small amount of target data, and SA represents a case with no difference between the accuracies for the two methods. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively. We automatically generated a decision tree in eight ways, the most accurate of which was with SA label when the WSD accuracies of the two methods were totally equal, performed binary classification without SA, and classified cases without weighted word tokens. The top node in the tree that was generated indicated that simulation using ten manually labeled word tokens of the target data was an important clue enabling the optimal DA method to be predicted.", "selected": "We defined a case as a triple of the target word type of WSD, the source data, and the target data,", "paper_id": "2462277"}]}, {"question": "What does \"WSD\" stand for?", "question_id": "5WWPAq8lni", "question_type": 1, "answer_text": "WSD: Word sense disambiguation", "evidence": [{"section": "Abstract", "paragraph": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}, {"section": "Introduction", "paragraph": "However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}]}, {"question": "What does \"each instance\" refer to?", "question_id": "e0KsKMSGtK", "question_type": 2, "answer_text": "\"Each instance\" refers to the individual data contained in the triple. Instead of considering them altogether, they are investigated individually.", "evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "target word type of WSD, source data, and target data", "paper_id": "15193840"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance.", "selected": "for each triple of the target word type of WSD, source data, and target data,", "paper_id": "15193840"}, {"section": "Conclusion", "paragraph": "We described how the optimal method of DA could be determined depending on the properties of the source and target data using decision tree learning and found what properties affected the determination of the best method when Japanese WSD was performed. We defined a case as a triple of the target word type of WSD, the source data, and the target data, all of which were classified into two labels (TO and RS) or three labels (TO, RS, and SA). Here, the case with TO should only be trained with a small amount of target data, the case with RS should be trained with source data and a small amount of target data, and SA represents a case with no difference between the accuracies for the two methods. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively. We automatically generated a decision tree in eight ways, the most accurate of which was with SA label when the WSD accuracies of the two methods were totally equal, performed binary classification without SA, and classified cases without weighted word tokens. The top node in the tree that was generated indicated that simulation using ten manually labeled word tokens of the target data was an important clue enabling the optimal DA method to be predicted.", "selected": "We defined a case as a triple of the target word type of WSD, the source data, and the target data,", "paper_id": "2462277"}, {"section": "Abstract", "paragraph": "Domain adaptation (DA), which involves adapting a classifier developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of confidence of multiple classifiers for each instance. We compared three classifiers for three DA methods, where 1) a classifier was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classifier was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classifier was trained with selected source data that were sufficiently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of confidence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was significantly higher than when the original methods were used collectively.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}, {"section": "Introduction", "paragraph": "However, the optimal method of DA varied according to the properties of the data in the source domain (the source data) and the data in the target domain (the target data) when DA for word sense disambiguation (WSD) was carried out.", "selected": "word sense disambiguation (WSD)", "paper_id": "15193840"}], "y": "The main difference is that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD [(Word sense disambiguation)], source data, and target data, whereas the authors determined the method for each instance [the individual data contained in the triple]."}
{"idx": "112709", "paper_id": "227230401", "title": "Formality Style Transfer with Shared Latent Space", "abstract": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "context_section_header": "", "context_paragraph": "To verify the effectiveness and generalization of our method, we conduct experiments in three different settings: Data Limited, Data Augmentation, and Pre-training. In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset. When we use large-scale non-parallel data to enhance our method in the data augmentation and pre-training settings, our method still consistently outperforms the baselines by 1 BLEU score. The ablation test further studies the effectiveness of the joint training, the auto-encoding training, and the auxiliary losses in different scenarios, showing the robustness of our method.", "sentence": "In the data-limited scenario, experimental results show that our method is significantly better than previous work (Rao and Tetreault, 2018) by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "snippet_rewrite": "In the data-limited scenario, experimental results show that the authors' method is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains (namely, F&R and E&M) of the GYAFC dataset.", "cited_ids": [{"paper_id": "4859003", "citation": "(Rao and Tetreault, 2018)"}], "questions": [{"question": "What does F&R stand for?", "question_id": "QJveBARNkz", "question_type": 1.0, "answer_text": "F&R stands for Family & Relationships category of informal sentences.", "evidence": [{"section": "Related Work", "paragraph": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk. The workers are presented with detailed instructions, as well  as examples. To ensure quality control, four experts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required standards. They also provided the workers with reasons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly performed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions.", "selected": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk.", "paper_id": "4859003"}]}, {"question": "What does E&M stand for?", "question_id": "pk1ETlyGVv", "question_type": 1.0, "answer_text": "E&M stands for Entertainment & Music category of informal sentences.", "evidence": [{"section": "Related Work", "paragraph": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk. The workers are presented with detailed instructions, as well  as examples. To ensure quality control, four experts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required standards. They also provided the workers with reasons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly performed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions.", "selected": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories", "paper_id": "4859003"}]}, {"question": "What is the authors' method?", "question_id": "VxCqlAwvTe", "question_type": 2, "answer_text": "The authors' method is called Sequence-to-Sequence with Shared Latent Space, and it includes two auxiliary losses with joint training of bi-directional transfer and auto-encoding.", "evidence": [{"section": "Abstract", "paragraph": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "selected": "In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding.", "paper_id": "227230401"}, {"section": "Our Approach", "paragraph": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS). Figure 1 depicts the overall framework of S2S-SLS. It has a shared encoder f enc , but two decoders f i dec and f f dec for the informal and formal styles, respectively. We will then describe our matching losses in detail. They ensure that the shared latent space of formal and informal styles does capture semantic information while eliminating the style.", "selected": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS).", "paper_id": "227230401"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk. The workers are presented with detailed instructions, as well  as examples. To ensure quality control, four experts, two of which are the authors of this paper, reviewed the rewrites of the workers and rejected those that they felt did not meet the required standards. They also provided the workers with reasons for rejection so that they would not repeat the same mistakes. Any worker who repeatedly performed poorly was eventually blocked from doing the task. We use this train set to train our models for the style transfer tasks in both directions.", "selected": "Next, we randomly sample a subset of 53,000 informal sentences each from the Entertainment & Music (E&M) and Family & Relationships (F&R) categories and collect one formal rewrite per sentence using Amazon Mechanical Turk.", "paper_id": "4859003"}, {"section": "Abstract", "paragraph": "Conventional approaches for formality style transfer borrow models from neural machine translation, which typically requires massive parallel data for training. However, the dataset for formality style transfer is considerably smaller than translation corpora. Moreover, we observe that informal and formal sentences closely resemble each other, which is different from the translation task where two languages have different vocabularies and grammars. In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding. Experimental results show that S2S-SLS (with either RNN or Transformer architectures) consistently outperforms baselines in various settings, especially when we have limited data.", "selected": "In this paper, we present a new approach, Sequence-to-Sequence with Shared Latent Space (S2S-SLS), for formality style transfer, where we propose two auxiliary losses and adopt joint training of bi-directional transfer and auto-encoding.", "paper_id": "227230401"}, {"section": "Our Approach", "paragraph": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS). Figure 1 depicts the overall framework of S2S-SLS. It has a shared encoder f enc , but two decoders f i dec and f f dec for the informal and formal styles, respectively. We will then describe our matching losses in detail. They ensure that the shared latent space of formal and informal styles does capture semantic information while eliminating the style.", "selected": "Our method generally follows the sequence-to-sequence (Seq2Seq) framework, but explores the shared latent space for both formal and informal sentences. We call our method Seq2Seq with Shared Latent Space (S2S-SLS).", "paper_id": "227230401"}], "y": "In the data-limited scenario, experimental results show that the authors' method [Sequence-to-Sequence with Shared Latent Space trained with two auxiliary losses with joint training of bi-directional transfer and auto-encoding] is significantly better than Rao and Tetreault, 2018 by 4 and 7 BLEU scores on the two domains [categories of informal sentences] (namely, F&R [family and relationships] and E&M [entertainment and music]) of the GYAFC dataset."}
{"idx": "113060", "paper_id": "202538019", "title": "Mixture Content Selection for Diverse Sequence Generation", "abstract": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "context_section_header": "", "context_paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "sentence": "While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation.", "snippet_rewrite": "Shen et al. (2019) makes a RNN decoder as a MoE, whereas the authors make a SELECTOR as a MoE to diversify content selection and let the encoder-decoder models one-to-one generation.", "cited_ids": [{"paper_id": "67787922", "citation": "Shen et al. (2019)"}], "questions": [{"question": "What does \"MoE\" stand for?", "question_id": "qcZ7zMU/Ij", "question_type": 1, "answer_text": "Mixture of Experts", "evidence": [{"section": "Related Work", "paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "selected": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps.", "paper_id": "202538019"}]}, {"question": "What is \"a SELECTOR?\"", "question_id": "0nuJozFz9/", "question_type": 1, "answer_text": "SELECTOR is a general plug-and-play module that wraps around and guides an existing encoder-decoder model. This is a generic module that is specialized for increasing generation diversity.", "evidence": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": "We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model", "paper_id": "202538019"}]}, {"question": "What is \"one-to-one generation?\"", "question_id": "uW85CSDZaZ", "question_type": 1, "answer_text": "This is the generation method that employes a standard encoder-decoder model to generate a target sequence given each selected content from the source.", "evidence": [{"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}]}, {"question": "What is the content that is being selected?", "question_id": "KA0t7+Mu76", "question_type": 3, "answer_text": "Individual tokens are selected from the source to be fed into the encoder-decoder model.", "evidence": [{"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": ". The generation stage uses a standard encoder-decoder model given each selected content from the source sequence.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "selected": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}]}], "unique_evidence": [{"section": "Related Work", "paragraph": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps. Yang et al. (2018) introduce soft mixture of softmax on top of the output layer of RNN language model. ; Shen et al. (2019) introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder (Shen et al., 2019) that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012;Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.", "selected": "Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991;Eigen et al., 2014) to diversify decoding steps.", "paper_id": "202538019"}, {"section": "Abstract", "paragraph": "Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.", "selected": "We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.", "selected": "The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping).", "paper_id": "202538019"}, {"section": "Introduction", "paragraph": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "selected": "Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018;Vijayakumar et al., 2018) or mixture of decoders Shen et al., 2019). These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.", "paper_id": "202538019"}], "y": "Shen et al. (2019) makes a RNN decoder as a MOE [deep Mixture of Experts], whereas the authors make a SELECTOR [a general plug-and-play module specialized for diversification that wraps around and guides an existing encoder-decoder model] as a MoE to diversify content selection [selection of individual tokens from the source to be fed into the encoder-decoder model] and enable the encoder-decoder model's one-to-one generation."}
{"idx": "114795", "paper_id": "196471395", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning", "abstract": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "context_section_header": "", "context_paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "sentence": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "snippet_rewrite": "The authors' system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, their multi-agent system supports any slotfilling / information-seeking domain.", "cited_ids": [{"paper_id": "1294169", "citation": "(Henderson et al., 2014)"}], "questions": [{"question": "What is the authors' system?", "question_id": "d+L547/CZx", "question_type": 2, "answer_text": "The author's system consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty.", "evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language.", "paper_id": "196471395"}]}, {"question": "What is \"dstc2\" domain?", "question_id": "N0apey0EKr", "question_type": 1, "answer_text": "DSTC2 is not a domain, but the seed data which contains the dialogues between humans asking for information and the machine providing it.", "evidence": [{"section": "Introduction", "paragraph": "Employing a user simulator is an established method for dialogue policy learning (Schatzmann et al., 2007, among others) and end-to-end dialogue training (Asri et al., 2016;Liu and Lane, 2018b). Training two conversational agents concurrently has been proposed by Georgila et al. (2014); training them via natural language communication was partially realized by Liu and Lane (2017), as they train agents that receive text input but generate dialogue acts. However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator. Inspired by Hakkani-T\u00fcr (2018), each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal. This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents. The architecture of each agent is mirrored as shown in Figure 1, so the effort of adding agents with new roles is minimal. As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information. Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "selected": "As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "paper_id": "196471395"}]}, {"question": "What is their multi-agent system?", "question_id": "GU316DPPTi", "question_type": 1, "answer_text": "The multi-agent system is a type of system that has each agent playing a different role and has their own objective. These agents create languages not present in the original data.", "evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time).", "paper_id": "196471395"}]}], "unique_evidence": [{"section": "Abstract", "paragraph": "Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent\u2019s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent\u2019s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.", "selected": "To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (\u201cassistant\u201d, \u201ctourist\u201d, \u201ceater\u201d, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty", "paper_id": "196471395"}, {"section": "Related Work", "paragraph": "2 System Overview Figure 1 shows the general architecture and information flow of our system, composed of two agents who communicate via written language. Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain. The Language Understanding and Generation components are trained offline as described in the following sections, while the dialogue policies of the agents are trained online during their interaction. Given that our language generation component is model-based rather than retrieval-based or template-based, we believe that the quality of the generated language and dialogues is encouraging (see appendix for some example dialogues).", "selected": "Our system operates in the well-known DSTC2 domain (Henderson et al., 2014) which concerns information about restaurants in Cambridge; however, our multi-agent system supports any slotfilling / information-seeking domain.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Employing a user simulator is an established method for dialogue policy learning (Schatzmann et al., 2007, among others) and end-to-end dialogue training (Asri et al., 2016;Liu and Lane, 2018b). Training two conversational agents concurrently has been proposed by Georgila et al. (2014); training them via natural language communication was partially realized by Liu and Lane (2017), as they train agents that receive text input but generate dialogue acts. However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator. Inspired by Hakkani-T\u00fcr (2018), each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal. This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents. The architecture of each agent is mirrored as shown in Figure 1, so the effort of adding agents with new roles is minimal. As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information. Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "selected": "As seed data, we use data from DSTC2 (Henderson et al., 2014), which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "paper_id": "196471395"}, {"section": "Introduction", "paragraph": "Machine learning for conversational agents has seen great advances (e.g. Tur and Mori, 2011;Gao et al., 2019;Singh et al., 1999;Oh and Rudnicky, 2000;Zen et al., 2009;Reiter and Dale, 2000;Rieser and Lemon, 2010), especially when adopting deep learning models (Deng and Mesnil et al., 2015;Wen et al., 2015Wen et al., , 2017Papangelis et al., 2018;Liu and Lane, 2018b;Li et al., 2017;Williams et al., 2017;Liu and Lane, 2018a). Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations. Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world may be complex but is stationary. In this work, we model conversational interaction as a stochastic game (e.g. Bowling and Veloso, 2000) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language. We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm (Bowling and Veloso, 2001), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time). While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "selected": "the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "paper_id": "196471395"}], "y": "The authors' system [consists of two agents that communicate via written language with each agent needing to learn to operate with multiple source of uncertainty]. The system operates in the well-known DSTC2 domain [seed data that contains the dialogues between human asking for information and the machine providing it] which concerns information about restaurants in Cambridge. However, the authors' multi-agent system [a system with multiple agent create novel languages by each playing a different role and having their own objective]. Supports any slot filling / information-seeking domain."}
