{"idx": "1604.00400.2.1.1", "paper_id": "1604.00400", "title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.", "context_section_header": "Summarization Evaluation by Relevance Analysis (Sera)", "context_paragraph": "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related. Our method is based on the well established linguistic premise that semantically related words occur in similar contexts BIBREF5 . The context of the words can be considered as surrounding words, sentences in which they appear or the documents. For scientific summarization, we consider the context of the words as the scientific articles in which they appear. Thus, if two concepts appear in identical set of articles, they are semantically related. We consider the two summaries as similar if they refer to same set of articles even if the two summaries do not have high lexical overlaps. To capture if a summary relates to a article, we use information retrieval by considering the summaries as queries and the articles as documents and we rank the articles based on their relatedness to a given summary. For a given pair of system summary and the gold summary, similar rankings of the retrieved articles suggest that the summaries are semantically related, and thus the system summary is of higher quality.", "sentence": "On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.", "cited_ids": [], "y": "The authors indirectly evaluate the content relevance of terms [that are semantically related] between a system generated summary and the human summary using information retrieval. To accomplish this, they use the [generated] summaries as search queries and compare the overlaps of the retrieved results [to the corresponding human written summaries].", "snippet_surface": "On a high level, the authors indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, they use the summaries as search queries and compare the overlaps of the retrieved results.", "questions": {"oov6c32bb8": "How do the authors perform \"information retrieval\"?", "Vw0oNetOfN": "What does \"on a high level\" mean?"}}
{"idx": "1802.07862.1.1.1", "paper_id": "1802.07862", "title": "Multimodal Named Entity Recognition for Short Social Media Posts", "abstract": "We introduce a new task called Multimodal Named Entity Recognition (MNER) for noisy user-generated data such as tweets or Snapchat captions, which comprise short text with accompanying images. These social media posts often come in inconsistent or incomplete syntax and lexical notations with very limited surrounding textual contexts, bringing significant challenges for NER. To this end, we create a new dataset for MNER called SnapCaptions (Snapchat image-caption pairs submitted to public and crowd-sourced stories with fully annotated named entities). We then build upon the state-of-the-art Bi-LSTM word/character based NER models with 1) a deep image network which incorporates relevant visual context to augment textual information, and 2) a generic modality-attention module which learns to attenuate irrelevant modalities while amplifying the most informative ones to extract contexts from, adaptive to each sample and token. The proposed MNER model with modality attention significantly outperforms the state-of-the-art text-only NER models by successfully leveraging provided visual contexts, opening up potential applications of MNER on myriads of social media platforms.", "context_section_header": "Results: SnapCaptions Dataset", "context_paragraph": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token. In the example of \u201cdisney word essential = coffee\" with visual tags selfie, phone, person, the modality attention successfully attenuates distracting visual signals and focuses on textual modalities, consequently making correct predictions. The named entities in the examples of \u201cBeautiful night atop The Space Needle\" and \u201cSplash Mountain\" are challenging to predict because they are composed of common nouns (space, needle, splash, mountain), and thus they often need additional contexts to correctly predict. In the training data, visual contexts make stronger indicators for these named entities (space needle, splash mountain), and the modality attention module successfully attends more to stronger signals.", "sentence": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token.", "cited_ids": [], "y": "The authors confirm that the modality attention successfully attenuates [reduces] irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token for the image-aided model (W+C+V).", "snippet_surface": "The authors confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token for the image-aided model (W+C+V; upper row in Figure FIGREF19).", "questions": {"vJrz+pNHzx": "What does \"attenuates\" mean?"}}
{"idx": "271822", "paper_id": "235624320", "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types", "abstract": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task.", "context_section_header": "", "context_paragraph": "OpenKG Embeddings: Learning embeddings for OpenKGs has been a relatively under-explored area of research. Previous work using OpenKG embeddings has primarily focused on canonicalization. CESI (Vashishth et al., 2018) uses KG embedding models for the canonicalization of noun phrases in OpenKGs. The problem of incorporating canonicalization information into OpenKG embeddings was addressed by Gupta et al. (2019). Their method for OpenKG embeddings (i.e., CaRE) performs better than Ontological KG embedding baselines in terms of link prediction performance. The challenges in the link prediction for OpenKGs were discussed in Broscheit et al. (2020), and methods similar to CaRE were proposed. In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task.", "sentence": "In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task.", "cited_ids": [{"paper_id": "202763196", "citation": "(Gupta et al., 2019)"}], "y": "In spirit, CaRE ([an OpenKG embedding method consisting of three components; ]Gupta et al., 2019) comes closest to the authors' model, but does not address the problem of type compatibility [predicting a head (noun phrases) pair and tail noun phrase using a compatibility score] in the link prediction task.", "snippet_surface": "In spirit, CaRE (Gupta et al., 2019) comes closest to the authors' model; however, they do not address the problem of type compatibility in the link prediction task.", "questions": {"B/fqVQzKlR": "What is \"care\"?", "4T9JAsYA/w": "What is the authors' model?", "uQiTjUgIcX": "What is type compatibility?"}}
{"idx": "392826", "paper_id": "207901266", "title": "What does the language of foods say about us?", "abstract": "In this work we investigate the signal contained in the language of food on social media. We experiment with a dataset of 24 million food-related tweets, and make several observations. First,thelanguageoffoodhaspredictive power. We are able to predict if states in the United States (US) are above the medianratesfortype2diabetesmellitus(T2DM), income, poverty, and education \u2013 outperforming previous work by 4\u201318%. Second, we investigate the effect of socioeconomic factors (income, poverty, and education) on predicting state-level T2DM rates. Socioeconomic factors do improve T2DM prediction, with the greatestimprovementcomingfrompovertyinformation(6%),but,importantly,thelanguage of food adds distinct information that is not captured by socioeconomics. Third, we analyze how the language of food has changed over a five-year period (2013 \u2013 2017), which is indicative of the shift in eating habits in the US during that period. We find several food trends, and that the language of food is used differently by different groups such as differentgenders. Last,weprovideanonlinevisualization tool for real-time queries and semantic analysis.", "context_section_header": "", "context_paragraph": "With an average of 6,000 new tweets posted every second, Twitter 1 has become a digital footprint of everyday life for a representative sample of the United States (US) population (Mislove et al., 2011). Previously, Fried et al. (2014) demonstrated that the language of food on Twitter can be used to predict health risks, political orientation, and geographic location. Here, we use predictive models to extend this analysis -exploring the ways in which the language of food can shed insight on health and the changing trends in * Equal contribution. 1 https://twitter.com/ both food culture and language use in different communities over time. We apply this methodology to the particular use case of predicting communities which are risk for type 2 diabetes mellitus (T2DM), a serious medical condition which affects over 30 million Americans and whose diagnosis alone costs $327 billion each year 2 . We refer to T2DM as diabetes in the rest of the paper. We show that by combining knowledge from tweets with other social characteristics (e.g., average income, level of education) we can better predict risk of T2DM. The contributions of this work are four-fold: 1. We use the same methods proposed by Fried et al. (2014) with a much larger (7 times) tweet corpus gathered from 2013 -2017 to predict the risk of T2DM. We collected over 24 million tweets with meal-related hashtags (e.g., #breakfast, #lunch) and localized 5 million of them to states within the US. We show that more data helps, and that by training on this larger dataset the state-level T2DM risk prediction accuracy is improved by 4-18%, compared to the results in Fried et al. (2014). We also apply the same models to predict additional state-level indicators: income, poverty, and education levels in order to further investigate the predictive power of the language of food. On these prediction tasks, our model outperforms the majority baseline by 12-34%. We believe that this work may drive immediate policy decisions for the communities deemed at risk without awaiting for similar results from major health organizations, which take months or years to be generated and disseminated. 3 Equally as important, we believe that this state-level T2DM risk prediction task may improve predicting risks for individuals from their social media activity, a task which often suffers from sparsity (Bell et al., 2018). 2. Unlike (Fried et al., 2014), we also investigate the effect of socioeconomic factors on the diabetes prediction task itself. We observe that aggregated US social demographic information from average income 4 , poverty 5 , and education 6 is complementary to the information gained from tweet language used for predicting diabetes risk. We add the correlation between each of these socioeconomic factors and the diabetes 7 rate in US states as additional features in the models in (1). We demonstrate that the T2DM prediction model strongly benefits from the additional information, as prediction accuracy further increases by 2-6%. However, importantly, the model that relies solely on these indicators performs considerably worse than the model that includes features from the language of food, which demonstrates that the language of food provides distinct signal from these indicators. 3. Furthermore, with a dataset that spans nearly five years, we also analyze language trends over time. Specifically, using pointwise mutual information (PMI) and a custom-built collection of healthy/unhealthy food words, we investigate the strength of healthy/unhealthy food references on Twitter, and observe a downward trend for healthy food references and an upward trend for unhealthy food words in the US. 4. Lastly, we provide a visualization tool to help understand and visualize semantic relations between words and various categories such as how different genders refer to vegetarian vs. lowcarb diets. 8 Our tool is based on semantic axes plots (Heimerl and Gleicher, 2018).", "sentence": "2. Unlike (Fried et al., 2014), we also investigate the effect of socioeconomic factors on the diabetes prediction task itself.", "cited_ids": [{"paper_id": "14599127", "citation": "(Fried et al., 2014)"}], "y": "Unlike (Fried et al., 2014), the authors also investigate the effect of socioeconomic factors [income, poverty, and education] on the diabetes prediction task itself.", "snippet_surface": "Unlike (Fried et al., 2014), the authors also investigate the effect of socioeconomic factors on the diabetes prediction task itself.", "questions": {"lEUzLiMPlq": "What socioeconomic factors are bring referred to?"}}
{"idx": "486090", "paper_id": "227230410", "title": "A High Precision Pipeline for Financial Knowledge Graph Construction", "abstract": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.", "context_section_header": "", "context_paragraph": "Compared with (Benetka et al., 2017), the most closely related work, our pipeline extracts over 342,000 n-ary facts and covers more types of financial predicates -a total of 87 as opposed to 50 in (Benetka et al., 2017). Furthermore, our pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "sentence": "Furthermore, our pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "cited_ids": [{"paper_id": "26077690", "citation": "(Benetka et al., 2017)"}], "y": "The authors' pipeline [that combines Semantic Role Labeling (SRL) information extraction for verb predicates with typed patterns for noun-mediated relations] produces high precision extractions [of the relations between between financial entities from financial text], specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "snippet_surface": "Furthermore, the authors' pipeline produces high precision extractions, specifically 78% at the top-100 extractions, as opposed to 34% of the pipeline from (Benetka et al., 2017).", "questions": {"A1oAR0vcX5": "What is the \"authors' pipeline\"?", "1ELxDE24dq": "What extractions are being refered to?", "lhmGxrLDhv": "How is the high precision achieved?", "/CUFrwB9+/": "What pipeline is being refered to?"}}
