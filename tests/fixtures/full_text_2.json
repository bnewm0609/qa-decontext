{"title":"Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation","abstract":"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","full_text":[{"section_name":"Introduction","paragraphs":["In recent years, word embeddings (Bengio et al., 2003;Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) have had a huge impact in natural language processing (NLP) and related fields, being used in many tasks including sentiment analysis (Dos Santos and Gatti, 2014;Fu et al., 2018), information retrieval (Clinchant and Perronnin, 2013;Ye et al., 2016) and word sense disambiguation (Bhingardive et al., 2015;Chen et al., 2014;Iacobacci et al., 2016), among many others. Start-ing from word embeddings, researchers proposed various ways of aggregating word embedding vectors to obtain efficient sentence-level or documentlevel representations Cheng et al., 2018;Clinchant and Perronnin, 2013;Conneau et al., 2017;Cozma et al., 2018;Fu et al., 2018;Hill et al., 2016;Kiros et al., 2015;Kusner et al., 2015;Le and Mikolov, 2014;Shen et al., 2018;Torki, 2018;Zhao et al., 2015;Zhou et al., 2016Zhou et al., , 2018. Although the mean (or sum) of word vectors is commonly adopted because of its simplicity (Mitchell and Lapata, 2010), it seems that more complex approaches usually yield better performance (Cheng et al., 2018;Conneau et al., 2017;Cozma et al., 2018;Fu et al., 2018;Hill et al., 2016;Kiros et al., 2015;Torki, 2018;Zhao et al., 2015;Zhou et al., 2016Zhou et al., , 2018. To this end, we propose a simple yet effective approach for aggregating word embeddings into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010(Jégou et al., , 2012 used in computer vision to efficiently represent images for various image classification and retrieval tasks. To our knowledge, we are the first to adapt and use VLAD in the text domain.","Our document-level representation is constructed as follows. First, we apply a pre-trained word embedding model, such as GloVe (Pennington et al., 2014), on all the words from a set of training documents in order to obtain a set of training word vectors. The word vectors are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a text document is then computed by accumulating the differences between each codeword vector and each word vector that is both present in the document and associated to the respective codeword. Since our approach considers cluster centroids as reference for building the representation, it can easily accommodate new words, not seen during k-means training, simply by associating them to the nearest cluster centroids. Thus, VLAWE is robust to vocabulary distribution gaps between training and test, which can appear when the training set is particularly smaller or from a different domain. Certainly, the robustness holds as long as the word embeddings are pretrained on a very large set of documents, e.g. the entire Wikipedia.","We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier, namely Support Vector Machines (SVM), and show that it is useful for a diverse set of text classification tasks. We consider five benchmark data sets: Reuters-21578 (Lewis, 1997), RT-2k (Pang and Lee, 2004), MR (Pang and Lee, 2005), TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004). We compare VLAWE with recent stateof-the-art methods Cheng et al., 2018;Fu et al., 2018;Hill et al., 2016;Iyyer et al., 2015;Kim, 2014;Kiros et al., 2015;Le and Mikolov, 2014;Liu et al., 2017;Shen et al., 2018;Torki, 2018;Xue and Zhou, 2009;Zhao et al., 2015;Zhou et al., 2016Zhou et al., , 2018, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review (MR) data set, surpassing the state-of-the-art approach of Cheng et al. (2018) by almost 10%.","The rest of the paper is organized as follows. We present related works on learning documentlevel representations in Section 2. We describe the Vector of Locally-Aggregated Word Embeddings in Section 3. We present experiments and results on various text classification tasks in Section 4. Finally, we draw our conclusion in Section 5."]},{"section_name":"Related Work","paragraphs":["There are various works Cheng et al., 2018;Conneau et al., 2017;Fu et al., 2018;Hill et al., 2016;Iyyer et al., 2015;Kim, 2014;Kiros et al., 2015;Kusner et al., 2015;Le and Mikolov, 2014;Clinchant and Perronnin, 2013;Shen et al., 2018;Torki, 2018;Zhao et al., 2015;Zhou et al., 2018) that propose to build effective sentence-level or document-level representations based on word embeddings. While most of these approaches are based on deep learning (Cheng et al., 2018;Conneau et al., 2017;Hill et al., 2016;Iyyer et al., 2015;Kim, 2014;Kiros et al., 2015;Le and Mikolov, 2014;Zhao et al., 2015;Zhou et al., 2018), there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words (Butnaru and Ionescu, 2017) and by Fisher Vectors (Clinchant and Perronnin, 2013). The relationship between the bag-of-visual-words, Fisher Vectors and VLAD is discussed in (Jégou et al., 2012). The discussion can be transferred to describe the relantionship of our work and the closely-related works of  and Clinchant and Perronnin (2013)."]},{"section_name":"Method","paragraphs":["The Vector of Locally-Aggregated Descriptors (VLAD) (Jégou et al., 2010(Jégou et al., , 2012 was introduced in computer vision to efficiently represent images for various image classification and retrieval tasks. We propose to adapt the VLAD representation in order to represent text documents instead of images. Our adaptation consists of replacing the Scale-Invariant Feature Transform (SIFT) image descriptors (Lowe, 2004) useful for recognizing object patterns in images with word embeddings (Mikolov et al., 2013;Pennington et al., 2014) useful for recognizing semantic patterns in text documents. We coin the term Vector of Locally-Aggregated Word Embeddings (VLAWE) for the resulting document representation.","The VLAWE representation is derived as follows. First, each word in the collection of training documents is represented as a word vector using a pre-trained word embeddings model. The result is a set X = {x 1 , x 2 , ..., x n } of n word vectors. As for the VLAD model, the next step is to learn a codebook {µ 1 , µ 2 , ..., µ k } of representative metaword vectors (codewords) using k-means. Each codeword µ i is the centroid of the cluster C i ⊂ X:","µ i = 1 |C i | xt∈C i x t , ∀i ∈ {1, 2, ..., k}, (1)","where |C i | is the number of word vectors assigned to cluster C i and k is the number of clusters. Since word embeddings carry semantic information by projecting semantically-related words in the same region of the embedding space, it means that the resulting clusters contain semanticallyrelated words. The formed centroids are stored in a randomized forest of k-d trees to reduce search cost, as described in (Philbin et al., 2007;Popescu, 2014, 2015a). Each word embedding x t is associated to a single cluster C i , such that the Euclidean distance between x t and the corresponding codeword µ i is minimum, for all i ∈ {1, 2, ..., k}. For each document D and each codeword µ i , the differences x t − µ i of the vectors x t ∈ C i ∩ D and the codeword µ i are accumulated into column vectors:","v i,D = xt∈C i ∩D x t − µ i ,(2)","where D ⊂ X is the set of word embeddings in a given text document. The final VLAWE embedding for a given document D is obtained by stacking together the d-dimensional residual vectors v i,D , where d is equal to the dimension of the word embeddings:","φ D =      v 1,D v 2,D . . . v k,D      .(3)","Therefore, the VLAWE document embedding is has k · d components.","The VLAWE vector φ D undergoes two normalization steps. First, a power normalization is performed by applying the following operator independently on each component (element):","f (z) = sign(z) · |z| α ,(4)","where 0 ≤ α ≤ 1 and |z| is the absolute value of z. Since words in natural language follow the Zipf's law (Powers, 1998), it seems natural to apply the power normalization in order to reduce the influence of highly frequent words, e.g. common words or stopwords, which can corrupt the representation. As Jégou et al. (2012), we empirically observed that this step consistently improves the quality of the representation. The power normalized document embeddings are then L 2 -normalized. After obtaining the normalized VLAWE representations, we employ a classification method to learn a discriminative model for each specific text classification task."]},{"section_name":"Experiments","paragraphs":[]},{"section_name":"Data Sets","paragraphs":["We exhibit the performance of VLAWE on five public data sets: Reuters-21578 (Lewis, 1997), RT-2k (Pang and Lee, 2004), MR (Pang and Lee, 2005), TREC (Li and Roth, 2002) and Subj (Pang and Lee, 2004).","The Reuters-21578 data set (Lewis, 1997) contains articles collected from Reuters newswire. Following Joachims (1998) and Yang and Liu (1999), we select the categories (topics) that have at least one document in the training set and one in the test set, leading to a total of 90 categories. We use the ModeApte evaluation (Xue and Zhou, 2009), in which unlabeled documents are eliminated, leaving a total of 10787 documents. The collection is already divided into 7768 documents for training and 3019 documents for testing.","The RT-2k data set (Pang and Lee, 2004) consists of 2000 movie reviews taken from the IMDB movie review archives. There are 1000 positive reviews rated with four or five stars, and 1000 negative reviews rated with one or two stars. The task is to discriminate between positive and negative reviews.","The Movie Review (MR) data set (Pang and Lee, 2005) consists of 5331 positive and 5331 negative sentences. Each sentence is selected from one movie review. The task is to discriminate between positive and negative sentiment.","TREC (Li and Roth, 2002) is a question type classification data set, where questions are divided into 6 classes. The collection is already divided into 5452 questions for training and 500 questions for testing.","The Subjectivity (Subj) (Pang and Lee, 2004) data set contains 5000 objective and 5000 subjective sentences. The task is to classify a sentence as being either subjective or objective."]},{"section_name":"Evaluation and Implementation Details","paragraphs":["In the experiments, we used the pre-trained word embeddings computed with the GloVe toolkit provided by Pennington et al. (2014). The pre-trained GloVe model contains 300-dimensional vectors for 2.2 million words and phrases. Most of the steps required for building the VLAWE representation, such as the k-means clustering and the randomized forest of k-d trees, are implemented using the VLFeat library (Vedaldi and Fulkerson, 2008). We set the number of clusters (size of the codebook) to k = 10, leading to a VLAWE representation of k · d = 10 · 300 = 3000 components. Similar to Jégou et al. (2012), we set α = 0.5 for the power normalization step in Equation (4), which consistently leads to near-optimal results on all data sets. In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Zhou, 2009) 87.0 ----Paragraph vectors (Le and Mikolov, 2014) --74.8 91.8 90.5 CNN (Kim, 2014) -83.5 81.5 93.6 93.4 DAN (Iyyer et al., 2015) --80.1 --Combine-skip (Kiros et al., 2015) --76.5 92.2 93.6 Combine-skip + NB (Kiros et al., 2015) -  Table 1: Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018;Fu et al., 2018;Hill et al., 2016;Iyyer et al., 2015;Kim, 2014;Kiros et al., 2015;Le and Mikolov, 2014;Liu et al., 2017;Shen et al., 2018;Torki, 2018;Xue and Zhou, 2009;Zhao et al., 2015;Zhou et al., 2016Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets. The top three results on each data set are highlighted in red, green and blue, respectively. Best viewed in color.","set the SVM regularization parameter to C = 1 in all our experiments. In the SVM, we use the linear kernel. For optimal results, the VLAWE representation is combined with the BOSWE representation , which is based on the PQ kernel Popescu, 2013, 2015b).","We follow the same evaluation procedure as Kiros et al. (2015) and Hill et al. (2016), using 10fold cross-validation when a train and test split is not pre-defined for a given data set. As evaluation metrics, we employ the micro-averaged F 1 measure for the Reuters-21578 data set and the standard classification accuracy for the RT-2k, the MR, the TREC and the Subj data sets, in order to fairly compare with the related art."]},{"section_name":"Results","paragraphs":["We compare VLAWE with several state-of-theart methods Cheng et al., 2018;Fu et al., 2018;Hill et al., 2016;Iyyer et al., 2015;Kim, 2014;Kiros et al., 2015;Le and Mikolov, 2014;Liu et al., 2017;Shen et al., 2018;Torki, 2018;Xue and Zhou, 2009;Zhao et al., 2015;Zhou et al., 2016Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW). The corresponding results are presented in Table 1.","First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014;Hill et al., 2016). In most cases, our improvements over the baselines are higher than 5%. On the Reuters-21578 data set, we surpass the closely-related approach Method MR VLAWE (k = 2) 93.0 VALWE (PCA) 93.2 VLAWE (full, k = 10) 93.3 Table 2: Performance results (in %) of the full VLAWE representation (with k = 10) versus two compact versions of VLAWE, obtained either by setting k = 2 or by applying PCA.","of  by around 2%. On the RT-2k data set, we surpass the related works of Fu et al. (2018) and  by around 4%. To our knowledge, our accuracy of 94.1% on RT-2k (Pang and Lee, 2004) surpasses all previous results reported in literature. On the MR data set, we surpass most related works by more than 10%. To our knowledge, the best accuracy on MR reported in previous literature is 83.6%, and it is obtained by Cheng et al. (2018). We surpass the accuracy of Cheng et al. (2018) by almost 10%, reaching an accuracy of 93.3% using VLAWE. On the TREC data set, we reach the third best performance, after methods such as (Cheng et al., 2018;Zhou et al., 2016Zhou et al., , 2018. Our performance on TREC is about 2% lower than the stateof-the-art accuracy of 96.1%. On the Subj data set, we obtain an accuracy of 95.0%. There are two state-of-the-art methods (Cheng et al., 2018;Zhao et al., 2015) reporting better performance on Subj. Compared to the best one of them (Cheng et al., 2018), our accuracy is 1% lower. Overall, we consider that our results are noteworthy."]},{"section_name":"Discussion","paragraphs":["The k-means clustering algorithm and, on some data sets, the cross-validation procedure can induce accuracy variations due to the random choices involved. We have conducted experiments to determine how large are the accuracy variations. We observed that the accuracy can decrease by up to 1%, which does not bring any significant differences to the results reported in Table 1.","Even for a small number of clusters, e.g. k = 10, the VLAWE document representation can grow up to thousands of features, as the number of features is k · d, where d = 300 is the dimensionality of commonly used word embeddings. However, there are several document-level representations that usually have a dimensionality much smaller than k · d. Therefore, it is desirable to obtain a more compact VLAWE representation. We hereby propose two approaches that lead to more compact representations. The first one is simply based on reducing the number of clusters. By setting k = 2 for instance, we obtain a 600-dimensional representation. The second one is based on applying Principal Component Analysis (PCA), to reduce the dimension of the feature vectors. Using PCA, we propose to reduce the size of the VLAWE representation to 300 components. In Table 2, the resulting compact representations are compared against the full VLAWE representation on the MR data set. Although the compact VLAWE representations provide slightly lower results compared to the VLAWE representation based on 3000 components, we note that the differences are insignificant. Furthermore, both compact VLAWE representations are far above the state-of-the-art method (Cheng et al., 2018).","In Figure 1, we illustrate the performance variation on MR, when using different values for k. We notice that the accuracy tends to increase slightly, as we increase the number of clusters from 2 to 30. Overall, the VLAWE representation seems to be robust to the choice of k, always surpassing the state-of-the-art approach (Cheng et al., 2018)."]},{"section_name":"Conclusion","paragraphs":["We proposed a novel representation for text documents which is based on aggregating word embeddings using k-means and on computing the residuals between each word embedding allocated to a given cluster and the corresponding cluster centroid. Our experiments on five benchmark data sets prove that our approach yields competitive results with respect to the state-of-the-art methods."]},{"section_name":"Figure 1 :","paragraphs":["1Accuracy on MR for different numbers of k-means clusters.","TF + FA + CP + SVM (Xue and). We","Method","Reuters-21578 RT-2k MR TREC Subj","Average of word embeddings (baseline)","85.3","84.7 77.4 80.0 89.5","BOW (baseline)","86.5","84.1 77.1 89.3 89.3","AcknowledgmentsWe thank the reviewers for their useful comments. This research is supported by University of Bucharest, Faculty of Mathematics and Computer Science, through the 2019 Mobility Fund.","A Neural Probabilistic Language Model. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin, Journal of Machine Learning Research. 3Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Lan- guage Model. Journal of Machine Learning Re- search, 3:1137-1155.","Unsupervised Most Frequent Sense Detection using Word Embeddings. Sudha Bhingardive, Dhirendra Singh, V Rudramurthy, Pushpak Hanumant Harichandra Redkar, Bhattacharyya, Proceedings of NAACL. NAACLSudha Bhingardive, Dhirendra Singh, Rudramurthy V, Hanumant Harichandra Redkar, and Pushpak Bhattacharyya. 2015. Unsupervised Most Frequent Sense Detection using Word Embeddings. In Pro- ceedings of NAACL, pages 1238-1243.","From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings. Andrei Butnaru, Radu Tudor Ionescu, Proceedings of KES. KESAndrei Butnaru and Radu Tudor Ionescu. 2017. From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings. In Proceed- ings of KES, pages 1784-1793.","ShotgunWSD: An unsupervised algorithm for global word sense disambiguation inspired by DNA sequencing. Andrei Butnaru, Tudor Radu, Florentina Ionescu, Hristea, Proceedings of EACL. EACLAndrei Butnaru, Radu Tudor Ionescu, and Florentina Hristea. 2017. ShotgunWSD: An unsupervised al- gorithm for global word sense disambiguation in- spired by DNA sequencing. In Proceedings of EACL, pages 916-926.","LibSVM: A Library for Support Vector Machines. Chih-Chung Chang, Chih-Jen Lin, 27. Software available at. 2Chih-Chung Chang and Chih-Jen Lin. 2011. LibSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technol- ogy, 2:27:1-27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.","A Unified Model for Word Sense Representation and Disambiguation. Xinxiong Chen, Zhiyuan Liu, Maosong Sun, Proceedings of EMNLP. EMNLPXinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A Unified Model for Word Sense Representation and Disambiguation. In Proceedings of EMNLP, pages 1025-1035.","TreeNet: Learning Sentence Representations with Unconstrained Tree Structure. Zhou Cheng, Chun Yuan, Jiancheng Li, Haiqin Yang, Proceedings of IJCAI. IJCAIZhou Cheng, Chun Yuan, Jiancheng Li, and Haiqin Yang. 2018. TreeNet: Learning Sentence Represen- tations with Unconstrained Tree Structure. In Pro- ceedings of IJCAI, pages 4005-4011.","Aggregating continuous word embeddings for information retrieval. Stéphane Clinchant, Florent Perronnin, Proceedings of CVSC Workshop. CVSC WorkshopStéphane Clinchant and Florent Perronnin. 2013. Ag- gregating continuous word embeddings for informa- tion retrieval. In Proceedings of CVSC Workshop, pages 100-109.","A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Ronan Collobert, Jason Weston, Proceedings of ICML. ICMLRonan Collobert and Jason Weston. 2008. A Uni- fied Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of ICML, pages 160-167.","Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, Antoine Bordes, Proceedings of EMNLP. EMNLPAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceed- ings of EMNLP, pages 670-680.","Automated essay scoring with string kernels and word embeddings. Mȃdȃlina Cozma, Andrei Butnaru, Radu Tudor Ionescu, Proceedings of ACL. ACLMȃdȃlina Cozma, Andrei Butnaru, and Radu Tudor Ionescu. 2018. Automated essay scoring with string kernels and word embeddings. In Proceedings of ACL, pages 503-509.","Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. Cícero Nogueira, Dos Santos, Maira Gatti, Proceedings of COL-ING. COL-INGCícero Nogueira Dos Santos and Maira Gatti. 2014. Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In Proceedings of COL- ING, pages 69-78.","Bag of meta-words: A novel method to represent document for the sentiment classification. Mingsheng Fu, Hong Qu, Li Huang, Li Lu, Expert Systems with Applications. 113Mingsheng Fu, Hong Qu, Li Huang, and Li Lu. 2018. Bag of meta-words: A novel method to represent document for the sentiment classification. Expert Systems with Applications, 113:33-43.","Learning Distributed Representations of Sentences from Unlabelled Data. Felix Hill, Kyunghyun Cho, Anna Korhonen, Proceedings of NAACL. NAACLFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning Distributed Representations of Sentences from Unlabelled Data. In Proceedings of NAACL, pages 1367-1377.","Embeddings for Word Sense Disambiguation: An Evaluation Study. Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli, Proceedings of ACL. ACLIgnacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016. Embeddings for Word Sense Disambiguation: An Evaluation Study. In Proceed- ings of ACL, pages 897-907.","Kernels for Visual Words Histograms. Tudor Radu, Marius Ionescu, Popescu, Proceedings of ICIAP. ICIAPRadu Tudor Ionescu and Marius Popescu. 2013. Ker- nels for Visual Words Histograms. In Proceedings of ICIAP, pages 81-90.","Objectness to improve the bag of visual words model. Tudor Radu, Marius Ionescu, Popescu, Proceedings of ICIP. ICIPRadu Tudor Ionescu and Marius Popescu. 2014. Ob- jectness to improve the bag of visual words model. In Proceedings of ICIP, pages 3238-3242.","Have a SNAK. Encoding Spatial Information with the Spatial Non-alignment Kernel. Tudor Radu, Marius Ionescu, Popescu, Proceedings of ICIAP. ICIAPRadu Tudor Ionescu and Marius Popescu. 2015a. Have a SNAK. Encoding Spatial Information with the Spatial Non-alignment Kernel. In Proceedings of ICIAP, pages 97-108.","PQ kernel: a rank correlation kernel for visual word histograms. Tudor Radu, Marius Ionescu, Popescu, Pattern Recognition Letters. 55Radu Tudor Ionescu and Marius Popescu. 2015b. PQ kernel: a rank correlation kernel for visual word his- tograms. Pattern Recognition Letters, 55:51-57.","Local Learning to Improve Bag of Visual Words Model for Facial Expression Recognition. Marius Radu Tudor Ionescu, Cristian Popescu, Grozea, Proceedings of WREPL. WREPLRadu Tudor Ionescu, Marius Popescu, and Cristian Grozea. 2013. Local Learning to Improve Bag of Visual Words Model for Facial Expression Recogni- tion. In Proceedings of WREPL.","Deep Unordered Composition Rivals Syntactic Methods for Text Classification. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, Hal Daumé, Iii , Proceedings of ACL. ACLMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. 2015. Deep Unordered Compo- sition Rivals Syntactic Methods for Text Classifica- tion. In Proceedings of ACL, pages 1681-1691.","Aggregating local descriptors into a compact image representation. Hervé Jégou, Matthijs Douze, Cordelia Schmid, Patrick Pérez, Proceedings of CVPR. CVPRHervé Jégou, Matthijs Douze, Cordelia Schmid, and Patrick Pérez. 2010. Aggregating local descriptors into a compact image representation. In Proceed- ings of CVPR, pages 3304-3311.","Aggregating local image descriptors into compact codes. Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Perez, Cordelia Schmid, IEEE Transactions on Pattern Analysis and Machine Intelligence. 349Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Perez, and Cordelia Schmid. 2012. Aggregating local image descriptors into compact codes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(9):1704-1716.","Text Categorization with Suport Vector Machines: Learning with Many Relevant Features. Thorsten Joachims, Proceedings of ECML. ECMLLondon, UK, UKSpringer-VerlagThorsten Joachims. 1998. Text Categorization with Su- port Vector Machines: Learning with Many Rele- vant Features. In Proceedings of ECML, pages 137- 142, London, UK, UK. Springer-Verlag.","Convolutional Neural Networks for Sentence Classification. Yoon Kim, Proceedings of EMNLP. EMNLPYoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of EMNLP, pages 1746-1751.","Skip-Thought Vectors. Ryan Kiros, Yukun Zhu, R Ruslan, Richard Salakhutdinov, Raquel Zemel, Antonio Urtasun, Sanja Torralba, Fidler, Proceedings of NIPS. NIPSRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-Thought Vectors. In Proceedings of NIPS, pages 3294-3302.","From word embeddings to document distances. Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger, Proceedings of ICML. ICMLMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to doc- ument distances. In Proceedings of ICML, pages 957-966.","Distributed Representations of Sentences and Documents. Quoc Le, Tomas Mikolov, Proceedings of ICML. ICMLQuoc Le and Tomas Mikolov. 2014. Distributed Rep- resentations of Sentences and Documents. In Pro- ceedings of ICML, pages 1188-1196.","The Reuters-21578 text categorization test collection. David Lewis, David Lewis. 1997. The Reuters-21578 text catego- rization test collection. http://www.daviddlewis.co m/resources/testcollections/reuters21578/.","Learning question classifiers. Xin Li, Dan Roth, Proceedings of COLING. COLINGXin Li and Dan Roth. 2002. Learning question classi- fiers. In Proceedings of COLING, pages 1-7.","Dynamic compositional neural networks over tree structure. Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Proceedings of IJCAI. IJCAIPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Dynamic compositional neural networks over tree structure. In Proceedings of IJCAI, pages 4054- 4060.","Distinctive Image Features from Scale-Invariant Keypoints. David G Lowe, International Journal of Computer Vision. 602David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60(2):91-110.","Distributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, Proceedings of NIPS. NIPSTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Rep- resentations of Words and Phrases and their Com- positionality. In Proceedings of NIPS, pages 3111- 3119.","Composition in distributional models of semantics. Jeff Mitchell, Mirella Lapata, Cognitive Science. 348Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Sci- ence, 34(8):1388-1429.","A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. Bo Pang, Lillian Lee, Proceedings of ACL. ACLBo Pang and Lillian Lee. 2004. A Sentimental Educa- tion: Sentiment Analysis Using Subjectivity Sum- marization Based on Minimum Cuts. In Proceed- ings of ACL, pages 271-278.","Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales. Bo Pang, Lillian Lee, Proceedings of ACL. ACLBo Pang and Lillian Lee. 2005. Seeing Stars: Exploit- ing Class Relationships For Sentiment Categoriza- tion With Respect To Rating Scales. In Proceedings of ACL, pages 115-124.","GloVe: Global Vectors for Word Representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of EMNLP. EMNLPJeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of EMNLP, pages 1532-1543.","Object retrieval with large vocabularies and fast spatial matching. James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, Andrew Zisserman, Proceedings of CVPR. CVPRJames Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. 2007. Object re- trieval with large vocabularies and fast spatial matching. In Proceedings of CVPR, pages 1-8.","Applications and explanations of Zipf's law. David Powers, Proceedings of NeMLaP/CoNLL. NeMLaP/CoNLLDavid Powers. 1998. Applications and explanations of Zipf's law. In Proceedings of NeMLaP/CoNLL, pages 151-160.","Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms. Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang, Chunyuan Li, Ricardo Henao, Lawrence Carin, Proceedings of ACL. ACLDinghan Shen, Guoyin Wang, Wenlin Wang, Mar- tin Renqiang Min, Qinliang Su, Yizhe Zhang, Chun- yuan Li, Ricardo Henao, and Lawrence Carin. 2018. Baseline Needs More Love: On Simple Word- Embedding-Based Models and Associated Pooling Mechanisms. In Proceedings of ACL, pages 440- 450.","A Document Descriptor using Covariance of Word Vectors. Marwan Torki, Proceedings of ACL. ACLMarwan Torki. 2018. A Document Descriptor using Covariance of Word Vectors. In Proceedings of ACL, pages 527-532.","VLFeat: An Open and Portable Library of Computer Vision Algorithms. Andrea Vedaldi, B Fulkerson, Andrea Vedaldi and B. Fulkerson. 2008. VLFeat: An Open and Portable Library of Computer Vision Al- gorithms. http://www.vlfeat.org/.","Distributional features for text categorization. Xiao- , Bing Xue, Zhi-Hua Zhou, IEEE Transactions on Knowledge and Data Engineering. 213Xiao-Bing Xue and Zhi-Hua Zhou. 2009. Distri- butional features for text categorization. IEEE Transactions on Knowledge and Data Engineering, 21(3):428-442.","A re-examination of text categorization methods. Yiming Yang, Xin Liu, Proceedings of SI-GIR. SI-GIRYiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of SI- GIR, pages 42-49.","From word embeddings to document similarities for improved information retrieval in software engineering. Xin Ye, Hui Shen, Xiao Ma, Rȃzvan Bunescu, Chang Liu, Proceedings of ICSE. ICSEXin Ye, Hui Shen, Xiao Ma, Rȃzvan Bunescu, and Chang Liu. 2016. From word embeddings to docu- ment similarities for improved information retrieval in software engineering. In Proceedings of ICSE, pages 404-415.","Self-Adaptive Hierarchical Sentence Model. Han Zhao, Zhengdong Lu, Pascal Poupart, Proceedings of IJCAI. IJCAIHan Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-Adaptive Hierarchical Sentence Model. In Pro- ceedings of IJCAI, pages 4069-4076.","Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling. Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, Bo Xu, Proceedings of COLING. COLINGPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling. In Proceedings of COLING, pages 3485-3495.","Differentiated attentive representation learning for sentence classification. Qianrong Zhou, Xiaojie Wang, Xuan Dong, Proceedings of IJCAI. IJCAIQianrong Zhou, Xiaojie Wang, and Xuan Dong. 2018. Differentiated attentive representation learning for sentence classification. In Proceedings of IJCAI, pages 4630-4636."]}]}
